{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682e224e-1bb9-470c-b363-386ede0785a4",
   "metadata": {},
   "source": [
    "# Intro to Ray Serve\n",
    "\n",
    "This notebook will introduce you to Ray Serve, a framework for building and deploying scalable ML applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Overview of Ray Serve</li>\n",
    "    <li><b>Part 2:</b> Implement an MNISTClassifier service</li>\n",
    "    <li><b>Part 3:</b> Advanced features of Ray Serve</li>\n",
    "    <li><b>Part 4:</b> Ray Serve in Production</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060aea0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import ray\n",
    "import requests\n",
    "import torch\n",
    "from ray import serve\n",
    "from matplotlib import pyplot as plt\n",
    "from fastapi import FastAPI\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250cc03-e52c-4e30-a262-8d8e0a5a0837",
   "metadata": {},
   "source": [
    "## 1. Overview of Ray Serve\n",
    "\n",
    "Serve is a framework for serving ML applications. \n",
    "\n",
    "Here is a high-level overview of the architecture of a Ray Serve Application.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "An Application is a collection of one or more Deployments that are deployed together.\n",
    "\n",
    "### Deployments\n",
    "\n",
    "`Deployment` is the fundamental developer-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "Each deployment can have multiple replicas. \n",
    "\n",
    "A replica is implemented as a Ray actor with a queue to process incoming requests.\n",
    "\n",
    "Each replica can be configured with a set of compute resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380b141",
   "metadata": {},
   "source": [
    "### When to use Ray Serve?\n",
    "\n",
    "Ray Serve is designed to be used in the following scenarios:\n",
    "- Build end-to-end ML applications with a flexible and programmable python API\n",
    "- Flexibly scale up and down your compute resources to meet the demand of your application\n",
    "- Easy to develop on a local machine, and scale to a multi-node GPU cluster\n",
    "\n",
    "#### Key Ray Serve Features\n",
    "Ray Serve provides the following key features and optimizations:\n",
    "- [response streaming](https://docs.ray.io/en/latest/serve/tutorials/streaming.html)\n",
    "- [dynamic request batching](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html)\n",
    "- [multi-node/multi-GPU serving](https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html)\n",
    "- [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html)\n",
    "- [fractional compute resource usage](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da1a6",
   "metadata": {},
   "source": [
    "## 2. Implement an MNISTClassifier service\n",
    "\n",
    "Letâ€™s jump right in and get a simple ML service up and running on Ray Serve. \n",
    "\n",
    "Recall the `MNISTClassifier` we built to perform batch inference on the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb17a6-a71c-4a11-8ea8-b1b350a5fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OfflineMNISTClassifier:\n",
    "    def __init__(self, local_path: str):\n",
    "        self.model = torch.jit.load(local_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        return self.predict(batch)\n",
    "    \n",
    "    def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the model from s3 to the EFS storage\n",
    "!aws s3 cp s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt /mnt/cluster_storage/model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79961",
   "metadata": {},
   "source": [
    "Here is how we can use the `OfflineMNISTClassifier` to perform batch inference on a dataset of random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b16400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of random images\n",
    "ds = ray.data.from_items([{\"image\": np.random.rand(1, 28, 28)} for _ in range(100)])\n",
    "\n",
    "# Map the OfflineMNISTClassifier to the dataset\n",
    "ds = ds.map_batches(\n",
    "    OfflineMNISTClassifier,\n",
    "    fn_constructor_kwargs={\"local_path\": \"/mnt/cluster_storage/model.pt\"},\n",
    "    concurrency=1,\n",
    "    num_gpus=1,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# Take a look at the first 10 predictions\n",
    "ds.take_batch(10)[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a687",
   "metadata": {},
   "source": [
    "Now, if want to migrate to an online inference setting, we can transform this into a Ray Serve Deployment by applying the `@serve.deployment` decorator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68888dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment() # this is the decorator to add\n",
    "class OnlineMNISTClassifier:\n",
    "    def __init__(self, local_path: str):\n",
    "        self.model = torch.jit.load(local_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict[str, Any]: # __call__ now takes a Starlette Request object\n",
    "        batch = json.loads(await request.json()) # we will need to parse the JSON body of the request\n",
    "        return await self.predict(batch)\n",
    "    \n",
    "    async def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        # same code as OfflineMNISTClassifier.predict except we added async to the method\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf85ff1",
   "metadata": {},
   "source": [
    "We can now instantiate the `OnlineMNISTClassifier` as a Ray Serve Application using `.bind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment = OnlineMNISTClassifier.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\"num_gpus\": 1},\n",
    ")\n",
    "\n",
    "mnist_app = mnist_deployment.bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e8ac4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Note:** `.bind` is a method that takes in the arguments to pass to the Deployment constructor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e70529",
   "metadata": {},
   "source": [
    "We can then run the application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0cdb-822a-4439-aeab-9916dd8d059c",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a80e9-c26f-48d2-8985-ef4eab4dc580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2cb01",
   "metadata": {},
   "source": [
    "We can also test it as a gRPC endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342928ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "response = await mnist_deployment_handle.predict.remote(batch)\n",
    "response[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e170084",
   "metadata": {},
   "source": [
    "## 3. Advanced features of Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b22a2",
   "metadata": {},
   "source": [
    "### Using fractions of a GPU\n",
    "\n",
    "With Ray we can specify fractional compute resources for each deployment's replica. \n",
    "\n",
    "This is useful to help us fully utilize a GPU especially when running small models like our `MNISTClassifier` model.\n",
    "\n",
    "Here is how to specify only 10% of a GPU's compute resources for our `MNISTClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    num_replicas=4, # we can scale to up to 10 replicas on a single GPU\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a8d83",
   "metadata": {},
   "source": [
    "Next we update the running application by running serve.run with the new options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ad6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196a535",
   "metadata": {},
   "source": [
    "We can test the new application by sending a sample request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aad97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05041234",
   "metadata": {},
   "source": [
    "### Customizing autoscaling\n",
    "\n",
    "Ray Serve provides a simple way to autoscale the number of replicas in a deployment. It is primarily based on the target number of ongoing requests per replica.\n",
    "\n",
    "i.e. here is how we can set the autoscaling config for our `OnlineMNISTClassifier` deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8a244",
   "metadata": {},
   "source": [
    "We can also control more granularly the autoscaling logic by setting:\n",
    "- the upscale and downscale delays\n",
    "- the intervals at which the replica sends metrics reports about the current number of ongoing requests\n",
    "- the look-back period used to evaluate the current number of ongoing requests\n",
    "\n",
    "Here is an example of how to set these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5594d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "        \"upscale_delay_s\": 10,\n",
    "        \"downscale_delay_s\": 10,\n",
    "        \"metrics_interval_s\": 10,\n",
    "        \"look_back_period_s\": 10, \n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a643b4",
   "metadata": {},
   "source": [
    "We can additionally control the minimum and maximum number of replicas that can be scaled up and down. \n",
    "\n",
    "We can even specify to start scaling up from 0 replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "        \"initial_replicas\": 0, # scale up from 0 replicas\n",
    "        \"min_replicas\": 0,\n",
    "        \"max_replicas\": 10,\n",
    "        # extreme upscale speeds\n",
    "        \"upscale_delay_s\": 0,\n",
    "        \"metrics_interval_s\": 0.1,\n",
    "        \"look_back_period_s\": 0.1,\n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040d6ac",
   "metadata": {},
   "source": [
    "Let's run the application with the new autoscaling config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be6e25",
   "metadata": {},
   "source": [
    "Looking at the Ray Serve dashboard, we can see we are currently at 0 replicas - i.e. no GPU resources are being used.\n",
    "\n",
    "<img src='https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/autoscaling_at_0.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fd6a6",
   "metadata": {},
   "source": [
    "We can send out a larger number of requests to the `OnlineMNISTClassifier` deployment to see the autoscaling in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "[\n",
    "    mnist_deployment_handle.predict.remote(batch)\n",
    "    for _ in range(100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4e5e9",
   "metadata": {},
   "source": [
    "Looking at the Ray Serve dashboard, we can see that the number of replicas has scaled up to 10 as expected.\n",
    "\n",
    "<img src='https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/autoscaling_at_10.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52225df",
   "metadata": {},
   "source": [
    "Let's shutdown the service for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d1c58",
   "metadata": {},
   "source": [
    "### Composing Deployments\n",
    "\n",
    "Ray Serve allows us to compose Deployments together to build more complex applications.\n",
    "\n",
    "Lets compose our `OnlineMNISTClassifier` with an `OnlineMNISTPreprocessor` deployment that performs the necessary transformations on the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67670984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class OnlineMNISTPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        \n",
    "    async def run(self, batch: dict[str, Any]) -> dict[str, Any]:\n",
    "        images = batch[\"image\"]\n",
    "        images = [self.transform(np.array(image, dtype=np.uint8)).cpu().numpy() for image in images]\n",
    "        return {\"image\": images}\n",
    "\n",
    "preprocessor_app = OnlineMNISTPreprocessor.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_handle = serve.run(preprocessor_app, name='mnist_preprocessor', blocking=False, route_prefix=\"/preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92daf899",
   "metadata": {},
   "source": [
    "Let's load an image and pass it to the `ImageTransformDeployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "image_batch = ds.take_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first image using matplotlib\n",
    "plt.imshow(image_batch[\"image\"][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_batch = await preprocessor_handle.run.remote(image_batch)\n",
    "\n",
    "for image in normalized_batch[\"image\"]:\n",
    "    assert image.shape == (1, 28, 28) # channel, height, width\n",
    "    assert image.min() >= -1 and image.max() <= 1 # normalized to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2848fc",
   "metadata": {},
   "source": [
    "We will proceed to shutdown the preprocessor application to prove it will be automatically created by the ingress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e44763",
   "metadata": {},
   "source": [
    "Let's now build an ingress for our application that composes the `ImageTransformDeployment` and `OnlineMNISTClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88340028",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class ImageServiceIngress:\n",
    "    def __init__(self, preprocessor: OnlineMNISTPreprocessor, model: OnlineMNISTClassifier):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        batch = json.loads(await request.json())\n",
    "        response = await self.preprocessor.run.remote(batch)\n",
    "        return await self.model.predict.remote(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier_ingress = ImageServiceIngress.bind(\n",
    "    preprocessor=OnlineMNISTPreprocessor.bind(),\n",
    "    model=OnlineMNISTClassifier.options(\n",
    "        num_replicas=1,\n",
    "        ray_actor_options={\"num_gpus\": 0.1},\n",
    "    ).bind(local_path=\"/mnt/cluster_storage/model.pt\"),\n",
    ")\n",
    "\n",
    "handle = serve.run(image_classifier_ingress, name='image_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81a51f",
   "metadata": {},
   "source": [
    "Let's test the application by sending a sample HTTP request to our ingress endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_request = json.dumps({\"image\": image_batch[\"image\"].tolist()}) \n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe8773",
   "metadata": {},
   "source": [
    "### Integrating with FastAPI\n",
    "\n",
    "Ray Serve can be integrated with FastAPI to provide:\n",
    "- HTTP routing\n",
    "- Pydantic model validation\n",
    "- OpenAPI documentation\n",
    "\n",
    "To integrate a Deployment with FastAPI, we can use the `@serve.ingress` decorator to designate a FastAPI app as the entrypoint for HTTP requests to our Serve application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d163431",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class ImageServiceIngress:\n",
    "    def __init__(self, preprocessor: OnlineMNISTPreprocessor, model: OnlineMNISTClassifier):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "    \n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, request: Request):\n",
    "        batch = json.loads(await request.json())\n",
    "        response = await self.preprocessor.run.remote(batch)\n",
    "        out = await self.model.predict.remote(response)\n",
    "        return {\"predicted_label\": out[\"predicted_label\"].tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a31b87",
   "metadata": {},
   "source": [
    "We now can build the application and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier_ingress = ImageServiceIngress.bind(\n",
    "    preprocessor=OnlineMNISTPreprocessor.bind(),\n",
    "    model=OnlineMNISTClassifier.options(\n",
    "        num_replicas=1,\n",
    "        ray_actor_options={\"num_gpus\": 0.1},\n",
    "    ).bind(local_path=\"/mnt/cluster_storage/model.pt\"),\n",
    ")\n",
    "\n",
    "handle = serve.run(image_classifier_ingress, name='image_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012894c6",
   "metadata": {},
   "source": [
    "After running the application, we can get test it as an HTTP endpoint programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e217336",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_request = json.dumps({\"image\": image_batch[\"image\"].tolist()}) \n",
    "response = requests.post(\"http://localhost:8000/predict\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ff14a",
   "metadata": {},
   "source": [
    "We can also visit the auto-generated FastAPI docs at http://localhost:8000/docs to get an interactive UI to test our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2af689",
   "metadata": {},
   "source": [
    "## 4. Ray Serve in Production\n",
    "\n",
    "1. Klaviyo built their model serving platform with Ray Serve. See [this article from Klaviyo Engineering](https://klaviyo.tech/how-klaviyo-built-a-robust-model-serving-platform-with-ray-serve-c02ec65788b3)\n",
    "2. Samsara uses Ray Serve to bridge the gap of development to deployment of their models. See [this article from Samsara Engineering](https://www.samsara.com/blog/building-a-modern-machine-learning-platform-with-ray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f4a09",
   "metadata": {},
   "source": [
    "## Clean up \n",
    "\n",
    "Let's shutdown the application and clean up the resources we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()\n",
    "!rm -rf /mnt/cluster_storage/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
