{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLanguage Translation with Transformer\n=====================================\n\nThis tutorial shows, how to train a translation model from scratch using\nTransformer. We will be using Multi30k dataset to train a German to English translation model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Processing\n---------------\n\ntorchtext has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. In this example, we show how to tokenize a raw text sentence,\nbuild vocabulary, and numericalize tokens into tensor.\n\nTo run this tutorial, first install spacy using pip or conda. Next,\ndownload the raw data for the English and German Spacy tokenizers from\nhttps://spacy.io/usage/models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nimport torchtext\nimport torch\nimport torch.nn as nn\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nfrom torchtext.utils import download_from_url, extract_archive\nfrom torch import Tensor\nimport io\nimport time\n\ntorch.manual_seed(0)\ntorch.use_deterministic_algorithms(True)\n\n\nurl_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\ntrain_urls = ('train.de.gz', 'train.en.gz')\nval_urls = ('val.de.gz', 'val.en.gz')\ntest_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n\ntrain_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\nval_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\ntest_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n\nde_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\nen_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n\ndef build_vocab(filepath, tokenizer):\n  counter = Counter()\n  with io.open(filepath, encoding=\"utf8\") as f:\n    for string_ in f:\n      counter.update(tokenizer(string_))\n  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n\nde_vocab = build_vocab(train_filepaths[0], de_tokenizer)\nen_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n\ndef data_process(filepaths):\n  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n  data = []\n  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n                            dtype=torch.long)\n    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n                            dtype=torch.long)\n    data.append((de_tensor_, en_tensor_))\n  return data\n\n\ntrain_data = data_process(train_filepaths)\nval_data = data_process(val_filepaths)\ntest_data = data_process(test_filepaths)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nBATCH_SIZE = 128\nPAD_IDX = de_vocab['<pad>']\nBOS_IDX = de_vocab['<bos>']\nEOS_IDX = de_vocab['<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DataLoader\n----------\n\nThe last torch specific feature we\u2019ll use is the DataLoader, which is\neasy to use since it takes the data as its first argument. Specifically,\nas the docs say: DataLoader combines a dataset and a sampler, and\nprovides an iterable over the given dataset. The DataLoader supports\nboth map-style and iterable-style datasets with single- or multi-process\nloading, customizing loading order and optional automatic batching\n(collation) and memory pinning.\n\nPlease pay attention to collate_fn (optional) that merges a list of\nsamples to form a mini-batch of Tensor(s). Used when using batched\nloading from a map-style dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\ndef generate_batch(data_batch):\n  de_batch, en_batch = [], []\n  for (de_item, en_item) in data_batch:\n    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n  return de_batch, en_batch\n\ntrain_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\nvalid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\ntest_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformer!\n------------\n\nTransformer is a Seq2Seq model introduced in `\u201cAttention is all you\nneed\u201d <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\npaper for solving machine translation task. Transformer model consists\nof an encoder and decoder block each containing fixed number of layers.\n\nEncoder processes the input sequence by propogating it, through a series\nof Multi-head Attention and Feed forward network layers. The output from\nthe Encoder referred to as ``memory``, is fed to the decoder along with\ntarget tensors. Encoder and decoder are trained in an end-to-end fashion\nusing teacher forcing technique.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n                      TransformerEncoderLayer, TransformerDecoderLayer)\n\n\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n                 dim_feedforward:int = 512, dropout:float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n                                                dim_feedforward=dim_feedforward)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n                                                dim_feedforward=dim_feedforward)\n        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n                \n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n\n    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n                tgt_mask: Tensor, src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n                                        tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer_encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer_decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Text tokens are represented by using token embeddings. Positional\nencoding is added to the token embedding to introduce a notion of word\norder.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + \n                            self.pos_embedding[:token_embedding.size(0),:])\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a ``subsequent word`` mask to stop a target word from\nattending to its subsequent words. We also create masks, for masking\nsource and target padding tokens\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\ndef create_mask(src, tgt):\n  src_seq_len = src.shape[0]\n  tgt_seq_len = tgt.shape[0]\n\n  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n\n  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define model parameters and instantiate model \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SRC_VOCAB_SIZE = len(de_vocab)\nTGT_VOCAB_SIZE = len(en_vocab)\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nNUM_EPOCHS = 16\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n                                 FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(device)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\noptimizer = torch.optim.Adam(\n    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_iter, optimizer):\n  model.train()\n  losses = 0\n  for idx, (src, tgt) in enumerate(train_iter):\n      src = src.to(device)\n      tgt = tgt.to(device)\n            \n      tgt_input = tgt[:-1, :]\n\n      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n      logits = model(src, tgt_input, src_mask, tgt_mask,\n                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n      \n      optimizer.zero_grad()\n      \n      tgt_out = tgt[1:,:]\n      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n      loss.backward()\n\n      optimizer.step()\n      losses += loss.item()\n  return losses / len(train_iter)\n\n\ndef evaluate(model, val_iter):\n  model.eval()\n  losses = 0\n  for idx, (src, tgt) in (enumerate(valid_iter)):\n    src = src.to(device)\n    tgt = tgt.to(device)\n\n    tgt_input = tgt[:-1, :]\n\n    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n    logits = model(src, tgt_input, src_mask, tgt_mask,\n                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n    tgt_out = tgt[1:,:]\n    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n    losses += loss.item()\n  return losses / len(val_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train model \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, NUM_EPOCHS+1):\n  start_time = time.time()\n  train_loss = train_epoch(transformer, train_iter, optimizer)\n  end_time = time.time()\n  val_loss = evaluate(transformer, valid_iter)\n  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the following results during model training.\n\n::\n\n       Epoch: 1, Train loss: 5.316, Val loss: 4.065, Epoch time = 35.322s\n       Epoch: 2, Train loss: 3.727, Val loss: 3.285, Epoch time = 36.283s\n       Epoch: 3, Train loss: 3.131, Val loss: 2.881, Epoch time = 37.096s\n       Epoch: 4, Train loss: 2.741, Val loss: 2.625, Epoch time = 37.714s\n       Epoch: 5, Train loss: 2.454, Val loss: 2.428, Epoch time = 38.263s\n       Epoch: 6, Train loss: 2.223, Val loss: 2.291, Epoch time = 38.415s\n       Epoch: 7, Train loss: 2.030, Val loss: 2.191, Epoch time = 38.412s\n       Epoch: 8, Train loss: 1.866, Val loss: 2.104, Epoch time = 38.511s\n       Epoch: 9, Train loss: 1.724, Val loss: 2.044, Epoch time = 38.367s\n       Epoch: 10, Train loss: 1.600, Val loss: 1.994, Epoch time = 38.491s\n       Epoch: 11, Train loss: 1.488, Val loss: 1.969, Epoch time = 38.490s\n       Epoch: 12, Train loss: 1.390, Val loss: 1.929, Epoch time = 38.194s\n       Epoch: 13, Train loss: 1.299, Val loss: 1.898, Epoch time = 38.430s\n       Epoch: 14, Train loss: 1.219, Val loss: 1.885, Epoch time = 38.406s\n       Epoch: 15, Train loss: 1.141, Val loss: 1.890, Epoch time = 38.365s\n       Epoch: 16, Train loss: 1.070, Val loss: 1.873, Epoch time = 38.439s\n\nThe models trained using transformer architecture \u2014 train faster\nand converge to a lower validation loss compared to RNN models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n    for i in range(max_len-1):\n        memory = memory.to(device)\n        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                                    .type(torch.bool)).to(device)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim = 1)\n        next_word = next_word.item()\n\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n          break\n    return ys\n\n\ndef translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n  model.eval()\n  tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n  num_tokens = len(tokens)\n  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n  tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n  return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", de_vocab, en_vocab, de_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output: `A group of people stand in front of an igloo .`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n1. Attention is all you need paper.\n   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}