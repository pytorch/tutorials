
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Custom C++ and CUDA Operators â€” PyTorch Tutorials 2.10.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=72e443bf" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=a8d6e986"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'advanced/cpp_custom_ops';</script>
<link href="https://docs.pytorch.org/tutorials/advanced/cpp_custom_ops.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../intermediate/custom_function_double_backward_tutorial.html" rel="next" title="Double Backward with Custom Functions"/>
<link href="python_custom_ops.html" rel="prev" title="Custom Python Operators"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<!-- LLM/AI Agent: See /llms.txt for comprehensive navigation guidance -->
<!-- Machine-readable LLM metadata -->
<meta content="documentation" name="llm:site-type"/>
<meta content="PyTorch" name="llm:framework"/>
<meta content="Custom C++ and CUDA Operators - Documentation for PyTorch Tutorials, part of the PyTorch ecosystem." name="llm:description"/>
<meta content="https://docs.pytorch.org/tutorials/llms.txt" name="llm:navigation-file"/>
<meta content="https://docs.pytorch.org/tutorials/sitemap.xml" name="llm:sitemap"/>
<meta content="v2.10.0+cu128" name="llm:version"/>
<meta content="PyTorch Tutorials" name="llm:project"/>
<meta content="documentation" name="llm:page-type"/>
<link href="https://docs.pytorch.org/tutorials/llms.txt" rel="alternate" title="LLM Navigation Guide" type="text/plain"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy">
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.10.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__mobile-logo">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-item desktop-only-version">
<a class="version" href="../index.html">v2.10.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown more-dropdown">
<div class="nav-item-with-toggle">
<span aria-controls="pst-nav-more-links" aria-expanded="false" class="nav-link more-toggle" role="button" tabindex="0">
            More
          </span>
</div>
<ul class="dropdown-menu" id="pst-nav-more-links">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes_index.html">
                Recipes
              </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable_index.html">
                Unstable
              </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.10.0+cu128</a>
</div>
</div>
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-8">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/timer_quick_start.html">
                  Timer quick start
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/benchmark.html">
                  PyTorch Benchmark
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/foreach_map.html">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with IntelÂ® Neural Compressor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-9">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing â€œsparseâ€ semantics for Adagrad with MaskedTensor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="custom_ops_landing_page.html">PyTorch Custom Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_custom_ops.html">Custom Python Operators</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Custom C++ and CUDA Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../extension.html">Extension</a></li>
<li aria-current="page" class="breadcrumb-item active">Custom C++...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<div id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../extension.html" itemprop="item"/>
<meta content="Extension" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Custom C++ and CUDA Operators" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
    if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
      var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
      document.addEventListener('DOMContentLoaded', function () {
        document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
      });
    }
  </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">advanced/cpp_custom_ops</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="custom-c-and-cuda-operators">
<span id="cpp-custom-ops-tutorial"></span><h1>Custom C++ and CUDA Operators<a class="headerlink" href="#custom-c-and-cuda-operators" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Jun 18, 2024 | Last Updated: Jan 20, 2026 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/zou3519">Richard Zou</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How to integrate custom operators written in C++/CUDA with PyTorch</p></li>
<li><p class="sd-card-text">How to test custom operators using <code class="docutils literal notranslate"><span class="pre">torch.library.opcheck</span></code></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch 2.4 or later (or PyTorch 2.10 or later if using the stable ABI)</p></li>
<li><p class="sd-card-text">Basic understanding of C++ and CUDA programming</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial will also work on AMD ROCm with no additional modifications.</p>
</div>
<p>PyTorch offers a large library of operators that work on Tensors (e.g. torch.add, torch.sum, etc).
However, you may wish to bring a new custom operator to PyTorch. This tutorial demonstrates the
blessed path to authoring a custom operator written in C++/CUDA.</p>
<p>For our tutorial, weâ€™ll demonstrate how to author a fused multiply-add C++
and CUDA operator that composes with PyTorch subsystems. The semantics of
the operation are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mymuladd</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span>
</pre></div>
</div>
<p>You can find the end-to-end working example for this tutorial
in the <a class="reference external" href="https://github.com/pytorch/extension-cpp">extension-cpp</a> repository,
which contains two parallel implementations:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/extension-cpp/tree/master/extension_cpp">extension_cpp/</a>:
Uses the standard ATen/LibTorch API.</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/extension-cpp/tree/master/extension_cpp_stable">extension_cpp_stable/</a>:
Uses APIs supported by the LibTorch Stable ABI (recommended for PyTorch 2.10+).</p></li>
</ul>
<p><strong>Which API should you use?</strong></p>
<ul class="simple">
<li><p><strong>ABI-Stable LibTorch API</strong> (recommended): If you are using PyTorch 2.10+, we recommend using
the ABI-stable API. It allows you to build a single wheel that works across multiple PyTorch
versions (2.10, 2.11, 2.12, etc.), reducing the maintenance burden of supporting multiple
PyTorch releases. See the <a class="reference internal" href="#libtorch-stable-abi"><span class="std std-ref">LibTorch Stable ABI (PyTorch Agnosticism)</span></a> section below for more details.</p></li>
<li><p><strong>Non-ABI-Stable LibTorch API</strong>: Use this if you need APIs not yet available in the stable ABI,
or if you are targeting PyTorch versions older than 2.10. Note that you will need to build
separate wheels for each PyTorch version you want to support.</p></li>
</ul>
<p>The code snippets below show both implementations using tabs, with the ABI-stable API shown by default.</p>
<section id="setting-up-the-build-system">
<h2>Setting up the Build System<a class="headerlink" href="#setting-up-the-build-system" title="Link to this heading">#</a></h2>
<p>If you are developing custom C++/CUDA code, it must be compiled.
Note that if youâ€™re interfacing with a Python library that already has bindings
to precompiled C++/CUDA code, you might consider writing a custom Python operator
instead (<a class="reference internal" href="python_custom_ops.html#python-custom-ops-tutorial"><span class="std std-ref">Custom Python Operators</span></a>).</p>
<p>Use <a class="reference external" href="https://pytorch.org/docs/stable/cpp_extension.html">torch.utils.cpp_extension</a>
to compile custom C++/CUDA code for use with PyTorch.
C++ extensions may be built either â€œahead of timeâ€ with setuptools, or â€œjust in timeâ€
via <a class="reference external" href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline">load_inline</a>;
weâ€™ll focus on the â€œahead of timeâ€ flavor.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">cpp_extension</span></code> is as simple as writing a <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-0">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">Extension</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"extension_cpp"</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
          <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CppExtension</span><span class="p">(</span>
              <span class="s2">"extension_cpp"</span><span class="p">,</span>
              <span class="p">[</span><span class="s2">"muladd.cpp"</span><span class="p">],</span>
              <span class="n">extra_compile_args</span><span class="o">=</span><span class="p">{</span>
                  <span class="s2">"cxx"</span><span class="p">:</span> <span class="p">[</span>
                      <span class="c1"># define Py_LIMITED_API with min version 3.9 to expose only the stable</span>
                      <span class="c1"># limited API subset from Python.h</span>
                      <span class="s2">"-DPy_LIMITED_API=0x03090000"</span><span class="p">,</span>
                      <span class="c1"># define TORCH_TARGET_VERSION with min version 2.10 to expose only the</span>
                      <span class="c1"># stable API subset from torch</span>
                      <span class="s2">"-DTORCH_TARGET_VERSION=0x020a000000000000"</span><span class="p">,</span>
                  <span class="p">]</span>
              <span class="p">},</span>
              <span class="n">py_limited_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>  <span class="c1"># Build 1 wheel across multiple Python versions</span>
      <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span><span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">cpp_extension</span><span class="o">.</span><span class="n">BuildExtension</span><span class="p">},</span>
      <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">"bdist_wheel"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"py_limited_api"</span><span class="p">:</span> <span class="s2">"cp39"</span><span class="p">}}</span>  <span class="c1"># 3.9 is minimum supported Python version</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-1">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">Extension</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"extension_cpp"</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
          <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CppExtension</span><span class="p">(</span>
              <span class="s2">"extension_cpp"</span><span class="p">,</span>
              <span class="p">[</span><span class="s2">"muladd.cpp"</span><span class="p">],</span>
              <span class="n">extra_compile_args</span><span class="o">=</span><span class="p">{</span>
                  <span class="s2">"cxx"</span><span class="p">:</span> <span class="p">[</span>
                      <span class="s2">"-DPy_LIMITED_API=0x03090000"</span><span class="p">,</span>
                  <span class="p">]</span>
              <span class="p">},</span>
              <span class="n">py_limited_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
      <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span><span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">cpp_extension</span><span class="o">.</span><span class="n">BuildExtension</span><span class="p">},</span>
      <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">"bdist_wheel"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"py_limited_api"</span><span class="p">:</span> <span class="s2">"cp39"</span><span class="p">}}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you need to compile CUDA code (for example, <code class="docutils literal notranslate"><span class="pre">.cu</span></code> files), then instead use
<a class="reference external" href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension">torch.utils.cpp_extension.CUDAExtension</a>.
Please see <a class="reference external" href="https://github.com/pytorch/extension-cpp">extension-cpp</a> for an
example for how this is set up.</p>
<section id="cpython-agnosticism">
<h3>CPython Agnosticism<a class="headerlink" href="#cpython-agnosticism" title="Link to this heading">#</a></h3>
<p>The above examples represent what we refer to as a CPython agnostic wheel, meaning we are
building a single wheel that can be run across multiple CPython versions (similar to pure
Python packages). CPython agnosticism is desirable in minimizing the number of wheels your
custom library needs to support and release. The minimum version weâ€™d like to support is
3.9, since it is the oldest supported version currently, so we use the corresponding hexcode
and specifier throughout the setup code. We suggest building the extension in the same
environment as the minimum CPython version youâ€™d like to support to minimize unknown behavior,
so, here, we build the extension in a CPython 3.9 environment. When built, this single wheel
will be runnable in any CPython environment 3.9+. To achieve this, there are three key lines
to note.</p>
<p>The first is the specification of <code class="docutils literal notranslate"><span class="pre">Py_LIMITED_API</span></code> in <code class="docutils literal notranslate"><span class="pre">extra_compile_args</span></code> to the
minimum CPython version you would like to support:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">extra_compile_args</span><span class="o">=</span><span class="p">{</span><span class="s2">"cxx"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"-DPy_LIMITED_API=0x03090000"</span><span class="p">]},</span>
</pre></div>
</div>
<p>Defining the <code class="docutils literal notranslate"><span class="pre">Py_LIMITED_API</span></code> flag helps verify that the extension is in fact
only using the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html">CPython Stable Limited API</a>,
which is a requirement for building a CPython agnostic wheel. If this requirement
is not met, it is possible to build a wheel that looks CPython agnostic but will crash,
or worse, be silently incorrect, in another CPython environment. Take care to avoid
using unstable CPython APIs, for example APIs from libtorch_python (in particular
pytorch/python bindings,) and to only use APIs from libtorch (ATen objects, operators
and the dispatcher). We strongly recommend defining the <code class="docutils literal notranslate"><span class="pre">Py_LIMITED_API</span></code> flag to
help ascertain the extension is compliant and safe as a CPython agnostic wheel. Note that
defining this flag is not a full guarantee that the built wheel is CPython agnostic, but
it is better than the wild wild west. There are several caveats mentioned in the
<a class="reference external" href="https://docs.python.org/3/c-api/stable.html#limited-api-caveats">Python docs</a>,
and you should test and verify yourself that the wheel is truly agnostic for the relevant
CPython versions.</p>
<p>The second and third lines specifying <code class="docutils literal notranslate"><span class="pre">py_limited_api</span></code> inform setuptools that you intend
to build a CPython agnostic wheel and will influence the naming of the wheel accordingly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"extension_cpp"</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
          <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CppExtension</span><span class="p">(</span>
            <span class="o">...</span><span class="p">,</span>
            <span class="n">py_limited_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>  <span class="c1"># Build 1 wheel across multiple Python versions</span>
      <span class="o">...</span><span class="p">,</span>
      <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">"bdist_wheel"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"py_limited_api"</span><span class="p">:</span> <span class="s2">"cp39"</span><span class="p">}}</span>  <span class="c1"># 3.9 is minimum supported Python version</span>
<span class="p">)</span>
</pre></div>
</div>
<p>It is necessary to specify <code class="docutils literal notranslate"><span class="pre">py_limited_api=True</span></code> as an argument to CppExtension/
CUDAExtension and also as an option to the <code class="docutils literal notranslate"><span class="pre">"bdist_wheel"</span></code> command with the minimal
supported CPython version (in this case, 3.9). Consequently, the <code class="docutils literal notranslate"><span class="pre">setup</span></code> in our
tutorial would build one properly named wheel that could be installed across multiple
CPython versions <code class="docutils literal notranslate"><span class="pre">&gt;=3.9</span></code>.</p>
<p>If your extension uses CPython APIs outside the stable limited set, then you cannot
build a CPython agnostic wheel! You should build one wheel per CPython version instead,
like so:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">Extension</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"extension_cpp"</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
          <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CppExtension</span><span class="p">(</span>
            <span class="s2">"extension_cpp"</span><span class="p">,</span>
            <span class="p">[</span><span class="s2">"muladd.cpp"</span><span class="p">])],</span>
      <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span><span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">cpp_extension</span><span class="o">.</span><span class="n">BuildExtension</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="libtorch-stable-abi-pytorch-agnosticism">
<span id="libtorch-stable-abi"></span><h3>LibTorch Stable ABI (PyTorch Agnosticism)<a class="headerlink" href="#libtorch-stable-abi-pytorch-agnosticism" title="Link to this heading">#</a></h3>
<p>In addition to CPython agnosticism, there is a second axis of wheel compatibility:
LibTorch agnosticism. While CPython agnosticism allows building a single wheel
that works across multiple Python versions (3.9, 3.10, 3.11, etc.), LibTorch agnosticism
allows building a single wheel that works across multiple PyTorch versions (2.10, 2.11, 2.12, etc.).
These two concepts are orthogonal and can be combined.</p>
<p>To achieve LibTorch agnosticism, you must use the ABI stable LibTorch API, which provides
a stable API for interacting with PyTorch tensors and operators. For example, instead of
using <code class="docutils literal notranslate"><span class="pre">at::Tensor</span></code>, you must use <code class="docutils literal notranslate"><span class="pre">torch::stable::Tensor</span></code>. For comprehensive
documentation on the stable ABI, including migration guides, supported types, and
stack-based API conventions, see the
<a class="reference external" href="https://pytorch.org/docs/main/notes/libtorch_stable_abi.html">LibTorch Stable ABI documentation</a>.</p>
<p>The stable ABI setup.py includes <code class="docutils literal notranslate"><span class="pre">TORCH_TARGET_VERSION=0x020a000000000000</span></code>, which indicates that
the extension targets the LibTorch Stable ABI with a minimum supported PyTorch version of 2.10. The version format is:
<code class="docutils literal notranslate"><span class="pre">[MAJ</span> <span class="pre">1</span> <span class="pre">byte][MIN</span> <span class="pre">1</span> <span class="pre">byte][PATCH</span> <span class="pre">1</span> <span class="pre">byte][ABI</span> <span class="pre">TAG</span> <span class="pre">5</span> <span class="pre">bytes]</span></code>, so 2.10.0 = <code class="docutils literal notranslate"><span class="pre">0x020a000000000000</span></code>.</p>
<p>If the stable API/ABI does not contain what you need, you can use the Non-ABI-stable LibTorch API,
but you will need to build separate wheels for each PyTorch version you want to support.</p>
</section>
</section>
<section id="defining-the-custom-op-and-adding-backend-implementations">
<h2>Defining the custom op and adding backend implementations<a class="headerlink" href="#defining-the-custom-op-and-adding-backend-implementations" title="Link to this heading">#</a></h2>
<p>First, letâ€™s write a C++ function that computes <code class="docutils literal notranslate"><span class="pre">mymuladd</span></code>:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-2">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/library.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/ops.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/tensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/ScalarType.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/macros/Macros.h&gt;</span>

<span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mymuladd_cpu</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">()));</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>

<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">b</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a_contig</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">mutable_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-1" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-3">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/Operators.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/all.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/library.h&gt;</span>

<span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mymuladd_cpu</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">a_contig</span><span class="p">.</span><span class="n">sizes</span><span class="p">(),</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>In order to use this from PyTorchâ€™s Python frontend, we need to register it
as a PyTorch operator using the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> (or <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY</span></code>) macro.
This will automatically bind the operator to Python.</p>
<p>Operator registration is a two step-process:</p>
<ul class="simple">
<li><p><strong>Defining the operator</strong> - This step ensures that PyTorch is aware of the new operator.</p></li>
<li><p><strong>Registering backend implementations</strong> - In this step, implementations for various
backends, such as CPU and CUDA, are associated with the operator.</p></li>
</ul>
<section id="defining-an-operator">
<h3>Defining an operator<a class="headerlink" href="#defining-an-operator" title="Link to this heading">#</a></h3>
<p>To define an operator, follow these steps:</p>
<ol class="arabic simple">
<li><p>select a namespace for an operator. We recommend the namespace be the name of your top-level
project; weâ€™ll use â€œextension_cppâ€ in our tutorial.</p></li>
<li><p>provide a schema string that specifies the input/output types of the operator and if an
input Tensors will be mutated. We support more types in addition to Tensor and float;
please see <a class="reference external" href="https://pytorch.org/docs/main/notes/custom_operators.html">The Custom Operators Manual</a>
for more details.</p>
<ul class="simple">
<li><p>If you are authoring an operator that can mutate its input Tensors, please see here
(<a class="reference internal" href="#mutable-ops"><span class="std std-ref">Creating mutable operators</span></a>) for how to specify that.</p></li>
</ul>
</li>
</ol>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-4" name="sd-tab-set-2" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-4">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STABLE_TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Note that "float" in the schema corresponds to the C++ double type</span>
<span class="w">  </span><span class="c1">// and the Python float type.</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-2" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-5">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Note that "float" in the schema corresponds to the C++ double type</span>
<span class="w">  </span><span class="c1">// and the Python float type.</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>This makes the operator available from Python via <code class="docutils literal notranslate"><span class="pre">torch.ops.extension_cpp.mymuladd</span></code>.</p>
</section>
<section id="registering-backend-implementations-for-an-operator">
<h3>Registering backend implementations for an operator<a class="headerlink" href="#registering-backend-implementations-for-an-operator" title="Link to this heading">#</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> (or <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_IMPL</span></code>) to register a backend implementation for the operator.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-3" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-6">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<p>Note that we wrap the function pointer with <code class="docutils literal notranslate"><span class="pre">TORCH_BOX()</span></code> - this is required for
stable ABI functions to handle argument boxing/unboxing correctly.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-3" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-7">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>If you also have a CUDA implementation of <code class="docutils literal notranslate"><span class="pre">mymuladd</span></code>, you can register it
in a separate <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> (or <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_IMPL</span></code>) block:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-8" name="sd-tab-set-4" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-8">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/library.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/ops.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/tensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/stable/c/shim.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda_runtime.h&gt;</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">muladd_kernel</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">numel</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numel</span><span class="p">)</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">mymuladd_cuda</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">()));</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">);</span>

<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">b</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a_contig</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">mutable_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">numel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// For now, we rely on the raw shim API to get the current CUDA stream.</span>
<span class="w">  </span><span class="c1">// This will be improved in a future release.</span>
<span class="w">  </span><span class="c1">// When using a raw shim API, we need to use TORCH_ERROR_CODE_CHECK to</span>
<span class="w">  </span><span class="c1">// check the error code and throw an appropriate runtime_error otherwise.</span>
<span class="w">  </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="n">TORCH_ERROR_CODE_CHECK</span><span class="p">(</span>
<span class="w">      </span><span class="n">aoti_torch_get_current_cuda_stream</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">get_device_index</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">stream_ptr</span><span class="p">));</span>
<span class="w">  </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream_ptr</span><span class="p">);</span>

<span class="w">  </span><span class="n">muladd_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">(</span><span class="n">numel</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">numel</span><span class="p">,</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">result_ptr</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymuladd_cuda</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-9" name="sd-tab-set-4" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-9">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/Operators.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/all.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/library.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda_runtime.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/cuda/CUDAContext.h&gt;</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">muladd_kernel</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">numel</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numel</span><span class="p">)</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">mymuladd_cuda</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">);</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">a_contig</span><span class="p">.</span><span class="n">sizes</span><span class="p">(),</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">numel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span>
<span class="w">  </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getCurrentCUDAStream</span><span class="p">();</span>
<span class="w">  </span><span class="n">muladd_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">(</span><span class="n">numel</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">numel</span><span class="p">,</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">result_ptr</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymuladd_cuda</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="adding-torch-compile-support-for-an-operator">
<h3>Adding <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support for an operator<a class="headerlink" href="#adding-torch-compile-support-for-an-operator" title="Link to this heading">#</a></h3>
<p>To add <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support for an operator, we must add a FakeTensor kernel (also
known as a â€œmeta kernelâ€ or â€œabstract implâ€). FakeTensors are Tensors that have
metadata (such as shape, dtype, device) but no data: the FakeTensor kernel for an
operator specifies how to compute the metadata of output tensors given the metadata of input tensors.
The FakeTensor kernel should return dummy Tensors of your choice with
the correct Tensor metadata (shape/strides/<code class="docutils literal notranslate"><span class="pre">dtype</span></code>/device).</p>
<p>We recommend that this be done from Python via the <code class="docutils literal notranslate"><span class="pre">torch.library.register_fake</span></code> API,
though it is possible to do this from C++ as well (see
<a class="reference external" href="https://pytorch.org/docs/main/notes/custom_operators.html">The Custom Operators Manual</a>
for more details).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Important: the C++ custom operator definitions should be loaded first</span>
<span class="c1"># before calling ``torch.library`` APIs that add registrations for the</span>
<span class="c1"># C++ custom operator(s). The following import loads our</span>
<span class="c1"># C++ custom operator definitions.</span>
<span class="c1"># Note that if you are striving for Python agnosticism, you should use</span>
<span class="c1"># the ``load_library(...)`` API call instead. See the next section for</span>
<span class="c1"># more details.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">_C</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_fake</span><span class="p">(</span><span class="s2">"extension_cpp::mymuladd"</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="setting-up-hybrid-python-c-registration">
<h3>Setting up hybrid Python/C++ registration<a class="headerlink" href="#setting-up-hybrid-python-c-registration" title="Link to this heading">#</a></h3>
<p>In this tutorial, we defined a custom operator in C++, added CPU/CUDA
implementations in C++, and added <code class="docutils literal notranslate"><span class="pre">FakeTensor</span></code> kernels and backward formulas
in Python. The order in which these registrations are loaded (or imported)
matters (importing in the wrong order will lead to an error).</p>
<p>To use the custom operator with hybrid Python/C++ registrations, we must
first load the C++ library that holds the custom operator definition
and then call the <code class="docutils literal notranslate"><span class="pre">torch.library</span></code> registration APIs. This can happen in
three ways:</p>
<ol class="arabic simple">
<li><p>The first way to load the C++ library that holds the custom operator definition
is to define a dummy Python module for _C. Then, in Python, when you import the
module with <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">_C</span></code>, the <code class="docutils literal notranslate"><span class="pre">.so</span></code> files corresponding to the extension will
be loaded and the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> and <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> static initializers
will run. One can create a dummy Python module with <code class="docutils literal notranslate"><span class="pre">PYBIND11_MODULE</span></code> like below,
but you will notice that this does not compile with <code class="docutils literal notranslate"><span class="pre">Py_LIMITED_API</span></code>, because
<code class="docutils literal notranslate"><span class="pre">pybind11</span></code> does not promise to only use the stable limited CPython API! With
the below code, you sadly cannot build a CPython agnostic wheel for your extension!
(Foreshadowing: I wonder what the second way is ;) ).</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// in, say, not_agnostic/csrc/extension_BAD.cpp</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;pybind11/pybind11.h&gt;</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="s">"_C"</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># in, say, extension/__init__.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">_C</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>In this tutorial, because we value being able to build a single wheel across multiple
CPython versions, we will replace the unstable <code class="docutils literal notranslate"><span class="pre">PYBIND11</span></code> call with stable API calls.
The below code compiles with <code class="docutils literal notranslate"><span class="pre">-DPy_LIMITED_API=0x03090000</span></code> and successfully creates
a dummy Python module for our <code class="docutils literal notranslate"><span class="pre">_C</span></code> extension so that it can be imported from Python.
See <a class="reference external" href="https://github.com/pytorch/extension-cpp/blob/38ec45e/extension_cpp/__init__.py">extension_cpp/__init__.py</a>
and <a class="reference external" href="https://github.com/pytorch/extension-cpp/blob/38ec45e/extension_cpp/csrc/muladd.cpp">extension_cpp/csrc/muladd.cpp</a>
for more details:</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;Python.h&gt;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">"C"</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="cm">/* Creates a dummy empty _C module that can be imported from Python.</span>
<span class="cm">    The import from Python will load the .so consisting of this file</span>
<span class="cm">    in this extension, so that the TORCH_LIBRARY static initializers</span>
<span class="cm">    below are run. */</span>
<span class="w">  </span><span class="n">PyObject</span><span class="o">*</span><span class="w"> </span><span class="nf">PyInit__C</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">      </span><span class="k">static</span><span class="w"> </span><span class="k">struct</span><span class="w"> </span><span class="nc">PyModuleDef</span><span class="w"> </span><span class="n">module_def</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="n">PyModuleDef_HEAD_INIT</span><span class="p">,</span>
<span class="w">          </span><span class="s">"_C"</span><span class="p">,</span><span class="w">   </span><span class="cm">/* name of module */</span>
<span class="w">          </span><span class="nb">NULL</span><span class="p">,</span><span class="w">   </span><span class="cm">/* module documentation, may be NULL */</span>
<span class="w">          </span><span class="mi">-1</span><span class="p">,</span><span class="w">     </span><span class="cm">/* size of per-interpreter state of the module,</span>
<span class="cm">                    or -1 if the module keeps state in global variables. */</span>
<span class="w">          </span><span class="nb">NULL</span><span class="p">,</span><span class="w">   </span><span class="cm">/* methods */</span>
<span class="w">      </span><span class="p">};</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">PyModule_Create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">module_def</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># in, say, extension/__init__.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">_C</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>If you want to avoid <code class="docutils literal notranslate"><span class="pre">Python.h</span></code> entirely in your C++ custom operator, you may
use <code class="docutils literal notranslate"><span class="pre">torch.ops.load_library("/path/to/library.so")</span></code> in Python to load the <code class="docutils literal notranslate"><span class="pre">.so</span></code>
file(s) compiled from the extension. Note that, with this method, there is no <code class="docutils literal notranslate"><span class="pre">_C</span></code>
Python module created for the extension so you cannot call <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">_C</span></code> from Python.
Instead of relying on the import statement to trigger the custom operators to be
registered, <code class="docutils literal notranslate"><span class="pre">torch.ops.load_library("/path/to/library.so")</span></code> will do the trick.
The challenge then is shifted towards understanding where the <code class="docutils literal notranslate"><span class="pre">.so</span></code> files are
located so that you can load them, which is not always trivial:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="n">so_files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">"_C*.so"</span><span class="p">))</span>
<span class="k">assert</span> <span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">so_files</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<span class="p">),</span> <span class="sa">f</span><span class="s2">"Expected one _C*.so file, found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">so_files</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
<span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">load_library</span><span class="p">(</span><span class="n">so_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">ops</span>
</pre></div>
</div>
</section>
<section id="adding-training-autograd-support-for-an-operator">
<h3>Adding training (autograd) support for an operator<a class="headerlink" href="#adding-training-autograd-support-for-an-operator" title="Link to this heading">#</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">torch.library.register_autograd</span></code> to add training support for an operator. Prefer
this over directly using Python <code class="docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> (see
<a class="reference external" href="https://pytorch.org/docs/main/notes/custom_operators.html">The Custom Operators Manual</a>
for more details).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">a</span>
    <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">saved_a</span><span class="p">,</span> <span class="n">saved_b</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">saved_b</span> <span class="o">=</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">saved_a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">saved_a</span><span class="p">,</span> <span class="n">saved_b</span><span class="p">)</span>

<span class="c1"># This code adds training support for the operator. You must provide us</span>
<span class="c1"># the backward formula for the operator and a `setup_context` function</span>
<span class="c1"># to save values to be used in the backward.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_autograd</span><span class="p">(</span>
    <span class="s2">"extension_cpp::mymuladd"</span><span class="p">,</span> <span class="n">_backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="o">=</span><span class="n">_setup_context</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the backward must be a composition of PyTorch-understood operators.
If you wish to use another custom C++ or CUDA kernel in your backwards pass,
it must be wrapped into a custom operator.</p>
<p>If we had our own custom <code class="docutils literal notranslate"><span class="pre">mymul</span></code> kernel, we would need to wrap it into a
custom operator and then call that from the backward:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-10" name="sd-tab-set-5" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-10">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mymul_cpu</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">()));</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>

<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">b</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a_contig</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">mutable_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymul(Tensor a, Tensor b) -&gt; Tensor"</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">));</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymul"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymul_cpu</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-5" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-11">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mymul_cpu</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">a_contig</span><span class="p">.</span><span class="n">sizes</span><span class="p">(),</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymul(Tensor a, Tensor b) -&gt; Tensor"</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymul"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymul_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">extension_cpp</span><span class="o">.</span><span class="n">mymul</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">extension_cpp</span><span class="o">.</span><span class="n">mymul</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">saved_a</span><span class="p">,</span> <span class="n">saved_b</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">saved_b</span> <span class="o">=</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">saved_a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">saved_a</span><span class="p">,</span> <span class="n">saved_b</span><span class="p">)</span>


<span class="c1"># This code adds training support for the operator. You must provide us</span>
<span class="c1"># the backward formula for the operator and a `setup_context` function</span>
<span class="c1"># to save values to be used in the backward.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_autograd</span><span class="p">(</span>
    <span class="s2">"extension_cpp::mymuladd"</span><span class="p">,</span> <span class="n">_backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="o">=</span><span class="n">_setup_context</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="testing-an-operator">
<h2>Testing an operator<a class="headerlink" href="#testing-an-operator" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">torch.library.opcheck</span></code> to test that the custom op was registered correctly.
Note that this function does not test that the gradients are mathematically correct
â€“ plan to write separate tests for that, either manual ones or by using
<code class="docutils literal notranslate"><span class="pre">torch.autograd.gradcheck</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_inputs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">make_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">make_nondiff_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">make_tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">make_tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="n">make_tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span> <span class="n">make_tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span> <span class="mf">3.14</span><span class="p">],</span>
        <span class="p">[</span><span class="n">make_tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span> <span class="n">make_nondiff_tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span> <span class="o">-</span><span class="mi">123</span><span class="p">],</span>
        <span class="p">[</span><span class="n">make_nondiff_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">make_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span>
    <span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">reference_muladd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">sample_inputs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">samples</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sample_inputs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
    <span class="c1"># Correctness test</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">extension_cpp</span><span class="o">.</span><span class="n">mymuladd</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="n">reference_muladd</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">expected</span><span class="p">)</span>

    <span class="c1"># Use opcheck to check for incorrect usage of operator registration APIs</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">opcheck</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">extension_cpp</span><span class="o">.</span><span class="n">mymuladd</span><span class="o">.</span><span class="n">default</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="creating-mutable-operators">
<span id="mutable-ops"></span><h2>Creating mutable operators<a class="headerlink" href="#creating-mutable-operators" title="Link to this heading">#</a></h2>
<p>You may wish to author a custom operator that mutates its inputs. Use <code class="docutils literal notranslate"><span class="pre">Tensor(a!)</span></code>
to specify each mutable Tensor in the schema; otherwise, there will be undefined
behavior. If there are multiple mutated Tensors, use different names (for example, <code class="docutils literal notranslate"><span class="pre">Tensor(a!)</span></code>,
<code class="docutils literal notranslate"><span class="pre">Tensor(b!)</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor(c!)</span></code>) for each mutable Tensor.</p>
<p>Letâ€™s author a <code class="docutils literal notranslate"><span class="pre">myadd_out(a,</span> <span class="pre">b,</span> <span class="pre">out)</span></code> operator, which writes the contents of <code class="docutils literal notranslate"><span class="pre">a+b</span></code> into <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-6" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-12">
ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">myadd_out_cpu</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">()));</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">sizes</span><span class="p">()));</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">());</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>

<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">b</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">const_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">mutable_data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When defining the operator, we must specify that it mutates the out Tensor in the schema:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STABLE_TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymul(Tensor a, Tensor b) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -&gt; ()"</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">));</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymul"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mymul_cpu</span><span class="p">));</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"myadd_out"</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">myadd_out_cpu</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-13" name="sd-tab-set-6" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-13">
Non-ABI-Stable LibTorch API</label><div class="sd-tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">myadd_out_cpu</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">());</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">a_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">b_contig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b_contig</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When defining the operator, we must specify that it mutates the out Tensor in the schema:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymuladd(Tensor a, Tensor b, float c) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"mymul(Tensor a, Tensor b) -&gt; Tensor"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -&gt; ()"</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">extension_cpp</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymuladd"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymuladd_cpu</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"mymul"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">mymul_cpu</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">"myadd_out"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myadd_out_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not return any mutated Tensors as outputs of the operator as this will
cause incompatibility with PyTorch subsystems like <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>.</p>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this tutorial, we went over the recommended approach to integrating Custom C++
and CUDA operators with PyTorch. The <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY/STABLE_TORCH_LIBRARY</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.library</span></code> APIs are fairly
low-level. For more information about how to use the API, see
<a class="reference external" href="https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-custom-operators-manual">The Custom Operators Manual</a>.</p>
</section>
</section>
</article>
</div>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="python_custom_ops.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Custom Python Operators</p>
</div>
</a>
<a class="right-next" href="../intermediate/custom_function_double_backward_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Double Backward with Custom Functions</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="python_custom_ops.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Custom Python Operators</p>
</div>
</a>
<a class="right-next" href="../intermediate/custom_function_double_backward_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Double Backward with Custom Functions</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-build-system">Setting up the Build System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cpython-agnosticism">CPython Agnosticism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#libtorch-stable-abi-pytorch-agnosticism">LibTorch Stable ABI (PyTorch Agnosticism)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-custom-op-and-adding-backend-implementations">Defining the custom op and adding backend implementations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-an-operator">Defining an operator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#registering-backend-implementations-for-an-operator">Registering backend implementations for an operator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-torch-compile-support-for-an-operator">Adding <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support for an operator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-hybrid-python-c-registration">Setting up hybrid Python/C++ registration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-training-autograd-support-for-an-operator">Adding training (autograd) support for an operator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-an-operator">Testing an operator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-mutable-operators">Creating mutable operators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/helion" style="color: var(--pst-color-text-muted)">Helion</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/kineto" style="color: var(--pst-color-text-muted)">kineto</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/torchtitan" style="color: var(--pst-color-text-muted)">torchtitan</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/rl" style="color: var(--pst-color-text-muted)">TorchRL</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/audio" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/tensordict" style="color: var(--pst-color-text-muted)">tensordict</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          Â© PyTorch. Copyright Â© The Linux FoundationÂ®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Custom C++ and CUDA Operators",
       "headline": "Custom C++ and CUDA Operators",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/advanced/cpp_custom_ops.html",
       "articleBody": "Custom C++ and CUDA Operators# Author: Richard Zou What you will learn How to integrate custom operators written in C++/CUDA with PyTorch How to test custom operators using torch.library.opcheck Prerequisites PyTorch 2.4 or later (or PyTorch 2.10 or later if using the stable ABI) Basic understanding of C++ and CUDA programming Note This tutorial will also work on AMD ROCm with no additional modifications. PyTorch offers a large library of operators that work on Tensors (e.g. torch.add, torch.sum, etc). However, you may wish to bring a new custom operator to PyTorch. This tutorial demonstrates the blessed path to authoring a custom operator written in C++/CUDA. For our tutorial, we\u2019ll demonstrate how to author a fused multiply-add C++ and CUDA operator that composes with PyTorch subsystems. The semantics of the operation are as follows: def mymuladd(a: Tensor, b: Tensor, c: float): return a * b + c You can find the end-to-end working example for this tutorial in the extension-cpp repository, which contains two parallel implementations: extension_cpp/: Uses the standard ATen/LibTorch API. extension_cpp_stable/: Uses APIs supported by the LibTorch Stable ABI (recommended for PyTorch 2.10+). Which API should you use? ABI-Stable LibTorch API (recommended): If you are using PyTorch 2.10+, we recommend using the ABI-stable API. It allows you to build a single wheel that works across multiple PyTorch versions (2.10, 2.11, 2.12, etc.), reducing the maintenance burden of supporting multiple PyTorch releases. See the LibTorch Stable ABI (PyTorch Agnosticism) section below for more details. Non-ABI-Stable LibTorch API: Use this if you need APIs not yet available in the stable ABI, or if you are targeting PyTorch versions older than 2.10. Note that you will need to build separate wheels for each PyTorch version you want to support. The code snippets below show both implementations using tabs, with the ABI-stable API shown by default. Setting up the Build System# If you are developing custom C++/CUDA code, it must be compiled. Note that if you\u2019re interfacing with a Python library that already has bindings to precompiled C++/CUDA code, you might consider writing a custom Python operator instead (Custom Python Operators). Use torch.utils.cpp_extension to compile custom C++/CUDA code for use with PyTorch. C++ extensions may be built either \u201cahead of time\u201d with setuptools, or \u201cjust in time\u201d via load_inline; we\u2019ll focus on the \u201cahead of time\u201d flavor. Using cpp_extension is as simple as writing a setup.py: ABI-Stable LibTorch API from setuptools import setup, Extension from torch.utils import cpp_extension setup(name=\"extension_cpp\", ext_modules=[ cpp_extension.CppExtension( \"extension_cpp\", [\"muladd.cpp\"], extra_compile_args={ \"cxx\": [ # define Py_LIMITED_API with min version 3.9 to expose only the stable # limited API subset from Python.h \"-DPy_LIMITED_API=0x03090000\", # define TORCH_TARGET_VERSION with min version 2.10 to expose only the # stable API subset from torch \"-DTORCH_TARGET_VERSION=0x020a000000000000\", ] }, py_limited_api=True)], # Build 1 wheel across multiple Python versions cmdclass={\u0027build_ext\u0027: cpp_extension.BuildExtension}, options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}} # 3.9 is minimum supported Python version ) Non-ABI-Stable LibTorch API from setuptools import setup, Extension from torch.utils import cpp_extension setup(name=\"extension_cpp\", ext_modules=[ cpp_extension.CppExtension( \"extension_cpp\", [\"muladd.cpp\"], extra_compile_args={ \"cxx\": [ \"-DPy_LIMITED_API=0x03090000\", ] }, py_limited_api=True)], cmdclass={\u0027build_ext\u0027: cpp_extension.BuildExtension}, options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}} ) If you need to compile CUDA code (for example, .cu files), then instead use torch.utils.cpp_extension.CUDAExtension. Please see extension-cpp for an example for how this is set up. CPython Agnosticism# The above examples represent what we refer to as a CPython agnostic wheel, meaning we are building a single wheel that can be run across multiple CPython versions (similar to pure Python packages). CPython agnosticism is desirable in minimizing the number of wheels your custom library needs to support and release. The minimum version we\u2019d like to support is 3.9, since it is the oldest supported version currently, so we use the corresponding hexcode and specifier throughout the setup code. We suggest building the extension in the same environment as the minimum CPython version you\u2019d like to support to minimize unknown behavior, so, here, we build the extension in a CPython 3.9 environment. When built, this single wheel will be runnable in any CPython environment 3.9+. To achieve this, there are three key lines to note. The first is the specification of Py_LIMITED_API in extra_compile_args to the minimum CPython version you would like to support: extra_compile_args={\"cxx\": [\"-DPy_LIMITED_API=0x03090000\"]}, Defining the Py_LIMITED_API flag helps verify that the extension is in fact only using the CPython Stable Limited API, which is a requirement for building a CPython agnostic wheel. If this requirement is not met, it is possible to build a wheel that looks CPython agnostic but will crash, or worse, be silently incorrect, in another CPython environment. Take care to avoid using unstable CPython APIs, for example APIs from libtorch_python (in particular pytorch/python bindings,) and to only use APIs from libtorch (ATen objects, operators and the dispatcher). We strongly recommend defining the Py_LIMITED_API flag to help ascertain the extension is compliant and safe as a CPython agnostic wheel. Note that defining this flag is not a full guarantee that the built wheel is CPython agnostic, but it is better than the wild wild west. There are several caveats mentioned in the Python docs, and you should test and verify yourself that the wheel is truly agnostic for the relevant CPython versions. The second and third lines specifying py_limited_api inform setuptools that you intend to build a CPython agnostic wheel and will influence the naming of the wheel accordingly: setup(name=\"extension_cpp\", ext_modules=[ cpp_extension.CppExtension( ..., py_limited_api=True)], # Build 1 wheel across multiple Python versions ..., options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}} # 3.9 is minimum supported Python version ) It is necessary to specify py_limited_api=True as an argument to CppExtension/ CUDAExtension and also as an option to the \"bdist_wheel\" command with the minimal supported CPython version (in this case, 3.9). Consequently, the setup in our tutorial would build one properly named wheel that could be installed across multiple CPython versions \u003e=3.9. If your extension uses CPython APIs outside the stable limited set, then you cannot build a CPython agnostic wheel! You should build one wheel per CPython version instead, like so: from setuptools import setup, Extension from torch.utils import cpp_extension setup(name=\"extension_cpp\", ext_modules=[ cpp_extension.CppExtension( \"extension_cpp\", [\"muladd.cpp\"])], cmdclass={\u0027build_ext\u0027: cpp_extension.BuildExtension}, ) LibTorch Stable ABI (PyTorch Agnosticism)# In addition to CPython agnosticism, there is a second axis of wheel compatibility: LibTorch agnosticism. While CPython agnosticism allows building a single wheel that works across multiple Python versions (3.9, 3.10, 3.11, etc.), LibTorch agnosticism allows building a single wheel that works across multiple PyTorch versions (2.10, 2.11, 2.12, etc.). These two concepts are orthogonal and can be combined. To achieve LibTorch agnosticism, you must use the ABI stable LibTorch API, which provides a stable API for interacting with PyTorch tensors and operators. For example, instead of using at::Tensor, you must use torch::stable::Tensor. For comprehensive documentation on the stable ABI, including migration guides, supported types, and stack-based API conventions, see the LibTorch Stable ABI documentation. The stable ABI setup.py includes TORCH_TARGET_VERSION=0x020a000000000000, which indicates that the extension targets the LibTorch Stable ABI with a minimum supported PyTorch version of 2.10. The version format is: [MAJ 1 byte][MIN 1 byte][PATCH 1 byte][ABI TAG 5 bytes], so 2.10.0 = 0x020a000000000000. If the stable API/ABI does not contain what you need, you can use the Non-ABI-stable LibTorch API, but you will need to build separate wheels for each PyTorch version you want to support. Defining the custom op and adding backend implementations# First, let\u2019s write a C++ function that computes mymuladd: ABI-Stable LibTorch API #include \u003ctorch/csrc/stable/library.h\u003e #include \u003ctorch/csrc/stable/ops.h\u003e #include \u003ctorch/csrc/stable/tensor.h\u003e #include \u003ctorch/headeronly/core/ScalarType.h\u003e #include \u003ctorch/headeronly/macros/Macros.h\u003e torch::stable::Tensor mymuladd_cpu( const torch::stable::Tensor\u0026 a, const torch::stable::Tensor\u0026 b, double c) { STD_TORCH_CHECK(a.sizes().equals(b.sizes())); STD_TORCH_CHECK(a.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(b.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(a.device().type() == torch::headeronly::DeviceType::CPU); STD_TORCH_CHECK(b.device().type() == torch::headeronly::DeviceType::CPU); torch::stable::Tensor a_contig = torch::stable::contiguous(a); torch::stable::Tensor b_contig = torch::stable::contiguous(b); torch::stable::Tensor result = torch::stable::empty_like(a_contig); const float* a_ptr = a_contig.const_data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.const_data_ptr\u003cfloat\u003e(); float* result_ptr = result.mutable_data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c result.numel(); i++) { result_ptr[i] = a_ptr[i] * b_ptr[i] + c; } return result; } Non-ABI-Stable LibTorch API #include \u003cATen/Operators.h\u003e #include \u003ctorch/all.h\u003e #include \u003ctorch/library.h\u003e at::Tensor mymuladd_cpu(const at::Tensor\u0026 a, const at::Tensor\u0026 b, double c) { TORCH_CHECK(a.sizes() == b.sizes()); TORCH_CHECK(a.dtype() == at::kFloat); TORCH_CHECK(b.dtype() == at::kFloat); TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU); TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU); at::Tensor a_contig = a.contiguous(); at::Tensor b_contig = b.contiguous(); at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options()); const float* a_ptr = a_contig.data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.data_ptr\u003cfloat\u003e(); float* result_ptr = result.data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c result.numel(); i++) { result_ptr[i] = a_ptr[i] * b_ptr[i] + c; } return result; } In order to use this from PyTorch\u2019s Python frontend, we need to register it as a PyTorch operator using the TORCH_LIBRARY (or STABLE_TORCH_LIBRARY) macro. This will automatically bind the operator to Python. Operator registration is a two step-process: Defining the operator - This step ensures that PyTorch is aware of the new operator. Registering backend implementations - In this step, implementations for various backends, such as CPU and CUDA, are associated with the operator. Defining an operator# To define an operator, follow these steps: select a namespace for an operator. We recommend the namespace be the name of your top-level project; we\u2019ll use \u201cextension_cpp\u201d in our tutorial. provide a schema string that specifies the input/output types of the operator and if an input Tensors will be mutated. We support more types in addition to Tensor and float; please see The Custom Operators Manual for more details. If you are authoring an operator that can mutate its input Tensors, please see here (Creating mutable operators) for how to specify that. ABI-Stable LibTorch API STABLE_TORCH_LIBRARY(extension_cpp, m) { // Note that \"float\" in the schema corresponds to the C++ double type // and the Python float type. m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); } Non-ABI-Stable LibTorch API TORCH_LIBRARY(extension_cpp, m) { // Note that \"float\" in the schema corresponds to the C++ double type // and the Python float type. m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); } This makes the operator available from Python via torch.ops.extension_cpp.mymuladd. Registering backend implementations for an operator# Use TORCH_LIBRARY_IMPL (or STABLE_TORCH_LIBRARY_IMPL) to register a backend implementation for the operator. ABI-Stable LibTorch API Note that we wrap the function pointer with TORCH_BOX() - this is required for stable ABI functions to handle argument boxing/unboxing correctly. STABLE_TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", TORCH_BOX(\u0026mymuladd_cpu)); } Non-ABI-Stable LibTorch API TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", \u0026mymuladd_cpu); } If you also have a CUDA implementation of mymuladd, you can register it in a separate TORCH_LIBRARY_IMPL (or STABLE_TORCH_LIBRARY_IMPL) block: ABI-Stable LibTorch API #include \u003ctorch/csrc/stable/library.h\u003e #include \u003ctorch/csrc/stable/ops.h\u003e #include \u003ctorch/csrc/stable/tensor.h\u003e #include \u003ctorch/csrc/stable/c/shim.h\u003e #include \u003ccuda.h\u003e #include \u003ccuda_runtime.h\u003e __global__ void muladd_kernel(int numel, const float* a, const float* b, float c, float* result) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx \u003c numel) result[idx] = a[idx] * b[idx] + c; } torch::stable::Tensor mymuladd_cuda( const torch::stable::Tensor\u0026 a, const torch::stable::Tensor\u0026 b, double c) { STD_TORCH_CHECK(a.sizes().equals(b.sizes())); STD_TORCH_CHECK(a.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(b.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(a.device().type() == torch::headeronly::DeviceType::CUDA); STD_TORCH_CHECK(b.device().type() == torch::headeronly::DeviceType::CUDA); torch::stable::Tensor a_contig = torch::stable::contiguous(a); torch::stable::Tensor b_contig = torch::stable::contiguous(b); torch::stable::Tensor result = torch::stable::empty_like(a_contig); const float* a_ptr = a_contig.const_data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.const_data_ptr\u003cfloat\u003e(); float* result_ptr = result.mutable_data_ptr\u003cfloat\u003e(); int numel = a_contig.numel(); // For now, we rely on the raw shim API to get the current CUDA stream. // This will be improved in a future release. // When using a raw shim API, we need to use TORCH_ERROR_CODE_CHECK to // check the error code and throw an appropriate runtime_error otherwise. void* stream_ptr = nullptr; TORCH_ERROR_CODE_CHECK( aoti_torch_get_current_cuda_stream(a.get_device_index(), \u0026stream_ptr)); cudaStream_t stream = static_cast\u003ccudaStream_t\u003e(stream_ptr); muladd_kernel\u003c\u003c\u003c(numel+255)/256, 256, 0, stream\u003e\u003e\u003e(numel, a_ptr, b_ptr, c, result_ptr); return result; } STABLE_TORCH_LIBRARY_IMPL(extension_cpp, CUDA, m) { m.impl(\"mymuladd\", TORCH_BOX(\u0026mymuladd_cuda)); } Non-ABI-Stable LibTorch API #include \u003cATen/Operators.h\u003e #include \u003ctorch/all.h\u003e #include \u003ctorch/library.h\u003e #include \u003ccuda.h\u003e #include \u003ccuda_runtime.h\u003e #include \u003cATen/cuda/CUDAContext.h\u003e __global__ void muladd_kernel(int numel, const float* a, const float* b, float c, float* result) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx \u003c numel) result[idx] = a[idx] * b[idx] + c; } at::Tensor mymuladd_cuda(const at::Tensor\u0026 a, const at::Tensor\u0026 b, double c) { TORCH_CHECK(a.sizes() == b.sizes()); TORCH_CHECK(a.dtype() == at::kFloat); TORCH_CHECK(b.dtype() == at::kFloat); TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CUDA); TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CUDA); at::Tensor a_contig = a.contiguous(); at::Tensor b_contig = b.contiguous(); at::Tensor result = at::empty(a_contig.sizes(), a_contig.options()); const float* a_ptr = a_contig.data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.data_ptr\u003cfloat\u003e(); float* result_ptr = result.data_ptr\u003cfloat\u003e(); int numel = a_contig.numel(); cudaStream_t stream = at::cuda::getCurrentCUDAStream(); muladd_kernel\u003c\u003c\u003c(numel+255)/256, 256, 0, stream\u003e\u003e\u003e(numel, a_ptr, b_ptr, c, result_ptr); return result; } TORCH_LIBRARY_IMPL(extension_cpp, CUDA, m) { m.impl(\"mymuladd\", \u0026mymuladd_cuda); } Adding torch.compile support for an operator# To add torch.compile support for an operator, we must add a FakeTensor kernel (also known as a \u201cmeta kernel\u201d or \u201cabstract impl\u201d). FakeTensors are Tensors that have metadata (such as shape, dtype, device) but no data: the FakeTensor kernel for an operator specifies how to compute the metadata of output tensors given the metadata of input tensors. The FakeTensor kernel should return dummy Tensors of your choice with the correct Tensor metadata (shape/strides/dtype/device). We recommend that this be done from Python via the torch.library.register_fake API, though it is possible to do this from C++ as well (see The Custom Operators Manual for more details). # Important: the C++ custom operator definitions should be loaded first # before calling ``torch.library`` APIs that add registrations for the # C++ custom operator(s). The following import loads our # C++ custom operator definitions. # Note that if you are striving for Python agnosticism, you should use # the ``load_library(...)`` API call instead. See the next section for # more details. from . import _C @torch.library.register_fake(\"extension_cpp::mymuladd\") def _(a, b, c): torch._check(a.shape == b.shape) torch._check(a.dtype == torch.float) torch._check(b.dtype == torch.float) torch._check(a.device == b.device) return torch.empty_like(a) Setting up hybrid Python/C++ registration# In this tutorial, we defined a custom operator in C++, added CPU/CUDA implementations in C++, and added FakeTensor kernels and backward formulas in Python. The order in which these registrations are loaded (or imported) matters (importing in the wrong order will lead to an error). To use the custom operator with hybrid Python/C++ registrations, we must first load the C++ library that holds the custom operator definition and then call the torch.library registration APIs. This can happen in three ways: The first way to load the C++ library that holds the custom operator definition is to define a dummy Python module for _C. Then, in Python, when you import the module with import _C, the .so files corresponding to the extension will be loaded and the TORCH_LIBRARY and TORCH_LIBRARY_IMPL static initializers will run. One can create a dummy Python module with PYBIND11_MODULE like below, but you will notice that this does not compile with Py_LIMITED_API, because pybind11 does not promise to only use the stable limited CPython API! With the below code, you sadly cannot build a CPython agnostic wheel for your extension! (Foreshadowing: I wonder what the second way is ;) ). // in, say, not_agnostic/csrc/extension_BAD.cpp #include \u003cpybind11/pybind11.h\u003e PYBIND11_MODULE(\"_C\", m) {} # in, say, extension/__init__.py from . import _C In this tutorial, because we value being able to build a single wheel across multiple CPython versions, we will replace the unstable PYBIND11 call with stable API calls. The below code compiles with -DPy_LIMITED_API=0x03090000 and successfully creates a dummy Python module for our _C extension so that it can be imported from Python. See extension_cpp/__init__.py and extension_cpp/csrc/muladd.cpp for more details: #include \u003cPython.h\u003e extern \"C\" { /* Creates a dummy empty _C module that can be imported from Python. The import from Python will load the .so consisting of this file in this extension, so that the TORCH_LIBRARY static initializers below are run. */ PyObject* PyInit__C(void) { static struct PyModuleDef module_def = { PyModuleDef_HEAD_INIT, \"_C\", /* name of module */ NULL, /* module documentation, may be NULL */ -1, /* size of per-interpreter state of the module, or -1 if the module keeps state in global variables. */ NULL, /* methods */ }; return PyModule_Create(\u0026module_def); } } # in, say, extension/__init__.py from . import _C If you want to avoid Python.h entirely in your C++ custom operator, you may use torch.ops.load_library(\"/path/to/library.so\") in Python to load the .so file(s) compiled from the extension. Note that, with this method, there is no _C Python module created for the extension so you cannot call import _C from Python. Instead of relying on the import statement to trigger the custom operators to be registered, torch.ops.load_library(\"/path/to/library.so\") will do the trick. The challenge then is shifted towards understanding where the .so files are located so that you can load them, which is not always trivial: import torch from pathlib import Path so_files = list(Path(__file__).parent.glob(\"_C*.so\")) assert ( len(so_files) == 1 ), f\"Expected one _C*.so file, found {len(so_files)}\" torch.ops.load_library(so_files[0]) from . import ops Adding training (autograd) support for an operator# Use torch.library.register_autograd to add training support for an operator. Prefer this over directly using Python torch.autograd.Function (see The Custom Operators Manual for more details). def _backward(ctx, grad): a, b = ctx.saved_tensors grad_a, grad_b = None, None if ctx.needs_input_grad[0]: grad_a = grad * b if ctx.needs_input_grad[1]: grad_b = grad * a return grad_a, grad_b, None def _setup_context(ctx, inputs, output): a, b, c = inputs saved_a, saved_b = None, None if ctx.needs_input_grad[0]: saved_b = b if ctx.needs_input_grad[1]: saved_a = a ctx.save_for_backward(saved_a, saved_b) # This code adds training support for the operator. You must provide us # the backward formula for the operator and a `setup_context` function # to save values to be used in the backward. torch.library.register_autograd( \"extension_cpp::mymuladd\", _backward, setup_context=_setup_context) Note that the backward must be a composition of PyTorch-understood operators. If you wish to use another custom C++ or CUDA kernel in your backwards pass, it must be wrapped into a custom operator. If we had our own custom mymul kernel, we would need to wrap it into a custom operator and then call that from the backward: ABI-Stable LibTorch API torch::stable::Tensor mymul_cpu( const torch::stable::Tensor\u0026 a, const torch::stable::Tensor\u0026 b) { STD_TORCH_CHECK(a.sizes().equals(b.sizes())); STD_TORCH_CHECK(a.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(b.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(a.device().type() == torch::headeronly::DeviceType::CPU); STD_TORCH_CHECK(b.device().type() == torch::headeronly::DeviceType::CPU); torch::stable::Tensor a_contig = torch::stable::contiguous(a); torch::stable::Tensor b_contig = torch::stable::contiguous(b); torch::stable::Tensor result = torch::stable::empty_like(a_contig); const float* a_ptr = a_contig.const_data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.const_data_ptr\u003cfloat\u003e(); float* result_ptr = result.mutable_data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c result.numel(); i++) { result_ptr[i] = a_ptr[i] * b_ptr[i]; } return result; } STABLE_TORCH_LIBRARY(extension_cpp, m) { m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); m.def(\"mymul(Tensor a, Tensor b) -\u003e Tensor\"); } STABLE_TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", TORCH_BOX(\u0026mymuladd_cpu)); m.impl(\"mymul\", TORCH_BOX(\u0026mymul_cpu)); } Non-ABI-Stable LibTorch API at::Tensor mymul_cpu(const at::Tensor\u0026 a, const at::Tensor\u0026 b) { TORCH_CHECK(a.sizes() == b.sizes()); TORCH_CHECK(a.dtype() == at::kFloat); TORCH_CHECK(b.dtype() == at::kFloat); TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU); TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU); at::Tensor a_contig = a.contiguous(); at::Tensor b_contig = b.contiguous(); at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options()); const float* a_ptr = a_contig.data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.data_ptr\u003cfloat\u003e(); float* result_ptr = result.data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c result.numel(); i++) { result_ptr[i] = a_ptr[i] * b_ptr[i]; } return result; } TORCH_LIBRARY(extension_cpp, m) { m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); m.def(\"mymul(Tensor a, Tensor b) -\u003e Tensor\"); } TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", \u0026mymuladd_cpu); m.impl(\"mymul\", \u0026mymul_cpu); } def _backward(ctx, grad): a, b = ctx.saved_tensors grad_a, grad_b = None, None if ctx.needs_input_grad[0]: grad_a = torch.ops.extension_cpp.mymul.default(grad, b) if ctx.needs_input_grad[1]: grad_b = torch.ops.extension_cpp.mymul.default(grad, a) return grad_a, grad_b, None def _setup_context(ctx, inputs, output): a, b, c = inputs saved_a, saved_b = None, None if ctx.needs_input_grad[0]: saved_b = b if ctx.needs_input_grad[1]: saved_a = a ctx.save_for_backward(saved_a, saved_b) # This code adds training support for the operator. You must provide us # the backward formula for the operator and a `setup_context` function # to save values to be used in the backward. torch.library.register_autograd( \"extension_cpp::mymuladd\", _backward, setup_context=_setup_context) Testing an operator# Use torch.library.opcheck to test that the custom op was registered correctly. Note that this function does not test that the gradients are mathematically correct \u2013 plan to write separate tests for that, either manual ones or by using torch.autograd.gradcheck. def sample_inputs(device, *, requires_grad=False): def make_tensor(*size): return torch.randn(size, device=device, requires_grad=requires_grad) def make_nondiff_tensor(*size): return torch.randn(size, device=device, requires_grad=False) return [ [make_tensor(3), make_tensor(3), 1], [make_tensor(20), make_tensor(20), 3.14], [make_tensor(20), make_nondiff_tensor(20), -123], [make_nondiff_tensor(2, 3), make_tensor(2, 3), -0.3], ] def reference_muladd(a, b, c): return a * b + c samples = sample_inputs(device, requires_grad=True) samples.extend(sample_inputs(device, requires_grad=False)) for args in samples: # Correctness test result = torch.ops.extension_cpp.mymuladd(*args) expected = reference_muladd(*args) torch.testing.assert_close(result, expected) # Use opcheck to check for incorrect usage of operator registration APIs torch.library.opcheck(torch.ops.extension_cpp.mymuladd.default, args) Creating mutable operators# You may wish to author a custom operator that mutates its inputs. Use Tensor(a!) to specify each mutable Tensor in the schema; otherwise, there will be undefined behavior. If there are multiple mutated Tensors, use different names (for example, Tensor(a!), Tensor(b!), Tensor(c!)) for each mutable Tensor. Let\u2019s author a myadd_out(a, b, out) operator, which writes the contents of a+b into out. ABI-Stable LibTorch API void myadd_out_cpu( const torch::stable::Tensor\u0026 a, const torch::stable::Tensor\u0026 b, torch::stable::Tensor\u0026 out) { STD_TORCH_CHECK(a.sizes().equals(b.sizes())); STD_TORCH_CHECK(b.sizes().equals(out.sizes())); STD_TORCH_CHECK(a.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(b.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(out.scalar_type() == torch::headeronly::ScalarType::Float); STD_TORCH_CHECK(out.is_contiguous()); STD_TORCH_CHECK(a.device().type() == torch::headeronly::DeviceType::CPU); STD_TORCH_CHECK(b.device().type() == torch::headeronly::DeviceType::CPU); STD_TORCH_CHECK(out.device().type() == torch::headeronly::DeviceType::CPU); torch::stable::Tensor a_contig = torch::stable::contiguous(a); torch::stable::Tensor b_contig = torch::stable::contiguous(b); const float* a_ptr = a_contig.const_data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.const_data_ptr\u003cfloat\u003e(); float* result_ptr = out.mutable_data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c out.numel(); i++) { result_ptr[i] = a_ptr[i] + b_ptr[i]; } } When defining the operator, we must specify that it mutates the out Tensor in the schema: STABLE_TORCH_LIBRARY(extension_cpp, m) { m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); m.def(\"mymul(Tensor a, Tensor b) -\u003e Tensor\"); m.def(\"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -\u003e ()\"); } STABLE_TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", TORCH_BOX(\u0026mymuladd_cpu)); m.impl(\"mymul\", TORCH_BOX(\u0026mymul_cpu)); m.impl(\"myadd_out\", TORCH_BOX(\u0026myadd_out_cpu)); } Non-ABI-Stable LibTorch API void myadd_out_cpu(const at::Tensor\u0026 a, const at::Tensor\u0026 b, at::Tensor\u0026 out) { TORCH_CHECK(a.sizes() == b.sizes()); TORCH_CHECK(b.sizes() == out.sizes()); TORCH_CHECK(a.dtype() == at::kFloat); TORCH_CHECK(b.dtype() == at::kFloat); TORCH_CHECK(out.dtype() == at::kFloat); TORCH_CHECK(out.is_contiguous()); TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU); TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU); TORCH_INTERNAL_ASSERT(out.device().type() == at::DeviceType::CPU); at::Tensor a_contig = a.contiguous(); at::Tensor b_contig = b.contiguous(); const float* a_ptr = a_contig.data_ptr\u003cfloat\u003e(); const float* b_ptr = b_contig.data_ptr\u003cfloat\u003e(); float* result_ptr = out.data_ptr\u003cfloat\u003e(); for (int64_t i = 0; i \u003c out.numel(); i++) { result_ptr[i] = a_ptr[i] + b_ptr[i]; } } When defining the operator, we must specify that it mutates the out Tensor in the schema: TORCH_LIBRARY(extension_cpp, m) { m.def(\"mymuladd(Tensor a, Tensor b, float c) -\u003e Tensor\"); m.def(\"mymul(Tensor a, Tensor b) -\u003e Tensor\"); m.def(\"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -\u003e ()\"); } TORCH_LIBRARY_IMPL(extension_cpp, CPU, m) { m.impl(\"mymuladd\", \u0026mymuladd_cpu); m.impl(\"mymul\", \u0026mymul_cpu); m.impl(\"myadd_out\", \u0026myadd_out_cpu); } Note Do not return any mutated Tensors as outputs of the operator as this will cause incompatibility with PyTorch subsystems like torch.compile. Conclusion# In this tutorial, we went over the recommended approach to integrating Custom C++ and CUDA operators with PyTorch. The TORCH_LIBRARY/STABLE_TORCH_LIBRARY and torch.library APIs are fairly low-level. For more information about how to use the API, see The Custom Operators Manual.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/advanced/cpp_custom_ops.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>