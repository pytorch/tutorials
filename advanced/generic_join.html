
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Distributed Training with Uneven Inputs Using the Join Context Manager â€” PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'advanced/generic_join';</script>
<link href="https://pytorch.org/tutorials/advanced/generic_join.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../deep-dive.html" rel="next" title="Deep Dive"/>
<link href="rpc_ddp_tutorial.html" rel="prev" title="Combining Distributed DataParallel with Distributed RPC Framework"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/advanced/generic_join.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname === 'docs.pytorch.org' ||
        window.location.hostname === 'docs-preview.pytorch.org' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel (FSDP2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/TCPStore_libuv_backend.html">Introduction to Libuv TCPStore Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pipelining_tutorial.html">Introduction to Distributed Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../distributed.html">Distributed</a></li>
<li aria-current="page" class="breadcrumb-item active">Distributed...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../distributed.html" itemprop="item"/>
<meta content="Distributed" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Distributed Training with Uneven Inputs Using the Join Context Manager" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">advanced/generic_join</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="distributed-training-with-uneven-inputs-using-the-join-context-manager">
<h1>Distributed Training with Uneven Inputs Using the Join Context Manager<a class="headerlink" href="#distributed-training-with-uneven-inputs-using-the-join-context-manager" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Aug 04, 2021 | Last Updated: Sep 03, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/andwgu">Andrew Gu</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="../_images/pencil-16.png"><img alt="edit" src="../_images/pencil-16.png" style="width: 16px; height: 16px;"/></a> View and edit this tutorial in <a class="reference external" href="https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst">github</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">Join</span></code> is introduced in PyTorch 1.10 as a prototype feature. This
API is subject to change.</p>
</div>
<p>In this tutorial, you will see:</p>
<ul class="simple">
<li><p>An overview of the <a class="reference external" href="https://pytorch.org/docs/master/distributed.algorithms.join.html">Join</a> context manager.</p></li>
<li><p>An example of how to use the context manager with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.</p></li>
<li><p>An example of how to use the context manager with both
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code>.</p></li>
<li><p>An example of passing in keyword arguments to the context manager.</p></li>
<li><p>A dive into how the <a class="reference external" href="https://pytorch.org/docs/master/distributed.algorithms.join.html">Join</a> context manager works.</p></li>
<li><p>An example showing how to make a toy class compatible with the context
manager.</p></li>
</ul>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>PyTorch 1.10+</p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></p></li>
</ul>
</section>
<section id="what-is-join">
<h2>What is <code class="docutils literal notranslate"><span class="pre">Join</span></code>?<a class="headerlink" href="#what-is-join" title="Link to this heading">#</a></h2>
<p>In <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case">Getting Started with Distributed Data Parallel - Basic Use Case</a>, you saw
the general skeleton for using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a> to perform data
parallel training. This implicitly schedules all-reduces in each backward pass
to synchronize gradients across ranks. Such <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">collective communications</a> require participation
from all ranks in the process group, so if a rank has fewer inputs, then the
other ranks will hang or error (depending on the backend). More generally, this
problem persists for any class that performs per-iteration synchronous
collective communications.</p>
<p><code class="docutils literal notranslate"><span class="pre">Join</span></code> is a context manager to be used around your per-rank training loop to
facilitate training with uneven inputs. The context manager allows the ranks
that exhaust their inputs early (i.e. <em>join</em> early) to shadow the collective
communications performed by those that have not yet joined. The ways in which
the communications are shadowed are specified by hooks.</p>
</section>
<section id="using-join-with-distributeddataparallel">
<h2>Using <code class="docutils literal notranslate"><span class="pre">Join</span></code> with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code><a class="headerlink" href="#using-join-with-distributeddataparallel" title="Link to this heading">#</a></h2>
<p>PyTorchâ€™s <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a> works out-of-the-box with the <code class="docutils literal notranslate"><span class="pre">Join</span></code>
context manager. Here is an example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.algorithms.join</span><span class="w"> </span><span class="kn">import</span> <span class="n">Join</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="n">BACKEND</span> <span class="o">=</span> <span class="s2">"nccl"</span>
<span class="n">WORLD_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">NUM_INPUTS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">def</span><span class="w"> </span><span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'localhost'</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'29500'</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">BACKEND</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    <span class="c1"># Rank 1 gets one more input than rank 0</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_INPUTS</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)]</span>

    <span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">Join</span><span class="p">([</span><span class="n">model</span><span class="p">]):</span>
        <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">num_inputs</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> has exhausted all </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> of its inputs!"</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>This produces the following output (where the <code class="docutils literal notranslate"><span class="pre">print()</span></code> s from rank 0 and
rank 1 may be arbitrarily ordered):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Rank 0 has exhausted all 5 of its inputs!
Rank 1 has exhausted all 6 of its inputs!
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a> provided its own <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join">join()</a> context manager
prior to the introduction of this generic <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager. In the
above example, using <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">Join([model]):</span></code> is equivalent to using
<code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">model.join():</span></code>. One limitation of the existing
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel.join()</span></code> is that it does not allow multiple
participating classes, e.g. <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.optim.html">ZeroRedundancyOptimizer</a> together.</p>
</div>
</section>
<section id="using-join-with-distributeddataparallel-and-zeroredundancyoptimizer">
<h2>Using <code class="docutils literal notranslate"><span class="pre">Join</span></code> with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code><a class="headerlink" href="#using-join-with-distributeddataparallel-and-zeroredundancyoptimizer" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager works not only with a single class but also with
multiple classes together. PyTorchâ€™s <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code> is also
compatible with the context manager, so here, we examine how to modify the
previous example to use both <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and
<code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">ZeroRedundancyOptimizer</span> <span class="k">as</span> <span class="n">ZeRO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>

<span class="k">def</span><span class="w"> </span><span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'localhost'</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'29500'</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">BACKEND</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">ZeRO</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="c1"># Rank 1 gets one more input than rank 0</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_INPUTS</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)]</span>

    <span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Pass both `model` and `optim` into `Join()`</span>
    <span class="k">with</span> <span class="n">Join</span><span class="p">([</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">]):</span>
        <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">num_inputs</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> has exhausted all </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> of its inputs!"</span><span class="p">)</span>
</pre></div>
</div>
<p>This will yield the same output as before. The notable change was
additionally passing in the <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code> instance into
<code class="docutils literal notranslate"><span class="pre">Join()</span></code>.</p>
</section>
<section id="passing-keyword-arguments">
<h2>Passing Keyword Arguments<a class="headerlink" href="#passing-keyword-arguments" title="Link to this heading">#</a></h2>
<p>Classes may provide keyword arguments that modify their behavior in the context
manager at run time. For example, <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> provides an
argument <code class="docutils literal notranslate"><span class="pre">divide_by_initial_world_size</span></code>, which determines if gradients are
divided by the initial world size or by the effective world size (i.e. number
of non-joined ranks). Such keyword arguments can be passed directly into the
context manager.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">Join</span><span class="p">([</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">],</span> <span class="n">divide_by_initial_world_size</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The keyword arguments passed into the context manager are shared across
all participating classes. This should not be a limitation since we do
not expect cases where multiple <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> s need differing settings
of the same argument. Nonetheless, this is something to keep in mind.</p>
</div>
</section>
<section id="how-does-join-work">
<h2>How Does <code class="docutils literal notranslate"><span class="pre">Join</span></code> Work?<a class="headerlink" href="#how-does-join-work" title="Link to this heading">#</a></h2>
<p>Now that we have seen some preliminary examples of how to use the <code class="docutils literal notranslate"><span class="pre">Join</span></code>
context manager, let us delve deeper into how it works. This will provide a
greater insight into the full capability that it offers and prepare you to make
your own custom classes compatible. Here, we will go over the <code class="docutils literal notranslate"><span class="pre">Join</span></code> class as
well as the supporting classes <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> and <code class="docutils literal notranslate"><span class="pre">JoinHook</span></code>.</p>
<section id="joinable">
<h3><code class="docutils literal notranslate"><span class="pre">Joinable</span></code><a class="headerlink" href="#joinable" title="Link to this heading">#</a></h3>
<p>To begin, classes compatible with the <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager must inherit
from the abstract base class <code class="docutils literal notranslate"><span class="pre">Joinable</span></code>. In particular, a <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> must
implement:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">join_hook(self,</span> <span class="pre">**kwargs)</span> <span class="pre">-&gt;</span> <span class="pre">JoinHook</span></code></p></li>
</ul>
<p>This returns the <code class="docutils literal notranslate"><span class="pre">JoinHook</span></code> instance for the <code class="docutils literal notranslate"><span class="pre">Joinable</span></code>, determining how
joined processes should shadow the per-iteration collective communications
performed by the <code class="docutils literal notranslate"><span class="pre">Joinable</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">join_device(self)</span> <span class="pre">-&gt;</span> <span class="pre">torch.device</span></code></p></li>
</ul>
<p>This returns a device to be used by the <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager to perform
collective communications, e.g. <code class="docutils literal notranslate"><span class="pre">torch.device("cuda:0")</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.device("cpu")</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">join_process_group(self)</span> <span class="pre">-&gt;</span> <span class="pre">ProcessGroup</span></code></p></li>
</ul>
<p>This returns the process group to be used by the <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager to
perform collective communications.</p>
<p>In particular, the <code class="docutils literal notranslate"><span class="pre">join_device</span></code> and <code class="docutils literal notranslate"><span class="pre">join_process_group</span></code> are required
attributes to ensure that the context manager can schedule collective
communications between joined and non-joined processes. One usage is to count
the number of non-joined processes on each iteration using an all-reduce.
Another usage is for implementing the mechanism required for
<code class="docutils literal notranslate"><span class="pre">throw_on_early_termination=True</span></code>, which we will explain later below.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code> already inherit
from <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> and implement the above methods, which is why we could
directly use them in the previous examples.</p>
<p><code class="docutils literal notranslate"><span class="pre">Joinable</span></code> classes should make sure to call the <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> constructor
since it initializes a <code class="docutils literal notranslate"><span class="pre">JoinConfig</span></code> instance, which is used internally by
the context manager to ensure correctness. This will be saved in each
<code class="docutils literal notranslate"><span class="pre">Joinable</span></code> as a field <code class="docutils literal notranslate"><span class="pre">_join_config</span></code>.</p>
</section>
<section id="joinhook">
<h3><code class="docutils literal notranslate"><span class="pre">JoinHook</span></code><a class="headerlink" href="#joinhook" title="Link to this heading">#</a></h3>
<p>Next, let us break down the <code class="docutils literal notranslate"><span class="pre">JoinHook</span></code> class. A <code class="docutils literal notranslate"><span class="pre">JoinHook</span></code> provides two
entry points into a context manager:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">main_hook(self)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></li>
</ul>
<p>This hook is called repeatedly by each joined rank while there exists a rank
that has not yet joined. It is meant to shadow the collective communications
performed by the <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> in each training iteration (e.g. in one forward
pass, backward pass, and optimizer step).</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">post_hook(self,</span> <span class="pre">is_last_joiner:</span> <span class="pre">bool)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></li>
</ul>
<p>This hook is called once all ranks have joined. It is passed an additional
<code class="docutils literal notranslate"><span class="pre">bool</span></code> argument <code class="docutils literal notranslate"><span class="pre">is_last_joiner</span></code>, which indicates if the rank was one of
the last to join. The argument may be useful for synchronization.</p>
<p>To give concrete examples of what these hooks may look like, the provided
<code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code> main hook performs an optimizer step per normal
since the joined rank is still responsible for updating and synchronizing its
shard of the parameters, and the provided <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> post-hook
broadcasts the final updated model from one of the last joining ranks to ensure
that it is the same across all ranks.</p>
</section>
<section id="join">
<h3><code class="docutils literal notranslate"><span class="pre">Join</span></code><a class="headerlink" href="#join" title="Link to this heading">#</a></h3>
<p>Finally, let us examine how these fit into the <code class="docutils literal notranslate"><span class="pre">Join</span></code> class itself.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__(self,</span> <span class="pre">joinables:</span> <span class="pre">List[Joinable],</span> <span class="pre">enable:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">throw_on_early_termination:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False)</span></code></p></li>
</ul>
<p>As we saw in the previous examples, the constructor takes in a list of the
<code class="docutils literal notranslate"><span class="pre">Joinable</span></code> s that participate in the training loop. These should be the
classes that perform collective communications in each iteration.</p>
<p><code class="docutils literal notranslate"><span class="pre">enable</span></code> is a <code class="docutils literal notranslate"><span class="pre">bool</span></code> that can be set to <code class="docutils literal notranslate"><span class="pre">False</span></code> if you know that there
will not be uneven inputs, in which case the context manager becomes vacuous
similar to <code class="docutils literal notranslate"><span class="pre">contextlib.nullcontext()</span></code>. This also may disable join-related
computation in the participating <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> s.</p>
<p><code class="docutils literal notranslate"><span class="pre">throw_on_early_termination</span></code> is a <code class="docutils literal notranslate"><span class="pre">bool</span></code> that can be set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to
have each rank raise an exception the moment that uneven inputs are detected.
This is useful for cases that do not conform to the context managerâ€™s
requirements, which is most typically when there are collective communications
from different classes that may be arbitrarily interleaved, such as when using
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> with a model that has <code class="docutils literal notranslate"><span class="pre">SyncBatchNorm</span></code> layers. In
such cases, this argument should be set to <code class="docutils literal notranslate"><span class="pre">True</span></code> so that the application
logic can catch the exception and determine how to proceed.</p>
<ul class="simple">
<li><p>The core logic occurs in the <code class="docutils literal notranslate"><span class="pre">__exit__()</span></code> method, which loops while there
exists a non-joined rank, calling each <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> â€˜s main hook, and
then once all ranks have joined, calls their post hooks. Both the main hooks
and post-hooks are iterated over in the order that the <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> s are
passed in.</p></li>
<li><p>The context manager requires a heartbeat from non-joined processes. As such,
each <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> class should make a call to <code class="docutils literal notranslate"><span class="pre">Join.notify_join_context()</span></code>
before its per-iteration collective communications. The context manager will
ensure that only the first <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> passed in actually sends the
heartbeat.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As mentioned above regarding <code class="docutils literal notranslate"><span class="pre">throw_on_early_termination</span></code>, the
<code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager is not compatible with certain compositions of
classes. The <code class="docutils literal notranslate"><span class="pre">Joinable</span></code> â€˜s <code class="docutils literal notranslate"><span class="pre">JoinHook</span></code> s must be serializable since each
hook is fully executed before proceeding to the next. In other words, two
hooks cannot overlap. Moreover, currently, both the main hooks and post-
hooks are iterated over in the same deterministic order. If this appears to
be a major limitation, we may modify the API to permit a customizable
ordering.</p>
</div>
</section>
</section>
<section id="making-a-toy-class-work-with-join">
<h2>Making a Toy Class Work with <code class="docutils literal notranslate"><span class="pre">Join</span></code><a class="headerlink" href="#making-a-toy-class-work-with-join" title="Link to this heading">#</a></h2>
<p>Since the previous section introduced several concepts, let us see them in
practice with a toy example. Here, we will implement a class that counts the
number of inputs that are seen across all ranks before its rank joins. This
should provide a basic idea of how you may make your own class compatible
with the <code class="docutils literal notranslate"><span class="pre">Join</span></code> context manager.</p>
<p>Specifically, the following code has each rank print out (1) the number of
inputs across all ranks that seen before it joins and (2) the total number
of inputs across all ranks.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.algorithms.join</span><span class="w"> </span><span class="kn">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">Joinable</span><span class="p">,</span> <span class="n">JoinHook</span>

<span class="n">BACKEND</span> <span class="o">=</span> <span class="s2">"nccl"</span>
<span class="n">WORLD_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">NUM_INPUTS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CounterJoinHook</span><span class="p">(</span><span class="n">JoinHook</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Join hook for :class:`Counter`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        counter (Counter): the :class:`Counter` object using this hook.</span>
<span class="sd">        sync_max_count (bool): whether to sync the max count once all ranks</span>
<span class="sd">            join.</span>
<span class="sd">    """</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">counter</span><span class="p">,</span>
        <span class="n">sync_max_count</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="n">counter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_max_count</span> <span class="o">=</span> <span class="n">sync_max_count</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">main_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Shadows the counter's all-reduce by all-reducing a dim-1 zero tensor.</span>
<span class="sd">        """</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_last_joiner</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Synchronizes the max count across all :class:`Counter` s if</span>
<span class="sd">        ``sync_max_count=True``.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">sync_max_count</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="n">common_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">find_common_rank</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">is_last_joiner</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">common_rank</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">max_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">count</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">max_count</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">common_rank</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Counter</span><span class="p">(</span><span class="n">Joinable</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Example :class:`Joinable` that counts the number of training iterations</span>
<span class="sd">    that it participates in.</span>
<span class="sd">    """</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">process_group</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Counter</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Counts the number of inputs processed on this iteration by all ranks</span>
<span class="sd">        by all-reducing a dim-1 one tensor; increments its own internal count.</span>
<span class="sd">        """</span>
        <span class="n">Join</span><span class="o">.</span><span class="n">notify_join_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">t</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">join_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">JoinHook</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Return a join hook that shadows the all-reduce in :meth:`__call__`.</span>

<span class="sd">        This join hook supports the following keyword arguments:</span>
<span class="sd">            sync_max_count (bool, optional): whether to synchronize the maximum</span>
<span class="sd">                count across all ranks once all ranks join; default is ``False``.</span>
<span class="sd">        """</span>
        <span class="n">sync_max_count</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"sync_max_count"</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">CounterJoinHook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sync_max_count</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">join_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">join_process_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">find_common_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">to_consider</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Returns the max rank of the ones to consider over the process group.</span>
<span class="sd">        """</span>
        <span class="n">common_rank</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">rank</span> <span class="k">if</span> <span class="n">to_consider</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">common_rank</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="n">common_rank</span> <span class="o">=</span> <span class="n">common_rank</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">common_rank</span>

<span class="k">def</span><span class="w"> </span><span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">WORLD_SIZE</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'localhost'</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'29500'</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">BACKEND</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">)</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">"</span><span class="p">),</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_INPUTS</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)]</span>

    <span class="k">with</span> <span class="n">Join</span><span class="p">([</span><span class="n">counter</span><span class="p">],</span> <span class="n">sync_max_count</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">counter</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">count</span><span class="o">.</span><span class="n">item</span><span class="p">())</span><span class="si">}</span><span class="s2"> inputs processed before rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> joined!"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">max_count</span><span class="o">.</span><span class="n">item</span><span class="p">())</span><span class="si">}</span><span class="s2"> inputs processed across all ranks!"</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>Since rank 0 sees 5 inputs and rank 1 sees 6, this yields the output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>10 inputs processed before rank 0 joined!
11 inputs processed across all ranks!
11 inputs processed before rank 1 joined!
11 inputs processed across all ranks!
</pre></div>
</div>
<p>Some key points to highlight:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">Counter</span></code> instance performs a single all-reduce per iteration, so the
main hook performs a single all-reduce as well to shadow it.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Counter</span></code> class makes a call to <code class="docutils literal notranslate"><span class="pre">Join.notify_join_context()</span></code> at the
beginning of its <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> method since that is a place before its per-
iteration collective communications (i.e. its all-reduce).</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">is_last_joiner</span></code> argument is used to determine the broadcast source in
the post-hooks.</p></li>
<li><p>We pass in the <code class="docutils literal notranslate"><span class="pre">sync_max_count</span></code> keyword argument to the context manager,
which is then forwarded to <code class="docutils literal notranslate"><span class="pre">Counter</span></code> â€˜s join hook.</p></li>
</ul>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="rpc_ddp_tutorial.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Combining Distributed DataParallel with Distributed RPC Framework</p>
</div>
</a>
<a class="right-next" href="../deep-dive.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Deep Dive</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
    
      
        Â© Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="rpc_ddp_tutorial.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Combining Distributed DataParallel with Distributed RPC Framework</p>
</div>
</a>
<a class="right-next" href="../deep-dive.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Deep Dive</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements">Requirements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-join">What is <code class="docutils literal notranslate"><span class="pre">Join</span></code>?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-join-with-distributeddataparallel">Using <code class="docutils literal notranslate"><span class="pre">Join</span></code> with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-join-with-distributeddataparallel-and-zeroredundancyoptimizer">Using <code class="docutils literal notranslate"><span class="pre">Join</span></code> with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">ZeroRedundancyOptimizer</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#passing-keyword-arguments">Passing Keyword Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-join-work">How Does <code class="docutils literal notranslate"><span class="pre">Join</span></code> Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joinable"><code class="docutils literal notranslate"><span class="pre">Joinable</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joinhook"><code class="docutils literal notranslate"><span class="pre">JoinHook</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#join"><code class="docutils literal notranslate"><span class="pre">Join</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-toy-class-work-with-join">Making a Toy Class Work with <code class="docutils literal notranslate"><span class="pre">Join</span></code></a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            Â© PyTorch. Copyright Â© The Linux FoundationÂ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "name": "Distributed Training with Uneven Inputs Using the Join Context Manager",
         "headline": "Distributed Training with Uneven Inputs Using the Join Context Manager",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/advanced/generic_join.html",
         "articleBody": "Distributed Training with Uneven Inputs Using the Join Context Manager# Author: Andrew Gu Note View and edit this tutorial in github. Note Join is introduced in PyTorch 1.10 as a prototype feature. This API is subject to change. In this tutorial, you will see: An overview of the Join context manager. An example of how to use the context manager with DistributedDataParallel. An example of how to use the context manager with both DistributedDataParallel and ZeroRedundancyOptimizer. An example of passing in keyword arguments to the context manager. A dive into how the Join context manager works. An example showing how to make a toy class compatible with the context manager. Requirements# PyTorch 1.10+ Getting Started with Distributed Data Parallel Shard Optimizer States with ZeroRedundancyOptimizer What is Join?# In Getting Started with Distributed Data Parallel - Basic Use Case, you saw the general skeleton for using DistributedDataParallel to perform data parallel training. This implicitly schedules all-reduces in each backward pass to synchronize gradients across ranks. Such collective communications require participation from all ranks in the process group, so if a rank has fewer inputs, then the other ranks will hang or error (depending on the backend). More generally, this problem persists for any class that performs per-iteration synchronous collective communications. Join is a context manager to be used around your per-rank training loop to facilitate training with uneven inputs. The context manager allows the ranks that exhaust their inputs early (i.e. join early) to shadow the collective communications performed by those that have not yet joined. The ways in which the communications are shadowed are specified by hooks. Using Join with DistributedDataParallel# PyTorch\u2019s DistributedDataParallel works out-of-the-box with the Join context manager. Here is an example usage: import os import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.distributed.algorithms.join import Join from torch.nn.parallel import DistributedDataParallel as DDP BACKEND = \"nccl\" WORLD_SIZE = 2 NUM_INPUTS = 5 def worker(rank): os.environ[\u0027MASTER_ADDR\u0027] = \u0027localhost\u0027 os.environ[\u0027MASTER_PORT\u0027] = \u002729500\u0027 dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE) model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank]) # Rank 1 gets one more input than rank 0 inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)] num_inputs = 0 with Join([model]): for input in inputs: num_inputs += 1 loss = model(input).sum() loss.backward() print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\") def main(): mp.spawn(worker, nprocs=WORLD_SIZE, join=True) if __name__ == \"__main__\": main() This produces the following output (where the print() s from rank 0 and rank 1 may be arbitrarily ordered): Rank 0 has exhausted all 5 of its inputs! Rank 1 has exhausted all 6 of its inputs! Note DistributedDataParallel provided its own join() context manager prior to the introduction of this generic Join context manager. In the above example, using with Join([model]): is equivalent to using with model.join():. One limitation of the existing DistributedDataParallel.join() is that it does not allow multiple participating classes, e.g. DistributedDataParallel and ZeroRedundancyOptimizer together. Using Join with DistributedDataParallel and ZeroRedundancyOptimizer# The Join context manager works not only with a single class but also with multiple classes together. PyTorch\u2019s ZeroRedundancyOptimizer is also compatible with the context manager, so here, we examine how to modify the previous example to use both DistributedDataParallel and ZeroRedundancyOptimizer: from torch.distributed.optim import ZeroRedundancyOptimizer as ZeRO from torch.optim import Adam def worker(rank): os.environ[\u0027MASTER_ADDR\u0027] = \u0027localhost\u0027 os.environ[\u0027MASTER_PORT\u0027] = \u002729500\u0027 dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE) model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank]) optim = ZeRO(model.parameters(), Adam, lr=0.01) # Rank 1 gets one more input than rank 0 inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)] num_inputs = 0 # Pass both `model` and `optim` into `Join()` with Join([model, optim]): for input in inputs: num_inputs += 1 loss = model(input).sum() loss.backward() optim.step() print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\") This will yield the same output as before. The notable change was additionally passing in the ZeroRedundancyOptimizer instance into Join(). Passing Keyword Arguments# Classes may provide keyword arguments that modify their behavior in the context manager at run time. For example, DistributedDataParallel provides an argument divide_by_initial_world_size, which determines if gradients are divided by the initial world size or by the effective world size (i.e. number of non-joined ranks). Such keyword arguments can be passed directly into the context manager. with Join([model, optim], divide_by_initial_world_size=False): for input in inputs: ... Warning The keyword arguments passed into the context manager are shared across all participating classes. This should not be a limitation since we do not expect cases where multiple Joinable s need differing settings of the same argument. Nonetheless, this is something to keep in mind. How Does Join Work?# Now that we have seen some preliminary examples of how to use the Join context manager, let us delve deeper into how it works. This will provide a greater insight into the full capability that it offers and prepare you to make your own custom classes compatible. Here, we will go over the Join class as well as the supporting classes Joinable and JoinHook. Joinable# To begin, classes compatible with the Join context manager must inherit from the abstract base class Joinable. In particular, a Joinable must implement: join_hook(self, **kwargs) -\u003e JoinHook This returns the JoinHook instance for the Joinable, determining how joined processes should shadow the per-iteration collective communications performed by the Joinable. join_device(self) -\u003e torch.device This returns a device to be used by the Join context manager to perform collective communications, e.g. torch.device(\"cuda:0\") or torch.device(\"cpu\"). join_process_group(self) -\u003e ProcessGroup This returns the process group to be used by the Join context manager to perform collective communications. In particular, the join_device and join_process_group are required attributes to ensure that the context manager can schedule collective communications between joined and non-joined processes. One usage is to count the number of non-joined processes on each iteration using an all-reduce. Another usage is for implementing the mechanism required for throw_on_early_termination=True, which we will explain later below. DistributedDataParallel and ZeroRedundancyOptimizer already inherit from Joinable and implement the above methods, which is why we could directly use them in the previous examples. Joinable classes should make sure to call the Joinable constructor since it initializes a JoinConfig instance, which is used internally by the context manager to ensure correctness. This will be saved in each Joinable as a field _join_config. JoinHook# Next, let us break down the JoinHook class. A JoinHook provides two entry points into a context manager: main_hook(self) -\u003e None This hook is called repeatedly by each joined rank while there exists a rank that has not yet joined. It is meant to shadow the collective communications performed by the Joinable in each training iteration (e.g. in one forward pass, backward pass, and optimizer step). post_hook(self, is_last_joiner: bool) -\u003e None This hook is called once all ranks have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank was one of the last to join. The argument may be useful for synchronization. To give concrete examples of what these hooks may look like, the provided ZeroRedundancyOptimizer main hook performs an optimizer step per normal since the joined rank is still responsible for updating and synchronizing its shard of the parameters, and the provided DistributedDataParallel post-hook broadcasts the final updated model from one of the last joining ranks to ensure that it is the same across all ranks. Join# Finally, let us examine how these fit into the Join class itself. __init__(self, joinables: List[Joinable], enable: bool = True, throw_on_early_termination: bool = False) As we saw in the previous examples, the constructor takes in a list of the Joinable s that participate in the training loop. These should be the classes that perform collective communications in each iteration. enable is a bool that can be set to False if you know that there will not be uneven inputs, in which case the context manager becomes vacuous similar to contextlib.nullcontext(). This also may disable join-related computation in the participating Joinable s. throw_on_early_termination is a bool that can be set to True to have each rank raise an exception the moment that uneven inputs are detected. This is useful for cases that do not conform to the context manager\u2019s requirements, which is most typically when there are collective communications from different classes that may be arbitrarily interleaved, such as when using DistributedDataParallel with a model that has SyncBatchNorm layers. In such cases, this argument should be set to True so that the application logic can catch the exception and determine how to proceed. The core logic occurs in the __exit__() method, which loops while there exists a non-joined rank, calling each Joinable \u2018s main hook, and then once all ranks have joined, calls their post hooks. Both the main hooks and post-hooks are iterated over in the order that the Joinable s are passed in. The context manager requires a heartbeat from non-joined processes. As such, each Joinable class should make a call to Join.notify_join_context() before its per-iteration collective communications. The context manager will ensure that only the first Joinable passed in actually sends the heartbeat. Warning As mentioned above regarding throw_on_early_termination, the Join context manager is not compatible with certain compositions of classes. The Joinable \u2018s JoinHook s must be serializable since each hook is fully executed before proceeding to the next. In other words, two hooks cannot overlap. Moreover, currently, both the main hooks and post- hooks are iterated over in the same deterministic order. If this appears to be a major limitation, we may modify the API to permit a customizable ordering. Making a Toy Class Work with Join# Since the previous section introduced several concepts, let us see them in practice with a toy example. Here, we will implement a class that counts the number of inputs that are seen across all ranks before its rank joins. This should provide a basic idea of how you may make your own class compatible with the Join context manager. Specifically, the following code has each rank print out (1) the number of inputs across all ranks that seen before it joins and (2) the total number of inputs across all ranks. import os import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.distributed.algorithms.join import Join, Joinable, JoinHook BACKEND = \"nccl\" WORLD_SIZE = 2 NUM_INPUTS = 5 class CounterJoinHook(JoinHook): r\"\"\" Join hook for :class:`Counter`. Arguments: counter (Counter): the :class:`Counter` object using this hook. sync_max_count (bool): whether to sync the max count once all ranks join. \"\"\" def __init__( self, counter, sync_max_count ): self.counter = counter self.sync_max_count = sync_max_count def main_hook(self): r\"\"\" Shadows the counter\u0027s all-reduce by all-reducing a dim-1 zero tensor. \"\"\" t = torch.zeros(1, device=self.counter.device) dist.all_reduce(t) def post_hook(self, is_last_joiner: bool): r\"\"\" Synchronizes the max count across all :class:`Counter` s if ``sync_max_count=True``. \"\"\" if not self.sync_max_count: return rank = dist.get_rank(self.counter.process_group) common_rank = self.counter.find_common_rank(rank, is_last_joiner) if rank == common_rank: self.counter.max_count = self.counter.count.detach().clone() dist.broadcast(self.counter.max_count, src=common_rank) class Counter(Joinable): r\"\"\" Example :class:`Joinable` that counts the number of training iterations that it participates in. \"\"\" def __init__(self, device, process_group): super(Counter, self).__init__() self.device = device self.process_group = process_group self.count = torch.tensor([0], device=device).float() self.max_count = torch.tensor([0], device=device).float() def __call__(self): r\"\"\" Counts the number of inputs processed on this iteration by all ranks by all-reducing a dim-1 one tensor; increments its own internal count. \"\"\" Join.notify_join_context(self) t = torch.ones(1, device=self.device).float() dist.all_reduce(t) self.count += t def join_hook(self, **kwargs) -\u003e JoinHook: r\"\"\" Return a join hook that shadows the all-reduce in :meth:`__call__`. This join hook supports the following keyword arguments: sync_max_count (bool, optional): whether to synchronize the maximum count across all ranks once all ranks join; default is ``False``. \"\"\" sync_max_count = kwargs.get(\"sync_max_count\", False) return CounterJoinHook(self, sync_max_count) @property def join_device(self) -\u003e torch.device: return self.device @property def join_process_group(self): return self.process_group def find_common_rank(self, rank, to_consider): r\"\"\" Returns the max rank of the ones to consider over the process group. \"\"\" common_rank = torch.tensor([rank if to_consider else -1], device=self.device) dist.all_reduce(common_rank, op=dist.ReduceOp.MAX, group=self.process_group) common_rank = common_rank.item() return common_rank def worker(rank): assert torch.cuda.device_count() \u003e= WORLD_SIZE os.environ[\u0027MASTER_ADDR\u0027] = \u0027localhost\u0027 os.environ[\u0027MASTER_PORT\u0027] = \u002729500\u0027 dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE) counter = Counter(torch.device(f\"cuda:{rank}\"), dist.group.WORLD) inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)] with Join([counter], sync_max_count=True): for _ in inputs: counter() print(f\"{int(counter.count.item())} inputs processed before rank {rank} joined!\") print(f\"{int(counter.max_count.item())} inputs processed across all ranks!\") def main(): mp.spawn(worker, nprocs=WORLD_SIZE, join=True) if __name__ == \"__main__\": main() Since rank 0 sees 5 inputs and rank 1 sees 6, this yields the output: 10 inputs processed before rank 0 joined! 11 inputs processed across all ranks! 11 inputs processed before rank 1 joined! 11 inputs processed across all ranks! Some key points to highlight: A Counter instance performs a single all-reduce per iteration, so the main hook performs a single all-reduce as well to shadow it. The Counter class makes a call to Join.notify_join_context() at the beginning of its __call__() method since that is a place before its per- iteration collective communications (i.e. its all-reduce). The is_last_joiner argument is used to determine the broadcast source in the post-hooks. We pass in the sync_max_count keyword argument to the context manager, which is then forwarded to Counter \u2018s join hook.",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/advanced/generic_join.html"
         },
         "datePublished": "2023-01-01T00:00:00Z",
         "dateModified": "2023-01-01T00:00:00Z"
       }
   </script>
</body>
</body></html>