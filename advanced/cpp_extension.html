
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Custom C++ and CUDA Extensions — PyTorch Tutorials 2.7.0+cu126 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=07b0cd76"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'advanced/cpp_extension';</script>
<link href="https://pytorch.org/tutorials/advanced/cpp_extension.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="dispatcher.html" rel="next" title="Registering a Dispatched Operator in C++"/>
<link href="../intermediate/custom_function_conv_bn_tutorial.html" rel="prev" title="Fusing Convolution and Batch Norm using Custom Function"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/advanced/cpp_extension.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects.
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.7.0+cu126');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.7.0+cu126</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="custom_ops_landing_page.html">PyTorch Custom Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_custom_ops.html">Custom Python Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_custom_ops.html">Custom C++ and CUDA Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../extension.html">Extension</a></li>
<li aria-current="page" class="breadcrumb-item active">Custom C++...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../extension.html" itemprop="item"/>
<meta content="Extension" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Custom C++ and CUDA Extensions" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">advanced/cpp_extension</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="custom-c-and-cuda-extensions">
<h1>Custom C++ and CUDA Extensions<a class="headerlink" href="#custom-c-and-cuda-extensions" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Apr 26, 2018 | Last Updated: Mar 19, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author</strong>: <a class="reference external" href="https://www.goldsborough.me/">Peter Goldsborough</a></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This tutorial is deprecated as of PyTorch 2.4. Please see <a class="reference internal" href="custom_ops_landing_page.html#custom-ops-landing-page"><span class="std std-ref">PyTorch Custom Operators</span></a>
for the newest up-to-date guides on extending PyTorch with Custom C++/CUDA Extensions.</p>
</div>
<p>PyTorch provides a plethora of operations related to neural networks, arbitrary
tensor algebra, data wrangling and other purposes. However, you may still find
yourself in need of a more customized operation. For example, you might want to
use a novel activation function you found in a paper, or implement an operation
you developed as part of your research.</p>
<p>The easiest way of integrating such a custom operation in PyTorch is to write it
in Python by extending <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> as outlined <a class="reference external" href="https://pytorch.org/docs/master/notes/extending.html">here</a>. This gives you the full
power of automatic differentiation (spares you from writing derivative
functions) as well as the usual expressiveness of Python. However, there may be
times when your operation is better implemented in C++. For example, your code
may need to be <em>really</em> fast because it is called very frequently in your model
or is very expensive even for few calls. Another plausible reason is that it
depends on or interacts with other C or C++ libraries. To address such cases,
PyTorch provides a very easy way of writing custom <em>C++ extensions</em>.</p>
<p>C++ extensions are a mechanism we have developed to allow users (you) to create
PyTorch operators defined <em>out-of-source</em>, i.e. separate from the PyTorch
backend. This approach is <em>different</em> from the way native PyTorch operations are
implemented. C++ extensions are intended to spare you much of the boilerplate
associated with integrating an operation with PyTorch’s backend while providing
you with a high degree of flexibility for your PyTorch-based projects.
Nevertheless, once you have defined your operation as a C++ extension, turning
it into a native PyTorch function is largely a matter of code organization,
which you can tackle after the fact if you decide to contribute your operation
upstream.</p>
<section id="motivation-and-example">
<h2>Motivation and Example<a class="headerlink" href="#motivation-and-example" title="Link to this heading">#</a></h2>
<p>The rest of this note will walk through a practical example of writing and using
a C++ (and CUDA) extension. If you are being chased or someone will fire you if
you don’t get that op done by the end of the day, you can skip this section and
head straight to the implementation details in the next section.</p>
<p>Let’s say you’ve come up with a new kind of recurrent unit that you found to
have superior properties compared to the state of the art. This recurrent unit
is similar to an LSTM, but differs in that it lacks a <em>forget gate</em> and uses an
<em>Exponential Linear Unit</em> (ELU) as its internal activation function. Because
this unit never forgets, we’ll call it <em>LLTM</em>, or <em>Long-Long-Term-Memory</em> unit.</p>
<p>The two ways in which LLTMs differ from vanilla LSTMs are significant enough
that we can’t configure PyTorch’s <code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code> for our purposes, so we’ll have to
create a custom cell. The first and easiest approach for this – and likely in
all cases a good first step – is to implement our desired functionality in
plain PyTorch with Python. For this, we need to subclass
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> and implement the forward pass of the LLTM. This would
look something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LLTM</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">state_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LLTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="c1"># 3 * state_size for input gate, output gate and candidate cell gate.</span>
        <span class="c1"># input_features + state_size because we will multiply with [input, h].</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">input_features</span> <span class="o">+</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="o">+</span><span class="n">stdv</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">old_h</span><span class="p">,</span> <span class="n">old_cell</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">old_h</span><span class="p">,</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute the input, output and candidate cell gates with one MM.</span>
        <span class="n">gate_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># Split the combined gate weight matrix into its components.</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="n">gate_weights</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">input_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># Here we use an ELU instead of the usual tanh.</span>
        <span class="n">candidate_cell</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

        <span class="c1"># Compute the new cell state.</span>
        <span class="n">new_cell</span> <span class="o">=</span> <span class="n">old_cell</span> <span class="o">+</span> <span class="n">candidate_cell</span> <span class="o">*</span> <span class="n">input_gate</span>
        <span class="c1"># Compute the new hidden state and output.</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_gate</span>

        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell</span>
</pre></div>
</div>
<p>which we could then use as expected:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_features</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">LLTM</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>

<span class="n">new_h</span><span class="p">,</span> <span class="n">new_C</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
<p>Naturally, if at all possible and plausible, you should use this approach to
extend PyTorch. Since PyTorch has highly optimized implementations of its
operations for CPU <em>and</em> GPU, powered by libraries such as <a class="reference external" href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a>, <a class="reference external" href="https://software.intel.com/en-us/mkl">Intel MKL</a> or <a class="reference external" href="https://github.com/Maratyszcza/NNPACK">NNPACK</a>, PyTorch code like above will often be
fast enough. However, we can also see why, under certain circumstances, there is
room for further performance improvements. The most obvious reason is that
PyTorch has no knowledge of the <em>algorithm</em> you are implementing. It knows only
of the individual operations you use to compose your algorithm. As such, PyTorch
must execute your operations individually, one after the other. Since each
individual call to the implementation (or <em>kernel</em>) of an operation, which may
involve the launch of a CUDA kernel, has a certain amount of overhead, this
overhead may become significant across many function calls. Furthermore, the
Python interpreter that is running our code can itself slow down our program.</p>
<p>A definite method of speeding things up is therefore to rewrite parts in C++ (or
CUDA) and <em>fuse</em> particular groups of operations. Fusing means combining the
implementations of many functions into a single function, which profits from
fewer kernel launches as well as other optimizations we can perform with
increased visibility of the global flow of data.</p>
<p>Let’s see how we can use C++ extensions to implement a <em>fused</em> version of the
LLTM. We’ll begin by writing it in plain C++, using the <a class="reference external" href="https://github.com/zdevito/ATen">ATen</a> library that powers much of PyTorch’s
backend, and see how easily it lets us translate our Python code. We’ll then
speed things up even more by moving parts of the model to CUDA kernel to benefit
from the massive parallelism GPUs provide.</p>
</section>
<section id="writing-a-c-extension">
<h2>Writing a C++ Extension<a class="headerlink" href="#writing-a-c-extension" title="Link to this heading">#</a></h2>
<p>C++ extensions come in two flavors: They can be built “ahead of time” with
<code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code>, or “just in time” via
<a class="reference external" href="https://docs.pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.utils.cpp_extension.load()</span></code></a>. We’ll begin with the first approach and
discuss the latter later.</p>
<section id="building-with-setuptools">
<h3>Building with <code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code><a class="headerlink" href="#building-with-setuptools" title="Link to this heading">#</a></h3>
<p>For the “ahead of time” flavor, we build our C++ extension by writing a
<code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script that uses setuptools to compile our C++ code. For the LLTM, it
looks as simple as this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">Extension</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'lltm_cpp'</span><span class="p">,</span>
      <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span><span class="n">cpp_extension</span><span class="o">.</span><span class="n">CppExtension</span><span class="p">(</span><span class="s1">'lltm_cpp'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'lltm.cpp'</span><span class="p">])],</span>
      <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span><span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">cpp_extension</span><span class="o">.</span><span class="n">BuildExtension</span><span class="p">})</span>
</pre></div>
</div>
<p>In this code, <code class="xref py py-class docutils literal notranslate"><span class="pre">CppExtension</span></code> is a convenience wrapper around
<code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> that passes the correct include paths and sets
the language of the extension to C++. The equivalent vanilla <code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code>
code would simply be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Extension</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="s1">'lltm_cpp'</span><span class="p">,</span>
   <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s1">'lltm.cpp'</span><span class="p">],</span>
   <span class="n">include_dirs</span><span class="o">=</span><span class="n">cpp_extension</span><span class="o">.</span><span class="n">include_paths</span><span class="p">(),</span>
   <span class="n">language</span><span class="o">=</span><span class="s1">'c++'</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">BuildExtension</span></code> performs a number of required configuration steps and
checks and also manages mixed compilation in the case of mixed C++/CUDA
extensions. And that’s all we really need to know about building C++ extensions
for now! Let’s now take a look at the implementation of our C++ extension,
which goes into <code class="docutils literal notranslate"><span class="pre">lltm.cpp</span></code>.</p>
</section>
<section id="writing-the-c-op">
<h3>Writing the C++ Op<a class="headerlink" href="#writing-the-c-op" title="Link to this heading">#</a></h3>
<p>Let’s start implementing the LLTM in C++! One function we’ll need for the
backward pass is the derivative of the sigmoid. This is a small enough piece of
code to discuss the overall environment that is available to us when writing C++
extensions:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">d_sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">s</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;torch/extension.h&gt;</span></code> is the one-stop header to include all the necessary PyTorch
bits to write C++ extensions. It includes:</p>
<ul class="simple">
<li><p>The ATen library, which is our primary API for tensor computation,</p></li>
<li><p><a class="reference external" href="https://github.com/pybind/pybind11">pybind11</a>, which is how we create Python bindings for our C++ code,</p></li>
<li><p>Headers that manage the details of interaction between ATen and pybind11.</p></li>
</ul>
<p>The implementation of <code class="xref py py-func docutils literal notranslate"><span class="pre">d_sigmoid()</span></code> shows how to use the ATen API.
PyTorch’s tensor and variable interface is generated automatically from the
ATen library, so we can more or less translate our Python implementation 1:1
into C++. Our primary datatype for all computations will be
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch::Tensor</span></code>. Its full API can be inspected <a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html">here</a>. Notice
also that we can include <code class="docutils literal notranslate"><span class="pre">&lt;iostream&gt;</span></code> or <em>any other C or C++ header</em> – we have
the full power of C++11 at our disposal.</p>
<p>Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.
To workaround the issue, move python binding logic to pure C++ file.
Example use:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/ATen.h&gt;</span>
<span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">SigmoidAlphaBlendForwardCuda</span><span class="p">(....)</span>
</pre></div>
</div>
<p>Instead of:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">SigmoidAlphaBlendForwardCuda</span><span class="p">(...)</span>
</pre></div>
</div>
<p>Currently open issue for nvcc bug <a class="reference external" href="https://github.com/pytorch/pytorch/issues/69460">here</a>.
Complete workaround code example <a class="reference external" href="https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48">here</a>.</p>
<section id="forward-pass">
<h4>Forward Pass<a class="headerlink" href="#forward-pass" title="Link to this heading">#</a></h4>
<p>Next we can port our entire forward pass to C++:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">cat</span><span class="p">({</span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">},</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gate_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">));</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="w"> </span><span class="cm">/*alpha=*/</span><span class="mf">1.0</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">input_gate</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">output_gate</span><span class="p">;</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">new_h</span><span class="p">,</span>
<span class="w">          </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">          </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">          </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">          </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">          </span><span class="n">X</span><span class="p">,</span>
<span class="w">          </span><span class="n">gate_weights</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="backward-pass">
<h4>Backward Pass<a class="headerlink" href="#backward-pass" title="Link to this heading">#</a></h4>
<p>The C++ extension API currently does not provide a way of automatically
generating a backwards function for us. As such, we have to also implement the
backward pass of our LLTM, which computes the derivative of the loss with
respect to each input of the forward pass. Ultimately, we will plop both the
forward and backward function into a <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> to create
a nice Python binding. The backward function is slightly more involved, so
we’ll not dig deeper into the code (if you are interested, <a class="reference external" href="https://www.cs.toronto.edu/~graves/phd.pdf">Alex Graves’ thesis</a> is a good read for more
information on this):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// tanh'(z) = 1 - tanh^2(z)</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">d_tanh</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z</span><span class="p">.</span><span class="n">tanh</span><span class="p">().</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">d_elu</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="p">.</span><span class="n">exp</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">mask</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">z</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">).</span><span class="n">type_as</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">e</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">;</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">  </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">  </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">d_elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gates</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="n">torch</span><span class="o">::</span><span class="n">cat</span><span class="p">({</span><span class="n">d_input_gate</span><span class="p">,</span><span class="w"> </span><span class="n">d_output_gate</span><span class="p">,</span><span class="w"> </span><span class="n">d_candidate_cell</span><span class="p">},</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">t</span><span class="p">().</span><span class="n">mm</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="cm">/*keepdim=*/</span><span class="nb">true</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_h</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">d_old_h</span><span class="p">,</span><span class="w"> </span><span class="n">d_input</span><span class="p">,</span><span class="w"> </span><span class="n">d_weights</span><span class="p">,</span><span class="w"> </span><span class="n">d_bias</span><span class="p">,</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="binding-to-python">
<h3>Binding to Python<a class="headerlink" href="#binding-to-python" title="Link to this heading">#</a></h3>
<p>Once you have your operation written in C++ and ATen, you can use pybind11 to
bind your C++ functions or classes into Python in a very simple manner.
Questions or issues you have about this part of PyTorch C++ extensions will
largely be addressed by <a class="reference external" href="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</a>.</p>
<p>For our extensions, the necessary binding code spans only four lines:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"forward"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_forward</span><span class="p">,</span><span class="w"> </span><span class="s">"LLTM forward"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"backward"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_backward</span><span class="p">,</span><span class="w"> </span><span class="s">"LLTM backward"</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>One bit to note here is the macro <code class="docutils literal notranslate"><span class="pre">TORCH_EXTENSION_NAME</span></code>. The torch extension
build will define it as the name you give your extension in the <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>
script. In this case, the value of <code class="docutils literal notranslate"><span class="pre">TORCH_EXTENSION_NAME</span></code> would be “lltm_cpp”.
This is to avoid having to maintain the name of the extension in two places
(the build script and your C++ code), as a mismatch between the two can lead to
nasty and hard to track issues.</p>
</section>
<section id="using-your-extension">
<h3>Using Your Extension<a class="headerlink" href="#using-your-extension" title="Link to this heading">#</a></h3>
<p>We are now set to import our extension in PyTorch. At this point, your directory
structure could look something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pytorch</span><span class="o">/</span>
  <span class="n">lltm</span><span class="o">-</span><span class="n">extension</span><span class="o">/</span>
    <span class="n">lltm</span><span class="o">.</span><span class="n">cpp</span>
    <span class="n">setup</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Now, run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> to build and install your extension. This
should look something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>running install
running bdist_egg
running egg_info
creating lltm_cpp.egg-info
writing lltm_cpp.egg-info/PKG-INFO
writing dependency_links to lltm_cpp.egg-info/dependency_links.txt
writing top-level names to lltm_cpp.egg-info/top_level.txt
writing manifest file 'lltm_cpp.egg-info/SOURCES.txt'
reading manifest file 'lltm_cpp.egg-info/SOURCES.txt'
writing manifest file 'lltm_cpp.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_ext
building 'lltm_cpp' extension
creating build
creating build/temp.linux-x86_64-3.7
gcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
creating build/lib.linux-x86_64-3.7
g++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/egg
copying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg
creating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so
byte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc
creating build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm_cpp.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm_cpp.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm_cpp.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
copying lltm_cpp.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt
zip_safe flag not set; analyzing archive contents...
__pycache__.lltm_cpp.cpython-37: module references __file__
creating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
removing '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it)
creating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
Extracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages
lltm-cpp 0.0.0 is already the active version in easy-install.pth

Installed ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
Processing dependencies for lltm-cpp==0.0.0
Finished processing dependencies for lltm-cpp==0.0.0
</pre></div>
</div>
<p>A small note on compilers: Due to ABI versioning issues, the compiler you use to
build your C++ extension must be <em>ABI-compatible</em> with the compiler PyTorch was
built with. In practice, this means that you must use GCC version 4.9 and above on Linux.
For Ubuntu 16.04 and other more-recent Linux distributions, this should be the
default compiler already. On MacOS, you must use clang (which does not have any ABI versioning issues). In the worst
case, you can build PyTorch from source with your compiler and then build the
extension with that same compiler.</p>
<p>Once your extension is built, you can simply import it in Python, using the
name you specified in your <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script. Just be sure to <code class="docutils literal notranslate"><span class="pre">import</span>
<span class="pre">torch</span></code> first, as this will resolve some symbols that the dynamic linker must
see:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="kn">import</span><span class="w"> </span><span class="nn">lltm_cpp</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">lltm_cpp</span><span class="o">.</span><span class="n">forward</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="o">&lt;</span><span class="n">function</span> <span class="n">lltm</span><span class="o">.</span><span class="n">PyCapsule</span><span class="o">.</span><span class="n">forward</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>If we call <code class="docutils literal notranslate"><span class="pre">help()</span></code> on the function or module, we can see that its signature
matches our C++ code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="n">help</span><span class="p">(</span><span class="n">lltm_cpp</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
<span class="n">forward</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="n">method</span> <span class="n">of</span> <span class="n">builtins</span><span class="o">.</span><span class="n">PyCapsule</span> <span class="n">instance</span>
    <span class="n">forward</span><span class="p">(</span><span class="n">arg0</span><span class="p">:</span> <span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">arg1</span><span class="p">:</span> <span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">arg2</span><span class="p">:</span> <span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">arg3</span><span class="p">:</span> <span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">arg4</span><span class="p">:</span> <span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="p">::</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="n">LLTM</span> <span class="n">forward</span>
</pre></div>
</div>
<p>Since we are now able to call our C++ functions from Python, we can wrap them
with <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> and <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> to make them first
class citizens of PyTorch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Our module!</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">lltm_cpp</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LLTMFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">old_h</span><span class="p">,</span> <span class="n">old_cell</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">lltm_cpp</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">old_h</span><span class="p">,</span> <span class="n">old_cell</span><span class="p">)</span>
        <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">variables</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">weights</span><span class="p">]</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">variables</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_h</span><span class="p">,</span> <span class="n">grad_cell</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">lltm_cpp</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">grad_h</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">grad_cell</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="o">*</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span>
        <span class="n">d_old_h</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_weights</span><span class="p">,</span> <span class="n">d_bias</span><span class="p">,</span> <span class="n">d_old_cell</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="k">return</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_weights</span><span class="p">,</span> <span class="n">d_bias</span><span class="p">,</span> <span class="n">d_old_h</span><span class="p">,</span> <span class="n">d_old_cell</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LLTM</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">state_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LLTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">input_features</span> <span class="o">+</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="o">+</span><span class="n">stdv</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">LLTMFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">*</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
<section id="performance-comparison">
<h4>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">#</a></h4>
<p>Now that we are able to use and call our C++ code from PyTorch, we can run a
small benchmark to see how much performance we gained from rewriting our op in
C++. We’ll run the LLTM forwards and backwards a few times and measure the
duration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">input_features</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_features</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">LLTM</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>

<span class="n">forward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">backward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">new_h</span><span class="p">,</span> <span class="n">new_C</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
    <span class="n">forward</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="p">(</span><span class="n">new_h</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">new_C</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">backward</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Forward: </span><span class="si">{:.3f}</span><span class="s1"> s | Backward </span><span class="si">{:.3f}</span><span class="s1"> s'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="n">backward</span><span class="p">))</span>
</pre></div>
</div>
<p>If we run this code with the original LLTM we wrote in pure Python at the start
of this post, we get the following numbers (on my machine):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">506.480</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">444.694</span> <span class="n">us</span>
</pre></div>
</div>
<p>and with our new C++ version:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">349.335</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">443.523</span> <span class="n">us</span>
</pre></div>
</div>
<p>We can already see a significant speedup for the forward function (more than
30%). For the backward function, a speedup is visible, albeit not a major one.
The backward pass I wrote above was not particularly optimized and could
definitely be improved. Also, PyTorch’s automatic differentiation engine can
automatically parallelize computation graphs, may use a more efficient flow of
operations overall, and is also implemented in C++, so it’s expected to be
fast. Nevertheless, this is a good start.</p>
</section>
<section id="performance-on-gpu-devices">
<h4>Performance on GPU Devices<a class="headerlink" href="#performance-on-gpu-devices" title="Link to this heading">#</a></h4>
<p>A wonderful fact about PyTorch’s <em>ATen</em> backend is that it abstracts the
computing device you are running on. This means the same code we wrote for CPU
can <em>also</em> run on GPU, and individual operations will correspondingly dispatch
to GPU-optimized implementations. For certain operations like matrix multiply
(like <code class="docutils literal notranslate"><span class="pre">mm</span></code> or <code class="docutils literal notranslate"><span class="pre">addmm</span></code>), this is a big win. Let’s take a look at how much
performance we gain from running our C++ code with CUDA tensors. No changes to
our implementation are required, we simply need to put our tensors in GPU
memory from Python, with either adding <code class="docutils literal notranslate"><span class="pre">device=cuda_device</span></code> argument at
creation time or using <code class="docutils literal notranslate"><span class="pre">.to(cuda_device)</span></code> after creation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">cuda_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>  <span class="c1"># device object representing GPU</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">input_features</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Note the device=cuda_device arguments here</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda_device</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda_device</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda_device</span><span class="p">)</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">LLTM</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda_device</span><span class="p">)</span>

<span class="n">forward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">backward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">new_h</span><span class="p">,</span> <span class="n">new_C</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">forward</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="p">(</span><span class="n">new_h</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">new_C</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">backward</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Forward: </span><span class="si">{:.3f}</span><span class="s1"> us | Backward </span><span class="si">{:.3f}</span><span class="s1"> us'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">forward</span> <span class="o">*</span> <span class="mf">1e6</span><span class="o">/</span><span class="mf">1e5</span><span class="p">,</span> <span class="n">backward</span> <span class="o">*</span> <span class="mf">1e6</span><span class="o">/</span><span class="mf">1e5</span><span class="p">))</span>
</pre></div>
</div>
<p>Once more comparing our plain PyTorch code with our C++ version, now both
running on CUDA devices, we again see performance gains. For Python/PyTorch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">187.719</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">410.815</span> <span class="n">us</span>
</pre></div>
</div>
<p>And C++/ATen:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">149.802</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">393.458</span> <span class="n">us</span>
</pre></div>
</div>
<p>That’s a great overall speedup compared to non-CUDA code. However, we can pull
even more performance out of our C++ code by writing custom CUDA kernels, which
we’ll dive into soon. Before that, let’s discuss another way of building your C++
extensions.</p>
</section>
</section>
<section id="jit-compiling-extensions">
<h3>JIT Compiling Extensions<a class="headerlink" href="#jit-compiling-extensions" title="Link to this heading">#</a></h3>
<p>Previously, I mentioned there were two ways of building C++ extensions: using
<code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code> or just in time (JIT). Having covered the former, let’s
elaborate on the latter. The JIT compilation mechanism provides you with a way
of compiling and loading your extensions on the fly by calling a simple
function in PyTorch’s API called <a class="reference external" href="https://docs.pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.utils.cpp_extension.load()</span></code></a>. For
the LLTM, this would look as simple as this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>

<span class="n">lltm_cpp</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"lltm_cpp"</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s2">"lltm.cpp"</span><span class="p">])</span>
</pre></div>
</div>
<p>Here, we provide the function with the same information as for
<code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code>. In the background, this will do the following:</p>
<ol class="arabic simple">
<li><p>Create a temporary directory <code class="docutils literal notranslate"><span class="pre">/tmp/torch_extensions/lltm</span></code>,</p></li>
<li><p>Emit a <a class="reference external" href="https://ninja-build.org/">Ninja</a> build file into that temporary directory,</p></li>
<li><p>Compile your source files into a shared library,</p></li>
<li><p>Import this shared library as a Python module.</p></li>
</ol>
<p>In fact, if you pass <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to <code class="xref py py-func docutils literal notranslate"><span class="pre">cpp_extension.load()</span></code>, you will
be informed about the process:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">torch_extensions</span> <span class="k">as</span> <span class="n">PyTorch</span> <span class="n">extensions</span> <span class="n">root</span><span class="o">...</span>
<span class="n">Emitting</span> <span class="n">ninja</span> <span class="n">build</span> <span class="n">file</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">torch_extensions</span><span class="o">/</span><span class="n">lltm_cpp</span><span class="o">/</span><span class="n">build</span><span class="o">.</span><span class="n">ninja</span><span class="o">...</span>
<span class="n">Building</span> <span class="n">extension</span> <span class="n">module</span> <span class="n">lltm_cpp</span><span class="o">...</span>
<span class="n">Loading</span> <span class="n">extension</span> <span class="n">module</span> <span class="n">lltm_cpp</span><span class="o">...</span>
</pre></div>
</div>
<p>The resulting Python module will be exactly the same as produced by setuptools,
but removes the requirement of having to maintain a separate <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> build
file. If your setup is more complicated and you do need the full power of
<code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code>, you <em>can</em> write your own <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> – but in many cases
this JIT technique will do just fine. The first time you run through this line,
it will take some time, as the extension is compiling in the background. Since
we use the Ninja build system to build your sources, re-compilation is
incremental and thus re-loading the extension when you run your Python module a
second time is fast and has low overhead if you didn’t change the extension’s
source files.</p>
</section>
</section>
<section id="writing-a-mixed-c-cuda-extension">
<h2>Writing a Mixed C++/CUDA extension<a class="headerlink" href="#writing-a-mixed-c-cuda-extension" title="Link to this heading">#</a></h2>
<p>To really take our implementation to the next level, we can hand-write parts of
our forward and backward passes with custom CUDA kernels. For the LLTM, this has
the prospect of being particularly effective, as there are a large number of
pointwise operations in sequence, that can all be fused and parallelized in a
single CUDA kernel. Let’s see how we could write such a CUDA kernel and
integrate it with PyTorch using this extension mechanism.</p>
<p>The general strategy for writing a CUDA extension is to first write a C++ file
which defines the functions that will be called from Python, and binds those
functions to Python with pybind11. Furthermore, this file will also <em>declare</em>
functions that are defined in CUDA (<code class="docutils literal notranslate"><span class="pre">.cu</span></code>) files. The C++ functions will then
do some checks and ultimately forward its calls to the CUDA functions. In the
CUDA files, we write our actual CUDA kernels. The <code class="xref py py-mod docutils literal notranslate"><span class="pre">cpp_extension</span></code> package
will then take care of compiling the C++ sources with a C++ compiler like
<code class="docutils literal notranslate"><span class="pre">gcc</span></code> and the CUDA sources with NVIDIA’s <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler. This ensures that
each compiler takes care of files it knows best to compile. Ultimately, they
will be linked into one shared library that is available to us from Python
code.</p>
<p>We’ll start with the C++ file, which we’ll call <code class="docutils literal notranslate"><span class="pre">lltm_cuda.cpp</span></code>, for example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="c1">// CUDA forward declarations</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_cuda_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_cuda_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">);</span>

<span class="c1">// C++ interface</span>

<span class="cp">#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")</span>
<span class="cp">#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")</span>
<span class="cp">#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">bias</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">old_h</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lltm_cuda_forward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">old_cell</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">grad_h</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">grad_cell</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">input_gate</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">output_gate</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">candidate_cell</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lltm_cuda_backward</span><span class="p">(</span>
<span class="w">      </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">      </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">      </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">      </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">X</span><span class="p">,</span>
<span class="w">      </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">      </span><span class="n">weights</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"forward"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_forward</span><span class="p">,</span><span class="w"> </span><span class="s">"LLTM forward (CUDA)"</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"backward"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_backward</span><span class="p">,</span><span class="w"> </span><span class="s">"LLTM backward (CUDA)"</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>As you can see, it is largely boilerplate, checks and forwarding to functions
that we’ll define in the CUDA file. We’ll name this file
<code class="docutils literal notranslate"><span class="pre">lltm_cuda_kernel.cu</span></code> (note the <code class="docutils literal notranslate"><span class="pre">.cu</span></code> extension!). NVCC can reasonably
compile C++11, thus we still have ATen and the C++ standard library available
to us (but not <code class="docutils literal notranslate"><span class="pre">torch.h</span></code>). Note that <code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code> cannot handle files
with the same name but different extensions, so if you use the <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>
method instead of the JIT method, you must give your CUDA file a different name
than your C++ file (for the JIT method, <code class="docutils literal notranslate"><span class="pre">lltm.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">lltm.cu</span></code> would work
fine). Let’s take a small peek at what this file will look like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda_runtime.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="n">__forceinline__</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Here we see the headers I just described, as well as the fact that we are using
CUDA-specific declarations like <code class="docutils literal notranslate"><span class="pre">__device__</span></code> and <code class="docutils literal notranslate"><span class="pre">__forceinline__</span></code> and
functions like <code class="docutils literal notranslate"><span class="pre">exp</span></code>. Let’s continue with a few more helper functions that
we’ll need:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="n">__forceinline__</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">s</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="n">__forceinline__</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">d_tanh</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">t</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="n">__forceinline__</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">elu</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">fmax</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fmin</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.0</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="n">__forceinline__</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">d_elu</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_relu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">d_relu</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(((</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.0</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.0</span><span class="p">)</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To now actually implement a function, we’ll again need two things: one function
that performs operations we don’t wish to explicitly write by hand and calls
into CUDA kernels, and then the actual CUDA kernel for the parts we want to
speed up. For the forward pass, the first function should look like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_cuda_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">cat</span><span class="p">({</span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">},</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">));</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">dim3</span><span class="w"> </span><span class="n">blocks</span><span class="p">((</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">);</span>

<span class="w">  </span><span class="n">AT_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">gates</span><span class="p">.</span><span class="n">type</span><span class="p">(),</span><span class="w"> </span><span class="s">"lltm_forward_cuda"</span><span class="p">,</span><span class="w"> </span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lltm_cuda_forward_kernel</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">gates</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">old_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">new_h</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">new_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">input_gate</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">output_gate</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">candidate_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">state_size</span><span class="p">);</span>
<span class="w">  </span><span class="p">}));</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">new_h</span><span class="p">,</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">gates</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The main point of interest here is the <code class="docutils literal notranslate"><span class="pre">AT_DISPATCH_FLOATING_TYPES</span></code> macro and
the kernel launch (indicated by the <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;...&gt;&gt;&gt;</span></code>). While ATen abstracts away
the device and datatype of the tensors we deal with, a tensor will, at runtime,
still be backed by memory of a concrete type on a concrete device. As such, we
need a way of determining at runtime what type a tensor is and then selectively
call functions with the corresponding correct type signature. Done manually,
this would (conceptually) look something like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">switch</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">type</span><span class="p">().</span><span class="n">scalarType</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">case</span><span class="w"> </span><span class="no">torch</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">Double</span><span class="p">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">function</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span>
<span class="w">  </span><span class="k">case</span><span class="w"> </span><span class="no">torch</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">Float</span><span class="p">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">function</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">());</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The purpose of <code class="docutils literal notranslate"><span class="pre">AT_DISPATCH_FLOATING_TYPES</span></code> is to take care of this dispatch
for us. It takes a type (<code class="docutils literal notranslate"><span class="pre">gates.type()</span></code> in our case), a name (for error
messages) and a lambda function. Inside this lambda function, the type alias
<code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> is available and is defined as the type that the tensor actually
is at runtime in that context. As such, if we have a template function (which
our CUDA kernel will be), we can instantiate it with this <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> alias,
and the correct function will be called. In this case, we also want to retrieve
the data pointers of the tensors as pointers of that <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> type. If you
wanted to dispatch over all types and not just floating point types (<code class="docutils literal notranslate"><span class="pre">Float</span></code>
and <code class="docutils literal notranslate"><span class="pre">Double</span></code>), you can use <code class="docutils literal notranslate"><span class="pre">AT_DISPATCH_ALL_TYPES</span></code>.</p>
<p>Note that we perform some operations with plain ATen. These operations will
still run on the GPU, but using ATen’s default implementations. This makes
sense because ATen will use highly optimized routines for things like matrix
multiplies (e.g. <code class="docutils literal notranslate"><span class="pre">addmm</span></code>) or convolutions which would be much harder to
implement and improve ourselves.</p>
<p>As for the kernel launch itself, we are here specifying that each CUDA block
will have 1024 threads, and that the entire GPU grid is split into as many
blocks of <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">x</span> <span class="pre">1024</span></code> threads as are required to fill our matrices with one
thread per component. For example, if our state size was 2048 and our batch
size 4, we’d launch a total of <code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">x</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">8</span></code> blocks with each 1024 threads. If
you’ve never heard of CUDA “blocks” or “grids” before, an <a class="reference external" href="https://devblogs.nvidia.com/even-easier-introduction-cuda">introductory read
about CUDA</a> may
help.</p>
<p>The actual CUDA kernel is fairly simple (if you’ve ever programmed GPUs before):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">lltm_cuda_forward_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">old_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">new_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">state_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">column</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">gates_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">state_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">column</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">state_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">input_gate</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">    </span><span class="n">output_gate</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">    </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">    </span><span class="n">new_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">old_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">input_gate</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">    </span><span class="n">new_h</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">index</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">output_gate</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>What’s primarily interesting here is that we are able to compute all of these
pointwise operations entirely in parallel for each individual component in our
gate matrices. If you imagine having to do this with a giant <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over
a million elements in serial, you can see why this would be much faster.</p>
<section id="using-accessors">
<h3>Using accessors<a class="headerlink" href="#using-accessors" title="Link to this heading">#</a></h3>
<p>You can see in the CUDA kernel that we work directly on pointers with the right
type. Indeed, working directly with high level type agnostic tensors inside cuda
kernels would be very inefficient.</p>
<p>However, this comes at a cost of ease of use and readability, especially for
highly dimensional data. In our example, we know for example that the contiguous
<code class="docutils literal notranslate"><span class="pre">gates</span></code> tensor has 3 dimensions:</p>
<ol class="arabic simple">
<li><p>batch, size of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and stride of <code class="docutils literal notranslate"><span class="pre">3*state_size</span></code></p></li>
<li><p>row, size of <code class="docutils literal notranslate"><span class="pre">3</span></code> and stride of <code class="docutils literal notranslate"><span class="pre">state_size</span></code></p></li>
<li><p>index, size  of <code class="docutils literal notranslate"><span class="pre">state_size</span></code> and stride of <code class="docutils literal notranslate"><span class="pre">1</span></code></p></li>
</ol>
<p>How can we access the element <code class="docutils literal notranslate"><span class="pre">gates[n][row][column]</span></code> inside the kernel then?
It turns out that you need the strides to access your element with some simple
arithmetic.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">gates</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">()[</span><span class="n">n</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">row</span><span class="o">*</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]</span>
</pre></div>
</div>
<p>In addition to being verbose, this expression needs stride to be explicitly
known, and thus passed to the kernel function within its arguments. You can see
that in the case of kernel functions accepting multiple tensors with different
sizes you will end up with a very long list of arguments.</p>
<p>Fortunately for us, ATen provides accessors that are created with a single
dynamic check that a Tensor is the type and number of dimensions.
Accessors then expose an API for accessing the Tensor elements efficiently
without having to convert to a single pointer:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">foo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">12</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">});</span>

<span class="c1">// assert foo is 2-dimensional and holds floats.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">foo_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">foo</span><span class="p">.</span><span class="n">accessor</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">();</span>
<span class="kt">float</span><span class="w"> </span><span class="n">trace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">foo_a</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// use the accessor foo_a to get tensor data.</span>
<span class="w">  </span><span class="n">trace</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">foo_a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Accessor objects have a relatively high level interface, with <code class="docutils literal notranslate"><span class="pre">.size()</span></code> and
<code class="docutils literal notranslate"><span class="pre">.stride()</span></code> methods and multi-dimensional indexing. The <code class="docutils literal notranslate"><span class="pre">.accessor&lt;&gt;</span></code>
interface is designed to access data efficiently on cpu tensor. The equivalent
for cuda tensors are <code class="docutils literal notranslate"><span class="pre">packed_accessor64&lt;&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">packed_accessor32&lt;&gt;</span></code>, which
produce Packed Accessors with either 64-bit or 32-bit integer indexing.</p>
<p>The fundamental difference with Accessor is that a Packed Accessor copies size
and stride data inside of its structure instead of pointing to it. It allows us
to pass it to a CUDA kernel function and use its interface inside it.</p>
<p>We can design a function that takes Packed Accessors instead of pointers.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">lltm_cuda_forward_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">old_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s decompose the template used here. the first two arguments <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> and
<code class="docutils literal notranslate"><span class="pre">2</span></code> are the same as regular Accessor. The argument
<code class="docutils literal notranslate"><span class="pre">torch::RestrictPtrTraits</span></code> indicates that the <code class="docutils literal notranslate"><span class="pre">__restrict__</span></code> keyword must be
used. Note also that we’ve used the <code class="docutils literal notranslate"><span class="pre">PackedAccessor32</span></code> variant which store the
sizes and strides in an <code class="docutils literal notranslate"><span class="pre">int32_t</span></code>. This is important as using the 64-bit
variant (<code class="docutils literal notranslate"><span class="pre">PackedAccessor64</span></code>) can make the kernel slower.</p>
<p>The function declaration becomes</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">lltm_cuda_forward_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">old_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">//batch index</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// column index</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">gates</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)){</span>
<span class="w">    </span><span class="n">input_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">    </span><span class="n">output_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">    </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">    </span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">old_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">input_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">    </span><span class="n">new_h</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">output_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The implementation is much more readable! This function is then called by
creating Packed Accessors with the <code class="docutils literal notranslate"><span class="pre">.packed_accessor32&lt;&gt;</span></code> method within the
host function.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_cuda_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">cat</span><span class="p">({</span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">},</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gate_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">));</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">});</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">dim3</span><span class="w"> </span><span class="n">blocks</span><span class="p">((</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">);</span>

<span class="w">  </span><span class="n">AT_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">gates</span><span class="p">.</span><span class="n">type</span><span class="p">(),</span><span class="w"> </span><span class="s">"lltm_forward_cuda"</span><span class="p">,</span><span class="w"> </span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lltm_cuda_forward_kernel</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">gates</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">old_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">new_h</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">new_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">input_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">output_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">candidate_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">());</span>
<span class="w">  </span><span class="p">}));</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">new_h</span><span class="p">,</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">gates</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The backwards pass follows much the same pattern and I won’t elaborate further
on it:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">lltm_cuda_backward_kernel</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">d_gates</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">//batch index</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// column index</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)){</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_new_cell</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">d_tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>


<span class="w">    </span><span class="n">d_old_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>

<span class="w">    </span><span class="n">d_gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">    </span><span class="n">d_gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">    </span><span class="n">d_gates</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_elu</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_cuda_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">new_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gates</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">dim3</span><span class="w"> </span><span class="n">blocks</span><span class="p">((</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">);</span>

<span class="w">  </span><span class="n">AT_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">type</span><span class="p">(),</span><span class="w"> </span><span class="s">"lltm_backward_cuda"</span><span class="p">,</span><span class="w"> </span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lltm_cuda_backward_kernel</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">d_old_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">d_gates</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">grad_h</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">grad_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">new_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">input_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">output_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">candidate_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">gates</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">torch</span><span class="o">::</span><span class="n">RestrictPtrTraits</span><span class="o">&gt;</span><span class="p">());</span>
<span class="w">  </span><span class="p">}));</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">*</span><span class="n">state_size</span><span class="p">});</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">t</span><span class="p">().</span><span class="n">mm</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="cm">/*keepdim=*/</span><span class="nb">true</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">d_old_h</span><span class="p">,</span><span class="w"> </span><span class="n">d_input</span><span class="p">,</span><span class="w"> </span><span class="n">d_weights</span><span class="p">,</span><span class="w"> </span><span class="n">d_bias</span><span class="p">,</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">,</span><span class="w"> </span><span class="n">d_gates</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="integrating-a-c-cuda-operation-with-pytorch">
<h3>Integrating a C++/CUDA Operation with PyTorch<a class="headerlink" href="#integrating-a-c-cuda-operation-with-pytorch" title="Link to this heading">#</a></h3>
<p>Integration of our CUDA-enabled op with PyTorch is again very straightforward.
If you want to write a <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script, it could look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">BuildExtension</span><span class="p">,</span> <span class="n">CUDAExtension</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'lltm'</span><span class="p">,</span>
    <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="n">CUDAExtension</span><span class="p">(</span><span class="s1">'lltm_cuda'</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">'lltm_cuda.cpp'</span><span class="p">,</span>
            <span class="s1">'lltm_cuda_kernel.cu'</span><span class="p">,</span>
        <span class="p">])</span>
    <span class="p">],</span>
    <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">BuildExtension</span>
    <span class="p">})</span>
</pre></div>
</div>
<p>Instead of <code class="xref py py-func docutils literal notranslate"><span class="pre">CppExtension()</span></code>, we now use <code class="xref py py-func docutils literal notranslate"><span class="pre">CUDAExtension()</span></code>. We can just
specify the <code class="docutils literal notranslate"><span class="pre">.cu</span></code> file along with the <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> files – the library takes
care of all the hassle this entails for you. The JIT mechanism is even
simpler:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>

<span class="n">lltm</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'lltm'</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s1">'lltm_cuda.cpp'</span><span class="p">,</span> <span class="s1">'lltm_cuda_kernel.cu'</span><span class="p">])</span>
</pre></div>
</div>
<section id="id4">
<h4>Performance Comparison<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>Our hope was that parallelizing and fusing the pointwise operations of our code
with CUDA would improve the performance of our LLTM. Let’s see if that holds
true. We can run the code I listed earlier to run a benchmark. Our fastest
version earlier was the CUDA-based C++ code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">149.802</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">393.458</span> <span class="n">us</span>
</pre></div>
</div>
<p>And now with our custom CUDA kernel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Forward</span><span class="p">:</span> <span class="mf">129.431</span> <span class="n">us</span> <span class="o">|</span> <span class="n">Backward</span> <span class="mf">304.641</span> <span class="n">us</span>
</pre></div>
</div>
<p>More performance increases!</p>
</section>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>You should now be equipped with a good overview of PyTorch’s C++ extension
mechanism as well as a motivation for using them. You can find the code
examples displayed in this note <a class="reference external" href="https://github.com/pytorch/extension-cpp">here</a>. If you have questions, please use
<a class="reference external" href="https://discuss.pytorch.org">the forums</a>. Also be sure to check our <a class="reference external" href="https://pytorch.org/cppdocs/notes/faq.html">FAQ</a> in case you run into any issues.
A blog on writing extensions for AMD ROCm can be found <a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/cpp-extn/readme.html">here</a>.</p>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/custom_function_conv_bn_tutorial.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Fusing Convolution and Batch Norm using Custom Function</p>
</div>
</a>
<a class="right-next" href="dispatcher.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Registering a Dispatched Operator in C++</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/custom_function_conv_bn_tutorial.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Fusing Convolution and Batch Norm using Custom Function</p>
</div>
</a>
<a class="right-next" href="dispatcher.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Registering a Dispatched Operator in C++</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-example">Motivation and Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-a-c-extension">Writing a C++ Extension</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-with-setuptools">Building with <code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-the-c-op">Writing the C++ Op</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward Pass</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binding-to-python">Binding to Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-your-extension">Using Your Extension</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance Comparison</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-on-gpu-devices">Performance on GPU Devices</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jit-compiling-extensions">JIT Compiling Extensions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-a-mixed-c-cuda-extension">Writing a Mixed C++/CUDA extension</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-accessors">Using accessors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrating-a-c-cuda-operation-with-pytorch">Integrating a C++/CUDA Operation with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Performance Comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "headline": "Custom C++ and CUDA Extensions",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/advanced/cpp_extension.html",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/advanced/cpp_extension.html"
         },
        "datePublished": "",
         "dateModified": "",
         "articleBody": "Custom C++ and CUDA Extensions# Author: Peter Goldsborough Warning This tutorial is deprecated as of PyTorch 2.4. Please see PyTorch Custom Operators for the newest up-to-date guides on extending PyTorch with Custom C++/CUDA Extensions. PyTorch provides a plethora of operations related to neural networks, arbitrary tensor algebra, data wrangling and other purposes. However, you may still find yourself in need of a more customized operation. For example, you might want to use a novel activation function you found in a paper, or implement an operation you developed as part of your research. The easiest way of integrating such a custom operation in PyTorch is to write it in Python by extending Function and Module as outlined here. This gives you the full power of automatic differentiation (spares you from writing derivative functions) as well as the usual expressiveness of Python. However, there may be times when your operation is better implemented in C++. For example, your code may need to be really fast because it is called very frequently in your model or is very expensive even for few calls. Another plausible reason is that it depends on or interacts with other C or C++ libraries. To address such cases, PyTorch provides a very easy way of writing custom C++ extensions. C++ extensions are a mechanism we have developed to allow users (you) to create PyTorch operators defined out-of-source, i.e. separate from the PyTorch backend. This approach is different from the way native PyTorch operations are implemented. C++ extensions are intended to spare you much of the boilerplate associated with integrating an operation with PyTorch’s backend while providing you with a high degree of flexibility for your PyTorch-based projects. Nevertheless, once you have defined your operation as a C++ extension, turning it into a native PyTorch function is largely a matter of code organization, which you can tackle after the fact if you decide to contribute your operation upstream. Motivation and Example# The rest of this note will walk through a practical example of writing and using a C++ (and CUDA) extension. If you are being chased or someone will fire you if you don’t get that op done by the end of the day, you can skip this section and head straight to the implementation details in the next section. Let’s say you’ve come up with a new kind of recurrent unit that you found to have superior properties compared to the state of the art. This recurrent unit is similar to an LSTM, but differs in that it lacks a forget gate and uses an Exponential Linear Unit (ELU) as its internal activation function. Because this unit never forgets, we’ll call it LLTM, or Long-Long-Term-Memory unit. The two ways in which LLTMs differ from vanilla LSTMs are significant enough that we can’t configure PyTorch’s LSTMCell for our purposes, so we’ll have to create a custom cell. The first and easiest approach for this – and likely in all cases a good first step – is to implement our des..."
       }
   </script>
</body>
</body></html>