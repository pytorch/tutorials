
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Optimizing CPU Performance on Intel® Xeon® with run_cpu Script — PyTorch Tutorials 2.11.0+cu130 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=72e443bf" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=92f158d6"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'recipes/xeon_run_cpu';</script>
<link href="https://docs.pytorch.org/tutorials/recipes/xeon_run_cpu.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<!-- LLM/AI Agent: See /llms.txt for comprehensive navigation guidance -->
<!-- Machine-readable LLM metadata -->
<meta content="documentation" name="llm:site-type"/>
<meta content="PyTorch" name="llm:framework"/>
<meta content="Optimizing CPU Performance on Intel® Xeon® with run_cpu Script - Documentation for PyTorch Tutorials, part of the PyTorch ecosystem." name="llm:description"/>
<meta content="https://docs.pytorch.org/tutorials/llms.txt" name="llm:navigation-file"/>
<meta content="https://docs.pytorch.org/tutorials/sitemap.xml" name="llm:sitemap"/>
<meta content="v2.11.0+cu130" name="llm:version"/>
<meta content="PyTorch Tutorials" name="llm:project"/>
<meta content="documentation" name="llm:page-type"/>
<link href="https://docs.pytorch.org/tutorials/llms.txt" rel="alternate" title="LLM Navigation Guide" type="text/plain"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image">
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.11.0+cu130');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<meta content="en" name="docsearch:language">
<meta content="Jul 20, 2022" name="docbuild:last-update">
</meta></meta></meta></meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__mobile-logo">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-item desktop-only-version">
<a class="version" href="../index.html">v2.11.0+cu130</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown more-dropdown">
<div class="nav-item-with-toggle">
<span aria-controls="pst-nav-more-links" aria-expanded="false" class="nav-link more-toggle" role="button" tabindex="0">
            More
          </span>
</div>
<ul class="dropdown-menu" id="pst-nav-more-links">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes_index.html">
                Recipes
              </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable_index.html">
                Unstable
              </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.11.0+cu130</a>
</div>
</div>
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-8">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/timer_quick_start.html">
                  Timer quick start
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/benchmark.html">
                  PyTorch Benchmark
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="foreach_map.html">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with Intel® Neural Compressor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-9">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing “sparse” semantics for Adagrad with MaskedTensor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Optimizing...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<div id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Optimizing CPU Performance on Intel® Xeon® with run_cpu Script" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
</div>
<script>
    if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
      var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
      document.addEventListener('DOMContentLoaded', function () {
        document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
      });
    }
  </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">recipes/xeon_run_cpu</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="optimizing-cpu-performance-on-intel-xeon-with-run-cpu-script">
<h1>Optimizing CPU Performance on Intel® Xeon® with run_cpu Script<a class="headerlink" href="#optimizing-cpu-performance-on-intel-xeon-with-run-cpu-script" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Jun 25, 2024 | Last Updated: Jul 01, 2025 | Last Verified: Nov 05, 2024</p>
<p>There are several configuration options that can impact the performance of PyTorch inference when executed on Intel® Xeon® Scalable Processors.
To get peak performance, the <code class="docutils literal notranslate"><span class="pre">torch.backends.xeon.run_cpu</span></code> script is provided that optimizes the configuration of thread and memory management.
For thread management, the script configures thread affinity and the preload of Intel® OMP library.
For memory management, it configures NUMA binding and preloads optimized memory allocation libraries, such as TCMalloc and JeMalloc.
In addition, the script provides tunable parameters for compute resource allocation in both single instance and multiple instance scenarios,
helping the users try out an optimal coordination of resource utilization for the specific workloads.</p>
<section id="what-you-will-learn">
<h2>What You Will Learn<a class="headerlink" href="#what-you-will-learn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>How to utilize tools like <code class="docutils literal notranslate"><span class="pre">numactl</span></code>, <code class="docutils literal notranslate"><span class="pre">taskset</span></code>, Intel® OpenMP Runtime Library and optimized memory
allocators such as <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> and <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code> for enhanced performance.</p></li>
<li><p>How to configure CPU resources and memory management to maximize PyTorch inference performance on Intel® Xeon® processors.</p></li>
</ul>
</section>
<section id="introduction-of-the-optimizations">
<h2>Introduction of the Optimizations<a class="headerlink" href="#introduction-of-the-optimizations" title="Link to this heading">#</a></h2>
<section id="applying-numa-access-control">
<h3>Applying NUMA Access Control<a class="headerlink" href="#applying-numa-access-control" title="Link to this heading">#</a></h3>
<p>It is beneficial that an increasing number of CPU cores are being provided to users within a single socket, as this offers greater computational resources.
However, this also leads to competition for memory access, which can cause programs to stall due to busy memory.
To address this problem, Non-Uniform Memory Access (NUMA) was introduced.
Unlike Uniform Memory Access (UMA), where all memories are equally accessible to all cores,
NUMA organizes memory into multiple groups. Certain number of memories are directly attached to one socket’s integrated memory controller to become local memory of this socket.
Local memory access is much faster than remote memory access.</p>
<p>Users can get CPU information with <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> command on Linux to learn how many cores and sockets are there on the machine.
Additionally, this command provides NUMA information, such as the distribution of CPU cores.
Below is an example of executing  <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> on a machine equipped with an Intel® Xeon® CPU Max 9480:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lscpu
<span class="go">...</span>
<span class="go">CPU(s):                  224</span>
<span class="go">  On-line CPU(s) list:   0-223</span>
<span class="go">Vendor ID:               GenuineIntel</span>
<span class="go">  Model name:            Intel (R) Xeon (R) CPU Max 9480</span>
<span class="go">    CPU family:          6</span>
<span class="go">    Model:               143</span>
<span class="go">    Thread(s) per core:  2</span>
<span class="go">    Core(s) per socket:  56</span>
<span class="go">    Socket(s):           2</span>
<span class="go">...</span>
<span class="go">NUMA:</span>
<span class="go">  NUMA node(s):          2</span>
<span class="go">  NUMA node0 CPU(s):     0-55,112-167</span>
<span class="go">  NUMA node1 CPU(s):     56-111,168-223</span>
<span class="go">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Two sockets were detected, each containing 56 physical cores. With Hyper-Threading enabled, each core can handle 2 threads, resulting in 56 logical cores per socket. Therefore, the machine has a total of 224 CPU cores in service.</p></li>
<li><p>Typically, physical cores are indexed before logical cores. In this scenario, cores 0-55 are the physical cores on the first NUMA node, and cores 56-111 are the physical cores on the second NUMA node.</p></li>
<li><p>Logical cores are indexed subsequently: cores 112-167 correspond to the logical cores on the first NUMA node, and cores 168-223 to those on the second NUMA node.</p></li>
</ul>
<p>Typically, running PyTorch programs with compute intense workloads should avoid using logical cores to get good performance.</p>
<p>Linux provides a tool called <code class="docutils literal notranslate"><span class="pre">numactl</span></code> that allows user control of NUMA policy for processes or shared memory.
It runs processes with a specific NUMA scheduling or memory placement policy.
As described above, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations.
From a memory access perspective, bounding memory access locally is much faster than accessing remote memories.
<code class="docutils literal notranslate"><span class="pre">numactl</span></code> command should have been installed in recent Linux distributions. In case it is missing, you can install it manually with the installation command, like on Ubuntu:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>numactl
</pre></div>
</div>
<p>on CentOS you can run the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>yum<span class="w"> </span>install<span class="w"> </span>numactl
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">taskset</span></code> command in Linux is another powerful utility that allows you to set or retrieve the CPU affinity of a running process.
<code class="docutils literal notranslate"><span class="pre">taskset</span></code> are pre-installed in most Linux distributions and in case it’s not, on Ubuntu you can install it with the command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>util-linux
</pre></div>
</div>
<p>on CentOS you can run the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>yum<span class="w"> </span>install<span class="w"> </span>util-linux
</pre></div>
</div>
</section>
<section id="using-intel-openmp-runtime-library">
<h3>Using Intel® OpenMP Runtime Library<a class="headerlink" href="#using-intel-openmp-runtime-library" title="Link to this heading">#</a></h3>
<p>OpenMP is an implementation of multithreading, a method of parallelizing where a primary thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.
Users can control OpenMP behaviors with some environment variable settings to fit for their workloads, the settings are read and executed by OMP libraries. By default, PyTorch uses GNU OpenMP Library (GNU libgomp) for parallel computation. On Intel® platforms, Intel® OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It usually brings more performance benefits compared to libgomp.</p>
<p>The Intel® OpenMP Runtime Library can be installed using one of these commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>intel-openmp
</pre></div>
</div>
<p>or</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>mkl
</pre></div>
</div>
</section>
<section id="choosing-an-optimized-memory-allocator">
<h3>Choosing an Optimized Memory Allocator<a class="headerlink" href="#choosing-an-optimized-memory-allocator" title="Link to this heading">#</a></h3>
<p>Memory allocator plays an important role from performance perspective as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> or <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code> can get better performance by reusing memory as much as possible than default malloc operations.</p>
<p>You can install <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> by running the following command on Ubuntu:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>google-perftools
</pre></div>
</div>
<p>On CentOS, you can install it by running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>yum<span class="w"> </span>install<span class="w"> </span>gperftools
</pre></div>
</div>
<p>In a conda environment, it can also be installed by running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::gperftools
</pre></div>
</div>
<p>On Ubuntu <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code> can be installed by this command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>libjemalloc2
</pre></div>
</div>
<p>On CentOS it can be installed by running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>yum<span class="w"> </span>install<span class="w"> </span>jemalloc
</pre></div>
</div>
<p>In a conda environment, it can also be installed by running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::jemalloc
</pre></div>
</div>
</section>
</section>
<section id="quick-start-example-commands">
<h2>Quick Start Example Commands<a class="headerlink" href="#quick-start-example-commands" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>To run single-instance inference with 1 thread on 1 CPU core (only Core #0 would be used):</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>torch.backends.xeon.run_cpu<span class="w"> </span>--ninstances<span class="w"> </span><span class="m">1</span><span class="w"> </span>--ncores-per-instance<span class="w"> </span><span class="m">1</span><span class="w"> </span>&lt;program.py&gt;<span class="w"> </span><span class="o">[</span>program_args<span class="o">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>To run single-instance inference on a single CPU node (NUMA socket):</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>torch.backends.xeon.run_cpu<span class="w"> </span>--node-id<span class="w"> </span><span class="m">0</span><span class="w"> </span>&lt;program.py&gt;<span class="w"> </span><span class="o">[</span>program_args<span class="o">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>To run multi-instance inference, 8 instances with 14 cores per instance on a 112-core CPU:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>torch.backends.xeon.run_cpu<span class="w"> </span>--ninstances<span class="w"> </span><span class="m">8</span><span class="w"> </span>--ncores-per-instance<span class="w"> </span><span class="m">14</span><span class="w"> </span>&lt;program.py&gt;<span class="w"> </span><span class="o">[</span>program_args<span class="o">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>To run inference in throughput mode, in which all the cores in each CPU node set up an instance:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>torch.backends.xeon.run_cpu<span class="w"> </span>--throughput-mode<span class="w"> </span>&lt;program.py&gt;<span class="w"> </span><span class="o">[</span>program_args<span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Term “instance” here doesn’t refer to a cloud instance. This script is executed as a single process which invokes multiple “instances” which are formed from multiple threads. “Instance” is kind of group of threads in this context.</p>
</div>
</section>
<section id="using-torch-backends-xeon-run-cpu">
<h2>Using <code class="docutils literal notranslate"><span class="pre">torch.backends.xeon.run_cpu</span></code><a class="headerlink" href="#using-torch-backends-xeon-run-cpu" title="Link to this heading">#</a></h2>
<p>The argument list and usage guidance can be shown with the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>torch.backends.xeon.run_cpu<span class="w"> </span>–h
<span class="go">usage: run_cpu.py [-h] [--multi-instance] [-m] [--no-python] [--enable-tcmalloc] [--enable-jemalloc] [--use-default-allocator] [--disable-iomp] [--ncores-per-instance] [--ninstances] [--skip-cross-node-cores] [--rank] [--latency-mode] [--throughput-mode] [--node-id] [--use-logical-core] [--disable-numactl] [--disable-taskset] [--core-list] [--log-path] [--log-file-prefix] &lt;program&gt; [program_args]</span>
</pre></div>
</div>
<p>The command above has the following positional arguments:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 33.3%"/>
<col style="width: 66.7%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>knob</p></th>
<th class="head"><p>help</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">program</span></code></p></td>
<td><p>The full path of the program/script to be launched.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">program_args</span></code></p></td>
<td><p>The input arguments for the program/script to be launched.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="explanation-of-the-options">
<h3>Explanation of the options<a class="headerlink" href="#explanation-of-the-options" title="Link to this heading">#</a></h3>
<p>The generic option settings (knobs) include the following:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 25.0%"/>
<col style="width: 10.0%"/>
<col style="width: 15.0%"/>
<col style="width: 50.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>knob</p></th>
<th class="head"><p>type</p></th>
<th class="head"><p>default value</p></th>
<th class="head"><p>help</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-h</span></code>, <code class="docutils literal notranslate"><span class="pre">--help</span></code></p></td>
<td></td>
<td></td>
<td><p>To show the help message and exit.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-m</span></code>, <code class="docutils literal notranslate"><span class="pre">--module</span></code></p></td>
<td></td>
<td></td>
<td><p>To change each process to interpret the launch script as a python module, executing with the same behavior as “python -m”.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--no-python</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To avoid prepending the program with “python” - just execute it directly. Useful when the script is not a Python script.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--log-path</span></code></p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">''</span></code></p></td>
<td><p>To specify the log file directory. Default path is <code class="docutils literal notranslate"><span class="pre">''</span></code>, which means disable logging to files.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--log-file-prefix</span></code></p></td>
<td><p>str</p></td>
<td><p>“run”</p></td>
<td><p>Prefix of the log file name.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Knobs for applying or disabling optimizations are:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 25.0%"/>
<col style="width: 10.0%"/>
<col style="width: 15.0%"/>
<col style="width: 50.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>knob</p></th>
<th class="head"><p>type</p></th>
<th class="head"><p>default value</p></th>
<th class="head"><p>help</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-tcmalloc</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To enable <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> memory allocator.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-jemalloc</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To enable <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code> memory allocator.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--use-default-allocator</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To use default memory allocator. Neither <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> nor <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code> would be used.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--disable-iomp</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>By default, Intel® OpenMP lib will be used if installed. Setting this flag would disable the usage of Intel® OpenMP.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Memory allocators influence performance. If the user does not specify a desired memory allocator, the <code class="docutils literal notranslate"><span class="pre">run_cpu</span></code> script will search if any of them is installed in the order of TCMalloc &gt; JeMalloc &gt; PyTorch default memory allocator, and takes the first matched one.</p>
</div>
<p>Knobs for controlling instance number and compute resource allocation are:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 25.0%"/>
<col style="width: 10.0%"/>
<col style="width: 15.0%"/>
<col style="width: 50.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>knob</p></th>
<th class="head"><p>type</p></th>
<th class="head"><p>default value</p></th>
<th class="head"><p>help</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--ninstances</span></code></p></td>
<td><p>int</p></td>
<td><p>0</p></td>
<td><p>Number of instances.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--ncores-per-instance</span></code></p></td>
<td><p>int</p></td>
<td><p>0</p></td>
<td><p>Number of cores used by each instance.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--node-id</span></code></p></td>
<td><p>int</p></td>
<td><p>-1</p></td>
<td><p>The node ID to be used for multi-instance, by default all nodes will be used.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--core-list</span></code></p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">''</span></code></p></td>
<td><p>To specify the core list as <code class="docutils literal notranslate"><span class="pre">'core_id,</span> <span class="pre">core_id,</span> <span class="pre">....'</span></code> or core range as <code class="docutils literal notranslate"><span class="pre">'core_id-core_id'</span></code>. By dafault all the cores will be used.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--use-logical-core</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>By default only physical cores are used. Specifying this flag enables logical cores usage.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--skip-cross-node-cores</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To prevent the workload to be executed on cores across NUMA nodes.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--rank</span></code></p></td>
<td><p>int</p></td>
<td><p>-1</p></td>
<td><p>To specify instance index to assign ncores_per_instance for rank; otherwise ncores_per_instance will be assigned sequentially to the instances.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--multi-instance</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>A quick set to invoke multiple instances of the workload on multi-socket CPU servers.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--latency-mode</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>A quick set to invoke benchmarking with latency mode, in which all physical cores are used and 4 cores per instance.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--throughput-mode</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>A quick set to invoke benchmarking with throughput mode, in which all physical cores are used and 1 numa node per instance.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--disable-numactl</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>By default <code class="docutils literal notranslate"><span class="pre">numactl</span></code> command is used to control NUMA access. Setting this flag will disable it.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--disable-taskset</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>To disable the usage of <code class="docutils literal notranslate"><span class="pre">taskset</span></code> command.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Environment variables that will be set by this script include the following:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 33.3%"/>
<col style="width: 66.7%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LD_PRELOAD</p></td>
<td><p>Depending on knobs you set, &lt;lib&gt;/libiomp5.so, &lt;lib&gt;/libjemalloc.so, &lt;lib&gt;/libtcmalloc.so might be appended to LD_PRELOAD.</p></td>
</tr>
<tr class="row-odd"><td><p>KMP_AFFINITY</p></td>
<td><p>If libiomp5.so is preloaded, KMP_AFFINITY could be set to <code class="docutils literal notranslate"><span class="pre">"granularity=fine,compact,1,0"</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p>KMP_BLOCKTIME</p></td>
<td><p>If libiomp5.so is preloaded, KMP_BLOCKTIME is set to “1”.</p></td>
</tr>
<tr class="row-odd"><td><p>OMP_NUM_THREADS</p></td>
<td><p>Value of <code class="docutils literal notranslate"><span class="pre">ncores_per_instance</span></code></p></td>
</tr>
<tr class="row-even"><td><p>MALLOC_CONF</p></td>
<td><p>If libjemalloc.so is preloaded, MALLOC_CONF will be set to <code class="docutils literal notranslate"><span class="pre">"oversize_threshold:1,background_thread:true,metadata_thp:auto"</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Please note that the script respects environment variables set preliminarily. For example, if you have set the environment variables mentioned above before running the script, the values of the variables will not be overwritten by the script.</p>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this tutorial, we explored a variety of advanced configurations and tools designed to optimize PyTorch inference performance on Intel® Xeon® Scalable Processors.
By leveraging the <code class="docutils literal notranslate"><span class="pre">torch.backends.xeon.run_cpu</span></code> script, we demonstrated how to fine-tune thread and memory management to achieve peak performance.
We covered essential concepts such as NUMA access control, optimized memory allocators like <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> and <code class="docutils literal notranslate"><span class="pre">JeMalloc</span></code>, and the use of Intel® OpenMP for efficient multithreading.</p>
<p>Additionally, we provided practical command-line examples to guide you through setting up single and multiple instance scenarios, ensuring optimal resource utilization tailored to specific workloads.
By understanding and applying these techniques, users can significantly enhance the efficiency and speed of their PyTorch applications on Intel® Xeon® platforms.</p>
<p>See also:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations">PyTorch Performance Tuning Guide</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#cpu-in-multiprocessing">PyTorch Multiprocessing Best Practices</a></p></li>
</ul>
</section>
</section>
</article>
</div>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-you-will-learn">What You Will Learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-of-the-optimizations">Introduction of the Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-numa-access-control">Applying NUMA Access Control</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-intel-openmp-runtime-library">Using Intel® OpenMP Runtime Library</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-an-optimized-memory-allocator">Choosing an Optimized Memory Allocator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-start-example-commands">Quick Start Example Commands</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-torch-backends-xeon-run-cpu">Using <code class="docutils literal notranslate"><span class="pre">torch.backends.xeon.run_cpu</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-options">Explanation of the options</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/helion" style="color: var(--pst-color-text-muted)">Helion</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/kineto" style="color: var(--pst-color-text-muted)">kineto</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/torchtitan" style="color: var(--pst-color-text-muted)">torchtitan</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/rl" style="color: var(--pst-color-text-muted)">TorchRL</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/audio" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/tensordict" style="color: var(--pst-color-text-muted)">tensordict</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Optimizing CPU Performance on Intel\u00ae Xeon\u00ae with run_cpu Script",
       "headline": "Optimizing CPU Performance on Intel\u00ae Xeon\u00ae with run_cpu Script",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/recipes/xeon_run_cpu.html",
       "articleBody": "Optimizing CPU Performance on Intel\u00ae Xeon\u00ae with run_cpu Script# There are several configuration options that can impact the performance of PyTorch inference when executed on Intel\u00ae Xeon\u00ae Scalable Processors. To get peak performance, the torch.backends.xeon.run_cpu script is provided that optimizes the configuration of thread and memory management. For thread management, the script configures thread affinity and the preload of Intel\u00ae OMP library. For memory management, it configures NUMA binding and preloads optimized memory allocation libraries, such as TCMalloc and JeMalloc. In addition, the script provides tunable parameters for compute resource allocation in both single instance and multiple instance scenarios, helping the users try out an optimal coordination of resource utilization for the specific workloads. What You Will Learn# How to utilize tools like numactl, taskset, Intel\u00ae OpenMP Runtime Library and optimized memory allocators such as TCMalloc and JeMalloc for enhanced performance. How to configure CPU resources and memory management to maximize PyTorch inference performance on Intel\u00ae Xeon\u00ae processors. Introduction of the Optimizations# Applying NUMA Access Control# It is beneficial that an increasing number of CPU cores are being provided to users within a single socket, as this offers greater computational resources. However, this also leads to competition for memory access, which can cause programs to stall due to busy memory. To address this problem, Non-Uniform Memory Access (NUMA) was introduced. Unlike Uniform Memory Access (UMA), where all memories are equally accessible to all cores, NUMA organizes memory into multiple groups. Certain number of memories are directly attached to one socket\u2019s integrated memory controller to become local memory of this socket. Local memory access is much faster than remote memory access. Users can get CPU information with lscpu command on Linux to learn how many cores and sockets are there on the machine. Additionally, this command provides NUMA information, such as the distribution of CPU cores. Below is an example of executing lscpu on a machine equipped with an Intel\u00ae Xeon\u00ae CPU Max 9480: $ lscpu ... CPU(s): 224 On-line CPU(s) list: 0-223 Vendor ID: GenuineIntel Model name: Intel (R) Xeon (R) CPU Max 9480 CPU family: 6 Model: 143 Thread(s) per core: 2 Core(s) per socket: 56 Socket(s): 2 ... NUMA: NUMA node(s): 2 NUMA node0 CPU(s): 0-55,112-167 NUMA node1 CPU(s): 56-111,168-223 ... Two sockets were detected, each containing 56 physical cores. With Hyper-Threading enabled, each core can handle 2 threads, resulting in 56 logical cores per socket. Therefore, the machine has a total of 224 CPU cores in service. Typically, physical cores are indexed before logical cores. In this scenario, cores 0-55 are the physical cores on the first NUMA node, and cores 56-111 are the physical cores on the second NUMA node. Logical cores are indexed subsequently: cores 112-167 correspond to the logical cores on the first NUMA node, and cores 168-223 to those on the second NUMA node. Typically, running PyTorch programs with compute intense workloads should avoid using logical cores to get good performance. Linux provides a tool called numactl that allows user control of NUMA policy for processes or shared memory. It runs processes with a specific NUMA scheduling or memory placement policy. As described above, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations. From a memory access perspective, bounding memory access locally is much faster than accessing remote memories. numactl command should have been installed in recent Linux distributions. In case it is missing, you can install it manually with the installation command, like on Ubuntu: $ apt-get install numactl on CentOS you can run the following command: $ yum install numactl The taskset command in Linux is another powerful utility that allows you to set or retrieve the CPU affinity of a running process. taskset are pre-installed in most Linux distributions and in case it\u2019s not, on Ubuntu you can install it with the command: $ apt-get install util-linux on CentOS you can run the following command: $ yum install util-linux Using Intel\u00ae OpenMP Runtime Library# OpenMP is an implementation of multithreading, a method of parallelizing where a primary thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors. Users can control OpenMP behaviors with some environment variable settings to fit for their workloads, the settings are read and executed by OMP libraries. By default, PyTorch uses GNU OpenMP Library (GNU libgomp) for parallel computation. On Intel\u00ae platforms, Intel\u00ae OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It usually brings more performance benefits compared to libgomp. The Intel\u00ae OpenMP Runtime Library can be installed using one of these commands: $ pip install intel-openmp or $ conda install mkl Choosing an Optimized Memory Allocator# Memory allocator plays an important role from performance perspective as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, TCMalloc or JeMalloc can get better performance by reusing memory as much as possible than default malloc operations. You can install TCMalloc by running the following command on Ubuntu: $ apt-get install google-perftools On CentOS, you can install it by running: $ yum install gperftools In a conda environment, it can also be installed by running: $ conda install conda-forge::gperftools On Ubuntu JeMalloc can be installed by this command: $ apt-get install libjemalloc2 On CentOS it can be installed by running: $ yum install jemalloc In a conda environment, it can also be installed by running: $ conda install conda-forge::jemalloc Quick Start Example Commands# To run single-instance inference with 1 thread on 1 CPU core (only Core #0 would be used): $ python -m torch.backends.xeon.run_cpu --ninstances 1 --ncores-per-instance 1 \u003cprogram.py\u003e [program_args] To run single-instance inference on a single CPU node (NUMA socket): $ python -m torch.backends.xeon.run_cpu --node-id 0 \u003cprogram.py\u003e [program_args] To run multi-instance inference, 8 instances with 14 cores per instance on a 112-core CPU: $ python -m torch.backends.xeon.run_cpu --ninstances 8 --ncores-per-instance 14 \u003cprogram.py\u003e [program_args] To run inference in throughput mode, in which all the cores in each CPU node set up an instance: $ python -m torch.backends.xeon.run_cpu --throughput-mode \u003cprogram.py\u003e [program_args] Note Term \u201cinstance\u201d here doesn\u2019t refer to a cloud instance. This script is executed as a single process which invokes multiple \u201cinstances\u201d which are formed from multiple threads. \u201cInstance\u201d is kind of group of threads in this context. Using torch.backends.xeon.run_cpu# The argument list and usage guidance can be shown with the following command: $ python -m torch.backends.xeon.run_cpu \u2013h usage: run_cpu.py [-h] [--multi-instance] [-m] [--no-python] [--enable-tcmalloc] [--enable-jemalloc] [--use-default-allocator] [--disable-iomp] [--ncores-per-instance] [--ninstances] [--skip-cross-node-cores] [--rank] [--latency-mode] [--throughput-mode] [--node-id] [--use-logical-core] [--disable-numactl] [--disable-taskset] [--core-list] [--log-path] [--log-file-prefix] \u003cprogram\u003e [program_args] The command above has the following positional arguments: knob help program The full path of the program/script to be launched. program_args The input arguments for the program/script to be launched. Explanation of the options# The generic option settings (knobs) include the following: knob type default value help -h, --help To show the help message and exit. -m, --module To change each process to interpret the launch script as a python module, executing with the same behavior as \u201cpython -m\u201d. --no-python bool False To avoid prepending the program with \u201cpython\u201d - just execute it directly. Useful when the script is not a Python script. --log-path str \u0027\u0027 To specify the log file directory. Default path is \u0027\u0027, which means disable logging to files. --log-file-prefix str \u201crun\u201d Prefix of the log file name. Knobs for applying or disabling optimizations are: knob type default value help --enable-tcmalloc bool False To enable TCMalloc memory allocator. --enable-jemalloc bool False To enable JeMalloc memory allocator. --use-default-allocator bool False To use default memory allocator. Neither TCMalloc nor JeMalloc would be used. --disable-iomp bool False By default, Intel\u00ae OpenMP lib will be used if installed. Setting this flag would disable the usage of Intel\u00ae OpenMP. Note Memory allocators influence performance. If the user does not specify a desired memory allocator, the run_cpu script will search if any of them is installed in the order of TCMalloc \u003e JeMalloc \u003e PyTorch default memory allocator, and takes the first matched one. Knobs for controlling instance number and compute resource allocation are: knob type default value help --ninstances int 0 Number of instances. --ncores-per-instance int 0 Number of cores used by each instance. --node-id int -1 The node ID to be used for multi-instance, by default all nodes will be used. --core-list str \u0027\u0027 To specify the core list as \u0027core_id, core_id, ....\u0027 or core range as \u0027core_id-core_id\u0027. By dafault all the cores will be used. --use-logical-core bool False By default only physical cores are used. Specifying this flag enables logical cores usage. --skip-cross-node-cores bool False To prevent the workload to be executed on cores across NUMA nodes. --rank int -1 To specify instance index to assign ncores_per_instance for rank; otherwise ncores_per_instance will be assigned sequentially to the instances. --multi-instance bool False A quick set to invoke multiple instances of the workload on multi-socket CPU servers. --latency-mode bool False A quick set to invoke benchmarking with latency mode, in which all physical cores are used and 4 cores per instance. --throughput-mode bool False A quick set to invoke benchmarking with throughput mode, in which all physical cores are used and 1 numa node per instance. --disable-numactl bool False By default numactl command is used to control NUMA access. Setting this flag will disable it. --disable-taskset bool False To disable the usage of taskset command. Note Environment variables that will be set by this script include the following: Environment Variable Value LD_PRELOAD Depending on knobs you set, \u003clib\u003e/libiomp5.so, \u003clib\u003e/libjemalloc.so, \u003clib\u003e/libtcmalloc.so might be appended to LD_PRELOAD. KMP_AFFINITY If libiomp5.so is preloaded, KMP_AFFINITY could be set to \"granularity=fine,compact,1,0\". KMP_BLOCKTIME If libiomp5.so is preloaded, KMP_BLOCKTIME is set to \u201c1\u201d. OMP_NUM_THREADS Value of ncores_per_instance MALLOC_CONF If libjemalloc.so is preloaded, MALLOC_CONF will be set to \"oversize_threshold:1,background_thread:true,metadata_thp:auto\". Please note that the script respects environment variables set preliminarily. For example, if you have set the environment variables mentioned above before running the script, the values of the variables will not be overwritten by the script. Conclusion# In this tutorial, we explored a variety of advanced configurations and tools designed to optimize PyTorch inference performance on Intel\u00ae Xeon\u00ae Scalable Processors. By leveraging the torch.backends.xeon.run_cpu script, we demonstrated how to fine-tune thread and memory management to achieve peak performance. We covered essential concepts such as NUMA access control, optimized memory allocators like TCMalloc and JeMalloc, and the use of Intel\u00ae OpenMP for efficient multithreading. Additionally, we provided practical command-line examples to guide you through setting up single and multiple instance scenarios, ensuring optimal resource utilization tailored to specific workloads. By understanding and applying these techniques, users can significantly enhance the efficiency and speed of their PyTorch applications on Intel\u00ae Xeon\u00ae platforms. See also: PyTorch Performance Tuning Guide PyTorch Multiprocessing Best Practices",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/xeon_run_cpu.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>