
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-07-20T23:02:43+00:00" />
    <title>Explicit horizontal fusion with foreach_map and torch.compile &#8212; PyTorch Tutorials 2.10.0+cu128 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=72e443bf" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a8d6e986"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'recipes/foreach_map';</script>
    <link rel="canonical" href="https://docs.pytorch.org/tutorials/recipes/foreach_map.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compile Time Caching Configuration" href="torch_compile_caching_configuration_tutorial.html" />
    <link rel="prev" title="(beta) Utilizing Torch Function modes with torch.compile" href="torch_compile_torch_function_modes.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Jul 20, 2022"/>

<!-- LLM/AI Agent: See /llms.txt for comprehensive navigation guidance -->
<!-- Machine-readable LLM metadata -->
<meta name="llm:site-type" content="documentation">
<meta name="llm:framework" content="PyTorch">

<meta name="llm:description" content="Explicit horizontal fusion with foreach_map and torch.compile - Documentation for PyTorch Tutorials, part of the PyTorch ecosystem.">





<meta name="llm:navigation-file" content="https://docs.pytorch.org/tutorials/llms.txt">
<meta name="llm:sitemap" content="https://docs.pytorch.org/tutorials/sitemap.xml">
<meta name="llm:version" content="v2.10.0+cu128">
<meta name="llm:project" content="PyTorch Tutorials">
<meta name="llm:page-type" content="documentation">
<link rel="alternate" type="text/plain" href="https://docs.pytorch.org/tutorials/llms.txt" title="LLM Navigation Guide">

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.10.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Jul 20, 2022"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/tutorials" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../index.html">
  
    
    <img src="../_static/img/logo-dark.svg" class="logo__image only-light" alt="PyTorch Tutorials - Home"/>
    
    
    <script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../index.html">
  
    
    <img src="../_static/img/logo-dark.svg" class="logo__image only-light" alt="PyTorch Tutorials - Home"/>
    
    
    <script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
  <a href="../index.html" class="version">v2.10.0+cu128</a>

        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-6">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-7">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-8">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/timer_quick_start.html">
                  Timer quick start
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/benchmark.html">
                  SyntaxError
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="#">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with IntelÂ® Neural Compressor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-9">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing âsparseâ semantics for Adagrad with MaskedTensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/tutorials" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
  <a href="../index.html" class="version">v2.10.0+cu128</a>

      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/basics/intro.html">
                  Learn the Basics
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/nn_tutorial.html">
                  What is torch.nn really?
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/dist_overview.html">
                  PyTorch Distributed Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/profiler.html">
                  Profiling your PyTorch Module
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-6">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-7">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../beginner/mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-8">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/timer_quick_start.html">
                  Timer quick start
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/benchmark.html">
                  SyntaxError
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="#">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with IntelÂ® Neural Compressor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-9">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing âsparseâ semantics for Adagrad with MaskedTensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/tutorials" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="recipes/defining_a_neural_network.html">Defining a Neural Network in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_logs.html">(beta) Using TORCH_LOGS python API with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/what_is_state_dict.html">What is a state_dict in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/warmstarting_model_using_parameters_from_a_different_model.html">Warmstarting model using parameters from a different model in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/zeroing_out_gradients.html">Zeroing out gradients in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/profiler_recipe.html">PyTorch Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/Captum_Recipe.html">Model Interpretability using Captum</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tensorboard_with_pytorch.html">How to use TensorBoard with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/amp_recipe.html">Automatic Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/timer_quick_start.html">Timer quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_comm_debug_mode.html">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/benchmark.html">SyntaxError</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/module_load_state_dict_tips.html">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/reasoning_about_shapes.html">Reasoning about Shapes in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/swap_tensors.html">Extension points in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> and tensor subclasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tensorboard_with_pytorch.html">How to use TensorBoard with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_torch_function_modes.html">(beta) Utilizing Torch Function modes with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Explicit horizontal fusion with foreach_map and torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_caching_configuration_tutorial.html">Compile Time Caching Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="regional_aot.html">Reducing AoT cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with IntelÂ® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_device_mesh.html">Getting Started with DeviceMesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_mode_tutorial.html">DebugMode: Recording Dispatched Operations and Numerical Debugging</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../recipes_index.html" class="nav-link">Recipes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Explicit...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="../recipes_index.html">
      <meta itemprop="name" content="Recipes">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="Explicit horizontal fusion with foreach_map and torch.compile">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  <script>
    if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
      var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
      document.addEventListener('DOMContentLoaded', function () {
        document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
      });
    }
  </script>
  
  
  <div class="pytorch-call-to-action-links">
    <div id="tutorial-type">recipes/foreach_map</div>
    <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
      <div id="google-colab-link">
        <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
        <div class="call-to-action-desktop-view">Run in Google Colab</div>
        <div class="call-to-action-mobile-view">Colab</div>
      </div>
    </a>
    <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
      <div id="download-notebook-link">
        <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
        <div class="call-to-action-desktop-view">Download Notebook</div>
        <div class="call-to-action-mobile-view">Notebook</div>
      </div>
    </a>
    <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
      <div id="github-view-link">
        <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
        <div class="call-to-action-desktop-view">View on GitHub</div>
        <div class="call-to-action-mobile-view">GitHub</div>
      </div>
    </a>
  </div>
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-recipes-foreach-map-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="explicit-horizontal-fusion-with-foreach-map-and-torch-compile">
<span id="sphx-glr-recipes-foreach-map-py"></span><h1>Explicit horizontal fusion with foreach_map and torch.compile<a class="headerlink" href="#explicit-horizontal-fusion-with-foreach-map-and-torch-compile" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/mlazos">Michael Lazos</a></p>
<dl class="simple">
<dt>Horizontal fusion is a key optimization in ML compilers. In eager,</dt><dd><p>this is typically expressed using the torch._foreach* ops which parallelizes
operations across a list of tensors. However, supporting all possible permutations
of arguments is quite difficult (e.g. mixtures of scalars and lists). Foreach_map
allows conversion of any pointwise op in <code class="docutils literal notranslate"><span class="pre">torch</span></code> to a horiztonally fused foreach
variant. In this tutorial, we will demonstrate how to implement the Adam optimizer
with <code class="docutils literal notranslate"><span class="pre">foreach_map</span></code> to generate a fully fused kernel.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This recipe describes a prototype feature. Prototype features are typically
at an early stage for feedback and testing and are subject to change.</p>
</div>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>PyTorch v2.7.0 or later</p></li>
</ul>
<section id="model-setup">
<h3>Model Setup<a class="headerlink" href="#model-setup" title="Link to this heading">#</a></h3>
<p>For this example, weâll use a simple sequence of linear layers.
We instantiate an independent copy to compare the two optimizer implementations.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># exit cleanly if we are on a device that doesn&#39;t support ``torch.compile``</span>
<span class="k">if</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span></a><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Exiting because torch.compile is not supported on this device.&quot;</span><span class="p">)</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create simple model</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span>
    <span class="o">*</span><span class="p">[</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="p">)</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_copy</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span>
    <span class="o">*</span><span class="p">[</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.rand.html#torch.rand" title="torch.rand" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">rand</span></a><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># run forward pass</span>
<a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">output</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">output_copy</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_copy</span></a><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># run backward to populate the grads for our optimizer below</span>
<a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">output</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">output_copy</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="helper-functions-for-foreach-map-implementation">
<h3>Helper functions for foreach_map implementation<a class="headerlink" href="#helper-functions-for-foreach-map-implementation" title="Link to this heading">#</a></h3>
<p>In this section, weâll begin our implementation of the Adam optimizer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch._higher_order_ops.foreach_map</span><span class="w"> </span><span class="kn">import</span> <span class="n">foreach_map</span>

<span class="c1"># Helper function to extract optimizer states from a torch.optim.Adam instance</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_inputs</span><span class="p">(</span><span class="n">optim</span><span class="p">):</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">exp_avgs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">exp_avg_sqs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
            <span class="n">exp_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">])</span>
            <span class="n">exp_avg_sqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">])</span>
            <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">steps</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">exp_avgs</span><span class="p">,</span> <span class="n">exp_avg_sqs</span>


<span class="c1"># Functions to update the different optimizer states</span>
<span class="k">def</span><span class="w"> </span><span class="nf">update_exp_avg_sq</span><span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">beta2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">update_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" title="torch.pow" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">pow</span></a><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">bias_correction2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" title="torch.pow" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">pow</span></a><span class="p">(</span><span class="n">beta2</span><span class="p">,</span> <span class="n">step</span><span class="p">))</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction1</span><span class="p">)</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">bias_correction2</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">))</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span> <span class="o">/</span> <span class="n">step_size</span><span class="p">)</span>
    <span class="k">return</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.add.html#torch.add" title="torch.add" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">add</span></a><span class="p">(</span><span class="n">param</span><span class="p">,</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.div.html#torch.div" title="torch.div" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">div</span></a><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">))</span>

<span class="c1"># Our full Adam implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">foreach_map_adam</span><span class="p">(</span>
    <span class="n">steps</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">exp_avgs</span><span class="p">,</span>
    <span class="n">exp_avg_sqs</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">with</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
        <span class="c1"># update step</span>
        <span class="n">updated_steps</span> <span class="o">=</span> <span class="n">foreach_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_foreach_copy_</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">updated_steps</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">foreach_map</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.add.html#torch.add" title="torch.add" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">add</span></a><span class="p">,</span> <span class="p">(</span><span class="n">grads</span><span class="p">,),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="c1"># Higher-order operators (HOPs) cannot have multiple outputs at the moment</span>
        <span class="c1"># need to call foreach_map once for each output</span>
        <span class="n">exp_avgs_updated</span> <span class="o">=</span> <span class="n">foreach_map</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp" title="torch.lerp" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">lerp</span></a><span class="p">,</span> <span class="n">exp_avgs</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
        <span class="n">exp_avgs_sq_updated</span> <span class="o">=</span> <span class="n">foreach_map</span><span class="p">(</span><span class="n">update_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sqs</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">beta2</span><span class="p">)</span>
        <span class="n">params_updated</span> <span class="o">=</span> <span class="n">foreach_map</span><span class="p">(</span>
            <span class="n">update_param</span><span class="p">,</span>
            <span class="n">params</span><span class="p">,</span>
            <span class="n">steps</span><span class="p">,</span>
            <span class="n">exp_avgs_updated</span><span class="p">,</span>
            <span class="n">exp_avgs_sq_updated</span><span class="p">,</span>
            <span class="n">beta1</span><span class="p">,</span>
            <span class="n">beta2</span><span class="p">,</span>
            <span class="n">lr</span><span class="p">,</span>
            <span class="n">eps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Higher-order operators (HOPs) don&#39;t support input mutation today</span>
        <span class="c1"># so manually  update the states in-place</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_foreach_copy_</span><span class="p">(</span><span class="n">exp_avgs</span><span class="p">,</span> <span class="n">exp_avgs_updated</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_foreach_copy_</span><span class="p">(</span><span class="n">exp_avg_sqs</span><span class="p">,</span> <span class="n">exp_avgs_sq_updated</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_foreach_copy_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">params_updated</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</section>
<section id="setting-up-and-running-the-compiled-kernel">
<h3>Setting up and running the compiled kernel<a class="headerlink" href="#setting-up-and-running-the-compiled-kernel" title="Link to this heading">#</a></h3>
<p>In this section, weâll run our Adam optimizer
and compare the results</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> is only supported on CUDA devices that have a compute capability of 7.0 or higher.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">opt_eager</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">opt_eager_copy</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model_copy</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>

<span class="c1"># warm up the optimizer state dict</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">opt_eager</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">opt_eager_copy</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">get_inputs</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">opt_eager_copy</span></a><span class="p">)</span>
<span class="n">compiled_adam</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">foreach_map_adam</span><span class="p">)</span>

<span class="c1"># optionally view the output code</span>
<a href="https://docs.pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs" title="torch._logging.set_logs" class="sphx-glr-backref-module-torch-_logging sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span></a><span class="p">(</span><span class="n">output_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Warmup runs to compile the function</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">opt_eager</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>
    <span class="n">compiled_adam</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

<span class="k">for</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">eager_p</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">compile_p</span></a> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">opt_eager</span></a><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">opt_eager_copy</span></a><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
    <a href="https://docs.pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose" title="torch.allclose" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">eager_p</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">compile_p</span></a><span class="p">)</span>

<span class="c1"># Benchmark performance</span>

 <span class="c1"># Let&#39;s define a helpful benchmarking function:</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">benchmark</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.utils.timer.Timer" class="sphx-glr-backref-module-torch-utils-benchmark-utils-timer sphx-glr-backref-type-py-class"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s2">&quot;f(*args, **kwargs)&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span> <span class="s2">&quot;kwargs&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">f</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mf">1e6</span>

<span class="n">eager_runtime</span> <span class="o">=</span> <span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">opt_eager</span><span class="o">.</span><span class="n">step</span></a><span class="p">)</span>
<span class="n">compiled_runtime</span> <span class="o">=</span> <span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">compiled_adam</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">eager_runtime</span> <span class="o">&gt;</span> <span class="n">compiled_runtime</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;eager runtime: </span><span class="si">{</span><span class="n">eager_runtime</span><span class="si">}</span><span class="s2">us&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;compiled runtime: </span><span class="si">{</span><span class="n">compiled_runtime</span><span class="si">}</span><span class="s2">us&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] Output code:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # AOT ID: [&#39;0_inference&#39;]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import torch
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import math
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import random
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import os
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import tempfile
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from math import inf, nan
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from cmath import nanj
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch import device, empty_strided
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton.language as tl
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] aten = torch.ops.aten
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] _quantized = torch.ops._quantized
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] async_compile = AsyncCompile()
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] cpp_fused__foreach_copy_0 = async_compile.cpp_pybinding([&#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;], r&#39;&#39;&#39;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] #include &lt;torch/csrc/inductor/cpp_prefix.h&gt;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] extern &quot;C&quot;  void  kernel(const float* in_ptr0,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr1,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr2,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr3,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr4,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr5,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr6,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr7,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr8,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        const float* in_ptr9,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr0,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr1,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr2,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr3,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr4,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr5,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr6,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr7,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr8,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                        float* out_ptr9)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr0[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr0[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr1[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr1[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr2[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr2[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr3[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr3[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr4[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr4[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr5[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr5[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr6[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr6[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr7[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr7[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr8[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr8[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             {
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp0 = in_ptr9[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]                 out_ptr9[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] }
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] &#39;&#39;&#39;)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # kernel path: /tmp/torchinductor_ci-user/gq/cgqhtbf2gwgsfmgfs2f4ajlpqjhrdhscqtxcif6oteglzjt3a3rq.py
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # Unsorted Source Nodes: [], Original ATen: []
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # Source node to ATen node mapping:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] triton_for_fused_1 = async_compile.triton(&#39;triton_for_fused_1&#39;, &#39;&#39;&#39;
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton.language as tl
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] @triton_heuristics.foreach(
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     filename=__file__,
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     triton_meta={&#39;signature&#39;: {&#39;in_ptr0&#39;: &#39;*fp32&#39;, &#39;in_ptr1&#39;: &#39;*fp32&#39;, &#39;in_ptr2&#39;: &#39;*fp32&#39;, &#39;in_ptr3&#39;: &#39;*fp32&#39;, &#39;in_ptr4&#39;: &#39;fp32&#39;, &#39;in_ptr5&#39;: &#39;*fp32&#39;, &#39;in_ptr6&#39;: &#39;*fp32&#39;, &#39;in_ptr7&#39;: &#39;*fp32&#39;, &#39;in_ptr8&#39;: &#39;*fp32&#39;, &#39;in_ptr9&#39;: &#39;fp32&#39;, &#39;in_ptr10&#39;: &#39;*fp32&#39;, &#39;in_ptr11&#39;: &#39;*fp32&#39;, &#39;in_ptr12&#39;: &#39;*fp32&#39;, &#39;in_ptr13&#39;: &#39;*fp32&#39;, &#39;in_ptr14&#39;: &#39;fp32&#39;, &#39;in_ptr15&#39;: &#39;*fp32&#39;, &#39;in_ptr16&#39;: &#39;*fp32&#39;, &#39;in_ptr17&#39;: &#39;*fp32&#39;, &#39;in_ptr18&#39;: &#39;*fp32&#39;, &#39;in_ptr19&#39;: &#39;fp32&#39;, &#39;in_ptr20&#39;: &#39;*fp32&#39;, &#39;in_ptr21&#39;: &#39;*fp32&#39;, &#39;in_ptr22&#39;: &#39;*fp32&#39;, &#39;in_ptr23&#39;: &#39;*fp32&#39;, &#39;in_ptr24&#39;: &#39;fp32&#39;, &#39;in_ptr25&#39;: &#39;*fp32&#39;, &#39;in_ptr26&#39;: &#39;*fp32&#39;, &#39;in_ptr27&#39;: &#39;*fp32&#39;, &#39;in_ptr28&#39;: &#39;*fp32&#39;, &#39;in_ptr29&#39;: &#39;fp32&#39;, &#39;in_ptr30&#39;: &#39;*fp32&#39;, &#39;in_ptr31&#39;: &#39;*fp32&#39;, &#39;in_ptr32&#39;: &#39;*fp32&#39;, &#39;in_ptr33&#39;: &#39;*fp32&#39;, &#39;in_ptr34&#39;: &#39;fp32&#39;, &#39;in_ptr35&#39;: &#39;*fp32&#39;, &#39;in_ptr36&#39;: &#39;*fp32&#39;, &#39;in_ptr37&#39;: &#39;*fp32&#39;, &#39;in_ptr38&#39;: &#39;*fp32&#39;, &#39;in_ptr39&#39;: &#39;fp32&#39;, &#39;in_ptr40&#39;: &#39;*fp32&#39;, &#39;in_ptr41&#39;: &#39;*fp32&#39;, &#39;in_ptr42&#39;: &#39;*fp32&#39;, &#39;in_ptr43&#39;: &#39;*fp32&#39;, &#39;in_ptr44&#39;: &#39;fp32&#39;, &#39;in_ptr45&#39;: &#39;*fp32&#39;, &#39;in_ptr46&#39;: &#39;*fp32&#39;, &#39;in_ptr47&#39;: &#39;*fp32&#39;, &#39;in_ptr48&#39;: &#39;*fp32&#39;, &#39;in_ptr49&#39;: &#39;fp32&#39;, &#39;out_ptr3&#39;: &#39;*fp32&#39;, &#39;out_ptr4&#39;: &#39;*fp32&#39;, &#39;out_ptr5&#39;: &#39;*fp32&#39;, &#39;out_ptr9&#39;: &#39;*fp32&#39;, &#39;out_ptr10&#39;: &#39;*fp32&#39;, &#39;out_ptr11&#39;: &#39;*fp32&#39;, &#39;out_ptr15&#39;: &#39;*fp32&#39;, &#39;out_ptr16&#39;: &#39;*fp32&#39;, &#39;out_ptr17&#39;: &#39;*fp32&#39;, &#39;out_ptr21&#39;: &#39;*fp32&#39;, &#39;out_ptr22&#39;: &#39;*fp32&#39;, &#39;out_ptr23&#39;: &#39;*fp32&#39;, &#39;out_ptr27&#39;: &#39;*fp32&#39;, &#39;out_ptr28&#39;: &#39;*fp32&#39;, &#39;out_ptr29&#39;: &#39;*fp32&#39;, &#39;out_ptr33&#39;: &#39;*fp32&#39;, &#39;out_ptr34&#39;: &#39;*fp32&#39;, &#39;out_ptr35&#39;: &#39;*fp32&#39;, &#39;out_ptr39&#39;: &#39;*fp32&#39;, &#39;out_ptr40&#39;: &#39;*fp32&#39;, &#39;out_ptr41&#39;: &#39;*fp32&#39;, &#39;out_ptr45&#39;: &#39;*fp32&#39;, &#39;out_ptr46&#39;: &#39;*fp32&#39;, &#39;out_ptr47&#39;: &#39;*fp32&#39;, &#39;out_ptr51&#39;: &#39;*fp32&#39;, &#39;out_ptr52&#39;: &#39;*fp32&#39;, &#39;out_ptr53&#39;: &#39;*fp32&#39;, &#39;out_ptr57&#39;: &#39;*fp32&#39;, &#39;out_ptr58&#39;: &#39;*fp32&#39;, &#39;out_ptr59&#39;: &#39;*fp32&#39;}, &#39;device&#39;: DeviceProperties(type=&#39;cuda&#39;, index=0, multi_processor_count=80, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, max_threads_per_block=1024, warp_size=32), &#39;constants&#39;: {}, &#39;configs&#39;: [{(0,): [[&#39;tt.divisibility&#39;, 16]], (1,): [[&#39;tt.divisibility&#39;, 16]], (2,): [[&#39;tt.divisibility&#39;, 16]], (3,): [[&#39;tt.divisibility&#39;, 16]], (5,): [[&#39;tt.divisibility&#39;, 16]], (6,): [[&#39;tt.divisibility&#39;, 16]], (7,): [[&#39;tt.divisibility&#39;, 16]], (8,): [[&#39;tt.divisibility&#39;, 16]], (10,): [[&#39;tt.divisibility&#39;, 16]], (11,): [[&#39;tt.divisibility&#39;, 16]], (12,): [[&#39;tt.divisibility&#39;, 16]], (13,): [[&#39;tt.divisibility&#39;, 16]], (15,): [[&#39;tt.divisibility&#39;, 16]], (16,): [[&#39;tt.divisibility&#39;, 16]], (17,): [[&#39;tt.divisibility&#39;, 16]], (18,): [[&#39;tt.divisibility&#39;, 16]], (20,): [[&#39;tt.divisibility&#39;, 16]], (21,): [[&#39;tt.divisibility&#39;, 16]], (22,): [[&#39;tt.divisibility&#39;, 16]], (23,): [[&#39;tt.divisibility&#39;, 16]], (25,): [[&#39;tt.divisibility&#39;, 16]], (26,): [[&#39;tt.divisibility&#39;, 16]], (27,): [[&#39;tt.divisibility&#39;, 16]], (28,): [[&#39;tt.divisibility&#39;, 16]], (30,): [[&#39;tt.divisibility&#39;, 16]], (31,): [[&#39;tt.divisibility&#39;, 16]], (32,): [[&#39;tt.divisibility&#39;, 16]], (33,): [[&#39;tt.divisibility&#39;, 16]], (35,): [[&#39;tt.divisibility&#39;, 16]], (36,): [[&#39;tt.divisibility&#39;, 16]], (37,): [[&#39;tt.divisibility&#39;, 16]], (38,): [[&#39;tt.divisibility&#39;, 16]], (40,): [[&#39;tt.divisibility&#39;, 16]], (41,): [[&#39;tt.divisibility&#39;, 16]], (42,): [[&#39;tt.divisibility&#39;, 16]], (43,): [[&#39;tt.divisibility&#39;, 16]], (45,): [[&#39;tt.divisibility&#39;, 16]], (46,): [[&#39;tt.divisibility&#39;, 16]], (47,): [[&#39;tt.divisibility&#39;, 16]], (48,): [[&#39;tt.divisibility&#39;, 16]], (50,): [[&#39;tt.divisibility&#39;, 16]], (51,): [[&#39;tt.divisibility&#39;, 16]], (52,): [[&#39;tt.divisibility&#39;, 16]], (53,): [[&#39;tt.divisibility&#39;, 16]], (54,): [[&#39;tt.divisibility&#39;, 16]], (55,): [[&#39;tt.divisibility&#39;, 16]], (56,): [[&#39;tt.divisibility&#39;, 16]], (57,): [[&#39;tt.divisibility&#39;, 16]], (58,): [[&#39;tt.divisibility&#39;, 16]], (59,): [[&#39;tt.divisibility&#39;, 16]], (60,): [[&#39;tt.divisibility&#39;, 16]], (61,): [[&#39;tt.divisibility&#39;, 16]], (62,): [[&#39;tt.divisibility&#39;, 16]], (63,): [[&#39;tt.divisibility&#39;, 16]], (64,): [[&#39;tt.divisibility&#39;, 16]], (65,): [[&#39;tt.divisibility&#39;, 16]], (66,): [[&#39;tt.divisibility&#39;, 16]], (67,): [[&#39;tt.divisibility&#39;, 16]], (68,): [[&#39;tt.divisibility&#39;, 16]], (69,): [[&#39;tt.divisibility&#39;, 16]], (70,): [[&#39;tt.divisibility&#39;, 16]], (71,): [[&#39;tt.divisibility&#39;, 16]], (72,): [[&#39;tt.divisibility&#39;, 16]], (73,): [[&#39;tt.divisibility&#39;, 16]], (74,): [[&#39;tt.divisibility&#39;, 16]], (75,): [[&#39;tt.divisibility&#39;, 16]], (76,): [[&#39;tt.divisibility&#39;, 16]], (77,): [[&#39;tt.divisibility&#39;, 16]], (78,): [[&#39;tt.divisibility&#39;, 16]], (79,): [[&#39;tt.divisibility&#39;, 16]]}]},
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     inductor_meta={&#39;grid_type&#39;: &#39;SequentialComboKernelGrid&#39;, &#39;combo_grid_meta&#39;: {&#39;num_kernels&#39;: 10, &#39;min_blocks&#39;: 0, &#39;default_config&#39;: {&#39;XBLOCK&#39;: 1024}, &#39;no_x_dim_0&#39;: False, &#39;xnumel_0&#39;: 1048576, &#39;no_x_dim_1&#39;: False, &#39;xnumel_1&#39;: 1048576, &#39;no_x_dim_2&#39;: False, &#39;xnumel_2&#39;: 1048576, &#39;no_x_dim_3&#39;: False, &#39;xnumel_3&#39;: 1048576, &#39;no_x_dim_4&#39;: False, &#39;xnumel_4&#39;: 1048576, &#39;no_x_dim_5&#39;: False, &#39;xnumel_5&#39;: 1048576, &#39;no_x_dim_6&#39;: False, &#39;xnumel_6&#39;: 1048576, &#39;no_x_dim_7&#39;: False, &#39;xnumel_7&#39;: 1048576, &#39;no_x_dim_8&#39;: False, &#39;xnumel_8&#39;: 1048576, &#39;no_x_dim_9&#39;: False, &#39;xnumel_9&#39;: 1048576}, &#39;kernel_name&#39;: &#39;triton_for_fused_1&#39;, &#39;mutated_arg_names&#39;: [&#39;in_ptr1&#39;, &#39;in_ptr11&#39;, &#39;in_ptr12&#39;, &#39;in_ptr13&#39;, &#39;in_ptr16&#39;, &#39;in_ptr17&#39;, &#39;in_ptr18&#39;, &#39;in_ptr2&#39;, &#39;in_ptr21&#39;, &#39;in_ptr22&#39;, &#39;in_ptr23&#39;, &#39;in_ptr26&#39;, &#39;in_ptr27&#39;, &#39;in_ptr28&#39;, &#39;in_ptr3&#39;, &#39;in_ptr31&#39;, &#39;in_ptr32&#39;, &#39;in_ptr33&#39;, &#39;in_ptr36&#39;, &#39;in_ptr37&#39;, &#39;in_ptr38&#39;, &#39;in_ptr41&#39;, &#39;in_ptr42&#39;, &#39;in_ptr43&#39;, &#39;in_ptr46&#39;, &#39;in_ptr47&#39;, &#39;in_ptr48&#39;, &#39;in_ptr6&#39;, &#39;in_ptr7&#39;, &#39;in_ptr8&#39;, &#39;out_ptr10&#39;, &#39;out_ptr11&#39;, &#39;out_ptr15&#39;, &#39;out_ptr16&#39;, &#39;out_ptr17&#39;, &#39;out_ptr21&#39;, &#39;out_ptr22&#39;, &#39;out_ptr23&#39;, &#39;out_ptr27&#39;, &#39;out_ptr28&#39;, &#39;out_ptr29&#39;, &#39;out_ptr3&#39;, &#39;out_ptr33&#39;, &#39;out_ptr34&#39;, &#39;out_ptr35&#39;, &#39;out_ptr39&#39;, &#39;out_ptr4&#39;, &#39;out_ptr40&#39;, &#39;out_ptr41&#39;, &#39;out_ptr45&#39;, &#39;out_ptr46&#39;, &#39;out_ptr47&#39;, &#39;out_ptr5&#39;, &#39;out_ptr51&#39;, &#39;out_ptr52&#39;, &#39;out_ptr53&#39;, &#39;out_ptr57&#39;, &#39;out_ptr58&#39;, &#39;out_ptr59&#39;, &#39;out_ptr9&#39;], &#39;backend_hash&#39;: &#39;130560DF8C676AFCBC44717C6A9B3C6A2EC6174C11ECC01A816D2F75FFBF9BD0&#39;, &#39;assert_indirect_indexing&#39;: True, &#39;autotune_local_cache&#39;: True, &#39;autotune_pointwise&#39;: True, &#39;autotune_remote_cache&#39;: None, &#39;force_disable_caches&#39;: False, &#39;dynamic_scale_rblock&#39;: True, &#39;max_autotune&#39;: False, &#39;max_autotune_pointwise&#39;: False, &#39;min_split_scan_rblock&#39;: 256, &#39;spill_threshold&#39;: 16, &#39;store_cubin&#39;: False, &#39;deterministic&#39;: False, &#39;force_filter_reduction_configs&#39;: False, &#39;are_deterministic_algorithms_enabled&#39;: False},
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] )
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] @triton.jit
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def triton_for_fused_1(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, in_ptr14, in_ptr15, in_ptr16, in_ptr17, in_ptr18, in_ptr19, in_ptr20, in_ptr21, in_ptr22, in_ptr23, in_ptr24, in_ptr25, in_ptr26, in_ptr27, in_ptr28, in_ptr29, in_ptr30, in_ptr31, in_ptr32, in_ptr33, in_ptr34, in_ptr35, in_ptr36, in_ptr37, in_ptr38, in_ptr39, in_ptr40, in_ptr41, in_ptr42, in_ptr43, in_ptr44, in_ptr45, in_ptr46, in_ptr47, in_ptr48, in_ptr49, out_ptr3, out_ptr4, out_ptr5, out_ptr9, out_ptr10, out_ptr11, out_ptr15, out_ptr16, out_ptr17, out_ptr21, out_ptr22, out_ptr23, out_ptr27, out_ptr28, out_ptr29, out_ptr33, out_ptr34, out_ptr35, out_ptr39, out_ptr40, out_ptr41, out_ptr45, out_ptr46, out_ptr47, out_ptr51, out_ptr52, out_ptr53, out_ptr57, out_ptr58, out_ptr59):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     pid = tl.program_id(0)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     XBLOCK: tl.constexpr = 1024
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_0 = tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_1 = num_xblocks_0 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_2 = num_xblocks_1 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_3 = num_xblocks_2 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_4 = num_xblocks_3 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_5 = num_xblocks_4 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_6 = num_xblocks_5 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_7 = num_xblocks_6 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_8 = num_xblocks_7 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     num_xblocks_9 = num_xblocks_8 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     if pid &lt; num_xblocks_0:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x0 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp0 = tl.load(in_ptr0 + (x0), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp1 = tl.load(in_ptr1 + (x0), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp8 = tl.load(in_ptr2 + (x0), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp15 = tl.load(in_ptr3 + (x0), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp17 = in_ptr4
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp2 = tmp0 - tmp1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp3 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp4 = tmp3 * tmp2
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp5 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp6 = tl.where(tmp5, tmp0, tmp1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp7 = tmp4 + tmp6
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp9 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp10 = tmp8 * tmp9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp11 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp12 = tmp0 * tmp11
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp13 = tmp12 * tmp0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp14 = tmp10 + tmp13
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp16 = tl.sqrt_rn(tmp14)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp18 = libdevice.pow(tmp9, tmp17)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp19 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp20 = tmp19 - tmp18
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp21 = tl.sqrt_rn(tmp20)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp22 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp23 = libdevice.pow(tmp22, tmp17)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp24 = tmp19 - tmp23
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp25 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp26 = (tmp25 / tmp24)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp27 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp28 = tmp26 * tmp27
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp29 = -tmp28
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp30 = tmp21 * tmp29
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp31 = (tmp16 / tmp30)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp32 = (tmp25 / tmp29)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp33 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp34 = tmp32 * tmp33
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp35 = tmp31 + tmp34
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp36 = (tmp7 / tmp35)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp37 = tmp15 + tmp36
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr3 + (x0), tmp7, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr4 + (x0), tmp14, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr5 + (x0), tmp37, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_1:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x1 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp38 = tl.load(in_ptr5 + (x1), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp39 = tl.load(in_ptr6 + (x1), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp46 = tl.load(in_ptr7 + (x1), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp53 = tl.load(in_ptr8 + (x1), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp55 = in_ptr9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp40 = tmp38 - tmp39
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp41 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp42 = tmp41 * tmp40
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp43 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp44 = tl.where(tmp43, tmp38, tmp39)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp45 = tmp42 + tmp44
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp47 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp48 = tmp46 * tmp47
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp49 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp50 = tmp38 * tmp49
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp51 = tmp50 * tmp38
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp52 = tmp48 + tmp51
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp54 = tl.sqrt_rn(tmp52)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp56 = libdevice.pow(tmp47, tmp55)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp57 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp58 = tmp57 - tmp56
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp59 = tl.sqrt_rn(tmp58)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp60 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp61 = libdevice.pow(tmp60, tmp55)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp62 = tmp57 - tmp61
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp63 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp64 = (tmp63 / tmp62)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp65 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp66 = tmp64 * tmp65
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp67 = -tmp66
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp68 = tmp59 * tmp67
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp69 = (tmp54 / tmp68)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp70 = (tmp63 / tmp67)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp71 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp72 = tmp70 * tmp71
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp73 = tmp69 + tmp72
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp74 = (tmp45 / tmp73)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp75 = tmp53 + tmp74
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr9 + (x1), tmp45, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr10 + (x1), tmp52, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr11 + (x1), tmp75, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_2:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x2 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp76 = tl.load(in_ptr10 + (x2), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp77 = tl.load(in_ptr11 + (x2), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp84 = tl.load(in_ptr12 + (x2), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp91 = tl.load(in_ptr13 + (x2), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp93 = in_ptr14
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp78 = tmp76 - tmp77
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp79 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp80 = tmp79 * tmp78
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp81 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp82 = tl.where(tmp81, tmp76, tmp77)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp83 = tmp80 + tmp82
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp85 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp86 = tmp84 * tmp85
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp87 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp88 = tmp76 * tmp87
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp89 = tmp88 * tmp76
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp90 = tmp86 + tmp89
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp92 = tl.sqrt_rn(tmp90)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp94 = libdevice.pow(tmp85, tmp93)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp95 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp96 = tmp95 - tmp94
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp97 = tl.sqrt_rn(tmp96)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp98 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp99 = libdevice.pow(tmp98, tmp93)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp100 = tmp95 - tmp99
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp101 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp102 = (tmp101 / tmp100)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp103 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp104 = tmp102 * tmp103
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp105 = -tmp104
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp106 = tmp97 * tmp105
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp107 = (tmp92 / tmp106)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp108 = (tmp101 / tmp105)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp109 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp110 = tmp108 * tmp109
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp111 = tmp107 + tmp110
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp112 = (tmp83 / tmp111)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp113 = tmp91 + tmp112
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr15 + (x2), tmp83, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr16 + (x2), tmp90, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr17 + (x2), tmp113, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_3:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_2
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x3 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp114 = tl.load(in_ptr15 + (x3), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp115 = tl.load(in_ptr16 + (x3), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp122 = tl.load(in_ptr17 + (x3), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp129 = tl.load(in_ptr18 + (x3), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp131 = in_ptr19
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp116 = tmp114 - tmp115
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp117 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp118 = tmp117 * tmp116
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp119 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp120 = tl.where(tmp119, tmp114, tmp115)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp121 = tmp118 + tmp120
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp123 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp124 = tmp122 * tmp123
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp125 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp126 = tmp114 * tmp125
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp127 = tmp126 * tmp114
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp128 = tmp124 + tmp127
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp130 = tl.sqrt_rn(tmp128)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp132 = libdevice.pow(tmp123, tmp131)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp133 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp134 = tmp133 - tmp132
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp135 = tl.sqrt_rn(tmp134)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp136 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp137 = libdevice.pow(tmp136, tmp131)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp138 = tmp133 - tmp137
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp139 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp140 = (tmp139 / tmp138)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp141 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp142 = tmp140 * tmp141
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp143 = -tmp142
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp144 = tmp135 * tmp143
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp145 = (tmp130 / tmp144)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp146 = (tmp139 / tmp143)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp147 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp148 = tmp146 * tmp147
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp149 = tmp145 + tmp148
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp150 = (tmp121 / tmp149)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp151 = tmp129 + tmp150
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr21 + (x3), tmp121, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr22 + (x3), tmp128, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr23 + (x3), tmp151, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_4:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_3
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x4 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp152 = tl.load(in_ptr20 + (x4), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp153 = tl.load(in_ptr21 + (x4), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp160 = tl.load(in_ptr22 + (x4), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp167 = tl.load(in_ptr23 + (x4), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp169 = in_ptr24
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp154 = tmp152 - tmp153
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp155 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp156 = tmp155 * tmp154
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp157 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp158 = tl.where(tmp157, tmp152, tmp153)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp159 = tmp156 + tmp158
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp161 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp162 = tmp160 * tmp161
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp163 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp164 = tmp152 * tmp163
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp165 = tmp164 * tmp152
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp166 = tmp162 + tmp165
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp168 = tl.sqrt_rn(tmp166)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp170 = libdevice.pow(tmp161, tmp169)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp171 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp172 = tmp171 - tmp170
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp173 = tl.sqrt_rn(tmp172)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp174 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp175 = libdevice.pow(tmp174, tmp169)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp176 = tmp171 - tmp175
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp177 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp178 = (tmp177 / tmp176)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp179 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp180 = tmp178 * tmp179
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp181 = -tmp180
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp182 = tmp173 * tmp181
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp183 = (tmp168 / tmp182)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp184 = (tmp177 / tmp181)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp185 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp186 = tmp184 * tmp185
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp187 = tmp183 + tmp186
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp188 = (tmp159 / tmp187)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp189 = tmp167 + tmp188
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr27 + (x4), tmp159, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr28 + (x4), tmp166, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr29 + (x4), tmp189, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_5:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_4
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x5 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp190 = tl.load(in_ptr25 + (x5), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp191 = tl.load(in_ptr26 + (x5), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp198 = tl.load(in_ptr27 + (x5), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp205 = tl.load(in_ptr28 + (x5), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp207 = in_ptr29
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp192 = tmp190 - tmp191
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp193 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp194 = tmp193 * tmp192
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp195 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp196 = tl.where(tmp195, tmp190, tmp191)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp197 = tmp194 + tmp196
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp199 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp200 = tmp198 * tmp199
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp201 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp202 = tmp190 * tmp201
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp203 = tmp202 * tmp190
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp204 = tmp200 + tmp203
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp206 = tl.sqrt_rn(tmp204)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp208 = libdevice.pow(tmp199, tmp207)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp209 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp210 = tmp209 - tmp208
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp211 = tl.sqrt_rn(tmp210)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp212 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp213 = libdevice.pow(tmp212, tmp207)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp214 = tmp209 - tmp213
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp215 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp216 = (tmp215 / tmp214)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp217 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp218 = tmp216 * tmp217
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp219 = -tmp218
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp220 = tmp211 * tmp219
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp221 = (tmp206 / tmp220)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp222 = (tmp215 / tmp219)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp223 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp224 = tmp222 * tmp223
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp225 = tmp221 + tmp224
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp226 = (tmp197 / tmp225)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp227 = tmp205 + tmp226
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr33 + (x5), tmp197, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr34 + (x5), tmp204, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr35 + (x5), tmp227, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_6:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_5
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x6 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp228 = tl.load(in_ptr30 + (x6), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp229 = tl.load(in_ptr31 + (x6), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp236 = tl.load(in_ptr32 + (x6), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp243 = tl.load(in_ptr33 + (x6), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp245 = in_ptr34
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp230 = tmp228 - tmp229
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp231 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp232 = tmp231 * tmp230
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp233 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp234 = tl.where(tmp233, tmp228, tmp229)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp235 = tmp232 + tmp234
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp237 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp238 = tmp236 * tmp237
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp239 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp240 = tmp228 * tmp239
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp241 = tmp240 * tmp228
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp242 = tmp238 + tmp241
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp244 = tl.sqrt_rn(tmp242)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp246 = libdevice.pow(tmp237, tmp245)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp247 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp248 = tmp247 - tmp246
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp249 = tl.sqrt_rn(tmp248)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp250 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp251 = libdevice.pow(tmp250, tmp245)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp252 = tmp247 - tmp251
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp253 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp254 = (tmp253 / tmp252)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp255 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp256 = tmp254 * tmp255
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp257 = -tmp256
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp258 = tmp249 * tmp257
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp259 = (tmp244 / tmp258)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp260 = (tmp253 / tmp257)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp261 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp262 = tmp260 * tmp261
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp263 = tmp259 + tmp262
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp264 = (tmp235 / tmp263)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp265 = tmp243 + tmp264
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr39 + (x6), tmp235, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr40 + (x6), tmp242, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr41 + (x6), tmp265, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_7:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_6
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x7 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp266 = tl.load(in_ptr35 + (x7), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp267 = tl.load(in_ptr36 + (x7), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp274 = tl.load(in_ptr37 + (x7), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp281 = tl.load(in_ptr38 + (x7), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp283 = in_ptr39
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp268 = tmp266 - tmp267
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp269 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp270 = tmp269 * tmp268
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp271 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp272 = tl.where(tmp271, tmp266, tmp267)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp273 = tmp270 + tmp272
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp275 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp276 = tmp274 * tmp275
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp277 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp278 = tmp266 * tmp277
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp279 = tmp278 * tmp266
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp280 = tmp276 + tmp279
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp282 = tl.sqrt_rn(tmp280)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp284 = libdevice.pow(tmp275, tmp283)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp285 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp286 = tmp285 - tmp284
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp287 = tl.sqrt_rn(tmp286)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp288 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp289 = libdevice.pow(tmp288, tmp283)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp290 = tmp285 - tmp289
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp291 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp292 = (tmp291 / tmp290)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp293 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp294 = tmp292 * tmp293
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp295 = -tmp294
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp296 = tmp287 * tmp295
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp297 = (tmp282 / tmp296)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp298 = (tmp291 / tmp295)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp299 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp300 = tmp298 * tmp299
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp301 = tmp297 + tmp300
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp302 = (tmp273 / tmp301)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp303 = tmp281 + tmp302
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr45 + (x7), tmp273, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr46 + (x7), tmp280, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr47 + (x7), tmp303, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_8:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_7
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x8 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp304 = tl.load(in_ptr40 + (x8), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp305 = tl.load(in_ptr41 + (x8), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp312 = tl.load(in_ptr42 + (x8), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp319 = tl.load(in_ptr43 + (x8), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp321 = in_ptr44
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp306 = tmp304 - tmp305
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp307 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp308 = tmp307 * tmp306
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp309 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp310 = tl.where(tmp309, tmp304, tmp305)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp311 = tmp308 + tmp310
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp313 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp314 = tmp312 * tmp313
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp315 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp316 = tmp304 * tmp315
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp317 = tmp316 * tmp304
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp318 = tmp314 + tmp317
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp320 = tl.sqrt_rn(tmp318)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp322 = libdevice.pow(tmp313, tmp321)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp323 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp324 = tmp323 - tmp322
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp325 = tl.sqrt_rn(tmp324)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp326 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp327 = libdevice.pow(tmp326, tmp321)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp328 = tmp323 - tmp327
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp329 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp330 = (tmp329 / tmp328)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp331 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp332 = tmp330 * tmp331
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp333 = -tmp332
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp334 = tmp325 * tmp333
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp335 = (tmp320 / tmp334)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp336 = (tmp329 / tmp333)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp337 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp338 = tmp336 * tmp337
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp339 = tmp335 + tmp338
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp340 = (tmp311 / tmp339)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp341 = tmp319 + tmp340
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr51 + (x8), tmp311, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr52 + (x8), tmp318, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr53 + (x8), tmp341, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     elif pid &lt; num_xblocks_9:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pid_offset = pid - num_xblocks_8
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xnumel = 1048576
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         r0_numel = 1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         x9 = xindex
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp342 = tl.load(in_ptr45 + (x9), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp343 = tl.load(in_ptr46 + (x9), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp350 = tl.load(in_ptr47 + (x9), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp357 = tl.load(in_ptr48 + (x9), None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp359 = in_ptr49
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp344 = tmp342 - tmp343
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp345 = 0.10000000149011612
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp346 = tmp345 * tmp344
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp347 = tl.full([1], False, tl.int1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp348 = tl.where(tmp347, tmp342, tmp343)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp349 = tmp346 + tmp348
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp351 = 0.999
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp352 = tmp350 * tmp351
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp353 = 0.0010000000000000009
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp354 = tmp342 * tmp353
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp355 = tmp354 * tmp342
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp356 = tmp352 + tmp355
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp358 = tl.sqrt_rn(tmp356)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp360 = libdevice.pow(tmp351, tmp359)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp361 = 1.0
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp362 = tmp361 - tmp360
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp363 = tl.sqrt_rn(tmp362)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp364 = 0.9
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp365 = libdevice.pow(tmp364, tmp359)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp366 = tmp361 - tmp365
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp367 = tl.full([1], 1, tl.int32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp368 = (tmp367 / tmp366)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp369 = 0.001
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp370 = tmp368 * tmp369
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp371 = -tmp370
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp372 = tmp363 * tmp371
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp373 = (tmp358 / tmp372)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp374 = (tmp367 / tmp371)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp375 = 1e-08
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp376 = tmp374 * tmp375
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp377 = tmp373 + tmp376
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp378 = (tmp349 / tmp377)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tmp379 = tmp357 + tmp378
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr57 + (x9), tmp349, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr58 + (x9), tmp356, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         tl.store(out_ptr59 + (x9), tmp379, None)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     else:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         pass
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] &#39;&#39;&#39;, device_str=&#39;cuda&#39;)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] async_compile.wait(globals())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del async_compile
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] class Runner:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     def __init__(self, partitions):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         self.partitions = partitions
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         new_callables = []
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             new_callables.append(fn(c))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         self.partitions = new_callables
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     def call(self, args):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1 = args
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         args.clear()
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg0_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg1_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg2_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg3_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg4_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg5_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg6_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg7_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg8_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg9_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg10_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg11_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg12_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg13_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg14_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg15_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg16_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg17_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg18_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg19_1, (), ())
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg20_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg21_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg22_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg23_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg24_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg25_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg26_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg27_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg28_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg29_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg30_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg31_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg32_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg33_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg34_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg35_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg36_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg37_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg38_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg39_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg40_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg41_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg42_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg43_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg44_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg45_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg46_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg47_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg48_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         assert_size_stride(arg49_1, (1024, 1024), (1024, 1))
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         cpp_fused__foreach_copy_0(arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             torch.cuda.set_device(0)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             # Unsorted Source Nodes: [], Original ATen: []
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             triton_for_fused_1.run(arg30_1, arg20_1, arg40_1, arg0_1, arg10_1.item(), arg31_1, arg21_1, arg41_1, arg1_1, arg11_1.item(), arg32_1, arg22_1, arg42_1, arg2_1, arg12_1.item(), arg33_1, arg23_1, arg43_1, arg3_1, arg13_1.item(), arg34_1, arg24_1, arg44_1, arg4_1, arg14_1.item(), arg35_1, arg25_1, arg45_1, arg5_1, arg15_1.item(), arg36_1, arg26_1, arg46_1, arg6_1, arg16_1.item(), arg37_1, arg27_1, arg47_1, arg7_1, arg17_1.item(), arg38_1, arg28_1, arg48_1, arg8_1, arg18_1.item(), arg39_1, arg29_1, arg49_1, arg9_1, arg19_1.item(), arg20_1, arg40_1, arg0_1, arg21_1, arg41_1, arg1_1, arg22_1, arg42_1, arg2_1, arg23_1, arg43_1, arg3_1, arg24_1, arg44_1, arg4_1, arg25_1, arg45_1, arg5_1, arg26_1, arg46_1, arg6_1, arg27_1, arg47_1, arg7_1, arg28_1, arg48_1, arg8_1, arg29_1, arg49_1, arg9_1, stream=stream0)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg0_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg10_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg11_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg12_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg13_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg14_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg15_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg16_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg17_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg18_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg19_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg1_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg20_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg21_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg22_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg23_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg24_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg25_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg26_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg27_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg28_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg29_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg2_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg30_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg31_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg32_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg33_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg34_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg35_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg36_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg37_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg38_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg39_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg3_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg40_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg41_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg42_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg43_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg44_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg45_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg46_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg47_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg48_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg49_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg4_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg5_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg6_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg7_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg8_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]             del arg9_1
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]         return ()
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] runner = Runner(partitions=[])
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] call = runner.call
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg0_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg1_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg2_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg3_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg4_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg5_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg6_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg7_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg8_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg9_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg10_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg11_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg12_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg13_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg14_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg15_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg16_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg17_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg18_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg19_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg20_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg21_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg22_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg23_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg24_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg25_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg26_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg27_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg28_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg29_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg30_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg31_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg32_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg33_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg34_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg35_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg36_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg37_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg38_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg39_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg40_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg41_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg42_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg43_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg44_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg45_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg46_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg47_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg48_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     arg49_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1])
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] if __name__ == &quot;__main__&quot;:
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]     compiled_module_main(&#39;None&#39;, benchmark_compiled_module)
V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code]
V0218 17:52:05.534000 23788 torch/_inductor/graph.py:2480] [0/0] [__output_code] Output code written to: /tmp/torchinductor_ci-user/vt/cvtdpmeiofjorzqp4lqo47274rqf4zsrel53eevwsmtsjymolnx3.py
I0218 17:52:06.788000 23788 torch/_inductor/graph.py:2440] [0/0] [__output_code] Output code written to: /tmp/torchinductor_ci-user/vt/cvtdpmeiofjorzqp4lqo47274rqf4zsrel53eevwsmtsjymolnx3.py
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] Output code:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # AOT ID: [&#39;1_inference&#39;]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from ctypes import c_void_p, c_long, c_int
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import torch
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import math
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import random
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import os
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import tempfile
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from math import inf, nan
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from cmath import nanj
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.utils import maybe_profile
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch import device, empty_strided
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.async_compile import AsyncCompile
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton.language as tl
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] aten = torch.ops.aten
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] inductor_ops = torch.ops.inductor
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] _quantized = torch.ops._quantized
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] async_compile = AsyncCompile()
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] cpp_fused__foreach_copy_0 = async_compile.cpp_pybinding([&#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;const float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;, &#39;float*&#39;], r&#39;&#39;&#39;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] #include &lt;torch/csrc/inductor/cpp_prefix.h&gt;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] extern &quot;C&quot;  void  kernel(const float* in_ptr0,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr1,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr2,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr3,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr4,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr5,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr6,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr7,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr8,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        const float* in_ptr9,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr0,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr1,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr2,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr3,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr4,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr5,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr6,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr7,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr8,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                        float* out_ptr9)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr0[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr0[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr1[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr1[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr2[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr2[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr3[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr3[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr4[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr4[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr5[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr5[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr6[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr6[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr7[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr7[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr8[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr8[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             {
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp0 = in_ptr9[static_cast&lt;int64_t&gt;(0L)];
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp1 = static_cast&lt;float&gt;(1.0);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 auto tmp2 = float(tmp0 + tmp1);
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]                 out_ptr9[static_cast&lt;int64_t&gt;(0L)] = tmp2;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] }
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] &#39;&#39;&#39;)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # kernel path: /tmp/torchinductor_ci-user/gq/cgqhtbf2gwgsfmgfs2f4ajlpqjhrdhscqtxcif6oteglzjt3a3rq.py
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # Unsorted Source Nodes: [], Original ATen: []
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # Source node to ATen node mapping:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] triton_for_fused_1 = async_compile.triton(&#39;triton_for_fused_1&#39;, &#39;&#39;&#39;
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton.language as tl
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] @triton_heuristics.foreach(
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     filename=__file__,
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     triton_meta={&#39;signature&#39;: {&#39;in_ptr0&#39;: &#39;*fp32&#39;, &#39;in_ptr1&#39;: &#39;*fp32&#39;, &#39;in_ptr2&#39;: &#39;*fp32&#39;, &#39;in_ptr3&#39;: &#39;*fp32&#39;, &#39;in_ptr4&#39;: &#39;fp32&#39;, &#39;in_ptr5&#39;: &#39;*fp32&#39;, &#39;in_ptr6&#39;: &#39;*fp32&#39;, &#39;in_ptr7&#39;: &#39;*fp32&#39;, &#39;in_ptr8&#39;: &#39;*fp32&#39;, &#39;in_ptr9&#39;: &#39;fp32&#39;, &#39;in_ptr10&#39;: &#39;*fp32&#39;, &#39;in_ptr11&#39;: &#39;*fp32&#39;, &#39;in_ptr12&#39;: &#39;*fp32&#39;, &#39;in_ptr13&#39;: &#39;*fp32&#39;, &#39;in_ptr14&#39;: &#39;fp32&#39;, &#39;in_ptr15&#39;: &#39;*fp32&#39;, &#39;in_ptr16&#39;: &#39;*fp32&#39;, &#39;in_ptr17&#39;: &#39;*fp32&#39;, &#39;in_ptr18&#39;: &#39;*fp32&#39;, &#39;in_ptr19&#39;: &#39;fp32&#39;, &#39;in_ptr20&#39;: &#39;*fp32&#39;, &#39;in_ptr21&#39;: &#39;*fp32&#39;, &#39;in_ptr22&#39;: &#39;*fp32&#39;, &#39;in_ptr23&#39;: &#39;*fp32&#39;, &#39;in_ptr24&#39;: &#39;fp32&#39;, &#39;in_ptr25&#39;: &#39;*fp32&#39;, &#39;in_ptr26&#39;: &#39;*fp32&#39;, &#39;in_ptr27&#39;: &#39;*fp32&#39;, &#39;in_ptr28&#39;: &#39;*fp32&#39;, &#39;in_ptr29&#39;: &#39;fp32&#39;, &#39;in_ptr30&#39;: &#39;*fp32&#39;, &#39;in_ptr31&#39;: &#39;*fp32&#39;, &#39;in_ptr32&#39;: &#39;*fp32&#39;, &#39;in_ptr33&#39;: &#39;*fp32&#39;, &#39;in_ptr34&#39;: &#39;fp32&#39;, &#39;in_ptr35&#39;: &#39;*fp32&#39;, &#39;in_ptr36&#39;: &#39;*fp32&#39;, &#39;in_ptr37&#39;: &#39;*fp32&#39;, &#39;in_ptr38&#39;: &#39;*fp32&#39;, &#39;in_ptr39&#39;: &#39;fp32&#39;, &#39;in_ptr40&#39;: &#39;*fp32&#39;, &#39;in_ptr41&#39;: &#39;*fp32&#39;, &#39;in_ptr42&#39;: &#39;*fp32&#39;, &#39;in_ptr43&#39;: &#39;*fp32&#39;, &#39;in_ptr44&#39;: &#39;fp32&#39;, &#39;in_ptr45&#39;: &#39;*fp32&#39;, &#39;in_ptr46&#39;: &#39;*fp32&#39;, &#39;in_ptr47&#39;: &#39;*fp32&#39;, &#39;in_ptr48&#39;: &#39;*fp32&#39;, &#39;in_ptr49&#39;: &#39;fp32&#39;, &#39;out_ptr3&#39;: &#39;*fp32&#39;, &#39;out_ptr4&#39;: &#39;*fp32&#39;, &#39;out_ptr5&#39;: &#39;*fp32&#39;, &#39;out_ptr9&#39;: &#39;*fp32&#39;, &#39;out_ptr10&#39;: &#39;*fp32&#39;, &#39;out_ptr11&#39;: &#39;*fp32&#39;, &#39;out_ptr15&#39;: &#39;*fp32&#39;, &#39;out_ptr16&#39;: &#39;*fp32&#39;, &#39;out_ptr17&#39;: &#39;*fp32&#39;, &#39;out_ptr21&#39;: &#39;*fp32&#39;, &#39;out_ptr22&#39;: &#39;*fp32&#39;, &#39;out_ptr23&#39;: &#39;*fp32&#39;, &#39;out_ptr27&#39;: &#39;*fp32&#39;, &#39;out_ptr28&#39;: &#39;*fp32&#39;, &#39;out_ptr29&#39;: &#39;*fp32&#39;, &#39;out_ptr33&#39;: &#39;*fp32&#39;, &#39;out_ptr34&#39;: &#39;*fp32&#39;, &#39;out_ptr35&#39;: &#39;*fp32&#39;, &#39;out_ptr39&#39;: &#39;*fp32&#39;, &#39;out_ptr40&#39;: &#39;*fp32&#39;, &#39;out_ptr41&#39;: &#39;*fp32&#39;, &#39;out_ptr45&#39;: &#39;*fp32&#39;, &#39;out_ptr46&#39;: &#39;*fp32&#39;, &#39;out_ptr47&#39;: &#39;*fp32&#39;, &#39;out_ptr51&#39;: &#39;*fp32&#39;, &#39;out_ptr52&#39;: &#39;*fp32&#39;, &#39;out_ptr53&#39;: &#39;*fp32&#39;, &#39;out_ptr57&#39;: &#39;*fp32&#39;, &#39;out_ptr58&#39;: &#39;*fp32&#39;, &#39;out_ptr59&#39;: &#39;*fp32&#39;}, &#39;device&#39;: DeviceProperties(type=&#39;cuda&#39;, index=0, multi_processor_count=80, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, max_threads_per_block=1024, warp_size=32), &#39;constants&#39;: {}, &#39;configs&#39;: [{(0,): [[&#39;tt.divisibility&#39;, 16]], (1,): [[&#39;tt.divisibility&#39;, 16]], (2,): [[&#39;tt.divisibility&#39;, 16]], (3,): [[&#39;tt.divisibility&#39;, 16]], (5,): [[&#39;tt.divisibility&#39;, 16]], (6,): [[&#39;tt.divisibility&#39;, 16]], (7,): [[&#39;tt.divisibility&#39;, 16]], (8,): [[&#39;tt.divisibility&#39;, 16]], (10,): [[&#39;tt.divisibility&#39;, 16]], (11,): [[&#39;tt.divisibility&#39;, 16]], (12,): [[&#39;tt.divisibility&#39;, 16]], (13,): [[&#39;tt.divisibility&#39;, 16]], (15,): [[&#39;tt.divisibility&#39;, 16]], (16,): [[&#39;tt.divisibility&#39;, 16]], (17,): [[&#39;tt.divisibility&#39;, 16]], (18,): [[&#39;tt.divisibility&#39;, 16]], (20,): [[&#39;tt.divisibility&#39;, 16]], (21,): [[&#39;tt.divisibility&#39;, 16]], (22,): [[&#39;tt.divisibility&#39;, 16]], (23,): [[&#39;tt.divisibility&#39;, 16]], (25,): [[&#39;tt.divisibility&#39;, 16]], (26,): [[&#39;tt.divisibility&#39;, 16]], (27,): [[&#39;tt.divisibility&#39;, 16]], (28,): [[&#39;tt.divisibility&#39;, 16]], (30,): [[&#39;tt.divisibility&#39;, 16]], (31,): [[&#39;tt.divisibility&#39;, 16]], (32,): [[&#39;tt.divisibility&#39;, 16]], (33,): [[&#39;tt.divisibility&#39;, 16]], (35,): [[&#39;tt.divisibility&#39;, 16]], (36,): [[&#39;tt.divisibility&#39;, 16]], (37,): [[&#39;tt.divisibility&#39;, 16]], (38,): [[&#39;tt.divisibility&#39;, 16]], (40,): [[&#39;tt.divisibility&#39;, 16]], (41,): [[&#39;tt.divisibility&#39;, 16]], (42,): [[&#39;tt.divisibility&#39;, 16]], (43,): [[&#39;tt.divisibility&#39;, 16]], (45,): [[&#39;tt.divisibility&#39;, 16]], (46,): [[&#39;tt.divisibility&#39;, 16]], (47,): [[&#39;tt.divisibility&#39;, 16]], (48,): [[&#39;tt.divisibility&#39;, 16]], (50,): [[&#39;tt.divisibility&#39;, 16]], (51,): [[&#39;tt.divisibility&#39;, 16]], (52,): [[&#39;tt.divisibility&#39;, 16]], (53,): [[&#39;tt.divisibility&#39;, 16]], (54,): [[&#39;tt.divisibility&#39;, 16]], (55,): [[&#39;tt.divisibility&#39;, 16]], (56,): [[&#39;tt.divisibility&#39;, 16]], (57,): [[&#39;tt.divisibility&#39;, 16]], (58,): [[&#39;tt.divisibility&#39;, 16]], (59,): [[&#39;tt.divisibility&#39;, 16]], (60,): [[&#39;tt.divisibility&#39;, 16]], (61,): [[&#39;tt.divisibility&#39;, 16]], (62,): [[&#39;tt.divisibility&#39;, 16]], (63,): [[&#39;tt.divisibility&#39;, 16]], (64,): [[&#39;tt.divisibility&#39;, 16]], (65,): [[&#39;tt.divisibility&#39;, 16]], (66,): [[&#39;tt.divisibility&#39;, 16]], (67,): [[&#39;tt.divisibility&#39;, 16]], (68,): [[&#39;tt.divisibility&#39;, 16]], (69,): [[&#39;tt.divisibility&#39;, 16]], (70,): [[&#39;tt.divisibility&#39;, 16]], (71,): [[&#39;tt.divisibility&#39;, 16]], (72,): [[&#39;tt.divisibility&#39;, 16]], (73,): [[&#39;tt.divisibility&#39;, 16]], (74,): [[&#39;tt.divisibility&#39;, 16]], (75,): [[&#39;tt.divisibility&#39;, 16]], (76,): [[&#39;tt.divisibility&#39;, 16]], (77,): [[&#39;tt.divisibility&#39;, 16]], (78,): [[&#39;tt.divisibility&#39;, 16]], (79,): [[&#39;tt.divisibility&#39;, 16]]}]},
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     inductor_meta={&#39;grid_type&#39;: &#39;SequentialComboKernelGrid&#39;, &#39;combo_grid_meta&#39;: {&#39;num_kernels&#39;: 10, &#39;min_blocks&#39;: 0, &#39;default_config&#39;: {&#39;XBLOCK&#39;: 1024}, &#39;no_x_dim_0&#39;: False, &#39;xnumel_0&#39;: 1048576, &#39;no_x_dim_1&#39;: False, &#39;xnumel_1&#39;: 1048576, &#39;no_x_dim_2&#39;: False, &#39;xnumel_2&#39;: 1048576, &#39;no_x_dim_3&#39;: False, &#39;xnumel_3&#39;: 1048576, &#39;no_x_dim_4&#39;: False, &#39;xnumel_4&#39;: 1048576, &#39;no_x_dim_5&#39;: False, &#39;xnumel_5&#39;: 1048576, &#39;no_x_dim_6&#39;: False, &#39;xnumel_6&#39;: 1048576, &#39;no_x_dim_7&#39;: False, &#39;xnumel_7&#39;: 1048576, &#39;no_x_dim_8&#39;: False, &#39;xnumel_8&#39;: 1048576, &#39;no_x_dim_9&#39;: False, &#39;xnumel_9&#39;: 1048576}, &#39;kernel_name&#39;: &#39;triton_for_fused_1&#39;, &#39;mutated_arg_names&#39;: [&#39;in_ptr1&#39;, &#39;in_ptr11&#39;, &#39;in_ptr12&#39;, &#39;in_ptr13&#39;, &#39;in_ptr16&#39;, &#39;in_ptr17&#39;, &#39;in_ptr18&#39;, &#39;in_ptr2&#39;, &#39;in_ptr21&#39;, &#39;in_ptr22&#39;, &#39;in_ptr23&#39;, &#39;in_ptr26&#39;, &#39;in_ptr27&#39;, &#39;in_ptr28&#39;, &#39;in_ptr3&#39;, &#39;in_ptr31&#39;, &#39;in_ptr32&#39;, &#39;in_ptr33&#39;, &#39;in_ptr36&#39;, &#39;in_ptr37&#39;, &#39;in_ptr38&#39;, &#39;in_ptr41&#39;, &#39;in_ptr42&#39;, &#39;in_ptr43&#39;, &#39;in_ptr46&#39;, &#39;in_ptr47&#39;, &#39;in_ptr48&#39;, &#39;in_ptr6&#39;, &#39;in_ptr7&#39;, &#39;in_ptr8&#39;, &#39;out_ptr10&#39;, &#39;out_ptr11&#39;, &#39;out_ptr15&#39;, &#39;out_ptr16&#39;, &#39;out_ptr17&#39;, &#39;out_ptr21&#39;, &#39;out_ptr22&#39;, &#39;out_ptr23&#39;, &#39;out_ptr27&#39;, &#39;out_ptr28&#39;, &#39;out_ptr29&#39;, &#39;out_ptr3&#39;, &#39;out_ptr33&#39;, &#39;out_ptr34&#39;, &#39;out_ptr35&#39;, &#39;out_ptr39&#39;, &#39;out_ptr4&#39;, &#39;out_ptr40&#39;, &#39;out_ptr41&#39;, &#39;out_ptr45&#39;, &#39;out_ptr46&#39;, &#39;out_ptr47&#39;, &#39;out_ptr5&#39;, &#39;out_ptr51&#39;, &#39;out_ptr52&#39;, &#39;out_ptr53&#39;, &#39;out_ptr57&#39;, &#39;out_ptr58&#39;, &#39;out_ptr59&#39;, &#39;out_ptr9&#39;], &#39;backend_hash&#39;: &#39;130560DF8C676AFCBC44717C6A9B3C6A2EC6174C11ECC01A816D2F75FFBF9BD0&#39;, &#39;assert_indirect_indexing&#39;: True, &#39;autotune_local_cache&#39;: True, &#39;autotune_pointwise&#39;: True, &#39;autotune_remote_cache&#39;: None, &#39;force_disable_caches&#39;: False, &#39;dynamic_scale_rblock&#39;: True, &#39;max_autotune&#39;: False, &#39;max_autotune_pointwise&#39;: False, &#39;min_split_scan_rblock&#39;: 256, &#39;spill_threshold&#39;: 16, &#39;store_cubin&#39;: False, &#39;deterministic&#39;: False, &#39;force_filter_reduction_configs&#39;: False, &#39;are_deterministic_algorithms_enabled&#39;: False},
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] )
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] @triton.jit
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def triton_for_fused_1(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, in_ptr14, in_ptr15, in_ptr16, in_ptr17, in_ptr18, in_ptr19, in_ptr20, in_ptr21, in_ptr22, in_ptr23, in_ptr24, in_ptr25, in_ptr26, in_ptr27, in_ptr28, in_ptr29, in_ptr30, in_ptr31, in_ptr32, in_ptr33, in_ptr34, in_ptr35, in_ptr36, in_ptr37, in_ptr38, in_ptr39, in_ptr40, in_ptr41, in_ptr42, in_ptr43, in_ptr44, in_ptr45, in_ptr46, in_ptr47, in_ptr48, in_ptr49, out_ptr3, out_ptr4, out_ptr5, out_ptr9, out_ptr10, out_ptr11, out_ptr15, out_ptr16, out_ptr17, out_ptr21, out_ptr22, out_ptr23, out_ptr27, out_ptr28, out_ptr29, out_ptr33, out_ptr34, out_ptr35, out_ptr39, out_ptr40, out_ptr41, out_ptr45, out_ptr46, out_ptr47, out_ptr51, out_ptr52, out_ptr53, out_ptr57, out_ptr58, out_ptr59):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     pid = tl.program_id(0)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     XBLOCK: tl.constexpr = 1024
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_0 = tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_1 = num_xblocks_0 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_2 = num_xblocks_1 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_3 = num_xblocks_2 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_4 = num_xblocks_3 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_5 = num_xblocks_4 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_6 = num_xblocks_5 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_7 = num_xblocks_6 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_8 = num_xblocks_7 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     num_xblocks_9 = num_xblocks_8 + tl.cdiv(1048576, XBLOCK)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     if pid &lt; num_xblocks_0:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x0 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp0 = tl.load(in_ptr0 + (x0), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp1 = tl.load(in_ptr1 + (x0), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp8 = tl.load(in_ptr2 + (x0), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp15 = tl.load(in_ptr3 + (x0), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp17 = in_ptr4
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp2 = tmp0 - tmp1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp3 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp4 = tmp3 * tmp2
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp5 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp6 = tl.where(tmp5, tmp0, tmp1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp7 = tmp4 + tmp6
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp9 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp10 = tmp8 * tmp9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp11 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp12 = tmp0 * tmp11
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp13 = tmp12 * tmp0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp14 = tmp10 + tmp13
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp16 = tl.sqrt_rn(tmp14)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp18 = libdevice.pow(tmp9, tmp17)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp19 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp20 = tmp19 - tmp18
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp21 = tl.sqrt_rn(tmp20)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp22 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp23 = libdevice.pow(tmp22, tmp17)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp24 = tmp19 - tmp23
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp25 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp26 = (tmp25 / tmp24)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp27 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp28 = tmp26 * tmp27
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp29 = -tmp28
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp30 = tmp21 * tmp29
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp31 = (tmp16 / tmp30)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp32 = (tmp25 / tmp29)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp33 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp34 = tmp32 * tmp33
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp35 = tmp31 + tmp34
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp36 = (tmp7 / tmp35)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp37 = tmp15 + tmp36
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr3 + (x0), tmp7, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr4 + (x0), tmp14, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr5 + (x0), tmp37, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_1:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x1 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp38 = tl.load(in_ptr5 + (x1), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp39 = tl.load(in_ptr6 + (x1), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp46 = tl.load(in_ptr7 + (x1), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp53 = tl.load(in_ptr8 + (x1), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp55 = in_ptr9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp40 = tmp38 - tmp39
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp41 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp42 = tmp41 * tmp40
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp43 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp44 = tl.where(tmp43, tmp38, tmp39)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp45 = tmp42 + tmp44
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp47 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp48 = tmp46 * tmp47
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp49 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp50 = tmp38 * tmp49
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp51 = tmp50 * tmp38
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp52 = tmp48 + tmp51
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp54 = tl.sqrt_rn(tmp52)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp56 = libdevice.pow(tmp47, tmp55)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp57 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp58 = tmp57 - tmp56
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp59 = tl.sqrt_rn(tmp58)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp60 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp61 = libdevice.pow(tmp60, tmp55)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp62 = tmp57 - tmp61
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp63 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp64 = (tmp63 / tmp62)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp65 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp66 = tmp64 * tmp65
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp67 = -tmp66
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp68 = tmp59 * tmp67
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp69 = (tmp54 / tmp68)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp70 = (tmp63 / tmp67)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp71 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp72 = tmp70 * tmp71
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp73 = tmp69 + tmp72
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp74 = (tmp45 / tmp73)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp75 = tmp53 + tmp74
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr9 + (x1), tmp45, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr10 + (x1), tmp52, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr11 + (x1), tmp75, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_2:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x2 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp76 = tl.load(in_ptr10 + (x2), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp77 = tl.load(in_ptr11 + (x2), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp84 = tl.load(in_ptr12 + (x2), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp91 = tl.load(in_ptr13 + (x2), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp93 = in_ptr14
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp78 = tmp76 - tmp77
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp79 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp80 = tmp79 * tmp78
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp81 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp82 = tl.where(tmp81, tmp76, tmp77)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp83 = tmp80 + tmp82
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp85 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp86 = tmp84 * tmp85
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp87 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp88 = tmp76 * tmp87
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp89 = tmp88 * tmp76
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp90 = tmp86 + tmp89
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp92 = tl.sqrt_rn(tmp90)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp94 = libdevice.pow(tmp85, tmp93)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp95 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp96 = tmp95 - tmp94
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp97 = tl.sqrt_rn(tmp96)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp98 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp99 = libdevice.pow(tmp98, tmp93)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp100 = tmp95 - tmp99
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp101 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp102 = (tmp101 / tmp100)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp103 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp104 = tmp102 * tmp103
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp105 = -tmp104
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp106 = tmp97 * tmp105
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp107 = (tmp92 / tmp106)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp108 = (tmp101 / tmp105)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp109 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp110 = tmp108 * tmp109
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp111 = tmp107 + tmp110
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp112 = (tmp83 / tmp111)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp113 = tmp91 + tmp112
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr15 + (x2), tmp83, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr16 + (x2), tmp90, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr17 + (x2), tmp113, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_3:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_2
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x3 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp114 = tl.load(in_ptr15 + (x3), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp115 = tl.load(in_ptr16 + (x3), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp122 = tl.load(in_ptr17 + (x3), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp129 = tl.load(in_ptr18 + (x3), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp131 = in_ptr19
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp116 = tmp114 - tmp115
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp117 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp118 = tmp117 * tmp116
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp119 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp120 = tl.where(tmp119, tmp114, tmp115)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp121 = tmp118 + tmp120
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp123 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp124 = tmp122 * tmp123
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp125 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp126 = tmp114 * tmp125
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp127 = tmp126 * tmp114
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp128 = tmp124 + tmp127
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp130 = tl.sqrt_rn(tmp128)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp132 = libdevice.pow(tmp123, tmp131)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp133 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp134 = tmp133 - tmp132
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp135 = tl.sqrt_rn(tmp134)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp136 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp137 = libdevice.pow(tmp136, tmp131)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp138 = tmp133 - tmp137
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp139 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp140 = (tmp139 / tmp138)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp141 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp142 = tmp140 * tmp141
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp143 = -tmp142
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp144 = tmp135 * tmp143
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp145 = (tmp130 / tmp144)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp146 = (tmp139 / tmp143)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp147 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp148 = tmp146 * tmp147
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp149 = tmp145 + tmp148
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp150 = (tmp121 / tmp149)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp151 = tmp129 + tmp150
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr21 + (x3), tmp121, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr22 + (x3), tmp128, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr23 + (x3), tmp151, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_4:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_3
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x4 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp152 = tl.load(in_ptr20 + (x4), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp153 = tl.load(in_ptr21 + (x4), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp160 = tl.load(in_ptr22 + (x4), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp167 = tl.load(in_ptr23 + (x4), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp169 = in_ptr24
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp154 = tmp152 - tmp153
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp155 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp156 = tmp155 * tmp154
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp157 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp158 = tl.where(tmp157, tmp152, tmp153)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp159 = tmp156 + tmp158
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp161 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp162 = tmp160 * tmp161
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp163 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp164 = tmp152 * tmp163
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp165 = tmp164 * tmp152
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp166 = tmp162 + tmp165
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp168 = tl.sqrt_rn(tmp166)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp170 = libdevice.pow(tmp161, tmp169)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp171 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp172 = tmp171 - tmp170
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp173 = tl.sqrt_rn(tmp172)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp174 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp175 = libdevice.pow(tmp174, tmp169)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp176 = tmp171 - tmp175
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp177 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp178 = (tmp177 / tmp176)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp179 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp180 = tmp178 * tmp179
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp181 = -tmp180
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp182 = tmp173 * tmp181
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp183 = (tmp168 / tmp182)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp184 = (tmp177 / tmp181)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp185 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp186 = tmp184 * tmp185
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp187 = tmp183 + tmp186
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp188 = (tmp159 / tmp187)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp189 = tmp167 + tmp188
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr27 + (x4), tmp159, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr28 + (x4), tmp166, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr29 + (x4), tmp189, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_5:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_4
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x5 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp190 = tl.load(in_ptr25 + (x5), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp191 = tl.load(in_ptr26 + (x5), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp198 = tl.load(in_ptr27 + (x5), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp205 = tl.load(in_ptr28 + (x5), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp207 = in_ptr29
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp192 = tmp190 - tmp191
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp193 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp194 = tmp193 * tmp192
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp195 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp196 = tl.where(tmp195, tmp190, tmp191)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp197 = tmp194 + tmp196
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp199 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp200 = tmp198 * tmp199
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp201 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp202 = tmp190 * tmp201
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp203 = tmp202 * tmp190
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp204 = tmp200 + tmp203
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp206 = tl.sqrt_rn(tmp204)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp208 = libdevice.pow(tmp199, tmp207)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp209 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp210 = tmp209 - tmp208
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp211 = tl.sqrt_rn(tmp210)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp212 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp213 = libdevice.pow(tmp212, tmp207)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp214 = tmp209 - tmp213
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp215 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp216 = (tmp215 / tmp214)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp217 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp218 = tmp216 * tmp217
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp219 = -tmp218
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp220 = tmp211 * tmp219
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp221 = (tmp206 / tmp220)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp222 = (tmp215 / tmp219)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp223 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp224 = tmp222 * tmp223
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp225 = tmp221 + tmp224
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp226 = (tmp197 / tmp225)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp227 = tmp205 + tmp226
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr33 + (x5), tmp197, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr34 + (x5), tmp204, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr35 + (x5), tmp227, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_6:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_5
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x6 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp228 = tl.load(in_ptr30 + (x6), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp229 = tl.load(in_ptr31 + (x6), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp236 = tl.load(in_ptr32 + (x6), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp243 = tl.load(in_ptr33 + (x6), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp245 = in_ptr34
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp230 = tmp228 - tmp229
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp231 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp232 = tmp231 * tmp230
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp233 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp234 = tl.where(tmp233, tmp228, tmp229)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp235 = tmp232 + tmp234
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp237 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp238 = tmp236 * tmp237
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp239 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp240 = tmp228 * tmp239
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp241 = tmp240 * tmp228
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp242 = tmp238 + tmp241
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp244 = tl.sqrt_rn(tmp242)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp246 = libdevice.pow(tmp237, tmp245)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp247 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp248 = tmp247 - tmp246
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp249 = tl.sqrt_rn(tmp248)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp250 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp251 = libdevice.pow(tmp250, tmp245)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp252 = tmp247 - tmp251
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp253 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp254 = (tmp253 / tmp252)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp255 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp256 = tmp254 * tmp255
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp257 = -tmp256
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp258 = tmp249 * tmp257
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp259 = (tmp244 / tmp258)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp260 = (tmp253 / tmp257)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp261 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp262 = tmp260 * tmp261
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp263 = tmp259 + tmp262
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp264 = (tmp235 / tmp263)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp265 = tmp243 + tmp264
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr39 + (x6), tmp235, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr40 + (x6), tmp242, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr41 + (x6), tmp265, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_7:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_6
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x7 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp266 = tl.load(in_ptr35 + (x7), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp267 = tl.load(in_ptr36 + (x7), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp274 = tl.load(in_ptr37 + (x7), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp281 = tl.load(in_ptr38 + (x7), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp283 = in_ptr39
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp268 = tmp266 - tmp267
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp269 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp270 = tmp269 * tmp268
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp271 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp272 = tl.where(tmp271, tmp266, tmp267)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp273 = tmp270 + tmp272
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp275 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp276 = tmp274 * tmp275
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp277 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp278 = tmp266 * tmp277
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp279 = tmp278 * tmp266
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp280 = tmp276 + tmp279
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp282 = tl.sqrt_rn(tmp280)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp284 = libdevice.pow(tmp275, tmp283)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp285 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp286 = tmp285 - tmp284
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp287 = tl.sqrt_rn(tmp286)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp288 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp289 = libdevice.pow(tmp288, tmp283)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp290 = tmp285 - tmp289
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp291 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp292 = (tmp291 / tmp290)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp293 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp294 = tmp292 * tmp293
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp295 = -tmp294
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp296 = tmp287 * tmp295
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp297 = (tmp282 / tmp296)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp298 = (tmp291 / tmp295)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp299 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp300 = tmp298 * tmp299
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp301 = tmp297 + tmp300
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp302 = (tmp273 / tmp301)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp303 = tmp281 + tmp302
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr45 + (x7), tmp273, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr46 + (x7), tmp280, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr47 + (x7), tmp303, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_8:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_7
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x8 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp304 = tl.load(in_ptr40 + (x8), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp305 = tl.load(in_ptr41 + (x8), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp312 = tl.load(in_ptr42 + (x8), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp319 = tl.load(in_ptr43 + (x8), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp321 = in_ptr44
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp306 = tmp304 - tmp305
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp307 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp308 = tmp307 * tmp306
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp309 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp310 = tl.where(tmp309, tmp304, tmp305)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp311 = tmp308 + tmp310
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp313 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp314 = tmp312 * tmp313
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp315 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp316 = tmp304 * tmp315
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp317 = tmp316 * tmp304
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp318 = tmp314 + tmp317
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp320 = tl.sqrt_rn(tmp318)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp322 = libdevice.pow(tmp313, tmp321)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp323 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp324 = tmp323 - tmp322
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp325 = tl.sqrt_rn(tmp324)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp326 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp327 = libdevice.pow(tmp326, tmp321)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp328 = tmp323 - tmp327
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp329 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp330 = (tmp329 / tmp328)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp331 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp332 = tmp330 * tmp331
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp333 = -tmp332
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp334 = tmp325 * tmp333
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp335 = (tmp320 / tmp334)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp336 = (tmp329 / tmp333)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp337 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp338 = tmp336 * tmp337
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp339 = tmp335 + tmp338
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp340 = (tmp311 / tmp339)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp341 = tmp319 + tmp340
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr51 + (x8), tmp311, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr52 + (x8), tmp318, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr53 + (x8), tmp341, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     elif pid &lt; num_xblocks_9:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pid_offset = pid - num_xblocks_8
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xnumel = 1048576
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         r0_numel = 1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xoffset = pid_offset * XBLOCK
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         xmask = tl.full([XBLOCK], True, tl.int1)[:]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         x9 = xindex
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp342 = tl.load(in_ptr45 + (x9), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp343 = tl.load(in_ptr46 + (x9), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp350 = tl.load(in_ptr47 + (x9), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp357 = tl.load(in_ptr48 + (x9), None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp359 = in_ptr49
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp344 = tmp342 - tmp343
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp345 = 0.10000000149011612
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp346 = tmp345 * tmp344
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp347 = tl.full([1], False, tl.int1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp348 = tl.where(tmp347, tmp342, tmp343)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp349 = tmp346 + tmp348
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp351 = 0.999
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp352 = tmp350 * tmp351
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp353 = 0.0010000000000000009
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp354 = tmp342 * tmp353
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp355 = tmp354 * tmp342
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp356 = tmp352 + tmp355
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp358 = tl.sqrt_rn(tmp356)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp360 = libdevice.pow(tmp351, tmp359)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp361 = 1.0
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp362 = tmp361 - tmp360
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp363 = tl.sqrt_rn(tmp362)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp364 = 0.9
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp365 = libdevice.pow(tmp364, tmp359)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp366 = tmp361 - tmp365
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp367 = tl.full([1], 1, tl.int32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp368 = (tmp367 / tmp366)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp369 = 0.001
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp370 = tmp368 * tmp369
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp371 = -tmp370
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp372 = tmp363 * tmp371
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp373 = (tmp358 / tmp372)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp374 = (tmp367 / tmp371)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp375 = 1e-08
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp376 = tmp374 * tmp375
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp377 = tmp373 + tmp376
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp378 = (tmp349 / tmp377)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tmp379 = tmp357 + tmp378
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr57 + (x9), tmp349, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr58 + (x9), tmp356, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         tl.store(out_ptr59 + (x9), tmp379, None)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     else:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         pass
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] &#39;&#39;&#39;, device_str=&#39;cuda&#39;)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] async_compile.wait(globals())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del async_compile
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] class Runner:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     def __init__(self, partitions):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         self.partitions = partitions
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     def recursively_apply_fns(self, fns):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         new_callables = []
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         for fn, c in zip(fns, self.partitions):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             new_callables.append(fn(c))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         self.partitions = new_callables
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     def call(self, args):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1 = args
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         args.clear()
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg0_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg1_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg2_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg3_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg4_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg5_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg6_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg7_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg8_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg9_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg10_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg11_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg12_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg13_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg14_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg15_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg16_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg17_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg18_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg19_1, (), ())
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg20_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg21_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg22_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg23_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg24_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg25_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg26_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg27_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg28_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg29_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg30_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg31_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg32_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg33_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg34_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg35_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg36_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg37_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg38_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg39_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg40_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg41_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg42_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg43_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg44_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg45_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg46_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg47_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg48_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         assert_size_stride(arg49_1, (1024, 1024), (1024, 1))
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         cpp_fused__foreach_copy_0(arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         with torch.cuda._DeviceGuard(0):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             torch.cuda.set_device(0)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             # Unsorted Source Nodes: [], Original ATen: []
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             stream0 = get_raw_stream(0)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             triton_for_fused_1.run(arg30_1, arg20_1, arg40_1, arg0_1, arg10_1.item(), arg31_1, arg21_1, arg41_1, arg1_1, arg11_1.item(), arg32_1, arg22_1, arg42_1, arg2_1, arg12_1.item(), arg33_1, arg23_1, arg43_1, arg3_1, arg13_1.item(), arg34_1, arg24_1, arg44_1, arg4_1, arg14_1.item(), arg35_1, arg25_1, arg45_1, arg5_1, arg15_1.item(), arg36_1, arg26_1, arg46_1, arg6_1, arg16_1.item(), arg37_1, arg27_1, arg47_1, arg7_1, arg17_1.item(), arg38_1, arg28_1, arg48_1, arg8_1, arg18_1.item(), arg39_1, arg29_1, arg49_1, arg9_1, arg19_1.item(), arg20_1, arg40_1, arg0_1, arg21_1, arg41_1, arg1_1, arg22_1, arg42_1, arg2_1, arg23_1, arg43_1, arg3_1, arg24_1, arg44_1, arg4_1, arg25_1, arg45_1, arg5_1, arg26_1, arg46_1, arg6_1, arg27_1, arg47_1, arg7_1, arg28_1, arg48_1, arg8_1, arg29_1, arg49_1, arg9_1, stream=stream0)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg0_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg10_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg11_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg12_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg13_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg14_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg15_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg16_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg17_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg18_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg19_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg1_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg20_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg21_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg22_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg23_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg24_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg25_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg26_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg27_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg28_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg29_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg2_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg30_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg31_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg32_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg33_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg34_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg35_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg36_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg37_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg38_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg39_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg3_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg40_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg41_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg42_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg43_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg44_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg45_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg46_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg47_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg48_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg49_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg4_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg5_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg6_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg7_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg8_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]             del arg9_1
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]         return ()
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] runner = Runner(partitions=[])
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] call = runner.call
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     from torch._dynamo.testing import rand_strided
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     from torch._inductor.utils import print_performance
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg0_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg1_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg2_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg3_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg4_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg5_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg6_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg7_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg8_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg9_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg10_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg11_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg12_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg13_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg14_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg15_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg16_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg17_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg18_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg19_1 = rand_strided((), (), device=&#39;cpu&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg20_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg21_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg22_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg23_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg24_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg25_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg26_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg27_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg28_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg29_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg30_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg31_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg32_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg33_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg34_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg35_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg36_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg37_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg38_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg39_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg40_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg41_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg42_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg43_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg44_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg45_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg46_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg47_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg48_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     arg49_1 = rand_strided((1024, 1024), (1024, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1])
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] if __name__ == &quot;__main__&quot;:
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]     compiled_module_main(&#39;None&#39;, benchmark_compiled_module)
V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code]
V0218 17:52:09.358000 23788 torch/_inductor/graph.py:2480] [0/1] [__output_code] Output code written to: /tmp/torchinductor_ci-user/ux/cuxhxe2uod67dudxwleao7vhfdxcbknp46g57v4y2g2humuvkdsg.py
I0218 17:52:09.485000 23788 torch/_inductor/graph.py:2440] [0/1] [__output_code] Output code written to: /tmp/torchinductor_ci-user/ux/cuxhxe2uod67dudxwleao7vhfdxcbknp46g57v4y2g2humuvkdsg.py
eager runtime: 1203.669024998817us
compiled runtime: 764.874690845276us
</pre></div>
</div>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<p>In this tutorial, we successfully implemented a custom fully-fused Adam optimizer using foreach_map.
By leveraging the power of foreach_map and torch.compile, we were able to create an optimized version of the Adam
optimizer that can be used in various machine learning applications. This tutorial provides a comprehensive guide
on how to use foreach_map and torch.compile to optimize machine learning models, and serves as a
valuable resource for developers looking to improve the performance of their models with horizontal fusion.</p>
<p>See also:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/compiling_optimizer.html">Compiled optimizer tutorial</a> - an intro into the compiled optimizer.</p></li>
<li><p><a class="reference external" href="https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669">Compiling the optimizer with PT2</a> - deeper technical details on the compiled optimizer.</p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 10.628 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-foreach-map-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/162cf335b789dd055d4192f77cb0251c/foreach_map.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">foreach_map.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/bcb9aa4fd3968b85310b970dbd86bbc3/foreach_map.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">foreach_map.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/faee5eeb51c8f314872395cc1b776677/foreach_map.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">foreach_map.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</section>


                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="torch_compile_torch_function_modes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">(beta) Utilizing Torch Function modes with torch.compile</p>
      </div>
    </a>
    <a class="right-next"
       href="torch_compile_caching_configuration_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Compile Time Caching Configuration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="torch_compile_torch_function_modes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">(beta) Utilizing Torch Function modes with torch.compile</p>
      </div>
    </a>
    <a class="right-next"
       href="torch_compile_caching_configuration_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Compile Time Caching Configuration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">Model Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions-for-foreach-map-implementation">Helper functions for foreach_map implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-and-running-the-compiled-kernel">Setting up and running the compiled kernel</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/helion" style="color: var(--pst-color-text-muted)">Helion</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/kineto" style="color: var(--pst-color-text-muted)">kineto</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/torchtitan" style="color: var(--pst-color-text-muted)">torchtitan</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/rl" style="color: var(--pst-color-text-muted)">TorchRL</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/audio" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/tensordict" style="color: var(--pst-color-text-muted)">tensordict</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright Â© The Linux FoundationÂ®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâs Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      Â© Copyright 2024, PyTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Explicit horizontal fusion with foreach_map and torch.compile",
       "headline": "Explicit horizontal fusion with foreach_map and torch.compile",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/recipes/foreach_map.html",
       "articleBody": "Note Go to the end to download the full example code. Explicit horizontal fusion with foreach_map and torch.compile# Author: Michael Lazos Horizontal fusion is a key optimization in ML compilers. In eager,this is typically expressed using the torch._foreach* ops which parallelizes operations across a list of tensors. However, supporting all possible permutations of arguments is quite difficult (e.g. mixtures of scalars and lists). Foreach_map allows conversion of any pointwise op in torch to a horiztonally fused foreach variant. In this tutorial, we will demonstrate how to implement the Adam optimizer with foreach_map to generate a fully fused kernel. Note This recipe describes a prototype feature. Prototype features are typically at an early stage for feedback and testing and are subject to change. Prerequisites# PyTorch v2.7.0 or later Model Setup# For this example, we\u2019ll use a simple sequence of linear layers. We instantiate an independent copy to compare the two optimizer implementations. import torch # exit cleanly if we are on a device that doesn\u0027t support ``torch.compile`` if torch.cuda.get_device_capability() \u003c (7, 0): print(\"Exiting because torch.compile is not supported on this device.\") import sys sys.exit(0) # Create simple model model = torch.nn.Sequential( *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)] ) model_copy = torch.nn.Sequential( *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)] ) input = torch.rand(1024, device=\"cuda\") # run forward pass output = model(input) output_copy = model_copy(input) # run backward to populate the grads for our optimizer below output.sum().backward() output_copy.sum().backward() Helper functions for foreach_map implementation# In this section, we\u2019ll begin our implementation of the Adam optimizer. from torch._higher_order_ops.foreach_map import foreach_map # Helper function to extract optimizer states from a torch.optim.Adam instance def get_inputs(optim): steps = [] params = [] grads = [] exp_avgs = [] exp_avg_sqs = [] for group in optim.param_groups: for p in group[\"params\"]: params.append(p) grads.append(p.grad) state = optim.state[p] exp_avgs.append(state[\"exp_avg\"]) exp_avg_sqs.append(state[\"exp_avg_sq\"]) steps.append(state[\"step\"]) return steps, params, exp_avgs, exp_avg_sqs # Functions to update the different optimizer states def update_exp_avg_sq(exp_avg_sq, grad, beta2): return exp_avg_sq.mul(beta2).addcmul(grad, grad, value=1 - beta2) def update_param(param, step, exp_avg, exp_avg_sq, beta1, beta2, lr, eps): bias_correction1 = 1 - torch.pow(beta1, step) bias_correction2 = (1 - torch.pow(beta2, step)).sqrt() step_size = (lr / bias_correction1).neg() denom = (exp_avg_sq.sqrt() / (bias_correction2 * step_size)).add(eps / step_size) return torch.add(param, torch.div(exp_avg, denom)) # Our full Adam implementation def foreach_map_adam( steps, params, exp_avgs, exp_avg_sqs, weight_decay=0, beta1=0.9, beta2=0.999, lr=1e-3, eps=1e-8, ): with torch.no_grad(): grads = [param.grad for param in params] # update step updated_steps = foreach_map(lambda x: x + 1, steps) torch._foreach_copy_(steps, updated_steps) if weight_decay != 0: foreach_map(torch.add, (grads,), alpha=weight_decay) # Higher-order operators (HOPs) cannot have multiple outputs at the moment # need to call foreach_map once for each output exp_avgs_updated = foreach_map(torch.lerp, exp_avgs, grads, 1 - beta1) exp_avgs_sq_updated = foreach_map(update_exp_avg_sq, exp_avg_sqs, grads, beta2) params_updated = foreach_map( update_param, params, steps, exp_avgs_updated, exp_avgs_sq_updated, beta1, beta2, lr, eps, ) # Higher-order operators (HOPs) don\u0027t support input mutation today # so manually update the states in-place torch._foreach_copy_(exp_avgs, exp_avgs_updated) torch._foreach_copy_(exp_avg_sqs, exp_avgs_sq_updated) torch._foreach_copy_(params, params_updated) return Setting up and running the compiled kernel# In this section, we\u2019ll run our Adam optimizer and compare the results Note torch.compile is only supported on CUDA devices that have a compute capability of 7.0 or higher. opt_eager = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01)) opt_eager_copy = torch.optim.Adam(model_copy.parameters(), lr=torch.tensor(0.01)) # warm up the optimizer state dict opt_eager.step() opt_eager_copy.step() inputs = get_inputs(opt_eager_copy) compiled_adam = torch.compile(foreach_map_adam) # optionally view the output code torch._logging.set_logs(output_code=True) # Warmup runs to compile the function for _ in range(5): opt_eager.step() compiled_adam(*inputs) for eager_p, compile_p in zip(opt_eager.param_groups[0][\"params\"], opt_eager_copy.param_groups[0][\"params\"]): torch.allclose(eager_p, compile_p) # Benchmark performance # Let\u0027s define a helpful benchmarking function: import torch.utils.benchmark as benchmark def benchmark_torch_function_in_microseconds(f, *args, **kwargs): t0 = benchmark.Timer( stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f} ) return t0.blocked_autorange().mean * 1e6 eager_runtime = benchmark_torch_function_in_microseconds(opt_eager.step) compiled_runtime = benchmark_torch_function_in_microseconds(lambda: compiled_adam(*inputs)) assert eager_runtime \u003e compiled_runtime print(f\"eager runtime: {eager_runtime}us\") print(f\"compiled runtime: {compiled_runtime}us\") V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] Output code: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # AOT ID: [\u00270_inference\u0027] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import torch V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import math V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import random V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import os V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import tempfile V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from math import inf, nan V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from cmath import nanj V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.utils import maybe_profile V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch import device, empty_strided V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton.language as tl V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] aten = torch.ops.aten V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] inductor_ops = torch.ops.inductor V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] _quantized = torch.ops._quantized V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] async_compile = AsyncCompile() V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] cpp_fused__foreach_copy_0 = async_compile.cpp_pybinding([\u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027], r\u0027\u0027\u0027 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] #include \u003ctorch/csrc/inductor/cpp_prefix.h\u003e V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] extern \"C\" void kernel(const float* in_ptr0, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr1, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr2, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr3, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr4, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr5, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr6, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr7, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr8, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] const float* in_ptr9, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr0, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr1, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr2, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr3, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr4, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr5, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr6, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr7, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr8, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] float* out_ptr9) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr0[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr0[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr1[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr1[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr2[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr2[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr3[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr3[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr4[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr4[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr5[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr5[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr6[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr6[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr7[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr7[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr8[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr8[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] { V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp0 = in_ptr9[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] out_ptr9[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] } V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] \u0027\u0027\u0027) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # kernel path: /tmp/torchinductor_ci-user/gq/cgqhtbf2gwgsfmgfs2f4ajlpqjhrdhscqtxcif6oteglzjt3a3rq.py V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # Unsorted Source Nodes: [], Original ATen: [] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # Source node to ATen node mapping: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] triton_for_fused_1 = async_compile.triton(\u0027triton_for_fused_1\u0027, \u0027\u0027\u0027 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] import triton.language as tl V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] @triton_heuristics.foreach( V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] filename=__file__, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] triton_meta={\u0027signature\u0027: {\u0027in_ptr0\u0027: \u0027*fp32\u0027, \u0027in_ptr1\u0027: \u0027*fp32\u0027, \u0027in_ptr2\u0027: \u0027*fp32\u0027, \u0027in_ptr3\u0027: \u0027*fp32\u0027, \u0027in_ptr4\u0027: \u0027fp32\u0027, \u0027in_ptr5\u0027: \u0027*fp32\u0027, \u0027in_ptr6\u0027: \u0027*fp32\u0027, \u0027in_ptr7\u0027: \u0027*fp32\u0027, \u0027in_ptr8\u0027: \u0027*fp32\u0027, \u0027in_ptr9\u0027: \u0027fp32\u0027, \u0027in_ptr10\u0027: \u0027*fp32\u0027, \u0027in_ptr11\u0027: \u0027*fp32\u0027, \u0027in_ptr12\u0027: \u0027*fp32\u0027, \u0027in_ptr13\u0027: \u0027*fp32\u0027, \u0027in_ptr14\u0027: \u0027fp32\u0027, \u0027in_ptr15\u0027: \u0027*fp32\u0027, \u0027in_ptr16\u0027: \u0027*fp32\u0027, \u0027in_ptr17\u0027: \u0027*fp32\u0027, \u0027in_ptr18\u0027: \u0027*fp32\u0027, \u0027in_ptr19\u0027: \u0027fp32\u0027, \u0027in_ptr20\u0027: \u0027*fp32\u0027, \u0027in_ptr21\u0027: \u0027*fp32\u0027, \u0027in_ptr22\u0027: \u0027*fp32\u0027, \u0027in_ptr23\u0027: \u0027*fp32\u0027, \u0027in_ptr24\u0027: \u0027fp32\u0027, \u0027in_ptr25\u0027: \u0027*fp32\u0027, \u0027in_ptr26\u0027: \u0027*fp32\u0027, \u0027in_ptr27\u0027: \u0027*fp32\u0027, \u0027in_ptr28\u0027: \u0027*fp32\u0027, \u0027in_ptr29\u0027: \u0027fp32\u0027, \u0027in_ptr30\u0027: \u0027*fp32\u0027, \u0027in_ptr31\u0027: \u0027*fp32\u0027, \u0027in_ptr32\u0027: \u0027*fp32\u0027, \u0027in_ptr33\u0027: \u0027*fp32\u0027, \u0027in_ptr34\u0027: \u0027fp32\u0027, \u0027in_ptr35\u0027: \u0027*fp32\u0027, \u0027in_ptr36\u0027: \u0027*fp32\u0027, \u0027in_ptr37\u0027: \u0027*fp32\u0027, \u0027in_ptr38\u0027: \u0027*fp32\u0027, \u0027in_ptr39\u0027: \u0027fp32\u0027, \u0027in_ptr40\u0027: \u0027*fp32\u0027, \u0027in_ptr41\u0027: \u0027*fp32\u0027, \u0027in_ptr42\u0027: \u0027*fp32\u0027, \u0027in_ptr43\u0027: \u0027*fp32\u0027, \u0027in_ptr44\u0027: \u0027fp32\u0027, \u0027in_ptr45\u0027: \u0027*fp32\u0027, \u0027in_ptr46\u0027: \u0027*fp32\u0027, \u0027in_ptr47\u0027: \u0027*fp32\u0027, \u0027in_ptr48\u0027: \u0027*fp32\u0027, \u0027in_ptr49\u0027: \u0027fp32\u0027, \u0027out_ptr3\u0027: \u0027*fp32\u0027, \u0027out_ptr4\u0027: \u0027*fp32\u0027, \u0027out_ptr5\u0027: \u0027*fp32\u0027, \u0027out_ptr9\u0027: \u0027*fp32\u0027, \u0027out_ptr10\u0027: \u0027*fp32\u0027, \u0027out_ptr11\u0027: \u0027*fp32\u0027, \u0027out_ptr15\u0027: \u0027*fp32\u0027, \u0027out_ptr16\u0027: \u0027*fp32\u0027, \u0027out_ptr17\u0027: \u0027*fp32\u0027, \u0027out_ptr21\u0027: \u0027*fp32\u0027, \u0027out_ptr22\u0027: \u0027*fp32\u0027, \u0027out_ptr23\u0027: \u0027*fp32\u0027, \u0027out_ptr27\u0027: \u0027*fp32\u0027, \u0027out_ptr28\u0027: \u0027*fp32\u0027, \u0027out_ptr29\u0027: \u0027*fp32\u0027, \u0027out_ptr33\u0027: \u0027*fp32\u0027, \u0027out_ptr34\u0027: \u0027*fp32\u0027, \u0027out_ptr35\u0027: \u0027*fp32\u0027, \u0027out_ptr39\u0027: \u0027*fp32\u0027, \u0027out_ptr40\u0027: \u0027*fp32\u0027, \u0027out_ptr41\u0027: \u0027*fp32\u0027, \u0027out_ptr45\u0027: \u0027*fp32\u0027, \u0027out_ptr46\u0027: \u0027*fp32\u0027, \u0027out_ptr47\u0027: \u0027*fp32\u0027, \u0027out_ptr51\u0027: \u0027*fp32\u0027, \u0027out_ptr52\u0027: \u0027*fp32\u0027, \u0027out_ptr53\u0027: \u0027*fp32\u0027, \u0027out_ptr57\u0027: \u0027*fp32\u0027, \u0027out_ptr58\u0027: \u0027*fp32\u0027, \u0027out_ptr59\u0027: \u0027*fp32\u0027}, \u0027device\u0027: DeviceProperties(type=\u0027cuda\u0027, index=0, multi_processor_count=80, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, max_threads_per_block=1024, warp_size=32), \u0027constants\u0027: {}, \u0027configs\u0027: [{(0,): [[\u0027tt.divisibility\u0027, 16]], (1,): [[\u0027tt.divisibility\u0027, 16]], (2,): [[\u0027tt.divisibility\u0027, 16]], (3,): [[\u0027tt.divisibility\u0027, 16]], (5,): [[\u0027tt.divisibility\u0027, 16]], (6,): [[\u0027tt.divisibility\u0027, 16]], (7,): [[\u0027tt.divisibility\u0027, 16]], (8,): [[\u0027tt.divisibility\u0027, 16]], (10,): [[\u0027tt.divisibility\u0027, 16]], (11,): [[\u0027tt.divisibility\u0027, 16]], (12,): [[\u0027tt.divisibility\u0027, 16]], (13,): [[\u0027tt.divisibility\u0027, 16]], (15,): [[\u0027tt.divisibility\u0027, 16]], (16,): [[\u0027tt.divisibility\u0027, 16]], (17,): [[\u0027tt.divisibility\u0027, 16]], (18,): [[\u0027tt.divisibility\u0027, 16]], (20,): [[\u0027tt.divisibility\u0027, 16]], (21,): [[\u0027tt.divisibility\u0027, 16]], (22,): [[\u0027tt.divisibility\u0027, 16]], (23,): [[\u0027tt.divisibility\u0027, 16]], (25,): [[\u0027tt.divisibility\u0027, 16]], (26,): [[\u0027tt.divisibility\u0027, 16]], (27,): [[\u0027tt.divisibility\u0027, 16]], (28,): [[\u0027tt.divisibility\u0027, 16]], (30,): [[\u0027tt.divisibility\u0027, 16]], (31,): [[\u0027tt.divisibility\u0027, 16]], (32,): [[\u0027tt.divisibility\u0027, 16]], (33,): [[\u0027tt.divisibility\u0027, 16]], (35,): [[\u0027tt.divisibility\u0027, 16]], (36,): [[\u0027tt.divisibility\u0027, 16]], (37,): [[\u0027tt.divisibility\u0027, 16]], (38,): [[\u0027tt.divisibility\u0027, 16]], (40,): [[\u0027tt.divisibility\u0027, 16]], (41,): [[\u0027tt.divisibility\u0027, 16]], (42,): [[\u0027tt.divisibility\u0027, 16]], (43,): [[\u0027tt.divisibility\u0027, 16]], (45,): [[\u0027tt.divisibility\u0027, 16]], (46,): [[\u0027tt.divisibility\u0027, 16]], (47,): [[\u0027tt.divisibility\u0027, 16]], (48,): [[\u0027tt.divisibility\u0027, 16]], (50,): [[\u0027tt.divisibility\u0027, 16]], (51,): [[\u0027tt.divisibility\u0027, 16]], (52,): [[\u0027tt.divisibility\u0027, 16]], (53,): [[\u0027tt.divisibility\u0027, 16]], (54,): [[\u0027tt.divisibility\u0027, 16]], (55,): [[\u0027tt.divisibility\u0027, 16]], (56,): [[\u0027tt.divisibility\u0027, 16]], (57,): [[\u0027tt.divisibility\u0027, 16]], (58,): [[\u0027tt.divisibility\u0027, 16]], (59,): [[\u0027tt.divisibility\u0027, 16]], (60,): [[\u0027tt.divisibility\u0027, 16]], (61,): [[\u0027tt.divisibility\u0027, 16]], (62,): [[\u0027tt.divisibility\u0027, 16]], (63,): [[\u0027tt.divisibility\u0027, 16]], (64,): [[\u0027tt.divisibility\u0027, 16]], (65,): [[\u0027tt.divisibility\u0027, 16]], (66,): [[\u0027tt.divisibility\u0027, 16]], (67,): [[\u0027tt.divisibility\u0027, 16]], (68,): [[\u0027tt.divisibility\u0027, 16]], (69,): [[\u0027tt.divisibility\u0027, 16]], (70,): [[\u0027tt.divisibility\u0027, 16]], (71,): [[\u0027tt.divisibility\u0027, 16]], (72,): [[\u0027tt.divisibility\u0027, 16]], (73,): [[\u0027tt.divisibility\u0027, 16]], (74,): [[\u0027tt.divisibility\u0027, 16]], (75,): [[\u0027tt.divisibility\u0027, 16]], (76,): [[\u0027tt.divisibility\u0027, 16]], (77,): [[\u0027tt.divisibility\u0027, 16]], (78,): [[\u0027tt.divisibility\u0027, 16]], (79,): [[\u0027tt.divisibility\u0027, 16]]}]}, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] inductor_meta={\u0027grid_type\u0027: \u0027SequentialComboKernelGrid\u0027, \u0027combo_grid_meta\u0027: {\u0027num_kernels\u0027: 10, \u0027min_blocks\u0027: 0, \u0027default_config\u0027: {\u0027XBLOCK\u0027: 1024}, \u0027no_x_dim_0\u0027: False, \u0027xnumel_0\u0027: 1048576, \u0027no_x_dim_1\u0027: False, \u0027xnumel_1\u0027: 1048576, \u0027no_x_dim_2\u0027: False, \u0027xnumel_2\u0027: 1048576, \u0027no_x_dim_3\u0027: False, \u0027xnumel_3\u0027: 1048576, \u0027no_x_dim_4\u0027: False, \u0027xnumel_4\u0027: 1048576, \u0027no_x_dim_5\u0027: False, \u0027xnumel_5\u0027: 1048576, \u0027no_x_dim_6\u0027: False, \u0027xnumel_6\u0027: 1048576, \u0027no_x_dim_7\u0027: False, \u0027xnumel_7\u0027: 1048576, \u0027no_x_dim_8\u0027: False, \u0027xnumel_8\u0027: 1048576, \u0027no_x_dim_9\u0027: False, \u0027xnumel_9\u0027: 1048576}, \u0027kernel_name\u0027: \u0027triton_for_fused_1\u0027, \u0027mutated_arg_names\u0027: [\u0027in_ptr1\u0027, \u0027in_ptr11\u0027, \u0027in_ptr12\u0027, \u0027in_ptr13\u0027, \u0027in_ptr16\u0027, \u0027in_ptr17\u0027, \u0027in_ptr18\u0027, \u0027in_ptr2\u0027, \u0027in_ptr21\u0027, \u0027in_ptr22\u0027, \u0027in_ptr23\u0027, \u0027in_ptr26\u0027, \u0027in_ptr27\u0027, \u0027in_ptr28\u0027, \u0027in_ptr3\u0027, \u0027in_ptr31\u0027, \u0027in_ptr32\u0027, \u0027in_ptr33\u0027, \u0027in_ptr36\u0027, \u0027in_ptr37\u0027, \u0027in_ptr38\u0027, \u0027in_ptr41\u0027, \u0027in_ptr42\u0027, \u0027in_ptr43\u0027, \u0027in_ptr46\u0027, \u0027in_ptr47\u0027, \u0027in_ptr48\u0027, \u0027in_ptr6\u0027, \u0027in_ptr7\u0027, \u0027in_ptr8\u0027, \u0027out_ptr10\u0027, \u0027out_ptr11\u0027, \u0027out_ptr15\u0027, \u0027out_ptr16\u0027, \u0027out_ptr17\u0027, \u0027out_ptr21\u0027, \u0027out_ptr22\u0027, \u0027out_ptr23\u0027, \u0027out_ptr27\u0027, \u0027out_ptr28\u0027, \u0027out_ptr29\u0027, \u0027out_ptr3\u0027, \u0027out_ptr33\u0027, \u0027out_ptr34\u0027, \u0027out_ptr35\u0027, \u0027out_ptr39\u0027, \u0027out_ptr4\u0027, \u0027out_ptr40\u0027, \u0027out_ptr41\u0027, \u0027out_ptr45\u0027, \u0027out_ptr46\u0027, \u0027out_ptr47\u0027, \u0027out_ptr5\u0027, \u0027out_ptr51\u0027, \u0027out_ptr52\u0027, \u0027out_ptr53\u0027, \u0027out_ptr57\u0027, \u0027out_ptr58\u0027, \u0027out_ptr59\u0027, \u0027out_ptr9\u0027], \u0027backend_hash\u0027: \u0027130560DF8C676AFCBC44717C6A9B3C6A2EC6174C11ECC01A816D2F75FFBF9BD0\u0027, \u0027assert_indirect_indexing\u0027: True, \u0027autotune_local_cache\u0027: True, \u0027autotune_pointwise\u0027: True, \u0027autotune_remote_cache\u0027: None, \u0027force_disable_caches\u0027: False, \u0027dynamic_scale_rblock\u0027: True, \u0027max_autotune\u0027: False, \u0027max_autotune_pointwise\u0027: False, \u0027min_split_scan_rblock\u0027: 256, \u0027spill_threshold\u0027: 16, \u0027store_cubin\u0027: False, \u0027deterministic\u0027: False, \u0027force_filter_reduction_configs\u0027: False, \u0027are_deterministic_algorithms_enabled\u0027: False}, V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] ) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] @triton.jit V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def triton_for_fused_1(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, in_ptr14, in_ptr15, in_ptr16, in_ptr17, in_ptr18, in_ptr19, in_ptr20, in_ptr21, in_ptr22, in_ptr23, in_ptr24, in_ptr25, in_ptr26, in_ptr27, in_ptr28, in_ptr29, in_ptr30, in_ptr31, in_ptr32, in_ptr33, in_ptr34, in_ptr35, in_ptr36, in_ptr37, in_ptr38, in_ptr39, in_ptr40, in_ptr41, in_ptr42, in_ptr43, in_ptr44, in_ptr45, in_ptr46, in_ptr47, in_ptr48, in_ptr49, out_ptr3, out_ptr4, out_ptr5, out_ptr9, out_ptr10, out_ptr11, out_ptr15, out_ptr16, out_ptr17, out_ptr21, out_ptr22, out_ptr23, out_ptr27, out_ptr28, out_ptr29, out_ptr33, out_ptr34, out_ptr35, out_ptr39, out_ptr40, out_ptr41, out_ptr45, out_ptr46, out_ptr47, out_ptr51, out_ptr52, out_ptr53, out_ptr57, out_ptr58, out_ptr59): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid = tl.program_id(0) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] XBLOCK: tl.constexpr = 1024 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_0 = tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_1 = num_xblocks_0 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_2 = num_xblocks_1 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_3 = num_xblocks_2 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_4 = num_xblocks_3 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_5 = num_xblocks_4 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_6 = num_xblocks_5 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_7 = num_xblocks_6 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_8 = num_xblocks_7 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] num_xblocks_9 = num_xblocks_8 + tl.cdiv(1048576, XBLOCK) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] if pid \u003c num_xblocks_0: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x0 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp0 = tl.load(in_ptr0 + (x0), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp1 = tl.load(in_ptr1 + (x0), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp8 = tl.load(in_ptr2 + (x0), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp15 = tl.load(in_ptr3 + (x0), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp17 = in_ptr4 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp2 = tmp0 - tmp1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp3 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp4 = tmp3 * tmp2 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp5 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp6 = tl.where(tmp5, tmp0, tmp1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp7 = tmp4 + tmp6 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp9 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp10 = tmp8 * tmp9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp11 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp12 = tmp0 * tmp11 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp13 = tmp12 * tmp0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp14 = tmp10 + tmp13 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp16 = tl.sqrt_rn(tmp14) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp18 = libdevice.pow(tmp9, tmp17) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp19 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp20 = tmp19 - tmp18 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp21 = tl.sqrt_rn(tmp20) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp22 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp23 = libdevice.pow(tmp22, tmp17) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp24 = tmp19 - tmp23 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp25 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp26 = (tmp25 / tmp24) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp27 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp28 = tmp26 * tmp27 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp29 = -tmp28 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp30 = tmp21 * tmp29 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp31 = (tmp16 / tmp30) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp32 = (tmp25 / tmp29) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp33 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp34 = tmp32 * tmp33 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp35 = tmp31 + tmp34 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp36 = (tmp7 / tmp35) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp37 = tmp15 + tmp36 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr3 + (x0), tmp7, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr4 + (x0), tmp14, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr5 + (x0), tmp37, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_1: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x1 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp38 = tl.load(in_ptr5 + (x1), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp39 = tl.load(in_ptr6 + (x1), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp46 = tl.load(in_ptr7 + (x1), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp53 = tl.load(in_ptr8 + (x1), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp55 = in_ptr9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp40 = tmp38 - tmp39 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp41 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp42 = tmp41 * tmp40 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp43 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp44 = tl.where(tmp43, tmp38, tmp39) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp45 = tmp42 + tmp44 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp47 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp48 = tmp46 * tmp47 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp49 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp50 = tmp38 * tmp49 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp51 = tmp50 * tmp38 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp52 = tmp48 + tmp51 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp54 = tl.sqrt_rn(tmp52) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp56 = libdevice.pow(tmp47, tmp55) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp57 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp58 = tmp57 - tmp56 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp59 = tl.sqrt_rn(tmp58) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp60 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp61 = libdevice.pow(tmp60, tmp55) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp62 = tmp57 - tmp61 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp63 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp64 = (tmp63 / tmp62) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp65 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp66 = tmp64 * tmp65 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp67 = -tmp66 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp68 = tmp59 * tmp67 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp69 = (tmp54 / tmp68) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp70 = (tmp63 / tmp67) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp71 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp72 = tmp70 * tmp71 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp73 = tmp69 + tmp72 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp74 = (tmp45 / tmp73) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp75 = tmp53 + tmp74 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr9 + (x1), tmp45, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr10 + (x1), tmp52, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr11 + (x1), tmp75, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_2: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x2 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp76 = tl.load(in_ptr10 + (x2), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp77 = tl.load(in_ptr11 + (x2), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp84 = tl.load(in_ptr12 + (x2), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp91 = tl.load(in_ptr13 + (x2), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp93 = in_ptr14 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp78 = tmp76 - tmp77 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp79 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp80 = tmp79 * tmp78 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp81 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp82 = tl.where(tmp81, tmp76, tmp77) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp83 = tmp80 + tmp82 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp85 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp86 = tmp84 * tmp85 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp87 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp88 = tmp76 * tmp87 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp89 = tmp88 * tmp76 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp90 = tmp86 + tmp89 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp92 = tl.sqrt_rn(tmp90) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp94 = libdevice.pow(tmp85, tmp93) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp95 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp96 = tmp95 - tmp94 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp97 = tl.sqrt_rn(tmp96) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp98 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp99 = libdevice.pow(tmp98, tmp93) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp100 = tmp95 - tmp99 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp101 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp102 = (tmp101 / tmp100) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp103 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp104 = tmp102 * tmp103 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp105 = -tmp104 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp106 = tmp97 * tmp105 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp107 = (tmp92 / tmp106) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp108 = (tmp101 / tmp105) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp109 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp110 = tmp108 * tmp109 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp111 = tmp107 + tmp110 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp112 = (tmp83 / tmp111) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp113 = tmp91 + tmp112 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr15 + (x2), tmp83, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr16 + (x2), tmp90, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr17 + (x2), tmp113, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_3: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_2 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x3 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp114 = tl.load(in_ptr15 + (x3), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp115 = tl.load(in_ptr16 + (x3), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp122 = tl.load(in_ptr17 + (x3), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp129 = tl.load(in_ptr18 + (x3), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp131 = in_ptr19 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp116 = tmp114 - tmp115 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp117 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp118 = tmp117 * tmp116 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp119 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp120 = tl.where(tmp119, tmp114, tmp115) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp121 = tmp118 + tmp120 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp123 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp124 = tmp122 * tmp123 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp125 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp126 = tmp114 * tmp125 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp127 = tmp126 * tmp114 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp128 = tmp124 + tmp127 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp130 = tl.sqrt_rn(tmp128) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp132 = libdevice.pow(tmp123, tmp131) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp133 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp134 = tmp133 - tmp132 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp135 = tl.sqrt_rn(tmp134) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp136 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp137 = libdevice.pow(tmp136, tmp131) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp138 = tmp133 - tmp137 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp139 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp140 = (tmp139 / tmp138) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp141 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp142 = tmp140 * tmp141 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp143 = -tmp142 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp144 = tmp135 * tmp143 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp145 = (tmp130 / tmp144) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp146 = (tmp139 / tmp143) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp147 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp148 = tmp146 * tmp147 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp149 = tmp145 + tmp148 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp150 = (tmp121 / tmp149) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp151 = tmp129 + tmp150 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr21 + (x3), tmp121, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr22 + (x3), tmp128, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr23 + (x3), tmp151, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_4: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_3 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x4 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp152 = tl.load(in_ptr20 + (x4), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp153 = tl.load(in_ptr21 + (x4), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp160 = tl.load(in_ptr22 + (x4), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp167 = tl.load(in_ptr23 + (x4), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp169 = in_ptr24 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp154 = tmp152 - tmp153 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp155 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp156 = tmp155 * tmp154 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp157 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp158 = tl.where(tmp157, tmp152, tmp153) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp159 = tmp156 + tmp158 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp161 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp162 = tmp160 * tmp161 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp163 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp164 = tmp152 * tmp163 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp165 = tmp164 * tmp152 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp166 = tmp162 + tmp165 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp168 = tl.sqrt_rn(tmp166) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp170 = libdevice.pow(tmp161, tmp169) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp171 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp172 = tmp171 - tmp170 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp173 = tl.sqrt_rn(tmp172) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp174 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp175 = libdevice.pow(tmp174, tmp169) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp176 = tmp171 - tmp175 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp177 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp178 = (tmp177 / tmp176) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp179 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp180 = tmp178 * tmp179 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp181 = -tmp180 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp182 = tmp173 * tmp181 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp183 = (tmp168 / tmp182) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp184 = (tmp177 / tmp181) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp185 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp186 = tmp184 * tmp185 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp187 = tmp183 + tmp186 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp188 = (tmp159 / tmp187) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp189 = tmp167 + tmp188 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr27 + (x4), tmp159, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr28 + (x4), tmp166, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr29 + (x4), tmp189, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_5: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_4 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x5 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp190 = tl.load(in_ptr25 + (x5), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp191 = tl.load(in_ptr26 + (x5), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp198 = tl.load(in_ptr27 + (x5), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp205 = tl.load(in_ptr28 + (x5), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp207 = in_ptr29 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp192 = tmp190 - tmp191 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp193 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp194 = tmp193 * tmp192 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp195 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp196 = tl.where(tmp195, tmp190, tmp191) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp197 = tmp194 + tmp196 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp199 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp200 = tmp198 * tmp199 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp201 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp202 = tmp190 * tmp201 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp203 = tmp202 * tmp190 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp204 = tmp200 + tmp203 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp206 = tl.sqrt_rn(tmp204) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp208 = libdevice.pow(tmp199, tmp207) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp209 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp210 = tmp209 - tmp208 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp211 = tl.sqrt_rn(tmp210) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp212 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp213 = libdevice.pow(tmp212, tmp207) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp214 = tmp209 - tmp213 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp215 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp216 = (tmp215 / tmp214) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp217 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp218 = tmp216 * tmp217 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp219 = -tmp218 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp220 = tmp211 * tmp219 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp221 = (tmp206 / tmp220) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp222 = (tmp215 / tmp219) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp223 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp224 = tmp222 * tmp223 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp225 = tmp221 + tmp224 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp226 = (tmp197 / tmp225) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp227 = tmp205 + tmp226 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr33 + (x5), tmp197, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr34 + (x5), tmp204, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr35 + (x5), tmp227, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_6: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_5 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x6 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp228 = tl.load(in_ptr30 + (x6), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp229 = tl.load(in_ptr31 + (x6), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp236 = tl.load(in_ptr32 + (x6), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp243 = tl.load(in_ptr33 + (x6), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp245 = in_ptr34 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp230 = tmp228 - tmp229 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp231 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp232 = tmp231 * tmp230 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp233 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp234 = tl.where(tmp233, tmp228, tmp229) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp235 = tmp232 + tmp234 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp237 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp238 = tmp236 * tmp237 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp239 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp240 = tmp228 * tmp239 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp241 = tmp240 * tmp228 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp242 = tmp238 + tmp241 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp244 = tl.sqrt_rn(tmp242) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp246 = libdevice.pow(tmp237, tmp245) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp247 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp248 = tmp247 - tmp246 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp249 = tl.sqrt_rn(tmp248) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp250 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp251 = libdevice.pow(tmp250, tmp245) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp252 = tmp247 - tmp251 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp253 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp254 = (tmp253 / tmp252) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp255 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp256 = tmp254 * tmp255 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp257 = -tmp256 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp258 = tmp249 * tmp257 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp259 = (tmp244 / tmp258) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp260 = (tmp253 / tmp257) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp261 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp262 = tmp260 * tmp261 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp263 = tmp259 + tmp262 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp264 = (tmp235 / tmp263) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp265 = tmp243 + tmp264 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr39 + (x6), tmp235, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr40 + (x6), tmp242, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr41 + (x6), tmp265, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_7: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_6 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x7 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp266 = tl.load(in_ptr35 + (x7), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp267 = tl.load(in_ptr36 + (x7), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp274 = tl.load(in_ptr37 + (x7), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp281 = tl.load(in_ptr38 + (x7), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp283 = in_ptr39 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp268 = tmp266 - tmp267 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp269 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp270 = tmp269 * tmp268 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp271 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp272 = tl.where(tmp271, tmp266, tmp267) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp273 = tmp270 + tmp272 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp275 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp276 = tmp274 * tmp275 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp277 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp278 = tmp266 * tmp277 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp279 = tmp278 * tmp266 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp280 = tmp276 + tmp279 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp282 = tl.sqrt_rn(tmp280) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp284 = libdevice.pow(tmp275, tmp283) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp285 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp286 = tmp285 - tmp284 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp287 = tl.sqrt_rn(tmp286) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp288 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp289 = libdevice.pow(tmp288, tmp283) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp290 = tmp285 - tmp289 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp291 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp292 = (tmp291 / tmp290) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp293 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp294 = tmp292 * tmp293 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp295 = -tmp294 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp296 = tmp287 * tmp295 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp297 = (tmp282 / tmp296) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp298 = (tmp291 / tmp295) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp299 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp300 = tmp298 * tmp299 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp301 = tmp297 + tmp300 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp302 = (tmp273 / tmp301) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp303 = tmp281 + tmp302 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr45 + (x7), tmp273, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr46 + (x7), tmp280, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr47 + (x7), tmp303, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_8: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_7 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x8 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp304 = tl.load(in_ptr40 + (x8), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp305 = tl.load(in_ptr41 + (x8), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp312 = tl.load(in_ptr42 + (x8), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp319 = tl.load(in_ptr43 + (x8), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp321 = in_ptr44 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp306 = tmp304 - tmp305 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp307 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp308 = tmp307 * tmp306 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp309 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp310 = tl.where(tmp309, tmp304, tmp305) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp311 = tmp308 + tmp310 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp313 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp314 = tmp312 * tmp313 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp315 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp316 = tmp304 * tmp315 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp317 = tmp316 * tmp304 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp318 = tmp314 + tmp317 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp320 = tl.sqrt_rn(tmp318) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp322 = libdevice.pow(tmp313, tmp321) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp323 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp324 = tmp323 - tmp322 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp325 = tl.sqrt_rn(tmp324) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp326 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp327 = libdevice.pow(tmp326, tmp321) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp328 = tmp323 - tmp327 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp329 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp330 = (tmp329 / tmp328) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp331 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp332 = tmp330 * tmp331 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp333 = -tmp332 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp334 = tmp325 * tmp333 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp335 = (tmp320 / tmp334) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp336 = (tmp329 / tmp333) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp337 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp338 = tmp336 * tmp337 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp339 = tmp335 + tmp338 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp340 = (tmp311 / tmp339) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp341 = tmp319 + tmp340 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr51 + (x8), tmp311, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr52 + (x8), tmp318, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr53 + (x8), tmp341, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] elif pid \u003c num_xblocks_9: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pid_offset = pid - num_xblocks_8 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xnumel = 1048576 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] r0_numel = 1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] x9 = xindex V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp342 = tl.load(in_ptr45 + (x9), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp343 = tl.load(in_ptr46 + (x9), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp350 = tl.load(in_ptr47 + (x9), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp357 = tl.load(in_ptr48 + (x9), None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp359 = in_ptr49 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp344 = tmp342 - tmp343 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp345 = 0.10000000149011612 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp346 = tmp345 * tmp344 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp347 = tl.full([1], False, tl.int1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp348 = tl.where(tmp347, tmp342, tmp343) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp349 = tmp346 + tmp348 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp351 = 0.999 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp352 = tmp350 * tmp351 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp353 = 0.0010000000000000009 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp354 = tmp342 * tmp353 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp355 = tmp354 * tmp342 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp356 = tmp352 + tmp355 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp358 = tl.sqrt_rn(tmp356) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp360 = libdevice.pow(tmp351, tmp359) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp361 = 1.0 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp362 = tmp361 - tmp360 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp363 = tl.sqrt_rn(tmp362) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp364 = 0.9 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp365 = libdevice.pow(tmp364, tmp359) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp366 = tmp361 - tmp365 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp367 = tl.full([1], 1, tl.int32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp368 = (tmp367 / tmp366) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp369 = 0.001 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp370 = tmp368 * tmp369 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp371 = -tmp370 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp372 = tmp363 * tmp371 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp373 = (tmp358 / tmp372) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp374 = (tmp367 / tmp371) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp375 = 1e-08 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp376 = tmp374 * tmp375 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp377 = tmp373 + tmp376 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp378 = (tmp349 / tmp377) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tmp379 = tmp357 + tmp378 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr57 + (x9), tmp349, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr58 + (x9), tmp356, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] tl.store(out_ptr59 + (x9), tmp379, None) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] else: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] pass V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] \u0027\u0027\u0027, device_str=\u0027cuda\u0027) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] async_compile.wait(globals()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del async_compile V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] class Runner: V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def __init__(self, partitions): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] self.partitions = partitions V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def recursively_apply_fns(self, fns): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] new_callables = [] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] for fn, c in zip(fns, self.partitions): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] new_callables.append(fn(c)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] self.partitions = new_callables V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def call(self, args): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1 = args V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] args.clear() V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg0_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg1_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg2_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg3_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg4_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg5_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg6_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg7_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg8_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg9_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg10_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg11_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg12_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg13_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg14_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg15_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg16_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg17_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg18_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg19_1, (), ()) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg20_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg21_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg22_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg23_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg24_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg25_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg26_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg27_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg28_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg29_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg30_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg31_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg32_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg33_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg34_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg35_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg36_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg37_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg38_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg39_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg40_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg41_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg42_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg43_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg44_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg45_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg46_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg47_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg48_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] assert_size_stride(arg49_1, (1024, 1024), (1024, 1)) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] cpp_fused__foreach_copy_0(arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] with torch.cuda._DeviceGuard(0): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] torch.cuda.set_device(0) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] # Unsorted Source Nodes: [], Original ATen: [] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] stream0 = get_raw_stream(0) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] triton_for_fused_1.run(arg30_1, arg20_1, arg40_1, arg0_1, arg10_1.item(), arg31_1, arg21_1, arg41_1, arg1_1, arg11_1.item(), arg32_1, arg22_1, arg42_1, arg2_1, arg12_1.item(), arg33_1, arg23_1, arg43_1, arg3_1, arg13_1.item(), arg34_1, arg24_1, arg44_1, arg4_1, arg14_1.item(), arg35_1, arg25_1, arg45_1, arg5_1, arg15_1.item(), arg36_1, arg26_1, arg46_1, arg6_1, arg16_1.item(), arg37_1, arg27_1, arg47_1, arg7_1, arg17_1.item(), arg38_1, arg28_1, arg48_1, arg8_1, arg18_1.item(), arg39_1, arg29_1, arg49_1, arg9_1, arg19_1.item(), arg20_1, arg40_1, arg0_1, arg21_1, arg41_1, arg1_1, arg22_1, arg42_1, arg2_1, arg23_1, arg43_1, arg3_1, arg24_1, arg44_1, arg4_1, arg25_1, arg45_1, arg5_1, arg26_1, arg46_1, arg6_1, arg27_1, arg47_1, arg7_1, arg28_1, arg48_1, arg8_1, arg29_1, arg49_1, arg9_1, stream=stream0) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg0_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg10_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg11_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg12_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg13_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg14_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg15_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg16_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg17_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg18_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg19_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg1_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg20_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg21_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg22_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg23_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg24_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg25_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg26_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg27_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg28_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg29_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg2_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg30_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg31_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg32_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg33_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg34_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg35_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg36_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg37_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg38_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg39_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg3_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg40_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg41_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg42_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg43_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg44_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg45_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg46_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg47_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg48_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg49_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg4_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg5_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg6_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg7_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg8_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] del arg9_1 V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] return () V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] runner = Runner(partitions=[]) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] call = runner.call V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10): V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._dynamo.testing import rand_strided V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.utils import print_performance V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg0_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg1_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg2_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg3_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg4_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg5_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg6_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg7_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg8_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg9_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg10_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg11_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg12_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg13_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg14_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg15_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg16_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg17_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg18_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg19_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg20_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg21_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg22_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg23_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg24_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg25_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg26_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg27_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg28_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg29_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg30_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg31_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg32_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg33_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg34_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg35_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg36_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg37_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg38_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg39_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg40_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg41_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg42_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg43_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg44_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg45_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg46_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg47_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg48_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] arg49_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1]) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] return print_performance(fn, times=times, repeat=repeat) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] if __name__ == \"__main__\": V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] from torch._inductor.wrapper_benchmark import compiled_module_main V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] compiled_module_main(\u0027None\u0027, benchmark_compiled_module) V0218 17:52:05.483000 23788 torch/_inductor/graph.py:2469] [0/0] [__output_code] V0218 17:52:05.534000 23788 torch/_inductor/graph.py:2480] [0/0] [__output_code] Output code written to: /tmp/torchinductor_ci-user/vt/cvtdpmeiofjorzqp4lqo47274rqf4zsrel53eevwsmtsjymolnx3.py I0218 17:52:06.788000 23788 torch/_inductor/graph.py:2440] [0/0] [__output_code] Output code written to: /tmp/torchinductor_ci-user/vt/cvtdpmeiofjorzqp4lqo47274rqf4zsrel53eevwsmtsjymolnx3.py V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] Output code: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # AOT ID: [\u00271_inference\u0027] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from ctypes import c_void_p, c_long, c_int V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import torch V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import math V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import random V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import os V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import tempfile V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from math import inf, nan V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from cmath import nanj V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.hooks import run_intermediate_hooks V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.utils import maybe_profile V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.codegen.memory_planning import _align as align V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch import device, empty_strided V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.async_compile import AsyncCompile V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.select_algorithm import extern_kernels V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton.language as tl V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] aten = torch.ops.aten V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] inductor_ops = torch.ops.inductor V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] _quantized = torch.ops._quantized V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] async_compile = AsyncCompile() V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] cpp_fused__foreach_copy_0 = async_compile.cpp_pybinding([\u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027const float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027, \u0027float*\u0027], r\u0027\u0027\u0027 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] #include \u003ctorch/csrc/inductor/cpp_prefix.h\u003e V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] extern \"C\" void kernel(const float* in_ptr0, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr1, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr2, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr3, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr4, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr5, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr6, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr7, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr8, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] const float* in_ptr9, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr0, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr1, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr2, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr3, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr4, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr5, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr6, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr7, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr8, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] float* out_ptr9) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr0[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr0[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr1[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr1[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr2[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr2[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr3[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr3[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr4[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr4[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr5[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr5[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr6[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr6[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr7[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr7[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr8[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr8[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] { V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp0 = in_ptr9[static_cast\u003cint64_t\u003e(0L)]; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp1 = static_cast\u003cfloat\u003e(1.0); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] auto tmp2 = float(tmp0 + tmp1); V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] out_ptr9[static_cast\u003cint64_t\u003e(0L)] = tmp2; V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] } V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] \u0027\u0027\u0027) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # kernel path: /tmp/torchinductor_ci-user/gq/cgqhtbf2gwgsfmgfs2f4ajlpqjhrdhscqtxcif6oteglzjt3a3rq.py V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # Unsorted Source Nodes: [], Original ATen: [] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # Source node to ATen node mapping: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] triton_for_fused_1 = async_compile.triton(\u0027triton_for_fused_1\u0027, \u0027\u0027\u0027 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] import triton.language as tl V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] @triton_heuristics.foreach( V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] filename=__file__, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] triton_meta={\u0027signature\u0027: {\u0027in_ptr0\u0027: \u0027*fp32\u0027, \u0027in_ptr1\u0027: \u0027*fp32\u0027, \u0027in_ptr2\u0027: \u0027*fp32\u0027, \u0027in_ptr3\u0027: \u0027*fp32\u0027, \u0027in_ptr4\u0027: \u0027fp32\u0027, \u0027in_ptr5\u0027: \u0027*fp32\u0027, \u0027in_ptr6\u0027: \u0027*fp32\u0027, \u0027in_ptr7\u0027: \u0027*fp32\u0027, \u0027in_ptr8\u0027: \u0027*fp32\u0027, \u0027in_ptr9\u0027: \u0027fp32\u0027, \u0027in_ptr10\u0027: \u0027*fp32\u0027, \u0027in_ptr11\u0027: \u0027*fp32\u0027, \u0027in_ptr12\u0027: \u0027*fp32\u0027, \u0027in_ptr13\u0027: \u0027*fp32\u0027, \u0027in_ptr14\u0027: \u0027fp32\u0027, \u0027in_ptr15\u0027: \u0027*fp32\u0027, \u0027in_ptr16\u0027: \u0027*fp32\u0027, \u0027in_ptr17\u0027: \u0027*fp32\u0027, \u0027in_ptr18\u0027: \u0027*fp32\u0027, \u0027in_ptr19\u0027: \u0027fp32\u0027, \u0027in_ptr20\u0027: \u0027*fp32\u0027, \u0027in_ptr21\u0027: \u0027*fp32\u0027, \u0027in_ptr22\u0027: \u0027*fp32\u0027, \u0027in_ptr23\u0027: \u0027*fp32\u0027, \u0027in_ptr24\u0027: \u0027fp32\u0027, \u0027in_ptr25\u0027: \u0027*fp32\u0027, \u0027in_ptr26\u0027: \u0027*fp32\u0027, \u0027in_ptr27\u0027: \u0027*fp32\u0027, \u0027in_ptr28\u0027: \u0027*fp32\u0027, \u0027in_ptr29\u0027: \u0027fp32\u0027, \u0027in_ptr30\u0027: \u0027*fp32\u0027, \u0027in_ptr31\u0027: \u0027*fp32\u0027, \u0027in_ptr32\u0027: \u0027*fp32\u0027, \u0027in_ptr33\u0027: \u0027*fp32\u0027, \u0027in_ptr34\u0027: \u0027fp32\u0027, \u0027in_ptr35\u0027: \u0027*fp32\u0027, \u0027in_ptr36\u0027: \u0027*fp32\u0027, \u0027in_ptr37\u0027: \u0027*fp32\u0027, \u0027in_ptr38\u0027: \u0027*fp32\u0027, \u0027in_ptr39\u0027: \u0027fp32\u0027, \u0027in_ptr40\u0027: \u0027*fp32\u0027, \u0027in_ptr41\u0027: \u0027*fp32\u0027, \u0027in_ptr42\u0027: \u0027*fp32\u0027, \u0027in_ptr43\u0027: \u0027*fp32\u0027, \u0027in_ptr44\u0027: \u0027fp32\u0027, \u0027in_ptr45\u0027: \u0027*fp32\u0027, \u0027in_ptr46\u0027: \u0027*fp32\u0027, \u0027in_ptr47\u0027: \u0027*fp32\u0027, \u0027in_ptr48\u0027: \u0027*fp32\u0027, \u0027in_ptr49\u0027: \u0027fp32\u0027, \u0027out_ptr3\u0027: \u0027*fp32\u0027, \u0027out_ptr4\u0027: \u0027*fp32\u0027, \u0027out_ptr5\u0027: \u0027*fp32\u0027, \u0027out_ptr9\u0027: \u0027*fp32\u0027, \u0027out_ptr10\u0027: \u0027*fp32\u0027, \u0027out_ptr11\u0027: \u0027*fp32\u0027, \u0027out_ptr15\u0027: \u0027*fp32\u0027, \u0027out_ptr16\u0027: \u0027*fp32\u0027, \u0027out_ptr17\u0027: \u0027*fp32\u0027, \u0027out_ptr21\u0027: \u0027*fp32\u0027, \u0027out_ptr22\u0027: \u0027*fp32\u0027, \u0027out_ptr23\u0027: \u0027*fp32\u0027, \u0027out_ptr27\u0027: \u0027*fp32\u0027, \u0027out_ptr28\u0027: \u0027*fp32\u0027, \u0027out_ptr29\u0027: \u0027*fp32\u0027, \u0027out_ptr33\u0027: \u0027*fp32\u0027, \u0027out_ptr34\u0027: \u0027*fp32\u0027, \u0027out_ptr35\u0027: \u0027*fp32\u0027, \u0027out_ptr39\u0027: \u0027*fp32\u0027, \u0027out_ptr40\u0027: \u0027*fp32\u0027, \u0027out_ptr41\u0027: \u0027*fp32\u0027, \u0027out_ptr45\u0027: \u0027*fp32\u0027, \u0027out_ptr46\u0027: \u0027*fp32\u0027, \u0027out_ptr47\u0027: \u0027*fp32\u0027, \u0027out_ptr51\u0027: \u0027*fp32\u0027, \u0027out_ptr52\u0027: \u0027*fp32\u0027, \u0027out_ptr53\u0027: \u0027*fp32\u0027, \u0027out_ptr57\u0027: \u0027*fp32\u0027, \u0027out_ptr58\u0027: \u0027*fp32\u0027, \u0027out_ptr59\u0027: \u0027*fp32\u0027}, \u0027device\u0027: DeviceProperties(type=\u0027cuda\u0027, index=0, multi_processor_count=80, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, max_threads_per_block=1024, warp_size=32), \u0027constants\u0027: {}, \u0027configs\u0027: [{(0,): [[\u0027tt.divisibility\u0027, 16]], (1,): [[\u0027tt.divisibility\u0027, 16]], (2,): [[\u0027tt.divisibility\u0027, 16]], (3,): [[\u0027tt.divisibility\u0027, 16]], (5,): [[\u0027tt.divisibility\u0027, 16]], (6,): [[\u0027tt.divisibility\u0027, 16]], (7,): [[\u0027tt.divisibility\u0027, 16]], (8,): [[\u0027tt.divisibility\u0027, 16]], (10,): [[\u0027tt.divisibility\u0027, 16]], (11,): [[\u0027tt.divisibility\u0027, 16]], (12,): [[\u0027tt.divisibility\u0027, 16]], (13,): [[\u0027tt.divisibility\u0027, 16]], (15,): [[\u0027tt.divisibility\u0027, 16]], (16,): [[\u0027tt.divisibility\u0027, 16]], (17,): [[\u0027tt.divisibility\u0027, 16]], (18,): [[\u0027tt.divisibility\u0027, 16]], (20,): [[\u0027tt.divisibility\u0027, 16]], (21,): [[\u0027tt.divisibility\u0027, 16]], (22,): [[\u0027tt.divisibility\u0027, 16]], (23,): [[\u0027tt.divisibility\u0027, 16]], (25,): [[\u0027tt.divisibility\u0027, 16]], (26,): [[\u0027tt.divisibility\u0027, 16]], (27,): [[\u0027tt.divisibility\u0027, 16]], (28,): [[\u0027tt.divisibility\u0027, 16]], (30,): [[\u0027tt.divisibility\u0027, 16]], (31,): [[\u0027tt.divisibility\u0027, 16]], (32,): [[\u0027tt.divisibility\u0027, 16]], (33,): [[\u0027tt.divisibility\u0027, 16]], (35,): [[\u0027tt.divisibility\u0027, 16]], (36,): [[\u0027tt.divisibility\u0027, 16]], (37,): [[\u0027tt.divisibility\u0027, 16]], (38,): [[\u0027tt.divisibility\u0027, 16]], (40,): [[\u0027tt.divisibility\u0027, 16]], (41,): [[\u0027tt.divisibility\u0027, 16]], (42,): [[\u0027tt.divisibility\u0027, 16]], (43,): [[\u0027tt.divisibility\u0027, 16]], (45,): [[\u0027tt.divisibility\u0027, 16]], (46,): [[\u0027tt.divisibility\u0027, 16]], (47,): [[\u0027tt.divisibility\u0027, 16]], (48,): [[\u0027tt.divisibility\u0027, 16]], (50,): [[\u0027tt.divisibility\u0027, 16]], (51,): [[\u0027tt.divisibility\u0027, 16]], (52,): [[\u0027tt.divisibility\u0027, 16]], (53,): [[\u0027tt.divisibility\u0027, 16]], (54,): [[\u0027tt.divisibility\u0027, 16]], (55,): [[\u0027tt.divisibility\u0027, 16]], (56,): [[\u0027tt.divisibility\u0027, 16]], (57,): [[\u0027tt.divisibility\u0027, 16]], (58,): [[\u0027tt.divisibility\u0027, 16]], (59,): [[\u0027tt.divisibility\u0027, 16]], (60,): [[\u0027tt.divisibility\u0027, 16]], (61,): [[\u0027tt.divisibility\u0027, 16]], (62,): [[\u0027tt.divisibility\u0027, 16]], (63,): [[\u0027tt.divisibility\u0027, 16]], (64,): [[\u0027tt.divisibility\u0027, 16]], (65,): [[\u0027tt.divisibility\u0027, 16]], (66,): [[\u0027tt.divisibility\u0027, 16]], (67,): [[\u0027tt.divisibility\u0027, 16]], (68,): [[\u0027tt.divisibility\u0027, 16]], (69,): [[\u0027tt.divisibility\u0027, 16]], (70,): [[\u0027tt.divisibility\u0027, 16]], (71,): [[\u0027tt.divisibility\u0027, 16]], (72,): [[\u0027tt.divisibility\u0027, 16]], (73,): [[\u0027tt.divisibility\u0027, 16]], (74,): [[\u0027tt.divisibility\u0027, 16]], (75,): [[\u0027tt.divisibility\u0027, 16]], (76,): [[\u0027tt.divisibility\u0027, 16]], (77,): [[\u0027tt.divisibility\u0027, 16]], (78,): [[\u0027tt.divisibility\u0027, 16]], (79,): [[\u0027tt.divisibility\u0027, 16]]}]}, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] inductor_meta={\u0027grid_type\u0027: \u0027SequentialComboKernelGrid\u0027, \u0027combo_grid_meta\u0027: {\u0027num_kernels\u0027: 10, \u0027min_blocks\u0027: 0, \u0027default_config\u0027: {\u0027XBLOCK\u0027: 1024}, \u0027no_x_dim_0\u0027: False, \u0027xnumel_0\u0027: 1048576, \u0027no_x_dim_1\u0027: False, \u0027xnumel_1\u0027: 1048576, \u0027no_x_dim_2\u0027: False, \u0027xnumel_2\u0027: 1048576, \u0027no_x_dim_3\u0027: False, \u0027xnumel_3\u0027: 1048576, \u0027no_x_dim_4\u0027: False, \u0027xnumel_4\u0027: 1048576, \u0027no_x_dim_5\u0027: False, \u0027xnumel_5\u0027: 1048576, \u0027no_x_dim_6\u0027: False, \u0027xnumel_6\u0027: 1048576, \u0027no_x_dim_7\u0027: False, \u0027xnumel_7\u0027: 1048576, \u0027no_x_dim_8\u0027: False, \u0027xnumel_8\u0027: 1048576, \u0027no_x_dim_9\u0027: False, \u0027xnumel_9\u0027: 1048576}, \u0027kernel_name\u0027: \u0027triton_for_fused_1\u0027, \u0027mutated_arg_names\u0027: [\u0027in_ptr1\u0027, \u0027in_ptr11\u0027, \u0027in_ptr12\u0027, \u0027in_ptr13\u0027, \u0027in_ptr16\u0027, \u0027in_ptr17\u0027, \u0027in_ptr18\u0027, \u0027in_ptr2\u0027, \u0027in_ptr21\u0027, \u0027in_ptr22\u0027, \u0027in_ptr23\u0027, \u0027in_ptr26\u0027, \u0027in_ptr27\u0027, \u0027in_ptr28\u0027, \u0027in_ptr3\u0027, \u0027in_ptr31\u0027, \u0027in_ptr32\u0027, \u0027in_ptr33\u0027, \u0027in_ptr36\u0027, \u0027in_ptr37\u0027, \u0027in_ptr38\u0027, \u0027in_ptr41\u0027, \u0027in_ptr42\u0027, \u0027in_ptr43\u0027, \u0027in_ptr46\u0027, \u0027in_ptr47\u0027, \u0027in_ptr48\u0027, \u0027in_ptr6\u0027, \u0027in_ptr7\u0027, \u0027in_ptr8\u0027, \u0027out_ptr10\u0027, \u0027out_ptr11\u0027, \u0027out_ptr15\u0027, \u0027out_ptr16\u0027, \u0027out_ptr17\u0027, \u0027out_ptr21\u0027, \u0027out_ptr22\u0027, \u0027out_ptr23\u0027, \u0027out_ptr27\u0027, \u0027out_ptr28\u0027, \u0027out_ptr29\u0027, \u0027out_ptr3\u0027, \u0027out_ptr33\u0027, \u0027out_ptr34\u0027, \u0027out_ptr35\u0027, \u0027out_ptr39\u0027, \u0027out_ptr4\u0027, \u0027out_ptr40\u0027, \u0027out_ptr41\u0027, \u0027out_ptr45\u0027, \u0027out_ptr46\u0027, \u0027out_ptr47\u0027, \u0027out_ptr5\u0027, \u0027out_ptr51\u0027, \u0027out_ptr52\u0027, \u0027out_ptr53\u0027, \u0027out_ptr57\u0027, \u0027out_ptr58\u0027, \u0027out_ptr59\u0027, \u0027out_ptr9\u0027], \u0027backend_hash\u0027: \u0027130560DF8C676AFCBC44717C6A9B3C6A2EC6174C11ECC01A816D2F75FFBF9BD0\u0027, \u0027assert_indirect_indexing\u0027: True, \u0027autotune_local_cache\u0027: True, \u0027autotune_pointwise\u0027: True, \u0027autotune_remote_cache\u0027: None, \u0027force_disable_caches\u0027: False, \u0027dynamic_scale_rblock\u0027: True, \u0027max_autotune\u0027: False, \u0027max_autotune_pointwise\u0027: False, \u0027min_split_scan_rblock\u0027: 256, \u0027spill_threshold\u0027: 16, \u0027store_cubin\u0027: False, \u0027deterministic\u0027: False, \u0027force_filter_reduction_configs\u0027: False, \u0027are_deterministic_algorithms_enabled\u0027: False}, V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] ) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] @triton.jit V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def triton_for_fused_1(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, in_ptr14, in_ptr15, in_ptr16, in_ptr17, in_ptr18, in_ptr19, in_ptr20, in_ptr21, in_ptr22, in_ptr23, in_ptr24, in_ptr25, in_ptr26, in_ptr27, in_ptr28, in_ptr29, in_ptr30, in_ptr31, in_ptr32, in_ptr33, in_ptr34, in_ptr35, in_ptr36, in_ptr37, in_ptr38, in_ptr39, in_ptr40, in_ptr41, in_ptr42, in_ptr43, in_ptr44, in_ptr45, in_ptr46, in_ptr47, in_ptr48, in_ptr49, out_ptr3, out_ptr4, out_ptr5, out_ptr9, out_ptr10, out_ptr11, out_ptr15, out_ptr16, out_ptr17, out_ptr21, out_ptr22, out_ptr23, out_ptr27, out_ptr28, out_ptr29, out_ptr33, out_ptr34, out_ptr35, out_ptr39, out_ptr40, out_ptr41, out_ptr45, out_ptr46, out_ptr47, out_ptr51, out_ptr52, out_ptr53, out_ptr57, out_ptr58, out_ptr59): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid = tl.program_id(0) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] XBLOCK: tl.constexpr = 1024 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_0 = tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_1 = num_xblocks_0 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_2 = num_xblocks_1 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_3 = num_xblocks_2 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_4 = num_xblocks_3 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_5 = num_xblocks_4 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_6 = num_xblocks_5 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_7 = num_xblocks_6 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_8 = num_xblocks_7 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] num_xblocks_9 = num_xblocks_8 + tl.cdiv(1048576, XBLOCK) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] if pid \u003c num_xblocks_0: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x0 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp0 = tl.load(in_ptr0 + (x0), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp1 = tl.load(in_ptr1 + (x0), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp8 = tl.load(in_ptr2 + (x0), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp15 = tl.load(in_ptr3 + (x0), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp17 = in_ptr4 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp2 = tmp0 - tmp1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp3 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp4 = tmp3 * tmp2 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp5 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp6 = tl.where(tmp5, tmp0, tmp1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp7 = tmp4 + tmp6 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp9 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp10 = tmp8 * tmp9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp11 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp12 = tmp0 * tmp11 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp13 = tmp12 * tmp0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp14 = tmp10 + tmp13 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp16 = tl.sqrt_rn(tmp14) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp18 = libdevice.pow(tmp9, tmp17) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp19 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp20 = tmp19 - tmp18 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp21 = tl.sqrt_rn(tmp20) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp22 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp23 = libdevice.pow(tmp22, tmp17) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp24 = tmp19 - tmp23 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp25 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp26 = (tmp25 / tmp24) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp27 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp28 = tmp26 * tmp27 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp29 = -tmp28 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp30 = tmp21 * tmp29 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp31 = (tmp16 / tmp30) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp32 = (tmp25 / tmp29) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp33 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp34 = tmp32 * tmp33 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp35 = tmp31 + tmp34 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp36 = (tmp7 / tmp35) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp37 = tmp15 + tmp36 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr3 + (x0), tmp7, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr4 + (x0), tmp14, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr5 + (x0), tmp37, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_1: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x1 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp38 = tl.load(in_ptr5 + (x1), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp39 = tl.load(in_ptr6 + (x1), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp46 = tl.load(in_ptr7 + (x1), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp53 = tl.load(in_ptr8 + (x1), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp55 = in_ptr9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp40 = tmp38 - tmp39 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp41 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp42 = tmp41 * tmp40 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp43 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp44 = tl.where(tmp43, tmp38, tmp39) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp45 = tmp42 + tmp44 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp47 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp48 = tmp46 * tmp47 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp49 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp50 = tmp38 * tmp49 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp51 = tmp50 * tmp38 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp52 = tmp48 + tmp51 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp54 = tl.sqrt_rn(tmp52) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp56 = libdevice.pow(tmp47, tmp55) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp57 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp58 = tmp57 - tmp56 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp59 = tl.sqrt_rn(tmp58) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp60 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp61 = libdevice.pow(tmp60, tmp55) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp62 = tmp57 - tmp61 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp63 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp64 = (tmp63 / tmp62) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp65 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp66 = tmp64 * tmp65 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp67 = -tmp66 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp68 = tmp59 * tmp67 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp69 = (tmp54 / tmp68) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp70 = (tmp63 / tmp67) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp71 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp72 = tmp70 * tmp71 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp73 = tmp69 + tmp72 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp74 = (tmp45 / tmp73) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp75 = tmp53 + tmp74 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr9 + (x1), tmp45, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr10 + (x1), tmp52, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr11 + (x1), tmp75, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_2: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x2 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp76 = tl.load(in_ptr10 + (x2), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp77 = tl.load(in_ptr11 + (x2), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp84 = tl.load(in_ptr12 + (x2), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp91 = tl.load(in_ptr13 + (x2), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp93 = in_ptr14 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp78 = tmp76 - tmp77 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp79 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp80 = tmp79 * tmp78 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp81 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp82 = tl.where(tmp81, tmp76, tmp77) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp83 = tmp80 + tmp82 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp85 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp86 = tmp84 * tmp85 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp87 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp88 = tmp76 * tmp87 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp89 = tmp88 * tmp76 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp90 = tmp86 + tmp89 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp92 = tl.sqrt_rn(tmp90) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp94 = libdevice.pow(tmp85, tmp93) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp95 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp96 = tmp95 - tmp94 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp97 = tl.sqrt_rn(tmp96) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp98 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp99 = libdevice.pow(tmp98, tmp93) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp100 = tmp95 - tmp99 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp101 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp102 = (tmp101 / tmp100) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp103 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp104 = tmp102 * tmp103 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp105 = -tmp104 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp106 = tmp97 * tmp105 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp107 = (tmp92 / tmp106) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp108 = (tmp101 / tmp105) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp109 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp110 = tmp108 * tmp109 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp111 = tmp107 + tmp110 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp112 = (tmp83 / tmp111) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp113 = tmp91 + tmp112 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr15 + (x2), tmp83, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr16 + (x2), tmp90, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr17 + (x2), tmp113, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_3: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_2 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x3 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp114 = tl.load(in_ptr15 + (x3), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp115 = tl.load(in_ptr16 + (x3), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp122 = tl.load(in_ptr17 + (x3), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp129 = tl.load(in_ptr18 + (x3), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp131 = in_ptr19 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp116 = tmp114 - tmp115 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp117 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp118 = tmp117 * tmp116 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp119 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp120 = tl.where(tmp119, tmp114, tmp115) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp121 = tmp118 + tmp120 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp123 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp124 = tmp122 * tmp123 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp125 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp126 = tmp114 * tmp125 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp127 = tmp126 * tmp114 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp128 = tmp124 + tmp127 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp130 = tl.sqrt_rn(tmp128) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp132 = libdevice.pow(tmp123, tmp131) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp133 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp134 = tmp133 - tmp132 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp135 = tl.sqrt_rn(tmp134) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp136 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp137 = libdevice.pow(tmp136, tmp131) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp138 = tmp133 - tmp137 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp139 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp140 = (tmp139 / tmp138) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp141 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp142 = tmp140 * tmp141 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp143 = -tmp142 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp144 = tmp135 * tmp143 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp145 = (tmp130 / tmp144) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp146 = (tmp139 / tmp143) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp147 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp148 = tmp146 * tmp147 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp149 = tmp145 + tmp148 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp150 = (tmp121 / tmp149) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp151 = tmp129 + tmp150 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr21 + (x3), tmp121, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr22 + (x3), tmp128, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr23 + (x3), tmp151, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_4: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_3 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x4 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp152 = tl.load(in_ptr20 + (x4), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp153 = tl.load(in_ptr21 + (x4), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp160 = tl.load(in_ptr22 + (x4), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp167 = tl.load(in_ptr23 + (x4), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp169 = in_ptr24 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp154 = tmp152 - tmp153 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp155 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp156 = tmp155 * tmp154 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp157 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp158 = tl.where(tmp157, tmp152, tmp153) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp159 = tmp156 + tmp158 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp161 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp162 = tmp160 * tmp161 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp163 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp164 = tmp152 * tmp163 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp165 = tmp164 * tmp152 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp166 = tmp162 + tmp165 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp168 = tl.sqrt_rn(tmp166) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp170 = libdevice.pow(tmp161, tmp169) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp171 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp172 = tmp171 - tmp170 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp173 = tl.sqrt_rn(tmp172) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp174 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp175 = libdevice.pow(tmp174, tmp169) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp176 = tmp171 - tmp175 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp177 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp178 = (tmp177 / tmp176) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp179 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp180 = tmp178 * tmp179 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp181 = -tmp180 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp182 = tmp173 * tmp181 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp183 = (tmp168 / tmp182) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp184 = (tmp177 / tmp181) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp185 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp186 = tmp184 * tmp185 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp187 = tmp183 + tmp186 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp188 = (tmp159 / tmp187) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp189 = tmp167 + tmp188 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr27 + (x4), tmp159, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr28 + (x4), tmp166, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr29 + (x4), tmp189, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_5: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_4 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x5 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp190 = tl.load(in_ptr25 + (x5), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp191 = tl.load(in_ptr26 + (x5), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp198 = tl.load(in_ptr27 + (x5), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp205 = tl.load(in_ptr28 + (x5), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp207 = in_ptr29 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp192 = tmp190 - tmp191 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp193 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp194 = tmp193 * tmp192 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp195 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp196 = tl.where(tmp195, tmp190, tmp191) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp197 = tmp194 + tmp196 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp199 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp200 = tmp198 * tmp199 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp201 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp202 = tmp190 * tmp201 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp203 = tmp202 * tmp190 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp204 = tmp200 + tmp203 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp206 = tl.sqrt_rn(tmp204) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp208 = libdevice.pow(tmp199, tmp207) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp209 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp210 = tmp209 - tmp208 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp211 = tl.sqrt_rn(tmp210) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp212 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp213 = libdevice.pow(tmp212, tmp207) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp214 = tmp209 - tmp213 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp215 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp216 = (tmp215 / tmp214) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp217 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp218 = tmp216 * tmp217 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp219 = -tmp218 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp220 = tmp211 * tmp219 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp221 = (tmp206 / tmp220) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp222 = (tmp215 / tmp219) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp223 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp224 = tmp222 * tmp223 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp225 = tmp221 + tmp224 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp226 = (tmp197 / tmp225) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp227 = tmp205 + tmp226 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr33 + (x5), tmp197, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr34 + (x5), tmp204, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr35 + (x5), tmp227, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_6: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_5 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x6 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp228 = tl.load(in_ptr30 + (x6), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp229 = tl.load(in_ptr31 + (x6), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp236 = tl.load(in_ptr32 + (x6), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp243 = tl.load(in_ptr33 + (x6), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp245 = in_ptr34 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp230 = tmp228 - tmp229 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp231 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp232 = tmp231 * tmp230 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp233 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp234 = tl.where(tmp233, tmp228, tmp229) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp235 = tmp232 + tmp234 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp237 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp238 = tmp236 * tmp237 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp239 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp240 = tmp228 * tmp239 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp241 = tmp240 * tmp228 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp242 = tmp238 + tmp241 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp244 = tl.sqrt_rn(tmp242) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp246 = libdevice.pow(tmp237, tmp245) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp247 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp248 = tmp247 - tmp246 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp249 = tl.sqrt_rn(tmp248) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp250 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp251 = libdevice.pow(tmp250, tmp245) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp252 = tmp247 - tmp251 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp253 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp254 = (tmp253 / tmp252) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp255 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp256 = tmp254 * tmp255 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp257 = -tmp256 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp258 = tmp249 * tmp257 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp259 = (tmp244 / tmp258) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp260 = (tmp253 / tmp257) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp261 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp262 = tmp260 * tmp261 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp263 = tmp259 + tmp262 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp264 = (tmp235 / tmp263) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp265 = tmp243 + tmp264 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr39 + (x6), tmp235, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr40 + (x6), tmp242, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr41 + (x6), tmp265, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_7: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_6 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x7 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp266 = tl.load(in_ptr35 + (x7), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp267 = tl.load(in_ptr36 + (x7), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp274 = tl.load(in_ptr37 + (x7), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp281 = tl.load(in_ptr38 + (x7), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp283 = in_ptr39 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp268 = tmp266 - tmp267 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp269 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp270 = tmp269 * tmp268 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp271 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp272 = tl.where(tmp271, tmp266, tmp267) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp273 = tmp270 + tmp272 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp275 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp276 = tmp274 * tmp275 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp277 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp278 = tmp266 * tmp277 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp279 = tmp278 * tmp266 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp280 = tmp276 + tmp279 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp282 = tl.sqrt_rn(tmp280) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp284 = libdevice.pow(tmp275, tmp283) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp285 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp286 = tmp285 - tmp284 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp287 = tl.sqrt_rn(tmp286) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp288 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp289 = libdevice.pow(tmp288, tmp283) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp290 = tmp285 - tmp289 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp291 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp292 = (tmp291 / tmp290) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp293 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp294 = tmp292 * tmp293 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp295 = -tmp294 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp296 = tmp287 * tmp295 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp297 = (tmp282 / tmp296) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp298 = (tmp291 / tmp295) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp299 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp300 = tmp298 * tmp299 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp301 = tmp297 + tmp300 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp302 = (tmp273 / tmp301) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp303 = tmp281 + tmp302 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr45 + (x7), tmp273, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr46 + (x7), tmp280, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr47 + (x7), tmp303, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_8: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_7 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x8 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp304 = tl.load(in_ptr40 + (x8), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp305 = tl.load(in_ptr41 + (x8), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp312 = tl.load(in_ptr42 + (x8), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp319 = tl.load(in_ptr43 + (x8), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp321 = in_ptr44 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp306 = tmp304 - tmp305 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp307 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp308 = tmp307 * tmp306 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp309 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp310 = tl.where(tmp309, tmp304, tmp305) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp311 = tmp308 + tmp310 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp313 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp314 = tmp312 * tmp313 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp315 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp316 = tmp304 * tmp315 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp317 = tmp316 * tmp304 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp318 = tmp314 + tmp317 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp320 = tl.sqrt_rn(tmp318) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp322 = libdevice.pow(tmp313, tmp321) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp323 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp324 = tmp323 - tmp322 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp325 = tl.sqrt_rn(tmp324) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp326 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp327 = libdevice.pow(tmp326, tmp321) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp328 = tmp323 - tmp327 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp329 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp330 = (tmp329 / tmp328) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp331 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp332 = tmp330 * tmp331 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp333 = -tmp332 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp334 = tmp325 * tmp333 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp335 = (tmp320 / tmp334) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp336 = (tmp329 / tmp333) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp337 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp338 = tmp336 * tmp337 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp339 = tmp335 + tmp338 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp340 = (tmp311 / tmp339) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp341 = tmp319 + tmp340 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr51 + (x8), tmp311, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr52 + (x8), tmp318, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr53 + (x8), tmp341, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] elif pid \u003c num_xblocks_9: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pid_offset = pid - num_xblocks_8 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xnumel = 1048576 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] r0_numel = 1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xoffset = pid_offset * XBLOCK V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] xmask = tl.full([XBLOCK], True, tl.int1)[:] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] x9 = xindex V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp342 = tl.load(in_ptr45 + (x9), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp343 = tl.load(in_ptr46 + (x9), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp350 = tl.load(in_ptr47 + (x9), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp357 = tl.load(in_ptr48 + (x9), None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp359 = in_ptr49 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp344 = tmp342 - tmp343 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp345 = 0.10000000149011612 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp346 = tmp345 * tmp344 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp347 = tl.full([1], False, tl.int1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp348 = tl.where(tmp347, tmp342, tmp343) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp349 = tmp346 + tmp348 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp351 = 0.999 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp352 = tmp350 * tmp351 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp353 = 0.0010000000000000009 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp354 = tmp342 * tmp353 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp355 = tmp354 * tmp342 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp356 = tmp352 + tmp355 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp358 = tl.sqrt_rn(tmp356) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp360 = libdevice.pow(tmp351, tmp359) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp361 = 1.0 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp362 = tmp361 - tmp360 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp363 = tl.sqrt_rn(tmp362) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp364 = 0.9 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp365 = libdevice.pow(tmp364, tmp359) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp366 = tmp361 - tmp365 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp367 = tl.full([1], 1, tl.int32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp368 = (tmp367 / tmp366) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp369 = 0.001 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp370 = tmp368 * tmp369 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp371 = -tmp370 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp372 = tmp363 * tmp371 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp373 = (tmp358 / tmp372) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp374 = (tmp367 / tmp371) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp375 = 1e-08 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp376 = tmp374 * tmp375 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp377 = tmp373 + tmp376 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp378 = (tmp349 / tmp377) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tmp379 = tmp357 + tmp378 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr57 + (x9), tmp349, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr58 + (x9), tmp356, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] tl.store(out_ptr59 + (x9), tmp379, None) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] else: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] pass V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] \u0027\u0027\u0027, device_str=\u0027cuda\u0027) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] async_compile.wait(globals()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del async_compile V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] class Runner: V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def __init__(self, partitions): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] self.partitions = partitions V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def recursively_apply_fns(self, fns): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] new_callables = [] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] for fn, c in zip(fns, self.partitions): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] new_callables.append(fn(c)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] self.partitions = new_callables V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def call(self, args): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1 = args V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] args.clear() V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg0_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg1_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg2_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg3_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg4_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg5_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg6_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg7_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg8_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg9_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg10_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg11_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg12_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg13_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg14_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg15_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg16_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg17_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg18_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg19_1, (), ()) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg20_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg21_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg22_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg23_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg24_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg25_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg26_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg27_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg28_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg29_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg30_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg31_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg32_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg33_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg34_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg35_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg36_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg37_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg38_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg39_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg40_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg41_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg42_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg43_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg44_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg45_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg46_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg47_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg48_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] assert_size_stride(arg49_1, (1024, 1024), (1024, 1)) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] cpp_fused__foreach_copy_0(arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] with torch.cuda._DeviceGuard(0): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] torch.cuda.set_device(0) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] # Unsorted Source Nodes: [], Original ATen: [] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] stream0 = get_raw_stream(0) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] triton_for_fused_1.run(arg30_1, arg20_1, arg40_1, arg0_1, arg10_1.item(), arg31_1, arg21_1, arg41_1, arg1_1, arg11_1.item(), arg32_1, arg22_1, arg42_1, arg2_1, arg12_1.item(), arg33_1, arg23_1, arg43_1, arg3_1, arg13_1.item(), arg34_1, arg24_1, arg44_1, arg4_1, arg14_1.item(), arg35_1, arg25_1, arg45_1, arg5_1, arg15_1.item(), arg36_1, arg26_1, arg46_1, arg6_1, arg16_1.item(), arg37_1, arg27_1, arg47_1, arg7_1, arg17_1.item(), arg38_1, arg28_1, arg48_1, arg8_1, arg18_1.item(), arg39_1, arg29_1, arg49_1, arg9_1, arg19_1.item(), arg20_1, arg40_1, arg0_1, arg21_1, arg41_1, arg1_1, arg22_1, arg42_1, arg2_1, arg23_1, arg43_1, arg3_1, arg24_1, arg44_1, arg4_1, arg25_1, arg45_1, arg5_1, arg26_1, arg46_1, arg6_1, arg27_1, arg47_1, arg7_1, arg28_1, arg48_1, arg8_1, arg29_1, arg49_1, arg9_1, stream=stream0) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg0_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg10_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg11_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg12_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg13_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg14_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg15_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg16_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg17_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg18_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg19_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg1_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg20_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg21_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg22_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg23_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg24_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg25_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg26_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg27_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg28_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg29_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg2_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg30_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg31_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg32_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg33_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg34_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg35_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg36_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg37_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg38_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg39_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg3_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg40_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg41_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg42_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg43_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg44_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg45_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg46_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg47_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg48_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg49_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg4_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg5_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg6_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg7_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg8_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] del arg9_1 V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] return () V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] runner = Runner(partitions=[]) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] call = runner.call V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] recursively_apply_fns = runner.recursively_apply_fns V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] def benchmark_compiled_module(times=10, repeat=10): V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._dynamo.testing import rand_strided V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.utils import print_performance V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg0_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg1_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg2_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg3_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg4_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg5_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg6_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg7_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg8_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg9_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg10_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg11_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg12_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg13_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg14_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg15_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg16_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg17_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg18_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg19_1 = rand_strided((), (), device=\u0027cpu\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg20_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg21_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg22_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg23_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg24_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg25_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg26_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg27_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg28_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg29_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg30_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg31_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg32_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg33_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg34_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg35_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg36_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg37_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg38_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg39_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg40_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg41_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg42_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg43_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg44_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg45_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg46_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg47_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg48_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] arg49_1 = rand_strided((1024, 1024), (1024, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1]) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] return print_performance(fn, times=times, repeat=repeat) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] if __name__ == \"__main__\": V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] from torch._inductor.wrapper_benchmark import compiled_module_main V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] compiled_module_main(\u0027None\u0027, benchmark_compiled_module) V0218 17:52:09.309000 23788 torch/_inductor/graph.py:2469] [0/1] [__output_code] V0218 17:52:09.358000 23788 torch/_inductor/graph.py:2480] [0/1] [__output_code] Output code written to: /tmp/torchinductor_ci-user/ux/cuxhxe2uod67dudxwleao7vhfdxcbknp46g57v4y2g2humuvkdsg.py I0218 17:52:09.485000 23788 torch/_inductor/graph.py:2440] [0/1] [__output_code] Output code written to: /tmp/torchinductor_ci-user/ux/cuxhxe2uod67dudxwleao7vhfdxcbknp46g57v4y2g2humuvkdsg.py eager runtime: 1203.669024998817us compiled runtime: 764.874690845276us Conclusion# In this tutorial, we successfully implemented a custom fully-fused Adam optimizer using foreach_map. By leveraging the power of foreach_map and torch.compile, we were able to create an optimized version of the Adam optimizer that can be used in various machine learning applications. This tutorial provides a comprehensive guide on how to use foreach_map and torch.compile to optimize machine learning models, and serves as a valuable resource for developers looking to improve the performance of their models with horizontal fusion. See also: Compiled optimizer tutorial - an intro into the compiled optimizer. Compiling the optimizer with PT2 - deeper technical details on the compiled optimizer. Total running time of the script: (0 minutes 10.628 seconds) Download Jupyter notebook: foreach_map.ipynb Download Python source code: foreach_map.py Download zipped: foreach_map.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/foreach_map.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>