
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "intermediate/text_to_speech_with_torchaudio.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_intermediate_text_to_speech_with_torchaudio.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_intermediate_text_to_speech_with_torchaudio.py:


Text-to-speech with torchaudio
==============================

**Author**: `Yao-Yuan Yang <https://github.com/yangarbiter>`__, `Moto
Hira <moto@fb.com>`__

.. GENERATED FROM PYTHON SOURCE LINES 11-41

Overview
--------

This tutorial shows how to build text-to-speech pipeline, using the
pretrained Tacotron2 in torchaudio.

The text-to-speech pipeline goes as follows: 1. Text preprocessing

First, the input text is encoded into a list of symbols. In this
tutorial, we will use English characters and phonemes as the symbols.

2. Spectrogram generation

From the encoded text, a spectrogram is generated. We use ``Tacotron2``
model for this.

3. Time-domain conversion

The last step is converting the spectrogram into the waveform. The
process to generate speech from spectrogram is also called Vocoder. In
this tutorial, three different vocoders are used,
```WaveRNN`` <https://pytorch.org/audio/stable/models/wavernn.html>`__,
```Griffin-Lim`` <https://pytorch.org/audio/stable/transforms.html#griffinlim>`__,
and
```Nvidia's WaveGlow`` <https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/>`__.

The following figure illustrates the whole process.

.. image:: https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png


.. GENERATED FROM PYTHON SOURCE LINES 44-51

Preparation
-----------

First, we install the necessary dependencies. In addition to
``torchaudio``, ``DeepPhonemizer`` is required to perform phoneme-based
encoding.


.. GENERATED FROM PYTHON SOURCE LINES 53-57

.. code-block:: bash

    %%bash
    pip3 install deep_phonemizer

.. GENERATED FROM PYTHON SOURCE LINES 57-72

.. code-block:: default


    import torch
    import torchaudio
    import matplotlib.pyplot as plt

    import IPython

    print(torch.__version__)
    print(torchaudio.__version__)

    torch.random.manual_seed(0)
    device = "cuda" if torch.cuda.is_available() else "cpu"







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.12.1+cu102
    0.12.1+cu102




.. GENERATED FROM PYTHON SOURCE LINES 73-76

Text Processing
---------------


.. GENERATED FROM PYTHON SOURCE LINES 79-97

Character-based encoding
~~~~~~~~~~~~~~~~~~~~~~~~

In this section, we will go through how the character-based encoding
works.

Since the pre-trained Tacotron2 model expects specific set of symbol
tables, the same functionalities available in ``torchaudio``. This
section is more for the explanation of the basis of encoding.

Firstly, we define the set of symbols. For example, we can use
``'_-!\'(),.:;? abcdefghijklmnopqrstuvwxyz'``. Then, we will map the
each character of the input text into the index of the corresponding
symbol in the table.

The following is an example of such processing. In the example, symbols
that are not in the table are ignored.


.. GENERATED FROM PYTHON SOURCE LINES 97-110

.. code-block:: default


    symbols = '_-!\'(),.:;? abcdefghijklmnopqrstuvwxyz'
    look_up = {s: i for i, s in enumerate(symbols)}
    symbols = set(symbols)

    def text_to_sequence(text):
      text = text.lower()
      return [look_up[s] for s in text if s in symbols]

    text = "Hello world! Text to speech!"
    print(text_to_sequence(text))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15, 2, 11, 31, 16, 35, 31, 11, 31, 26, 11, 30, 27, 16, 16, 14, 19, 2]




.. GENERATED FROM PYTHON SOURCE LINES 111-116

As mentioned in the above, the symbol table and indices must match
what the pretrained Tacotron2 model expects. ``torchaudio`` provides the
transform along with the pretrained model. For example, you can
instantiate and use such transform as follow.


.. GENERATED FROM PYTHON SOURCE LINES 116-126

.. code-block:: default


    processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()

    text = "Hello world! Text to speech!"
    processed, lengths = processor(text)

    print(processed)
    print(lengths)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15,  2, 11, 31, 16, 35, 31, 11,
             31, 26, 11, 30, 27, 16, 16, 14, 19,  2]])
    tensor([28], dtype=torch.int32)




.. GENERATED FROM PYTHON SOURCE LINES 127-134

The ``processor`` object takes either a text or list of texts as inputs.
When a list of texts are provided, the returned ``lengths`` variable
represents the valid length of each processed tokens in the output
batch.

The intermediate representation can be retrieved as follow.


.. GENERATED FROM PYTHON SOURCE LINES 134-138

.. code-block:: default


    print([processor.tokens[i] for i in processed[0, :lengths[0]]])






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 't', 'e', 'x', 't', ' ', 't', 'o', ' ', 's', 'p', 'e', 'e', 'c', 'h', '!']




.. GENERATED FROM PYTHON SOURCE LINES 139-158

Phoneme-based encoding
~~~~~~~~~~~~~~~~~~~~~~

Phoneme-based encoding is similar to character-based encoding, but it
uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)
model.

The detail of the G2P model is out of scope of this tutorial, we will
just look at what the conversion looks like.

Similar to the case of character-based encoding, the encoding process is
expected to match what a pretrained Tacotron2 model is trained on.
``torchaudio`` has an interface to create the process.

The following code illustrates how to make and use the process. Behind
the scene, a G2P model is created using ``DeepPhonemizer`` package, and
the pretrained weights published by the author of ``DeepPhonemizer`` is
fetched.


.. GENERATED FROM PYTHON SOURCE LINES 158-171

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH

    processor = bundle.get_text_processor()

    text = "Hello world! Text to speech!"
    with torch.inference_mode():
      processed, lengths = processor(text)

    print(processed)
    print(lengths)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


      0%|          | 0.00/63.6M [00:00<?, ?B/s]
      0%|          | 16.0k/63.6M [00:00<07:23, 150kB/s]
      0%|          | 48.0k/63.6M [00:00<04:40, 238kB/s]
      0%|          | 112k/63.6M [00:00<02:45, 403kB/s] 
      0%|          | 248k/63.6M [00:00<01:29, 745kB/s]
      1%|          | 536k/63.6M [00:00<00:45, 1.45MB/s]
      2%|1         | 1.05M/63.6M [00:00<00:24, 2.68MB/s]
      3%|3         | 2.16M/63.6M [00:00<00:12, 5.25MB/s]
      7%|6         | 4.33M/63.6M [00:00<00:06, 10.2MB/s]
     11%|#         | 6.81M/63.6M [00:00<00:04, 14.4MB/s]
     15%|#4        | 9.41M/63.6M [00:01<00:03, 17.6MB/s]
     19%|#8        | 11.9M/63.6M [00:01<00:02, 19.6MB/s]
     23%|##2       | 14.5M/63.6M [00:01<00:02, 21.2MB/s]
     27%|##7       | 17.2M/63.6M [00:01<00:02, 22.6MB/s]
     31%|###1      | 20.0M/63.6M [00:01<00:01, 23.7MB/s]
     36%|###6      | 22.9M/63.6M [00:01<00:01, 25.0MB/s]
     40%|####      | 25.8M/63.6M [00:01<00:01, 25.6MB/s]
     45%|####4     | 28.3M/63.6M [00:01<00:01, 25.4MB/s]
     49%|####8     | 31.1M/63.6M [00:01<00:01, 25.7MB/s]
     53%|#####3    | 34.0M/63.6M [00:02<00:01, 26.1MB/s]
     58%|#####7    | 36.8M/63.6M [00:02<00:01, 26.4MB/s]
     62%|######2   | 39.7M/63.6M [00:02<00:00, 26.8MB/s]
     67%|######7   | 42.7M/63.6M [00:02<00:00, 27.2MB/s]
     72%|#######1  | 45.5M/63.6M [00:02<00:00, 27.1MB/s]
     76%|#######6  | 48.5M/63.6M [00:02<00:00, 27.4MB/s]
     81%|########  | 51.3M/63.6M [00:02<00:00, 27.5MB/s]
     85%|########5 | 54.3M/63.6M [00:02<00:00, 27.6MB/s]
     90%|########9 | 57.2M/63.6M [00:02<00:00, 27.8MB/s]
     94%|#########4| 60.1M/63.6M [00:03<00:00, 27.7MB/s]
     99%|#########8| 62.9M/63.6M [00:03<00:00, 27.6MB/s]
    100%|##########| 63.6M/63.6M [00:03<00:00, 20.9MB/s]
    tensor([[54, 20, 65, 69, 11, 92, 44, 65, 38,  2, 11, 81, 40, 64, 79, 81, 11, 81,
             20, 11, 79, 77, 59, 37,  2]])
    tensor([25], dtype=torch.int32)




.. GENERATED FROM PYTHON SOURCE LINES 172-177

Notice that the encoded values are different from the example of
character-based encoding.

The intermediate representation looks like the following.


.. GENERATED FROM PYTHON SOURCE LINES 177-181

.. code-block:: default


    print([processor.tokens[i] for i in processed[0, :lengths[0]]])






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['HH', 'AH', 'L', 'OW', ' ', 'W', 'ER', 'L', 'D', '!', ' ', 'T', 'EH', 'K', 'S', 'T', ' ', 'T', 'AH', ' ', 'S', 'P', 'IY', 'CH', '!']




.. GENERATED FROM PYTHON SOURCE LINES 182-199

Spectrogram Generation
----------------------

``Tacotron2`` is the model we use to generate spectrogram from the
encoded text. For the detail of the model, please refer to `the
paper <https://arxiv.org/abs/1712.05884>`__.

It is easy to instantiate a Tacotron2 model with pretrained weight,
however, note that the input to Tacotron2 models are processed by the
matching text processor.

``torchaudio`` bundles the matching models and processors together so
that it is easy to create the pipeline.

(For the available bundles, and its usage, please refer to `the
documentation <https://pytorch.org/audio/stable/pipelines.html#tacotron2-text-to-speech>`__.)


.. GENERATED FROM PYTHON SOURCE LINES 199-216

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH
    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)

    text = "Hello world! Text to speech!"

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, _, _ = tacotron2.infer(processed, lengths)


    plt.imshow(spec[0].cpu().detach())





.. image-sg:: /intermediate/images/sphx_glr_text_to_speech_with_torchaudio_001.png
   :alt: text to speech with torchaudio
   :srcset: /intermediate/images/sphx_glr_text_to_speech_with_torchaudio_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth

      0%|          | 0.00/107M [00:00<?, ?B/s]
      6%|6         | 6.88M/107M [00:00<00:01, 71.9MB/s]
     13%|#2        | 13.7M/107M [00:00<00:01, 59.6MB/s]
     18%|#8        | 19.6M/107M [00:00<00:01, 49.1MB/s]
     23%|##2       | 24.4M/107M [00:00<00:01, 46.6MB/s]
     27%|##6       | 29.0M/107M [00:00<00:01, 43.9MB/s]
     31%|###       | 33.2M/107M [00:00<00:02, 35.0MB/s]
     34%|###4      | 36.8M/107M [00:00<00:02, 31.9MB/s]
     37%|###7      | 40.0M/107M [00:01<00:02, 31.6MB/s]
     40%|####      | 43.2M/107M [00:01<00:02, 32.0MB/s]
     43%|####3     | 46.7M/107M [00:01<00:01, 33.2MB/s]
     46%|####6     | 49.9M/107M [00:01<00:01, 33.4MB/s]
     50%|####9     | 53.3M/107M [00:01<00:01, 34.0MB/s]
     53%|#####2    | 56.8M/107M [00:01<00:01, 34.5MB/s]
     56%|#####6    | 60.3M/107M [00:01<00:01, 35.2MB/s]
     59%|#####9    | 63.7M/107M [00:01<00:01, 35.2MB/s]
     63%|######2   | 67.2M/107M [00:01<00:01, 35.5MB/s]
     66%|######5   | 70.6M/107M [00:02<00:01, 26.4MB/s]
     68%|######8   | 73.4M/107M [00:02<00:01, 25.0MB/s]
     71%|#######   | 76.1M/107M [00:02<00:01, 18.5MB/s]
     73%|#######2  | 78.2M/107M [00:02<00:01, 18.5MB/s]
     75%|#######5  | 81.0M/107M [00:02<00:01, 20.6MB/s]
     78%|#######8  | 84.1M/107M [00:02<00:01, 22.8MB/s]
     81%|########1 | 87.4M/107M [00:02<00:00, 25.6MB/s]
     84%|########4 | 90.5M/107M [00:03<00:00, 27.5MB/s]
     87%|########7 | 93.7M/107M [00:03<00:00, 29.0MB/s]
     90%|######### | 96.8M/107M [00:03<00:00, 29.8MB/s]
     93%|#########3| 100M/107M [00:03<00:00, 31.5MB/s] 
     96%|#########6| 104M/107M [00:03<00:00, 32.3MB/s]
    100%|#########9| 107M/107M [00:03<00:00, 33.6MB/s]
    100%|##########| 107M/107M [00:03<00:00, 31.7MB/s]

    <matplotlib.image.AxesImage object at 0x7f4810078b80>



.. GENERATED FROM PYTHON SOURCE LINES 217-220

Note that ``Tacotron2.infer`` method perfoms multinomial sampling,
therefor, the process of generating the spectrogram incurs randomness.


.. GENERATED FROM PYTHON SOURCE LINES 220-228

.. code-block:: default


    for _ in range(3):
      with torch.inference_mode():
        spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
      plt.imshow(spec[0].cpu().detach())
      plt.show()





.. image-sg:: /intermediate/images/sphx_glr_text_to_speech_with_torchaudio_002.png
   :alt: text to speech with torchaudio
   :srcset: /intermediate/images/sphx_glr_text_to_speech_with_torchaudio_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 229-238

Waveform Generation
-------------------

Once the spectrogram is generated, the last process is to recover the
waveform from the spectrogram.

``torchaudio`` provides vocoders based on ``GriffinLim`` and
``WaveRNN``.


.. GENERATED FROM PYTHON SOURCE LINES 241-247

WaveRNN
~~~~~~~

Continuing from the previous section, we can instantiate the matching
WaveRNN model from the same bundle.


.. GENERATED FROM PYTHON SOURCE LINES 247-267

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH

    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)
    vocoder = bundle.get_vocoder().to(device)

    text = "Hello world! Text to speech!"

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
      waveforms, lengths = vocoder(spec, spec_lengths)

    torchaudio.save("output_wavernn.wav", waveforms[0:1].cpu(), sample_rate=vocoder.sample_rate)
    IPython.display.display(IPython.display.Audio("output_wavernn.wav"))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/wavernn_10k_epochs_8bits_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/wavernn_10k_epochs_8bits_ljspeech.pth

      0%|          | 0.00/16.7M [00:00<?, ?B/s]
     37%|###6      | 6.16M/16.7M [00:00<00:00, 64.4MB/s]
     89%|########9 | 14.9M/16.7M [00:00<00:00, 80.2MB/s]
    100%|##########| 16.7M/16.7M [00:00<00:00, 69.4MB/s]
    <IPython.lib.display.Audio object>




.. GENERATED FROM PYTHON SOURCE LINES 268-274

Griffin-Lim
~~~~~~~~~~~

Using the Griffin-Lim vocoder is same as WaveRNN. You can instantiate
the vocode object with ``get_vocoder`` method and pass the spectrogram.


.. GENERATED FROM PYTHON SOURCE LINES 274-292

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH

    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)
    vocoder = bundle.get_vocoder().to(device)

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
    waveforms, lengths = vocoder(spec, spec_lengths)

    torchaudio.save("output_griffinlim.wav", waveforms[0:1].cpu(), sample_rate=vocoder.sample_rate)
    IPython.display.display(IPython.display.Audio("output_griffinlim.wav"))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_ljspeech.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_ljspeech.pth

      0%|          | 0.00/107M [00:00<?, ?B/s]
      6%|6         | 6.57M/107M [00:00<00:01, 68.9MB/s]
     14%|#3        | 14.7M/107M [00:00<00:01, 75.1MB/s]
     28%|##7       | 29.8M/107M [00:00<00:00, 112MB/s] 
     38%|###7      | 40.6M/107M [00:00<00:00, 106MB/s]
     52%|#####1    | 55.7M/107M [00:00<00:00, 124MB/s]
     65%|######5   | 70.0M/107M [00:00<00:00, 132MB/s]
     77%|#######7  | 83.2M/107M [00:00<00:00, 134MB/s]
     89%|########9 | 96.1M/107M [00:00<00:00, 123MB/s]
    100%|##########| 107M/107M [00:00<00:00, 123MB/s] 
    <IPython.lib.display.Audio object>




.. GENERATED FROM PYTHON SOURCE LINES 293-300

Waveglow
~~~~~~~~

Waveglow is a vocoder published by Nvidia. The pretrained weights are
published on Torch Hub. One can instantiate the model using ``torch.hub``
module.


.. GENERATED FROM PYTHON SOURCE LINES 300-327

.. code-block:: default

    if torch.cuda.is_available():
      waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')
    else:
      # Workaround to load model mapped on GPU
      # https://stackoverflow.com/a/61840832
      waveglow = torch.hub.load(
          "NVIDIA/DeepLearningExamples:torchhub",
          "nvidia_waveglow",
          model_math="fp32",
          pretrained=False,
      )
      checkpoint = torch.hub.load_state_dict_from_url(
          "https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427",
          progress=False,
          map_location=device,
      )
      state_dict = {key.replace("module.", ""): value for key, value in checkpoint["state_dict"].items()}

    waveglow = waveglow.remove_weightnorm(waveglow)
    waveglow = waveglow.to(device)
    waveglow.eval()

    with torch.no_grad():
      waveforms = waveglow.infer(spec)

    torchaudio.save("output_waveglow.wav", waveforms[0:1].cpu(), sample_rate=22050)
    IPython.display.display(IPython.display.Audio("output_waveglow.wav"))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torch/hub.py:266: UserWarning:

    You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour

    Downloading: "https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub" to /var/lib/jenkins/.cache/torch/hub/torchhub.zip
    /var/lib/jenkins/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning:

    pytorch_quantization module not found, quantization will not be available

    /var/lib/jenkins/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning:

    pytorch_quantization module not found, quantization will not be available

    Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427
    <IPython.lib.display.Audio object>





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  36.782 seconds)


.. _sphx_glr_download_intermediate_text_to_speech_with_torchaudio.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: text_to_speech_with_torchaudio.py <text_to_speech_with_torchaudio.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: text_to_speech_with_torchaudio.ipynb <text_to_speech_with_torchaudio.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
