
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "intermediate/reinforcement_ppo.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_intermediate_reinforcement_ppo.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_intermediate_reinforcement_ppo.py:


Reinforcement Learning (PPO) with TorchRL Tutorial
==================================================
**Author**: `Vincent Moens <https://github.com/vmoens>`_

This tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` to train a parametric policy
network to solve the Inverted Pendulum task from the `OpenAI-Gym/Farama-Gymnasium
control library <https://github.com/Farama-Foundation/Gymnasium>`__.

.. figure:: /_static/img/invpendulum.gif
   :alt: Inverted pendulum

   Inverted pendulum

Key learnings:

- How to create an environment in TorchRL, transform its outputs, and collect data from this env;
- How to make your classes talk to each other using :class:`tensordict.TensorDict`;
- The basics of building your training loop with TorchRL:

  - How to compute the advantage signal for policy gradient methods;
  - How to create a stochastic policy using a probabilistic neural network;
  - How to create a dynamic replay buffer and sample from it without repetition.

We will cover six crucial components of TorchRL:

* `environments <https://pytorch.org/rl/reference/envs.html>`__
* `transforms <https://pytorch.org/rl/reference/envs.html#transforms>`__
* `models (policy and value function) <https://pytorch.org/rl/reference/modules.html>`__
* `loss modules <https://pytorch.org/rl/reference/objectives.html>`__
* `data collectors <https://pytorch.org/rl/reference/collectors.html>`__
* `replay buffers <https://pytorch.org/rl/reference/data.html#replay-buffers>`__

.. GENERATED FROM PYTHON SOURCE LINES 38-106

If you are running this in Google Colab, make sure you install the following dependencies:

.. code-block:: bash

   !pip3 install torchrl
   !pip3 install gym[mujoco]
   !pip3 install tqdm

Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a
batch of data is being collected and directly consumed to train the policy to maximise
the expected return given some proximality constraints. You can think of it
as a sophisticated version of `REINFORCE <https://link.springer.com/content/pdf/10.1007/BF00992696.pdf>`_,
the foundational policy-optimization algorithm. For more information, see the
`Proximal Policy Optimization Algorithms <https://arxiv.org/abs/1707.06347>`_ paper.

PPO is usually regarded as a fast and efficient method for online, on-policy
reinforcement algorithm. TorchRL provides a loss-module that does all the work
for you, so that you can rely on this implementation and focus on solving your
problem rather than re-inventing the wheel every time you want to train a policy.

For completeness, here is a brief overview of what the loss computes, even though
this is taken care of by our :class:`ClipPPOLoss` moduleâ€”the algorithm works as follows:
1. we will sample a batch of data by playing the
policy in the environment for a given number of steps.
2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using
a clipped version of the REINFORCE loss.
3. The clipping will put a pessimistic bound on our loss: lower return estimates will
be favored compared to higher ones.
The precise formula of the loss is:

.. math::

    L(s,a,\theta_k,\theta) = \min\left(
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
    g(\epsilon, A^{\pi_{\theta_k}}(s,a))
    \right),

There are two components in that loss: in the first part of the minimum operator,
we simply compute an importance-weighted version of the REINFORCE loss (for example, a
REINFORCE loss that we have corrected for the fact that the current policy
configuration lags the one that was used for the data collection).
The second part of that minimum operator is a similar loss where we have clipped
the ratios when they exceeded or were below a given pair of thresholds.

This loss ensures that whether the advantage is positive or negative, policy
updates that would produce significant shifts from the previous configuration
are being discouraged.

This tutorial is structured as follows:

1. First, we will define a set of hyperparameters we will be using for training.

2. Next, we will focus on creating our environment, or simulator, using TorchRL's
   wrappers and transforms.

3. Next, we will design the policy network and the value model,
   which is indispensable to the loss function. These modules will be used
   to configure our loss module.

4. Next, we will create the replay buffer and data loader.

5. Finally, we will run our training loop and analyze the results.

Throughout this tutorial, we'll be using the :mod:`tensordict` library.
:class:`tensordict.TensorDict` is the lingua franca of TorchRL: it helps us abstract
what a module reads and writes and care less about the specific data
description and more about the algorithm itself.


.. GENERATED FROM PYTHON SOURCE LINES 106-132

.. code-block:: default


    from collections import defaultdict

    import matplotlib.pyplot as plt
    import torch
    from tensordict.nn import TensorDictModule
    from tensordict.nn.distributions import NormalParamExtractor
    from torch import nn
    from torchrl.collectors import SyncDataCollector
    from torchrl.data.replay_buffers import ReplayBuffer
    from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement
    from torchrl.data.replay_buffers.storages import LazyTensorStorage
    from torchrl.envs import (
        Compose,
        DoubleToFloat,
        ObservationNorm,
        StepCounter,
        TransformedEnv,
    )
    from torchrl.envs.libs.gym import GymEnv
    from torchrl.envs.utils import check_env_specs, set_exploration_mode
    from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator
    from torchrl.objectives import ClipPPOLoss
    from torchrl.objectives.value import GAE
    from tqdm import tqdm





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning:

    failed to set start method to spawn, and current start method for mp is fork.

    /opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:252: UserWarning:

    Got multiple backends for torchrl.envs.libs.gym._get_gym_envs. Using the last queried (<module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'> with version 0.27.0).

    /opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:252: UserWarning:

    Got multiple backends for torchrl.envs.libs.gym._build_gym_env. Using the last queried (<module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'> with version 0.27.0).

    /opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:252: UserWarning:

    Got multiple backends for torchrl.envs.libs.gym._set_seed_initial. Using the last queried (<module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'> with version 0.27.0).

    /opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:252: UserWarning:

    Got multiple backends for torchrl.envs.libs.gym._set_gym_args. Using the last queried (<module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'> with version 0.27.0).

    /opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:252: UserWarning:

    Got multiple backends for torchrl.envs.libs.gym._set_gym_default. Using the last queried (<module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'> with version 0.27.0).





.. GENERATED FROM PYTHON SOURCE LINES 133-144

Define Hyperparameters
----------------------

We set the hyperparameters for our algorithm. Depending on the resources
available, one may choose to execute the policy on GPU or on another
device.
The ``frame_skip`` will control how for how many frames is a single
action being executed. The rest of the arguments that count frames
must be corrected for this value (since one environment step will
actually return ``frame_skip`` frames).


.. GENERATED FROM PYTHON SOURCE LINES 144-150

.. code-block:: default


    device = "cpu" if not torch.has_cuda else "cuda:0"
    num_cells = 256  # number of cells in each layer
    lr = 3e-4
    max_grad_norm = 1.0








.. GENERATED FROM PYTHON SOURCE LINES 151-172

Data collection parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~

When collecting data, we will be able to choose how big each batch will be
by defining a ``frames_per_batch`` parameter. We will also define how many
frames (such as the number of interactions with the simulator) we will allow ourselves to
use. In general, the goal of an RL algorithm is to learn to solve the task
as fast as it can in terms of environment interactions: the lower the ``total_frames``
the better.
We also define a ``frame_skip``: in some contexts, repeating the same action
multiple times over the course of a trajectory may be beneficial as it makes
the behavior more consistent and less erratic. However, "skipping"
too many frames will hamper training by reducing the reactivity of the actor
to observation changes.

When using ``frame_skip`` it is good practice to
correct the other frame counts by the number of frames we are grouping
together. If we configure a total count of X frames for training but
use a ``frame_skip`` of Y, we will be actually collecting XY frames in total
which exceeds our predefined budget.


.. GENERATED FROM PYTHON SOURCE LINES 172-177

.. code-block:: default

    frame_skip = 1
    frames_per_batch = 1000 // frame_skip
    # For a complete training, bring the number of frames up to 1M
    total_frames = 50_000 // frame_skip








.. GENERATED FROM PYTHON SOURCE LINES 178-189

PPO parameters
~~~~~~~~~~~~~~

At each data collection (or batch collection) we will run the optimization
over a certain number of *epochs*, each time consuming the entire data we just
acquired in a nested training loop. Here, the ``sub_batch_size`` is different from the
``frames_per_batch`` here above: recall that we are working with a "batch of data"
coming from our collector, which size is defined by ``frames_per_batch``, and that
we will further split in smaller sub-batches during the inner training loop.
The size of these sub-batches is controlled by ``sub_batch_size``.


.. GENERATED FROM PYTHON SOURCE LINES 189-198

.. code-block:: default

    sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop
    num_epochs = 10  # optimisation steps per batch of data collected
    clip_epsilon = (
        0.2  # clip value for PPO loss: see the equation in the intro for more context.
    )
    gamma = 0.99
    lmbda = 0.95
    entropy_eps = 1e-4








.. GENERATED FROM PYTHON SOURCE LINES 199-210

Define an environment
---------------------

In RL, an *environment* is usually the way we refer to a simulator or a
control system. Various libraries provide simulation environments for reinforcement
learning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and
many others.
As a generalistic library, TorchRL's goal is to provide an interchangeable interface
to a large panel of RL simulators, allowing you to easily swap one environment
with another. For example, creating a wrapped gym environment can be achieved with few characters:


.. GENERATED FROM PYTHON SOURCE LINES 210-213

.. code-block:: default


    base_env = GymEnv("InvertedDoublePendulum-v4", device=device, frame_skip=frame_skip)








.. GENERATED FROM PYTHON SOURCE LINES 214-264

There are a few things to notice in this code: first, we created
the environment by calling the ``GymEnv`` wrapper. If extra keyword arguments
are passed, they will be transmitted to the ``gym.make`` method, hence covering
the most common env construction commands.
Alternatively, one could also directly create a gym environment using ``gym.make(env_name, **kwargs)``
and wrap it in a `GymWrapper` class.

Also the ``device`` argument: for gym, this only controls the device where
input action and observered states will be stored, but the execution will always
be done on CPU. The reason for this is simply that gym does not support on-device
execution, unless specified otherwise. For other libraries, we have control over
the execution device and, as much as we can, we try to stay consistent in terms of
storing and execution backends.

Transforms
~~~~~~~~~~

We will append some transforms to our environments to prepare the data for
the policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different
approach, more similar to other pytorch domain libraries, through the use of transforms.
To add transforms to an environment, one should simply wrap it in a :class:`TransformedEnv`
instance, and append the sequence of transforms to it. The transformed env will inherit
the device and meta-data of the wrapped env, and transform these depending on the sequence
of transforms it contains.

Normalization
~~~~~~~~~~~~~

The first to encode is a normalization transform.
As a rule of thumbs, it is preferable to have data that loosely
match a unit Gaussian distribution: to obtain this, we will
run a certain number of random steps in the environment and compute
the summary statistics of these observations.

We'll append two other transforms: the :class:`DoubleToFloat` transform will
convert double entries to single-precision numbers, ready to be read by the
policy. The :class:`StepCounter` transform will be used to count the steps before
the environment is terminated. We will use this measure as a supplementary measure
of performance.

As we will see later, many of the TorchRL's classes rely on :class:`tensordict.TensorDict`
to communicate. You could think of it as a python dictionary with some extra
tensor features. In practice, this means that many modules we will be working
with need to be told what key to read (``in_keys``) and what key to write
(``out_keys``) in the tensordict they will receive. Usually, if ``out_keys``
is omitted, it is assumed that the ``in_keys`` entries will be updated
in-place. For our transforms, the only entry we are interested in is referred
to as ``"observation"`` and our transform layers will be told to modify this
entry and this entry only:


.. GENERATED FROM PYTHON SOURCE LINES 264-275

.. code-block:: default


    env = TransformedEnv(
        base_env,
        Compose(
            # normalize observations
            ObservationNorm(in_keys=["observation"]),
            DoubleToFloat(in_keys=["observation"]),
            StepCounter(),
        ),
    )








.. GENERATED FROM PYTHON SOURCE LINES 276-280

As you may have noticed, we have created a normalization layer but we did not
set its normalization parameters. To do this, :class:`ObservationNorm` can
automatically gather the summary statistics of our environment:


.. GENERATED FROM PYTHON SOURCE LINES 280-282

.. code-block:: default

    env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)








.. GENERATED FROM PYTHON SOURCE LINES 283-288

The :class:`ObservationNorm` transform has now been populated with a
location and a scale that will be used to normalize the data.

Let us do a little sanity check for the shape of our summary stats:


.. GENERATED FROM PYTHON SOURCE LINES 288-290

.. code-block:: default

    print("normalization constant shape:", env.transform[0].loc.shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    normalization constant shape: torch.Size([11])




.. GENERATED FROM PYTHON SOURCE LINES 291-309

An environment is not only defined by its simulator and transforms, but also
by a series of metadata that describe what can be expected during its
execution.
For efficiency purposes, TorchRL is quite stringent when it comes to
environment specs, but you can easily check that your environment specs are
adequate.
In our example, the :class:`GymWrapper` and :class:`GymEnv` that inherits
from it already take care of setting the proper specs for your env so
you should not have to care about this.

Nevertheless, let's see a concrete example using our transformed
environment by looking at its specs.
There are three specs to look at: ``observation_spec`` which defines what
is to be expected when executing an action in the environment,
``reward_spec`` which indicates the reward domain and finally the
``input_spec`` (which contains the ``action_spec``) and which represents
everything an environment requires to execute a single step.


.. GENERATED FROM PYTHON SOURCE LINES 309-314

.. code-block:: default

    print("observation_spec:", env.observation_spec)
    print("reward_spec:", env.reward_spec)
    print("input_spec:", env.input_spec)
    print("action_spec (as defined by input_spec):", env.action_spec)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    observation_spec: CompositeSpec(
        observation: UnboundedContinuousTensorSpec(
             shape=torch.Size([11]), space=None, device=cuda:0, dtype=torch.float32, domain=continuous),
        step_count: UnboundedDiscreteTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))
    reward_spec: UnboundedContinuousTensorSpec(
         shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)
    input_spec: CompositeSpec(
        action: BoundedTensorSpec(
             shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous),
        step_count: UnboundedDiscreteTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))
    action_spec (as defined by input_spec): BoundedTensorSpec(
         shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)




.. GENERATED FROM PYTHON SOURCE LINES 315-318

the :func:`check_env_specs` function runs a small rollout and compares its output against the environemnt
specs. If no error is raised, we can be confident that the specs are properly defined:


.. GENERATED FROM PYTHON SOURCE LINES 318-320

.. code-block:: default

    check_env_specs(env)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    check_env_specs succeeded!




.. GENERATED FROM PYTHON SOURCE LINES 321-335

For fun, let's see what a simple random rollout looks like. You can
call `env.rollout(n_steps)` and get an overview of what the environment inputs
and outputs look like. Actions will automatically be drawn from the action spec
domain, so you don't need to care about designing a random sampler.

Typically, at each step, an RL environment receives an
action as input, and outputs an observation, a reward and a done state. The
observation may be composite, meaning that it could be composed of more than one
tensor. This is not a problem for TorchRL, since the whole set of observations
is automatically packed in the output :class:`tensordict.TensorDict`. After executing a rollout
(ie a sequence of environment steps and random action generations) over a given
number of steps, we will retrieve a :class:`tensordict.TensorDict` instance with a shape
that matches this trajectory length:


.. GENERATED FROM PYTHON SOURCE LINES 335-339

.. code-block:: default

    rollout = env.rollout(3)
    print("rollout of three steps:", rollout)
    print("Shape of the rollout TensorDict:", rollout.batch_size)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    rollout of three steps: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            next: TensorDict(
                fields={
                    done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),
                    observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),
                    reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),
                    step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},
                batch_size=torch.Size([3]),
                device=cuda:0,
                is_shared=True),
            observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),
            reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},
        batch_size=torch.Size([3]),
        device=cuda:0,
        is_shared=True)
    Shape of the rollout TensorDict: torch.Size([3])




.. GENERATED FROM PYTHON SOURCE LINES 340-374

Our rollout data has a shape of ``torch.Size([3])`, which matches the number of steps
we ran it for. The ``"next"`` entry points to the data coming after the current step.
In most cases, the ``"next""`` data at time `t` matches the data at ``t+1``, but this
may not be the case if we are using some specific transformations (e.g. mutli-step).

Policy
------

PPO utilizes a stochastic policy to handle exploration. This means that our
neural network will have to output the parameters of a distribution, rather
than a single value corresponding to the action taken.

As the data is continuous, we use a Tanh-Normal distribution to respect the
action space boundaries. TorchRL provides such distribution, and the only
thing we need to care about is to build a neural network that outputs the
right number of parameters for the policy to work with (a location, or mean,
and a scale):

.. math::

    f_{\theta}(\text{observation}) = \mu_{\theta}(\text{observation}), \sigma^{+}_{\theta}(\text{observation})

The only extra-difficulty that is brought up here is to split our output in two
equal parts and map the second to a scrictly positive space.

We design the policy in three steps:

1. Define a neural network ``D_obs`` -> ``2 * D_action``. Indeed, our ``loc`` (mu) and ``scale`` (sigma) both have dimension ``D_action``;

2. Append a :class:`NormalParamExtractor` to extract a location and a scale (ie splits the input in two equal parts
  and applies a positive transformation to the scale parameter);

3. Create a probabilistic :class:`TensorDictModule` that can create this distribution and sample from it.


.. GENERATED FROM PYTHON SOURCE LINES 374-386

.. code-block:: default


    actor_net = nn.Sequential(
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),
        NormalParamExtractor(),
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:

    Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.





.. GENERATED FROM PYTHON SOURCE LINES 387-392

To enable the policy to "talk" with the environment through the tensordict
data carrier, we wrap the ``nn.Module`` in a :class:`TensorDictModule`. This
class will simply ready the ``in_keys`` it is provided with and write the
outputs in-place at the registered ``out_keys``.


.. GENERATED FROM PYTHON SOURCE LINES 392-396

.. code-block:: default

    policy_module = TensorDictModule(
        actor_net, in_keys=["observation"], out_keys=["loc", "scale"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 397-411

We now need to build a distribution out of the location and scale of our
normal distribution. To do so, we instruct the :class:`ProbabilisticActor`
class to build a :class:`TanhNormal` out of the location and scale
parameters. We also provide the minimum and maximum values of this
distribution, which we gather from the environment specs.

The name of the ``in_keys`` (and hence the name of the ``out_keys`` from
the :class:`TensorDictModule` above) cannot be set to any value one may
like, as the :class:`TanhNormal` distribution constructor will expect the
``loc`` and ``scale`` keyword arguments. That being said,
:class:`ProbabilisticActor` also accepts ``Dict[str, str]`` typed ``in_keys``
where the key-value pair indicates what ``in_key`` string should be used for
every keyword argument that is to be used.


.. GENERATED FROM PYTHON SOURCE LINES 411-424

.. code-block:: default

    policy_module = ProbabilisticActor(
        module=policy_module,
        spec=env.action_spec,
        in_keys=["loc", "scale"],
        distribution_class=TanhNormal,
        distribution_kwargs={
            "min": env.action_spec.space.minimum,
            "max": env.action_spec.space.maximum,
        },
        return_log_prob=True,
        # we'll need the log-prob for the numerator of the importance weights
    )








.. GENERATED FROM PYTHON SOURCE LINES 425-436

Value network
-------------

The value network is a crucial component of the PPO algorithm, even though it
won't be used at inference time. This module will read the observations and
return an estimation of the discounted return for the following trajectory.
This allows us to amortize learning by relying on the some utility estimation
that is learnt on-the-fly during training. Our value network share the same
structure as the policy, but for simplicity we assign it its own set of
parameters.


.. GENERATED FROM PYTHON SOURCE LINES 436-451

.. code-block:: default

    value_net = nn.Sequential(
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(num_cells, device=device),
        nn.Tanh(),
        nn.LazyLinear(1, device=device),
    )

    value_module = ValueOperator(
        module=value_net,
        in_keys=["observation"],
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:

    Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.





.. GENERATED FROM PYTHON SOURCE LINES 452-457

let's try our policy and value modules. As we said earlier, the usage of
:class:`TensorDictModule` makes it possible to directly read the output
of the environment to run these modules, as they know what information to read
and where to write it:


.. GENERATED FROM PYTHON SOURCE LINES 457-460

.. code-block:: default

    print("Running policy:", policy_module(env.reset()))
    print("Running value:", value_module(env.reset()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running policy: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),
            reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),
            scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},
        batch_size=torch.Size([]),
        device=cuda:0,
        is_shared=True)
    Running value: TensorDict(
        fields={
            done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),
            observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),
            reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),
            step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},
        batch_size=torch.Size([]),
        device=cuda:0,
        is_shared=True)




.. GENERATED FROM PYTHON SOURCE LINES 461-490

Data collector
--------------

TorchRL provides a set of :class:`DataCollector` classes. Briefly, these
classes execute three operations: reset an environment, compute an action
given the latest observation, execute a step in the environment, and repeat
the last two steps until the environment reaches a stop signal (or ``"done"``
state).

They allow you to control how many frames to collect at each iteration
(through the ``frames_per_batch`` parameter),
when to reset the environment (through the ``max_frames_per_traj`` argument),
on which ``device`` the policy should be executed, etc. They are also
designed to work efficiently with batched and multiprocessed environments.

The simplest data collector is the :class:`SyncDataCollector`: it is an
iterator that you can use to get batches of data of a given length, and
that will stop once a total number of frames (``total_frames``) have been
collected.
Other data collectors (``MultiSyncDataCollector`` and
``MultiaSyncDataCollector``) will execute the same operations in synchronous
and asynchronous manner over a set of multiprocessed workers.

As for the policy and environment before, the data collector will return
:class:`tensordict.TensorDict` instances with a total number of elements that will
match ``frames_per_batch``. Using :class:`tensordict.TensorDict` to pass data to the
training loop allows you to write dataloading pipelines
that are 100% oblivious to the actual specificities of the rollout content.


.. GENERATED FROM PYTHON SOURCE LINES 490-499

.. code-block:: default

    collector = SyncDataCollector(
        env,
        policy_module,
        frames_per_batch=frames_per_batch,
        total_frames=total_frames,
        split_trajs=False,
        device=device,
    )








.. GENERATED FROM PYTHON SOURCE LINES 500-518

Replay buffer
-------------

Replay buffers are a common building piece of off-policy RL algorithms.
In on-policy contexts, a replay buffer is refilled every time a batch of
data is collected, and its data is repeatedly consumed for a certain number
of epochs.

TorchRL's replay buffers are built using a common container
:class:`ReplayBuffer` which takes as argument the components of the buffer:
a storage, a writer, a sampler and possibly some transforms. Only the
storage (which indicates the replay buffer capacity) is mandatory. We
also specify a sampler without repetition to avoid sampling multiple times
the same item in one epoch.
Using a replay buffer for PPO is not mandatory and we could simply
sample the sub-batches from the collected batch, but using these classes
make it easy for us to build the inner training loop in a reproducible way.


.. GENERATED FROM PYTHON SOURCE LINES 518-524

.. code-block:: default


    replay_buffer = ReplayBuffer(
        storage=LazyTensorStorage(frames_per_batch),
        sampler=SamplerWithoutReplacement(),
    )








.. GENERATED FROM PYTHON SOURCE LINES 525-546

Loss function
-------------

The PPO loss can be directly imported from torchrl for convenience using the
:class:`ClipPPOLoss` class. This is the easiest way of utilizing PPO:
it hides away the mathematical operations of PPO and the control flow that
goes with it.

PPO requires some "advantage estimation" to be computed. In short, an advantage
is a value that reflects an expectancy over the return value while dealing with
the bias / variance tradeoff.
To compute the advantage, one just needs to (1) build the advantage module, which
utilizes our value operator, and (2) pass each batch of data through it before each
epoch.
The GAE module will update the input tensordict with new ``"advantage"`` and
``"value_target"`` entries.
The ``"value_target"`` is a gradient-free tensor that represents the empirical
value that the value network should represent with the input observation.
Both of these will be used by :class:`ClipPPOLoss` to
return the policy and value losses.


.. GENERATED FROM PYTHON SOURCE LINES 546-570

.. code-block:: default


    advantage_module = GAE(
        gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True
    )

    loss_module = ClipPPOLoss(
        actor=policy_module,
        critic=value_module,
        advantage_key="advantage",
        clip_epsilon=clip_epsilon,
        entropy_bonus=bool(entropy_eps),
        entropy_coef=entropy_eps,
        # these keys match by default but we set this for completeness
        value_target_key=advantage_module.value_target_key,
        critic_coef=1.0,
        gamma=0.99,
        loss_critic_type="smooth_l1",
    )

    optim = torch.optim.Adam(loss_module.parameters(), lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optim, total_frames // frames_per_batch, 0.0
    )








.. GENERATED FROM PYTHON SOURCE LINES 571-589

Training loop
-------------
We now have all the pieces needed to code our training loop.
The steps include:

* Collect data

  * Compute advantage

    * Loop over the collected to compute loss values
    * Back propagate
    * Optimize
    * Repeat

  * Repeat

* Repeat


.. GENERATED FROM PYTHON SOURCE LINES 589-659

.. code-block:: default



    logs = defaultdict(list)
    pbar = tqdm(total=total_frames * frame_skip)
    eval_str = ""

    # We iterate over the collector until it reaches the total number of frames it was
    # designed to collect:
    for i, tensordict_data in enumerate(collector):
        # we now have a batch of data to work with. Let's learn something from it.
        for _ in range(num_epochs):
            # We'll need an "advantage" signal to make PPO work.
            # We re-compute it at each epoch as its value depends on the value
            # network which is updated in the inner loop.
            advantage_module(tensordict_data)
            data_view = tensordict_data.reshape(-1)
            replay_buffer.extend(data_view.cpu())
            for _ in range(frames_per_batch // sub_batch_size):
                subdata, *_ = replay_buffer.sample(sub_batch_size)
                loss_vals = loss_module(subdata.to(device))
                loss_value = (
                    loss_vals["loss_objective"]
                    + loss_vals["loss_critic"]
                    + loss_vals["loss_entropy"]
                )

                # Optimization: backward, grad clipping and optim step
                loss_value.backward()
                # this is not strictly mandatory but it's good practice to keep
                # your gradient norm bounded
                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)
                optim.step()
                optim.zero_grad()

        logs["reward"].append(tensordict_data["next", "reward"].mean().item())
        pbar.update(tensordict_data.numel() * frame_skip)
        cum_reward_str = (
            f"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})"
        )
        logs["step_count"].append(tensordict_data["step_count"].max().item())
        stepcount_str = f"step count (max): {logs['step_count'][-1]}"
        logs["lr"].append(optim.param_groups[0]["lr"])
        lr_str = f"lr policy: {logs['lr'][-1]: 4.4f}"
        if i % 10 == 0:
            # We evaluate the policy once every 10 batches of data.
            # Evaluation is rather simple: execute the policy without exploration
            # (take the expected value of the action distribution) for a given
            # number of steps (1000, which is our env horizon).
            # The ``rollout`` method of the env can take a policy as argument:
            # it will then execute this policy at each step.
            with set_exploration_mode("mean"), torch.no_grad():
                # execute a rollout with the trained policy
                eval_rollout = env.rollout(1000, policy_module)
                logs["eval reward"].append(eval_rollout["next", "reward"].mean().item())
                logs["eval reward (sum)"].append(
                    eval_rollout["next", "reward"].sum().item()
                )
                logs["eval step_count"].append(eval_rollout["step_count"].max().item())
                eval_str = (
                    f"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} "
                    f"(init: {logs['eval reward (sum)'][0]: 4.4f}), "
                    f"eval step-count: {logs['eval step_count'][-1]}"
                )
                del eval_rollout
        pbar.set_description(", ".join([eval_str, cum_reward_str, stepcount_str, lr_str]))

        # We're also using a learning rate scheduler. Like the gradient clipping,
        # this is a nice-to-have but nothing necessary for PPO to work.
        scheduler.step()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


      0%|          | 0/50000 [00:00<?, ?it/s]Creating a TensorStorage...

      2%|2         | 1000/50000 [00:07<05:47, 141.01it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.0758 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   2%|2         | 1000/50000 [00:07<05:47, 141.01it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.0758 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14<05:45, 139.08it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1356 (init= 9.0758), step count (max): 17, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14<05:45, 139.08it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1356 (init= 9.0758), step count (max): 17, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21<05:32, 141.47it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1575 (init= 9.0758), step count (max): 25, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21<05:32, 141.47it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1575 (init= 9.0758), step count (max): 25, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28<05:22, 142.70it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1790 (init= 9.0758), step count (max): 24, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28<05:22, 142.70it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1790 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35<05:16, 142.13it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2034 (init= 9.0758), step count (max): 25, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35<05:16, 142.13it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2034 (init= 9.0758), step count (max): 25, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42<05:08, 142.59it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2070 (init= 9.0758), step count (max): 25, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42<05:08, 142.59it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2070 (init= 9.0758), step count (max): 25, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49<04:59, 143.54it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1884 (init= 9.0758), step count (max): 30, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49<04:59, 143.54it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1884 (init= 9.0758), step count (max): 30, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56<04:53, 142.89it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2143 (init= 9.0758), step count (max): 32, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56<04:53, 142.89it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2143 (init= 9.0758), step count (max): 32, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03<04:45, 143.78it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1972 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03<04:45, 143.78it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.1972 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  20%|##        | 10000/50000 [01:09<04:37, 144.13it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2050 (init= 9.0758), step count (max): 35, lr policy:  0.0003:  20%|##        | 10000/50000 [01:09<04:37, 144.13it/s]
    eval cumulative reward:  120.5691 (init:  120.5691), eval step-count: 12, average reward= 9.2050 (init= 9.0758), step count (max): 35, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:16<04:31, 143.43it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2004 (init= 9.0758), step count (max): 44, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:17<04:31, 143.43it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2004 (init= 9.0758), step count (max): 44, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:23<04:23, 143.96it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2275 (init= 9.0758), step count (max): 30, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:23<04:23, 143.96it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2275 (init= 9.0758), step count (max): 30, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31<04:23, 140.55it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2235 (init= 9.0758), step count (max): 48, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31<04:23, 140.55it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2235 (init= 9.0758), step count (max): 48, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:38<04:20, 138.30it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2083 (init= 9.0758), step count (max): 54, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:38<04:20, 138.30it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2083 (init= 9.0758), step count (max): 54, lr policy:  0.0003:  30%|###       | 15000/50000 [01:45<04:09, 140.23it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2111 (init= 9.0758), step count (max): 37, lr policy:  0.0002:  30%|###       | 15000/50000 [01:45<04:09, 140.23it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2111 (init= 9.0758), step count (max): 37, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53<04:04, 138.97it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2274 (init= 9.0758), step count (max): 45, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53<04:04, 138.97it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2274 (init= 9.0758), step count (max): 45, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00<03:54, 140.47it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2264 (init= 9.0758), step count (max): 38, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00<03:54, 140.47it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2264 (init= 9.0758), step count (max): 38, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:06<03:45, 141.76it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2320 (init= 9.0758), step count (max): 38, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:06<03:45, 141.76it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2320 (init= 9.0758), step count (max): 38, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14<03:41, 140.09it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2064 (init= 9.0758), step count (max): 33, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14<03:41, 140.09it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2064 (init= 9.0758), step count (max): 33, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22<03:42, 134.86it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2158 (init= 9.0758), step count (max): 32, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22<03:42, 134.86it/s]
    eval cumulative reward:  119.9798 (init:  120.5691), eval step-count: 12, average reward= 9.2158 (init= 9.0758), step count (max): 32, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:30<03:39, 132.31it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2211 (init= 9.0758), step count (max): 40, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:30<03:39, 132.31it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2211 (init= 9.0758), step count (max): 40, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:38<03:36, 129.22it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2242 (init= 9.0758), step count (max): 36, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:38<03:36, 129.22it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2242 (init= 9.0758), step count (max): 36, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:46<03:31, 127.95it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2218 (init= 9.0758), step count (max): 30, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:46<03:31, 127.95it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2218 (init= 9.0758), step count (max): 30, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:54<03:23, 127.77it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2138 (init= 9.0758), step count (max): 29, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:54<03:23, 127.77it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2138 (init= 9.0758), step count (max): 29, lr policy:  0.0002:  50%|#####     | 25000/50000 [03:01<03:11, 130.73it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2261 (init= 9.0758), step count (max): 34, lr policy:  0.0002:  50%|#####     | 25000/50000 [03:01<03:11, 130.73it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2261 (init= 9.0758), step count (max): 34, lr policy:  0.0002:  52%|#####2    | 26000/50000 [03:08<02:58, 134.81it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2315 (init= 9.0758), step count (max): 41, lr policy:  0.0001:  52%|#####2    | 26000/50000 [03:08<02:58, 134.81it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2315 (init= 9.0758), step count (max): 41, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:15<02:46, 138.03it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2219 (init= 9.0758), step count (max): 44, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:15<02:46, 138.03it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2219 (init= 9.0758), step count (max): 44, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:22<02:37, 139.32it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2120 (init= 9.0758), step count (max): 30, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:22<02:37, 139.32it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2120 (init= 9.0758), step count (max): 30, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:29<02:28, 140.94it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2242 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:29<02:28, 140.94it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2242 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  60%|######    | 30000/50000 [03:36<02:20, 142.04it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2210 (init= 9.0758), step count (max): 29, lr policy:  0.0001:  60%|######    | 30000/50000 [03:36<02:20, 142.04it/s]
    eval cumulative reward:  157.4857 (init:  120.5691), eval step-count: 16, average reward= 9.2210 (init= 9.0758), step count (max): 29, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:43<02:13, 142.17it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2224 (init= 9.0758), step count (max): 26, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:43<02:13, 142.17it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2224 (init= 9.0758), step count (max): 26, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:49<02:05, 143.15it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2304 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:49<02:05, 143.15it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2304 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:56<01:57, 144.14it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2246 (init= 9.0758), step count (max): 29, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:56<01:57, 144.14it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2246 (init= 9.0758), step count (max): 29, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:03<01:50, 145.24it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2522 (init= 9.0758), step count (max): 37, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:03<01:50, 145.24it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2522 (init= 9.0758), step count (max): 37, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:10<01:43, 144.65it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2530 (init= 9.0758), step count (max): 58, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:10<01:43, 144.65it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2530 (init= 9.0758), step count (max): 58, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:17<01:37, 143.10it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2540 (init= 9.0758), step count (max): 55, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:17<01:37, 143.10it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2540 (init= 9.0758), step count (max): 55, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:24<01:30, 144.04it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2561 (init= 9.0758), step count (max): 53, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:24<01:30, 144.04it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2561 (init= 9.0758), step count (max): 53, lr policy:  0.0001:  76%|#######6  | 38000/50000 [04:31<01:22, 144.78it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2606 (init= 9.0758), step count (max): 41, lr policy:  0.0000:  76%|#######6  | 38000/50000 [04:31<01:22, 144.78it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2606 (init= 9.0758), step count (max): 41, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:38<01:15, 145.17it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2523 (init= 9.0758), step count (max): 46, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:38<01:15, 145.17it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2523 (init= 9.0758), step count (max): 46, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44<01:08, 145.92it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2504 (init= 9.0758), step count (max): 45, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44<01:08, 145.92it/s]
    eval cumulative reward:  157.1462 (init:  120.5691), eval step-count: 16, average reward= 9.2504 (init= 9.0758), step count (max): 45, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51<01:01, 146.27it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2411 (init= 9.0758), step count (max): 49, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51<01:01, 146.27it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2411 (init= 9.0758), step count (max): 49, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:59<00:56, 142.64it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2602 (init= 9.0758), step count (max): 73, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:59<00:56, 142.64it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2602 (init= 9.0758), step count (max): 73, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:06<00:48, 143.37it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2443 (init= 9.0758), step count (max): 45, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:06<00:48, 143.37it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2443 (init= 9.0758), step count (max): 45, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12<00:41, 143.98it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2496 (init= 9.0758), step count (max): 42, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12<00:41, 143.98it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2496 (init= 9.0758), step count (max): 42, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19<00:34, 144.26it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2651 (init= 9.0758), step count (max): 100, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19<00:34, 144.26it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2651 (init= 9.0758), step count (max): 100, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26<00:27, 145.05it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2603 (init= 9.0758), step count (max): 64, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26<00:27, 145.05it/s] 
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2603 (init= 9.0758), step count (max): 64, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33<00:20, 144.72it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2480 (init= 9.0758), step count (max): 39, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33<00:20, 144.72it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2480 (init= 9.0758), step count (max): 39, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:40<00:13, 144.30it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2488 (init= 9.0758), step count (max): 70, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:40<00:13, 144.30it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2488 (init= 9.0758), step count (max): 70, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:47<00:06, 145.21it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2495 (init= 9.0758), step count (max): 55, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:47<00:06, 145.21it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2495 (init= 9.0758), step count (max): 55, lr policy:  0.0000: 100%|##########| 50000/50000 [05:54<00:00, 146.17it/s]
    eval cumulative reward:  147.8875 (init:  120.5691), eval step-count: 15, average reward= 9.2586 (init= 9.0758), step count (max): 54, lr policy:  0.0000: 100%|##########| 50000/50000 [05:54<00:00, 146.17it/s]



.. GENERATED FROM PYTHON SOURCE LINES 660-667

Results
-------

Before the 1M step cap is reached, the algorithm should have reached a max
step count of 1000 steps, which is the maximum number of steps before the
trajectory is truncated.


.. GENERATED FROM PYTHON SOURCE LINES 667-682

.. code-block:: default

    plt.figure(figsize=(10, 10))
    plt.subplot(2, 2, 1)
    plt.plot(logs["reward"])
    plt.title("training rewards (average)")
    plt.subplot(2, 2, 2)
    plt.plot(logs["step_count"])
    plt.title("Max step count (training)")
    plt.subplot(2, 2, 3)
    plt.plot(logs["eval reward (sum)"])
    plt.title("Return (test)")
    plt.subplot(2, 2, 4)
    plt.plot(logs["eval step_count"])
    plt.title("Max step count (test)")
    plt.show()




.. image-sg:: /intermediate/images/sphx_glr_reinforcement_ppo_001.png
   :alt: training rewards (average), Max step count (training), Return (test), Max step count (test)
   :srcset: /intermediate/images/sphx_glr_reinforcement_ppo_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 683-703

Conclusion and next steps
-------------------------

In this tutorial, we have learned:

1. How to create and customize an environment with :py:mod:`torchrl`;
2. How to write a model and a loss function;
3. How to set up a typical training loop.

If you want to experiment with this tutorial a bit more, you can apply the following modifications:

* From an efficiency perspective,
  we could run several simulations in parallel to speed up data collection.
  Check :class:`torchrl.envs.ParallelEnv` for further information.

* From a logging perspective, one could add a :class:`torchrl.record.VideoRecorder` transform to
  the environment after asking for rendering to get a visual rendering of the
  inverted pendulum in action. Check :py:mod:`torchrl.record` to
  know more.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  58.054 seconds)


.. _sphx_glr_download_intermediate_reinforcement_ppo.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: reinforcement_ppo.py <reinforcement_ppo.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: reinforcement_ppo.ipynb <reinforcement_ppo.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
