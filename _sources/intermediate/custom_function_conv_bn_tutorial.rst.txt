
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "intermediate/custom_function_conv_bn_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_intermediate_custom_function_conv_bn_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_intermediate_custom_function_conv_bn_tutorial.py:


Fusing Convolution and Batch Norm using Custom Function
=======================================================

Fusing adjacent convolution and batch norm layers together is typically an
inference-time optimization to improve run-time. It is usually achieved
by eliminating the batch norm layer entirely and updating the weight
and bias of the preceding convolution [0]. However, this technique is not
applicable for training models.

In this tutorial, we will show a different technique to fuse the two layers
that can be applied during training. Rather than improved runtime, the
objective of this optimization is to reduce memory usage.

The idea behind this optimization is to see that both convolution and
batch norm (as well as many other ops) need to save a copy of their input
during forward for the backward pass. For large
batch sizes, these saved inputs are responsible for most of your memory usage,
so being able to avoid allocating another input tensor for every
convolution batch norm pair can be a significant reduction.

In this tutorial, we avoid this extra allocation by combining convolution
and batch norm into a single layer (as a custom function). In the forward
of this combined layer, we perform normal convolution and batch norm as-is,
with the only difference being that we will only save the inputs to the convolution.
To obtain the input of batch norm, which is necessary to backward through
it, we recompute convolution forward again during the backward pass.

It is important to note that the usage of this optimization is situational.
Though (by avoiding one buffer saved) we always reduce the memory allocated at
the end of the forward pass, there are cases when the *peak* memory allocated
may not actually be reduced. See the final section for more details.

For simplicity, in this tutorial we hardcode `bias=False`, `stride=1`, `padding=0`, `dilation=1`,
and `groups=1` for Conv2D. For BatchNorm2D, we hardcode `eps=1e-3`, `momentum=0.1`,
`affine=False`, and `track_running_statistics=False`. Another small difference
is that we add epsilon in the denominator outside of the square root in the computation
of batch norm.

[0] https://nenadmarkus.com/p/fusing-batchnorm-and-conv/

.. GENERATED FROM PYTHON SOURCE LINES 45-52

Backward Formula Implementation for Convolution
-------------------------------------------------------------------
Implementing a custom function requires us to implement the backward
ourselves. In this case, we need both the backward formulas for Conv2D
and BatchNorm2D. Eventually we'd chain them together in our unified
backward function, but below we first implement them as their own
custom functions so we can validate their correctness individually

.. GENERATED FROM PYTHON SOURCE LINES 52-74

.. code-block:: default

    import torch
    from torch.autograd.function import once_differentiable
    import torch.nn.functional as F

    def convolution_backward(grad_out, X, weight):
        grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)
        grad_X = F.conv_transpose2d(grad_out, weight)
        return grad_X, grad_input

    class Conv2D(torch.autograd.Function):
        @staticmethod
        def forward(ctx, X, weight):
            ctx.save_for_backward(X, weight)
            return F.conv2d(X, weight)

        # Use @once_differentiable by default unless we intend to double backward
        @staticmethod
        @once_differentiable
        def backward(ctx, grad_out):
            X, weight = ctx.saved_tensors
            return convolution_backward(grad_out, X, weight)








.. GENERATED FROM PYTHON SOURCE LINES 75-76

When testing with ``gradcheck``, it is important to use double precision

.. GENERATED FROM PYTHON SOURCE LINES 76-80

.. code-block:: default

    weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)
    X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)
    torch.autograd.gradcheck(Conv2D.apply, (X, weight))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    True



.. GENERATED FROM PYTHON SOURCE LINES 81-88

Backward Formula Implementation for Batch Norm
-------------------------------------------------------------------
Batch Norm has two modes: training and ``eval`` mode. In training mode
the sample statistics are a function of the inputs. In ``eval`` mode,
we use the saved running statistics, which are not a function of the inputs.
This makes non-training mode's backward significantly simpler. Below
we implement and test only the training mode case.

.. GENERATED FROM PYTHON SOURCE LINES 88-151

.. code-block:: default

    def unsqueeze_all(t):
        # Helper function to ``unsqueeze`` all the dimensions that we reduce over
        return t[None, :, None, None]

    def batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):
        # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``
        # in batch norm 2D forward. To simplify our derivation, we follow the
        # chain rule and compute the gradients as follows before accumulating
        # them all into a final grad_input.
        #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``
        #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``
        #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``
        # We then rewrite the formulas to use as few extra buffers as possible
        tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))
        tmp *= -1
        d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``
        # It is useful to delete tensors when you no longer need them with ``del``
        # For example, we could've done ``del tmp`` here because we won't use it later
        # In this case, it's not a big difference because ``tmp`` only has size of (C,)
        # The important thing is avoid allocating NCHW-sized tensors unnecessarily
        d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``
        # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer
        d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)
        d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)
        # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry

        # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``
        grad_input = X * unsqueeze_all(d_var * N)
        grad_input += unsqueeze_all(-d_var * sum)
        grad_input *= 2 / ((N - 1) * N)
        # (2) mean (see above)
        grad_input += d_mean_dx
        # (3) Add 'grad_out / <factor>' without allocating an extra buffer
        grad_input *= unsqueeze_all(sqrt_var + eps)
        grad_input += grad_out
        grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``
        return grad_input

    class BatchNorm(torch.autograd.Function):
        @staticmethod
        def forward(ctx, X, eps=1e-3):
            # Don't save ``keepdim`` values for backward
            sum = X.sum(dim=(0, 2, 3))
            var = X.var(unbiased=True, dim=(0, 2, 3))
            N = X.numel() / X.size(1)
            sqrt_var = torch.sqrt(var)
            ctx.save_for_backward(X)
            ctx.eps = eps
            ctx.sum = sum
            ctx.N = N
            ctx.sqrt_var = sqrt_var
            mean = sum / N
            denom = sqrt_var + eps
            out = X - unsqueeze_all(mean)
            out /= unsqueeze_all(denom)
            return out

        @staticmethod
        @once_differentiable
        def backward(ctx, grad_out):
            X, = ctx.saved_tensors
            return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)








.. GENERATED FROM PYTHON SOURCE LINES 152-153

Testing with ``gradcheck``

.. GENERATED FROM PYTHON SOURCE LINES 153-156

.. code-block:: default

    a = torch.rand(1, 2, 3, 4, requires_grad=True, dtype=torch.double)
    torch.autograd.gradcheck(BatchNorm.apply, (a,), fast_mode=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    True



.. GENERATED FROM PYTHON SOURCE LINES 157-164

Fusing Convolution and BatchNorm
-------------------------------------------------------------------
Now that the bulk of the work has been done, we can combine
them together. Note that in (1) we only save a single buffer
for backward, but this also means we recompute convolution forward
in (5). Also see that in (2), (3), (4), and (6), it's the same
exact code as the examples above.

.. GENERATED FROM PYTHON SOURCE LINES 164-203

.. code-block:: default

    class FusedConvBN2DFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, X, conv_weight, eps=1e-3):
            assert X.ndim == 4  # N, C, H, W
            # (1) Only need to save this single buffer for backward!
            ctx.save_for_backward(X, conv_weight)

            # (2) Exact same Conv2D forward from example above
            X = F.conv2d(X, conv_weight)
            # (3) Exact same BatchNorm2D forward from example above
            sum = X.sum(dim=(0, 2, 3))
            var = X.var(unbiased=True, dim=(0, 2, 3))
            N = X.numel() / X.size(1)
            sqrt_var = torch.sqrt(var)
            ctx.eps = eps
            ctx.sum = sum
            ctx.N = N
            ctx.sqrt_var = sqrt_var
            mean = sum / N
            denom = sqrt_var + eps
            # Try to do as many things in-place as possible
            # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`
            # avoids allocating one extra NCHW-sized buffer here
            out = X - unsqueeze_all(mean)
            out /= unsqueeze_all(denom)
            return out

        @staticmethod
        def backward(ctx, grad_out):
            X, conv_weight, = ctx.saved_tensors
            # (4) Batch norm backward
            # (5) We need to recompute conv
            X_conv_out = F.conv2d(X, conv_weight)
            grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,
                                           ctx.N, ctx.eps)
            # (6) Conv2d backward
            grad_X, grad_input = convolution_backward(grad_out, X, conv_weight)
            return grad_X, grad_input, None, None, None, None, None








.. GENERATED FROM PYTHON SOURCE LINES 204-206

The next step is to wrap our functional variant in a stateful
`nn.Module`

.. GENERATED FROM PYTHON SOURCE LINES 206-230

.. code-block:: default

    import torch.nn as nn
    import math

    class FusedConvBN(nn.Module):
        def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,
                     eps=1e-3, device=None, dtype=None):
            super(FusedConvBN, self).__init__()
            factory_kwargs = {'device': device, 'dtype': dtype}
            # Conv parameters
            weight_shape = (out_channels, in_channels, kernel_size, kernel_size)
            self.conv_weight = nn.Parameter(torch.empty(*weight_shape, **factory_kwargs))
            # Batch norm parameters
            num_features = out_channels
            self.num_features = num_features
            self.eps = eps
            # Initialize
            self.reset_parameters()

        def forward(self, X):
            return FusedConvBN2DFunction.apply(X, self.conv_weight, self.eps)

        def reset_parameters(self) -> None:
            nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))








.. GENERATED FROM PYTHON SOURCE LINES 231-232

Use ``gradcheck`` to validate the correctness of our backward formula

.. GENERATED FROM PYTHON SOURCE LINES 232-236

.. code-block:: default

    weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)
    X = torch.rand(2, 3, 4, 4, requires_grad=True, dtype=torch.double)
    torch.autograd.gradcheck(FusedConvBN2DFunction.apply, (X, weight))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    True



.. GENERATED FROM PYTHON SOURCE LINES 237-242

Testing out our new Layer
-------------------------------------------------------------------
Use ``FusedConvBN`` to train a basic network
The code below is after some light modifications to the example here:
https://github.com/pytorch/examples/tree/master/mnist

.. GENERATED FROM PYTHON SOURCE LINES 242-350

.. code-block:: default

    import torch.optim as optim
    from torchvision import datasets, transforms
    from torch.optim.lr_scheduler import StepLR

    # Record memory allocated at the end of the forward pass
    memory_allocated = [[],[]]

    class Net(nn.Module):
        def __init__(self, fused=True):
            super(Net, self).__init__()
            self.fused = fused
            if fused:
                self.convbn1 = FusedConvBN(1, 32, 3)
                self.convbn2 = FusedConvBN(32, 64, 3)
            else:
                self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(32, affine=False, track_running_stats=False)
                self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(64, affine=False, track_running_stats=False)
            self.fc1 = nn.Linear(9216, 128)
            self.dropout = nn.Dropout(0.5)
            self.fc2 = nn.Linear(128, 10)

        def forward(self, x):
            if self.fused:
                x = self.convbn1(x)
            else:
                x = self.conv1(x)
                x = self.bn1(x)
            F.relu_(x)
            if self.fused:
                x = self.convbn2(x)
            else:
                x = self.conv2(x)
                x = self.bn2(x)
            F.relu_(x)
            x = F.max_pool2d(x, 2)
            F.relu_(x)
            x = x.flatten(1)
            x = self.fc1(x)
            x = self.dropout(x)
            F.relu_(x)
            x = self.fc2(x)
            output = F.log_softmax(x, dim=1)
            if fused:
                memory_allocated[0].append(torch.cuda.memory_allocated())
            else:
                memory_allocated[1].append(torch.cuda.memory_allocated())
            return output

    def train(model, device, train_loader, optimizer, epoch):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            if batch_idx % 2 == 0:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    100. * batch_idx / len(train_loader), loss.item()))

    def test(model, device, test_loader):
        model.eval()
        test_loss = 0
        correct = 0
        # Use inference mode instead of no_grad, for free improved test-time performance
        with torch.inference_mode():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                # sum up batch loss
                test_loss += F.nll_loss(output, target, reduction='sum').item()
                # get the index of the max log-probability
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        test_loss /= len(test_loader.dataset)

        print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            test_loss, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    train_kwargs = {'batch_size': 2048}
    test_kwargs = {'batch_size': 2048}

    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                              transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                              transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz

      0%|          | 0/9912422 [00:00<?, ?it/s]
      1%|          | 65536/9912422 [00:00<00:19, 511661.59it/s]
      1%|1         | 131072/9912422 [00:00<00:19, 508942.13it/s]
      2%|1         | 196608/9912422 [00:00<00:19, 500805.01it/s]
      3%|2         | 262144/9912422 [00:00<00:19, 507738.42it/s]
      3%|3         | 327680/9912422 [00:00<00:19, 493172.15it/s]
      4%|3         | 393216/9912422 [00:00<00:18, 510183.99it/s]
      5%|4         | 458752/9912422 [00:00<00:18, 506898.73it/s]
      5%|5         | 524288/9912422 [00:01<00:18, 507304.31it/s]
      6%|5         | 589824/9912422 [00:01<00:18, 504210.91it/s]
      7%|6         | 655360/9912422 [00:01<00:18, 505433.80it/s]
      7%|7         | 720896/9912422 [00:01<00:18, 506644.27it/s]
      8%|7         | 786432/9912422 [00:01<00:18, 506099.47it/s]
      9%|8         | 851968/9912422 [00:01<00:18, 503111.82it/s]
      9%|9         | 917504/9912422 [00:01<00:17, 506039.93it/s]
     10%|9         | 983040/9912422 [00:01<00:17, 503605.01it/s]
     11%|#         | 1048576/9912422 [00:02<00:17, 506464.01it/s]
     11%|#1        | 1114112/9912422 [00:02<00:17, 505809.17it/s]
     12%|#1        | 1179648/9912422 [00:02<00:17, 505345.02it/s]
     13%|#2        | 1245184/9912422 [00:02<00:17, 505225.64it/s]
     13%|#3        | 1310720/9912422 [00:02<00:17, 504794.03it/s]
     14%|#3        | 1376256/9912422 [00:02<00:16, 504835.99it/s]
     15%|#4        | 1441792/9912422 [00:02<00:16, 504862.29it/s]
     15%|#5        | 1507328/9912422 [00:02<00:16, 502597.51it/s]
     16%|#5        | 1572864/9912422 [00:03<00:16, 504648.16it/s]
     17%|#6        | 1638400/9912422 [00:03<00:16, 500020.93it/s]
     17%|#7        | 1703936/9912422 [00:03<00:16, 506622.03it/s]
     18%|#7        | 1769472/9912422 [00:03<00:16, 504729.48it/s]
     19%|#8        | 1835008/9912422 [00:03<00:15, 505516.30it/s]
     19%|#9        | 1900544/9912422 [00:03<00:15, 505698.15it/s]
     20%|#9        | 1966080/9912422 [00:03<00:15, 505405.04it/s]
     20%|##        | 2031616/9912422 [00:04<00:15, 504076.09it/s]
     21%|##1       | 2097152/9912422 [00:04<00:15, 504699.53it/s]
     22%|##1       | 2162688/9912422 [00:04<00:15, 504286.13it/s]
     22%|##2       | 2228224/9912422 [00:04<00:15, 503819.51it/s]
     23%|##3       | 2293760/9912422 [00:04<00:15, 503423.68it/s]
     24%|##3       | 2359296/9912422 [00:04<00:15, 503129.84it/s]
     24%|##4       | 2424832/9912422 [00:04<00:14, 502874.94it/s]
     25%|##5       | 2490368/9912422 [00:04<00:14, 502747.70it/s]
     26%|##5       | 2555904/9912422 [00:05<00:14, 502862.54it/s]
     26%|##6       | 2621440/9912422 [00:05<00:14, 500521.94it/s]
     27%|##7       | 2686976/9912422 [00:05<00:14, 498977.40it/s]
     28%|##7       | 2752512/9912422 [00:05<00:14, 505100.83it/s]
     28%|##8       | 2818048/9912422 [00:05<00:14, 503510.93it/s]
     29%|##9       | 2883584/9912422 [00:05<00:13, 504092.22it/s]
     30%|##9       | 2949120/9912422 [00:05<00:13, 503685.40it/s]
     30%|###       | 3014656/9912422 [00:05<00:13, 503422.86it/s]
     31%|###1      | 3080192/9912422 [00:06<00:13, 502837.14it/s]
     32%|###1      | 3145728/9912422 [00:06<00:13, 502857.54it/s]
     32%|###2      | 3211264/9912422 [00:06<00:13, 502661.89it/s]
     33%|###3      | 3276800/9912422 [00:06<00:13, 502639.98it/s]
     34%|###3      | 3342336/9912422 [00:06<00:13, 502160.20it/s]
     34%|###4      | 3407872/9912422 [00:06<00:12, 502678.65it/s]
     35%|###5      | 3473408/9912422 [00:06<00:12, 502680.94it/s]
     36%|###5      | 3538944/9912422 [00:07<00:12, 502923.14it/s]
     36%|###6      | 3604480/9912422 [00:07<00:12, 502850.13it/s]
     37%|###7      | 3670016/9912422 [00:07<00:12, 503568.34it/s]
     38%|###7      | 3735552/9912422 [00:07<00:12, 503062.16it/s]
     38%|###8      | 3801088/9912422 [00:07<00:12, 503063.94it/s]
     39%|###9      | 3866624/9912422 [00:07<00:12, 502852.32it/s]
     40%|###9      | 3932160/9912422 [00:07<00:11, 502594.26it/s]
     40%|####      | 3997696/9912422 [00:07<00:11, 502678.39it/s]
     41%|####      | 4063232/9912422 [00:08<00:11, 502693.72it/s]
     42%|####1     | 4128768/9912422 [00:08<00:11, 502174.91it/s]
     42%|####2     | 4194304/9912422 [00:08<00:11, 502045.81it/s]
     43%|####2     | 4259840/9912422 [00:08<00:11, 502141.17it/s]
     44%|####3     | 4325376/9912422 [00:08<00:11, 502492.18it/s]
     44%|####4     | 4390912/9912422 [00:08<00:10, 502598.63it/s]
     45%|####4     | 4456448/9912422 [00:08<00:10, 502304.74it/s]
     46%|####5     | 4521984/9912422 [00:08<00:10, 502540.94it/s]
     46%|####6     | 4587520/9912422 [00:09<00:10, 502493.03it/s]
     47%|####6     | 4653056/9912422 [00:09<00:10, 502452.33it/s]
     48%|####7     | 4718592/9912422 [00:09<00:10, 502448.64it/s]
     48%|####8     | 4784128/9912422 [00:09<00:10, 499062.14it/s]
     49%|####8     | 4849664/9912422 [00:09<00:10, 502136.54it/s]
     50%|####9     | 4915200/9912422 [00:09<00:09, 502348.15it/s]
     50%|#####     | 4980736/9912422 [00:09<00:09, 503783.60it/s]
     51%|#####     | 5046272/9912422 [00:10<00:09, 502818.73it/s]
     52%|#####1    | 5111808/9912422 [00:10<00:09, 502524.47it/s]
     52%|#####2    | 5177344/9912422 [00:10<00:09, 504055.75it/s]
     53%|#####2    | 5242880/9912422 [00:10<00:09, 503807.24it/s]
     54%|#####3    | 5308416/9912422 [00:10<00:09, 503841.69it/s]
     54%|#####4    | 5373952/9912422 [00:10<00:09, 503295.66it/s]
     55%|#####4    | 5439488/9912422 [00:10<00:08, 504434.72it/s]
     56%|#####5    | 5505024/9912422 [00:10<00:08, 504370.48it/s]
     56%|#####6    | 5570560/9912422 [00:11<00:08, 504247.54it/s]
     57%|#####6    | 5636096/9912422 [00:11<00:08, 504639.94it/s]
     58%|#####7    | 5701632/9912422 [00:11<00:08, 505060.26it/s]
     58%|#####8    | 5767168/9912422 [00:11<00:08, 505371.35it/s]
     59%|#####8    | 5832704/9912422 [00:11<00:08, 504681.22it/s]
     60%|#####9    | 5898240/9912422 [00:11<00:07, 504953.93it/s]
     60%|######    | 5963776/9912422 [00:11<00:07, 504664.22it/s]
     61%|######    | 6029312/9912422 [00:11<00:07, 504536.90it/s]
     61%|######1   | 6094848/9912422 [00:12<00:07, 503413.20it/s]
     62%|######2   | 6160384/9912422 [00:12<00:07, 504579.61it/s]
     63%|######2   | 6225920/9912422 [00:12<00:07, 504930.02it/s]
     63%|######3   | 6291456/9912422 [00:12<00:07, 504616.93it/s]
     64%|######4   | 6356992/9912422 [00:12<00:07, 505414.65it/s]
     65%|######4   | 6422528/9912422 [00:12<00:06, 505213.20it/s]
     65%|######5   | 6488064/9912422 [00:12<00:06, 504816.83it/s]
     66%|######6   | 6553600/9912422 [00:13<00:06, 504802.42it/s]
     67%|######6   | 6619136/9912422 [00:13<00:06, 504999.33it/s]
     67%|######7   | 6684672/9912422 [00:13<00:06, 504091.76it/s]
     68%|######8   | 6750208/9912422 [00:13<00:06, 504911.06it/s]
     69%|######8   | 6815744/9912422 [00:13<00:06, 504754.06it/s]
     69%|######9   | 6881280/9912422 [00:13<00:05, 505755.93it/s]
     70%|#######   | 6946816/9912422 [00:13<00:05, 505531.89it/s]
     71%|#######   | 7012352/9912422 [00:13<00:05, 505292.69it/s]
     71%|#######1  | 7077888/9912422 [00:14<00:05, 505224.54it/s]
     72%|#######2  | 7143424/9912422 [00:14<00:05, 505030.38it/s]
     73%|#######2  | 7208960/9912422 [00:14<00:05, 503461.65it/s]
     73%|#######3  | 7274496/9912422 [00:14<00:05, 505441.53it/s]
     74%|#######4  | 7340032/9912422 [00:14<00:05, 504645.70it/s]
     75%|#######4  | 7405568/9912422 [00:14<00:05, 475480.21it/s]
     75%|#######5  | 7471104/9912422 [00:14<00:04, 515026.72it/s]
     76%|#######6  | 7536640/9912422 [00:14<00:04, 511982.49it/s]
     77%|#######6  | 7602176/9912422 [00:15<00:04, 510074.10it/s]
     77%|#######7  | 7667712/9912422 [00:15<00:04, 508639.06it/s]
     78%|#######8  | 7733248/9912422 [00:15<00:04, 507268.36it/s]
     79%|#######8  | 7798784/9912422 [00:15<00:04, 506743.92it/s]
     79%|#######9  | 7864320/9912422 [00:15<00:04, 503756.56it/s]
     80%|#######9  | 7929856/9912422 [00:15<00:03, 506882.89it/s]
     81%|########  | 7995392/9912422 [00:15<00:03, 505657.58it/s]
     81%|########1 | 8060928/9912422 [00:15<00:03, 506068.92it/s]
     82%|########1 | 8126464/9912422 [00:16<00:03, 505866.91it/s]
     83%|########2 | 8192000/9912422 [00:16<00:03, 505468.09it/s]
     83%|########3 | 8257536/9912422 [00:16<00:03, 505161.16it/s]
     84%|########3 | 8323072/9912422 [00:16<00:03, 504595.32it/s]
     85%|########4 | 8388608/9912422 [00:16<00:03, 504486.47it/s]
     85%|########5 | 8454144/9912422 [00:16<00:02, 504180.21it/s]
     86%|########5 | 8519680/9912422 [00:16<00:02, 504902.21it/s]
     87%|########6 | 8585216/9912422 [00:17<00:02, 503882.38it/s]
     87%|########7 | 8650752/9912422 [00:17<00:02, 505048.62it/s]
     88%|########7 | 8716288/9912422 [00:17<00:02, 505003.05it/s]
     89%|########8 | 8781824/9912422 [00:17<00:02, 504695.23it/s]
     89%|########9 | 8847360/9912422 [00:17<00:02, 504928.41it/s]
     90%|########9 | 8912896/9912422 [00:17<00:01, 505529.00it/s]
     91%|######### | 8978432/9912422 [00:17<00:01, 504681.71it/s]
     91%|#########1| 9043968/9912422 [00:17<00:01, 505202.06it/s]
     92%|#########1| 9109504/9912422 [00:18<00:01, 503583.84it/s]
     93%|#########2| 9175040/9912422 [00:18<00:01, 503758.65it/s]
     93%|#########3| 9240576/9912422 [00:18<00:01, 506199.68it/s]
     94%|#########3| 9306112/9912422 [00:18<00:01, 505748.35it/s]
     95%|#########4| 9371648/9912422 [00:18<00:01, 505216.91it/s]
     95%|#########5| 9437184/9912422 [00:18<00:00, 505331.98it/s]
     96%|#########5| 9502720/9912422 [00:18<00:00, 505191.86it/s]
     97%|#########6| 9568256/9912422 [00:18<00:00, 504897.60it/s]
     97%|#########7| 9633792/9912422 [00:19<00:00, 503777.22it/s]
     98%|#########7| 9699328/9912422 [00:19<00:00, 506439.44it/s]
     99%|#########8| 9764864/9912422 [00:19<00:00, 506446.33it/s]
     99%|#########9| 9830400/9912422 [00:19<00:00, 506117.13it/s]
    100%|##########| 9912422/9912422 [00:19<00:00, 507994.33it/s]
    Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz

      0%|          | 0/28881 [00:00<?, ?it/s]
    100%|##########| 28881/28881 [00:00<00:00, 137810800.71it/s]
    Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz

      0%|          | 0/1648877 [00:00<?, ?it/s]
    100%|##########| 1648877/1648877 [00:00<00:00, 151554607.34it/s]
    Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz

      0%|          | 0/4542 [00:00<?, ?it/s]
    100%|##########| 4542/4542 [00:00<00:00, 41777475.37it/s]
    Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw





.. GENERATED FROM PYTHON SOURCE LINES 351-367

A Comparison of Memory Usage
-------------------------------------------------------------------
If CUDA is enabled, print out memory usage for both `fused=True` and `fused=False`
For an example run on NVIDIA GeForce RTX 3070, NVIDIA CUDAÂ® Deep Neural Network library (cuDNN) 8.0.5: fused peak memory: 1.56GB,
unfused peak memory: 2.68GB

It is important to note that the *peak* memory usage for this model may vary depending
the specific cuDNN convolution algorithm used. For shallower models, it
may be possible for the peak memory allocated of the fused model to exceed
that of the unfused model! This is because the memory allocated to compute
certain cuDNN convolution algorithms can be high enough to "hide" the typical peak
you would expect to be near the start of the backward pass.

For this reason, we also record and display the memory allocated at the end
of the forward pass as an approximation, and to demonstrate that we indeed
allocate one fewer buffer per fused ``conv-bn`` pair.

.. GENERATED FROM PYTHON SOURCE LINES 367-395

.. code-block:: default

    from statistics import mean

    torch.backends.cudnn.enabled = True

    if use_cuda:
        peak_memory_allocated = []

        for fused in (True, False):
            torch.manual_seed(123456)

            model = Net(fused=fused).to(device)
            optimizer = optim.Adadelta(model.parameters(), lr=1.0)
            scheduler = StepLR(optimizer, step_size=1, gamma=0.7)

            for epoch in range(1):
                train(model, device, train_loader, optimizer, epoch)
                test(model, device, test_loader)
                scheduler.step()
            peak_memory_allocated.append(torch.cuda.max_memory_allocated())
            torch.cuda.reset_peak_memory_stats()
        print("cuDNN version:", torch.backends.cudnn.version())
        print()
        print("Peak memory allocated:")
        print(f"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB")
        print("Memory allocated at end of forward pass:")
        print(f"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Train Epoch: 0 [0/60000 (0%)]   Loss: 2.348735
    Train Epoch: 0 [4096/60000 (7%)]        Loss: 7.435781
    Train Epoch: 0 [8192/60000 (13%)]       Loss: 5.540894
    Train Epoch: 0 [12288/60000 (20%)]      Loss: 2.274223
    Train Epoch: 0 [16384/60000 (27%)]      Loss: 1.618885
    Train Epoch: 0 [20480/60000 (33%)]      Loss: 1.515203
    Train Epoch: 0 [24576/60000 (40%)]      Loss: 1.329276
    Train Epoch: 0 [28672/60000 (47%)]      Loss: 1.184942
    Train Epoch: 0 [32768/60000 (53%)]      Loss: 1.140154
    Train Epoch: 0 [36864/60000 (60%)]      Loss: 1.174118
    Train Epoch: 0 [40960/60000 (67%)]      Loss: 1.057965
    Train Epoch: 0 [45056/60000 (73%)]      Loss: 0.976334
    Train Epoch: 0 [49152/60000 (80%)]      Loss: 0.842555
    Train Epoch: 0 [53248/60000 (87%)]      Loss: 0.690169
    Train Epoch: 0 [57344/60000 (93%)]      Loss: 0.656998

    Test set: Average loss: 0.4197, Accuracy: 8681/10000 (87%)

    Train Epoch: 0 [0/60000 (0%)]   Loss: 2.349030
    Train Epoch: 0 [4096/60000 (7%)]        Loss: 7.435158
    Train Epoch: 0 [8192/60000 (13%)]       Loss: 5.443548
    Train Epoch: 0 [12288/60000 (20%)]      Loss: 2.457870
    Train Epoch: 0 [16384/60000 (27%)]      Loss: 1.739212
    Train Epoch: 0 [20480/60000 (33%)]      Loss: 1.448282
    Train Epoch: 0 [24576/60000 (40%)]      Loss: 1.312154
    Train Epoch: 0 [28672/60000 (47%)]      Loss: 1.145339
    Train Epoch: 0 [32768/60000 (53%)]      Loss: 1.495954
    Train Epoch: 0 [36864/60000 (60%)]      Loss: 1.251006
    Train Epoch: 0 [40960/60000 (67%)]      Loss: 1.077316
    Train Epoch: 0 [45056/60000 (73%)]      Loss: 0.890140
    Train Epoch: 0 [49152/60000 (80%)]      Loss: 0.836710
    Train Epoch: 0 [53248/60000 (87%)]      Loss: 0.736266
    Train Epoch: 0 [57344/60000 (93%)]      Loss: 0.798268

    Test set: Average loss: 0.4135, Accuracy: 8870/10000 (89%)

    cuDNN version: 8902

    Peak memory allocated:
    fused: 2.30GB, unfused: 1.77GB
    Memory allocated at end of forward pass:
    fused: 0.59GB, unfused: 0.96GB





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  12.192 seconds)


.. _sphx_glr_download_intermediate_custom_function_conv_bn_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: custom_function_conv_bn_tutorial.py <custom_function_conv_bn_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: custom_function_conv_bn_tutorial.ipynb <custom_function_conv_bn_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
