
<!DOCTYPE html>

<html data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Deep Learning with PyTorch — PyTorch Tutorials 2.9.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css?v=047068a3" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/documentation_options.js?v=c2809cec"></script>
<script src="../../_static/doctools.js?v=888ff710"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=f281be69"></script>
<script src="../../_static/katex.min.js?v=be8ff15f"></script>
<script src="../../_static/auto-render.min.js?v=ad136472"></script>
<script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'beginner/nlp/deep_learning_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html" rel="canonical"/>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<link crossorigin="anonymous" href="../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.9.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started/locally">Get Started</a>
</li>
<li>
<a href="https://docs.pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../../index.html">v2.9.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"></div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Deep...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Deep Learning with PyTorch" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
</div>
<script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">beginner/nlp/deep_learning_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-nlp-deep-learning-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="deep-learning-with-pytorch">
<span id="sphx-glr-beginner-nlp-deep-learning-tutorial-py"></span><h1>Deep Learning with PyTorch<a class="headerlink" href="#deep-learning-with-pytorch" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Apr 08, 2017 | Last Updated: Apr 24, 2018 | Last Verified: Nov 05, 2024</p>
<section id="deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">
<h2>Deep Learning Building Blocks: Affine maps, non-linearities and objectives<a class="headerlink" href="#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives" title="Link to this heading">#</a></h2>
<p>Deep learning consists of composing linearities with non-linearities in
clever ways. The introduction of non-linearities allows for powerful
models. In this section, we will play with these core components, make
up an objective function, and see how the model is trained.</p>
<section id="affine-maps">
<h3>Affine Maps<a class="headerlink" href="#affine-maps" title="Link to this heading">#</a></h3>
<p>One of the core workhorses of deep learning is the affine map, which is
a function <span class="math">\(f(x)\)</span> where</p>
<div class="math">
\[f(x) = Ax + b

\]</div>
<p>for a matrix <span class="math">\(A\)</span> and vectors <span class="math">\(x, b\)</span>. The parameters to be
learned here are <span class="math">\(A\)</span> and <span class="math">\(b\)</span>. Often, <span class="math">\(b\)</span> is refered to
as the <em>bias</em> term.</p>
<p>PyTorch and most other deep learning frameworks do things a little
differently than traditional linear algebra. It maps the rows of the
input instead of the columns. That is, the <span class="math">\(i\)</span>’th row of the
output below is the mapping of the <span class="math">\(i\)</span>’th row of the input under
<span class="math">\(A\)</span>, plus the bias term. Look at the example below.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Robert Guthrie</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator object at 0x7fd08bf8c9b0&gt;
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">lin</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># maps from R^5 to R^3, parameters A, b</span>
<span class="c1"># data is 2x5.  A maps from 5 to 3... can we map "data" under A?</span>
<span class="n">data</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">lin</span></a><span class="p">(</span><span class="n">data</span><span class="p">))</span>  <span class="c1"># yes</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.1755, -0.3268, -0.5069],
        [-0.6602,  0.2260,  0.1089]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</section>
<section id="non-linearities">
<h3>Non-Linearities<a class="headerlink" href="#non-linearities" title="Link to this heading">#</a></h3>
<p>First, note the following fact, which will explain why we need
non-linearities in the first place. Suppose we have two affine maps
<span class="math">\(f(x) = Ax + b\)</span> and <span class="math">\(g(x) = Cx + d\)</span>. What is
<span class="math">\(f(g(x))\)</span>?</p>
<div class="math">
\[f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)

\]</div>
<p><span class="math">\(AC\)</span> is a matrix and <span class="math">\(Ad + b\)</span> is a vector, so we see that
composing affine maps gives you an affine map.</p>
<p>From this, you can see that if you wanted your neural network to be long
chains of affine compositions, that this adds no new power to your model
than just doing a single affine map.</p>
<p>If we introduce non-linearities in between the affine layers, this is no
longer the case, and we can build much more powerful models.</p>
<p>There are a few core non-linearities.
<span class="math">\(\tanh(x), \sigma(x), \text{ReLU}(x)\)</span> are the most common. You are
probably wondering: “why these functions? I can think of plenty of other
non-linearities.” The reason for this is that they have gradients that
are easy to compute, and computing gradients is essential for learning.
For example</p>
<div class="math">
\[\frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))

\]</div>
<p>A quick note: although you may have learned some neural networks in your
intro to AI class where <span class="math">\(\sigma(x)\)</span> was the default non-linearity,
typically people shy away from it in practice. This is because the
gradient <em>vanishes</em> very quickly as the absolute value of the argument
grows. Small gradients means it is hard to learn. Most people default to
tanh or ReLU.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># In pytorch, most non-linearities are in torch.functional (we have it imported as F)</span>
<span class="c1"># Note that non-linearites typically don't have parameters like affine maps do.</span>
<span class="c1"># That is, they don't have weights that are updated during training.</span>
<span class="n">data</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[-0.5404, -2.2102],
        [ 2.1130, -0.0040]])
tensor([[0.0000, 0.0000],
        [2.1130, 0.0000]])
</pre></div>
</div>
</section>
<section id="softmax-and-probabilities">
<h3>Softmax and Probabilities<a class="headerlink" href="#softmax-and-probabilities" title="Link to this heading">#</a></h3>
<p>The function <span class="math">\(\text{Softmax}(x)\)</span> is also just a non-linearity, but
it is special in that it usually is the last operation done in a
network. This is because it takes in a vector of real numbers and
returns a probability distribution. Its definition is as follows. Let
<span class="math">\(x\)</span> be a vector of real numbers (positive, negative, whatever,
there are no constraints). Then the i’th component of
<span class="math">\(\text{Softmax}(x)\)</span> is</p>
<div class="math">
\[\frac{\exp(x_i)}{\sum_j \exp(x_j)}

\]</div>
<p>It should be clear that the output is a probability distribution: each
element is non-negative and the sum over all components is 1.</p>
<p>You could also think of it as just applying an element-wise
exponentiation operator to the input to make everything non-negative and
then dividing by the normalization constant.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Softmax is also in torch.nn.functional</span>
<span class="n">data</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><span class="n">F</span><span class="o">.</span><span class="n">softmax</span></a><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><span class="n">F</span><span class="o">.</span><span class="n">softmax</span></a><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>  <span class="c1"># Sums to 1 because it is a distribution!</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span></a><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># theres also log_softmax</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])
tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])
tensor(1.)
tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])
</pre></div>
</div>
</section>
<section id="objective-functions">
<h3>Objective Functions<a class="headerlink" href="#objective-functions" title="Link to this heading">#</a></h3>
<p>The objective function is the function that your network is being
trained to minimize (in which case it is often called a <em>loss function</em>
or <em>cost function</em>). This proceeds by first choosing a training
instance, running it through your neural network, and then computing the
loss of the output. The parameters of the model are then updated by
taking the derivative of the loss function. Intuitively, if your model
is completely confident in its answer, and its answer is wrong, your
loss will be high. If it is very confident in its answer, and its answer
is correct, the loss will be low.</p>
<p>The idea behind minimizing the loss function on your training examples
is that your network will hopefully generalize well and have small loss
on unseen examples in your dev set, test set, or in production. An
example loss function is the <em>negative log likelihood loss</em>, which is a
very common objective for multi-class classification. For supervised
multi-class classification, this means training the network to minimize
the negative log probability of the correct output (or equivalently,
maximize the log probability of the correct output).</p>
</section>
</section>
<section id="optimization-and-training">
<h2>Optimization and Training<a class="headerlink" href="#optimization-and-training" title="Link to this heading">#</a></h2>
<p>So what we can compute a loss function for an instance? What do we do
with that? We saw earlier that Tensors know how to compute gradients
with respect to the things that were used to compute it. Well,
since our loss is an Tensor, we can compute gradients with
respect to all of the parameters used to compute it! Then we can perform
standard gradient updates. Let <span class="math">\(\theta\)</span> be our parameters,
<span class="math">\(L(\theta)\)</span> the loss function, and <span class="math">\(\eta\)</span> a positive
learning rate. Then:</p>
<div class="math">
\[\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta L(\theta)

\]</div>
<p>There are a huge collection of algorithms and active research in
attempting to do something more than just this vanilla gradient update.
Many attempt to vary the learning rate based on what is happening at
train time. You don’t need to worry about what specifically these
algorithms are doing unless you are really interested. Torch provides
many in the torch.optim package, and they are all completely
transparent. Using the simplest gradient update is the same as the more
complicated algorithms. Trying different update algorithms and different
parameters for the update algorithms (like different initial learning
rates) is important in optimizing your network’s performance. Often,
just replacing vanilla SGD with an optimizer like Adam or RMSProp will
boost performance noticably.</p>
</section>
<section id="creating-network-components-in-pytorch">
<h2>Creating Network Components in PyTorch<a class="headerlink" href="#creating-network-components-in-pytorch" title="Link to this heading">#</a></h2>
<p>Before we move on to our focus on NLP, lets do an annotated example of
building a network in PyTorch using only affine maps and
non-linearities. We will also see how to compute a loss function, using
PyTorch’s built in negative log likelihood, and update parameters by
backpropagation.</p>
<p>All network components should inherit from nn.Module and override the
forward() method. That is about it, as far as the boilerplate is
concerned. Inheriting from nn.Module provides functionality to your
component. For example, it makes it keep track of its trainable
parameters, you can swap it between CPU and GPU with the <code class="docutils literal notranslate"><span class="pre">.to(device)</span></code>
method, where device can be a CPU device <code class="docutils literal notranslate"><span class="pre">torch.device("cpu")</span></code> or CUDA
device <code class="docutils literal notranslate"><span class="pre">torch.device("cuda:0")</span></code>.</p>
<p>Let’s write an annotated example of a network that takes in a sparse
bag-of-words representation and outputs a probability distribution over
two labels: “English” and “Spanish”. This model is just logistic
regression.</p>
<section id="example-logistic-regression-bag-of-words-classifier">
<h3>Example: Logistic Regression Bag-of-Words classifier<a class="headerlink" href="#example-logistic-regression-bag-of-words-classifier" title="Link to this heading">#</a></h3>
<p>Our model will map a sparse BoW representation to log probabilities over
labels. We assign each word in the vocab an index. For example, say our
entire vocab is two words “hello” and “world”, with indices 0 and 1
respectively. The BoW vector for the sentence “hello hello hello hello”
is</p>
<div class="math">
\[\left[ 4, 0 \right]

\]</div>
<p>For “hello world world hello”, it is</p>
<div class="math">
\[\left[ 2, 2 \right]

\]</div>
<p>etc. In general, it is</p>
<div class="math">
\[\left[ \text{Count}(\text{hello}), \text{Count}(\text{world}) \right]

\]</div>
<p>Denote this BOW vector as <span class="math">\(x\)</span>. The output of our network is:</p>
<div class="math">
\[\log \text{Softmax}(Ax + b)

\]</div>
<p>That is, we pass the input through an affine map and then do log
softmax.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">"me gusta comer en la cafeteria"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"SPANISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">"Give it to me"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"ENGLISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">"No creo que sea una buena idea"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"SPANISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">"No it is not a good idea to get lost at sea"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"ENGLISH"</span><span class="p">)]</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">"Yo creo que si"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"SPANISH"</span><span class="p">),</span>
             <span class="p">(</span><span class="s2">"it is lost on me"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s2">"ENGLISH"</span><span class="p">)]</span>

<span class="c1"># word_to_ix maps each word in the vocab to a unique integer, which will be its</span>
<span class="c1"># index into the Bag of words vector</span>
<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">data</span> <span class="o">+</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_ix</span><span class="p">:</span>
            <span class="n">word_to_ix</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>

<span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>
<span class="n">NUM_LABELS</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BoWClassifier</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>  <span class="c1"># inheriting from nn.Module!</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="c1"># calls the init function of nn.Module.  Dont get confused by syntax,</span>
        <span class="c1"># just always do it in an nn.Module</span>
        <span class="nb">super</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">BoWClassifier</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Define the parameters that you will need.  In this case, we need A and b,</span>
        <span class="c1"># the parameters of the affine mapping.</span>
        <span class="c1"># Torch defines nn.Linear(), which provides the affine map.</span>
        <span class="c1"># Make sure you understand why the input dimension is vocab_size</span>
        <span class="c1"># and the output is num_labels!</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">)</span>

        <span class="c1"># NOTE! The non-linearity log softmax does not have parameters! So we don't need</span>
        <span class="c1"># to worry about that here</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a><span class="p">):</span>
        <span class="c1"># Pass the input through the linear layer,</span>
        <span class="c1"># then pass that through log_softmax.</span>
        <span class="c1"># Many non-linearities and other functions are in torch.nn.functional</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">make_bow_vector</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">):</span>
    <span class="n">vec</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
        <span class="n">vec</span><span class="p">[</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vec</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">make_target</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">label_to_ix</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">label_to_ix</span><span class="p">[</span><span class="n">label</span><span class="p">]])</span>


<span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">BoWClassifier</span></a><span class="p">(</span><span class="n">NUM_LABELS</span><span class="p">,</span> <span class="n">VOCAB_SIZE</span><span class="p">)</span>

<span class="c1"># the model knows its parameters.  The first output below is A, the second is b.</span>
<span class="c1"># Whenever you assign a component to a class variable in the __init__ function</span>
<span class="c1"># of a module, which was done with the line</span>
<span class="c1"># self.linear = nn.Linear(...)</span>
<span class="c1"># Then through some Python magic from the PyTorch devs, your module</span>
<span class="c1"># (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters</span>
<span class="k">for</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">param</span></a> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">param</span></a><span class="p">)</span>

<span class="c1"># To run the model, pass in a BoW vector</span>
<span class="c1"># Here we don't need to train, so the code is wrapped in torch.no_grad()</span>
<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vector</span></a> <span class="o">=</span> <span class="n">make_bow_vector</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">word_to_ix</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vector</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}
Parameter containing:
tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,
         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,
          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,
          0.0119, -0.0334],
        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,
          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,
          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,
          0.1344,  0.0406]], requires_grad=True)
Parameter containing:
tensor([0.0631, 0.1465], requires_grad=True)
tensor([[-0.5378, -0.8771]])
</pre></div>
</div>
<p>Which of the above values corresponds to the log probability of ENGLISH,
and which to SPANISH? We never defined it, but we need to if we want to
train the thing.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">label_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"SPANISH"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"ENGLISH"</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<p>So lets train! To do this, we pass instances through to get log
probabilities, compute a loss function, compute the gradient of the loss
function, and then update the parameters with a gradient step. Loss
functions are provided by Torch in the nn package. nn.NLLLoss() is the
negative log likelihood loss we want. It also defines optimization
functions in torch.optim. Here, we will just use SGD.</p>
<p>Note that the <em>input</em> to NLLLoss is a vector of log probabilities, and a
target label. It doesn’t compute the log probabilities for us. This is
why the last layer of our network is log softmax. The loss function
nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log
softmax for you.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run on test data before we train, just to see a before-and-after</span>
<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="k">for</span> <span class="n">instance</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a> <span class="o">=</span> <span class="n">make_bow_vector</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a><span class="p">)</span>

<span class="c1"># Print the matrix column corresponding to "creo"</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">())[:,</span> <span class="n">word_to_ix</span><span class="p">[</span><span class="s2">"creo"</span><span class="p">]])</span>

<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><span class="n">loss_function</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD" title="torch.optim.SGD"><span class="n">optimizer</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD" title="torch.optim.SGD"><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Usually you want to pass over the training data several times.</span>
<span class="c1"># 100 is much bigger than on a real data set, but real datasets have more than</span>
<span class="c1"># two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">instance</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c1"># Step 1. Remember that PyTorch accumulates gradients.</span>
        <span class="c1"># We need to clear them out before each instance</span>
        <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad" title="torch.nn.Module.zero_grad"><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

        <span class="c1"># Step 2. Make our BOW vector and also we must wrap the target in a</span>
        <span class="c1"># Tensor as an integer. For example, if the target is SPANISH, then</span>
        <span class="c1"># we wrap the integer 0. The loss function then knows that the 0th</span>
        <span class="c1"># element of the log probabilities is the log probability</span>
        <span class="c1"># corresponding to SPANISH</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a> <span class="o">=</span> <span class="n">make_bow_vector</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target</span></a> <span class="o">=</span> <span class="n">make_target</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">label_to_ix</span><span class="p">)</span>

        <span class="c1"># Step 3. Run our forward pass.</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a><span class="p">)</span>

        <span class="c1"># Step 4. Compute the loss, gradients, and update the parameters by</span>
        <span class="c1"># calling optimizer.step()</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">loss</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><span class="n">loss_function</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"><span class="n">loss</span><span class="o">.</span><span class="n">backward</span></a><span class="p">()</span>
        <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step" title="torch.optim.SGD.step"><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>

<span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="k">for</span> <span class="n">instance</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a> <span class="o">=</span> <span class="n">make_bow_vector</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">bow_vec</span></a><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">log_probs</span></a><span class="p">)</span>

<span class="c1"># Index corresponding to Spanish goes up, English goes down!</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">())[:,</span> <span class="n">word_to_ix</span><span class="p">[</span><span class="s2">"creo"</span><span class="p">]])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[-0.9297, -0.5020]])
tensor([[-0.6388, -0.7506]])
tensor([-0.1488, -0.1313], grad_fn=&lt;SelectBackward0&gt;)
tensor([[-0.2093, -1.6669]])
tensor([[-2.5330, -0.0828]])
tensor([ 0.2803, -0.5605], grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
<p>We got the right answer! You can see that the log probability for
Spanish is much higher in the first example, and the log probability for
English is much higher in the second for the test data, as it should be.</p>
<p>Now you see how to make a PyTorch component, pass some data through it
and do gradient updates. We are ready to dig deeper into what deep NLP
has to offer.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 0.276 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-nlp-deep-learning-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/dd1c511de656ab48216de2866264b28f/deep_learning_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">deep_learning_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/f1703ff94c4a6544f91c89b37c3a1fcf/deep_learning_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">deep_learning_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/388445308536355abc2e460d51dfa282/deep_learning_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">deep_learning_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#affine-maps">Affine Maps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linearities">Non-Linearities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-and-probabilities">Softmax and Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-functions">Objective Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-and-training">Optimization and Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-network-components-in-pytorch">Creating Network Components in PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-logistic-regression-bag-of-words-classifier">Example: Logistic Regression Bag-of-Words classifier</a></li>
</ul>
</li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Deep Learning with PyTorch",
       "headline": "Deep Learning with PyTorch",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/nlp/deep_learning_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. Deep Learning with PyTorch# Deep Learning Building Blocks: Affine maps, non-linearities and objectives# Deep learning consists of composing linearities with non-linearities in clever ways. The introduction of non-linearities allows for powerful models. In this section, we will play with these core components, make up an objective function, and see how the model is trained. Affine Maps# One of the core workhorses of deep learning is the affine map, which is a function \\(f(x)\\) where \\[f(x) = Ax + b \\] for a matrix \\(A\\) and vectors \\(x, b\\). The parameters to be learned here are \\(A\\) and \\(b\\). Often, \\(b\\) is refered to as the bias term. PyTorch and most other deep learning frameworks do things a little differently than traditional linear algebra. It maps the rows of the input instead of the columns. That is, the \\(i\\)\u2019th row of the output below is the mapping of the \\(i\\)\u2019th row of the input under \\(A\\), plus the bias term. Look at the example below. # Author: Robert Guthrie import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) \u003ctorch._C.Generator object at 0x7fd08bf8c9b0\u003e lin = nn.Linear(5, 3) # maps from R^5 to R^3, parameters A, b # data is 2x5. A maps from 5 to 3... can we map \"data\" under A? data = torch.randn(2, 5) print(lin(data)) # yes tensor([[ 0.1755, -0.3268, -0.5069], [-0.6602, 0.2260, 0.1089]], grad_fn=\u003cAddmmBackward0\u003e) Non-Linearities# First, note the following fact, which will explain why we need non-linearities in the first place. Suppose we have two affine maps \\(f(x) = Ax + b\\) and \\(g(x) = Cx + d\\). What is \\(f(g(x))\\)? \\[f(g(x)) = A(Cx + d) + b = ACx + (Ad + b) \\] \\(AC\\) is a matrix and \\(Ad + b\\) is a vector, so we see that composing affine maps gives you an affine map. From this, you can see that if you wanted your neural network to be long chains of affine compositions, that this adds no new power to your model than just doing a single affine map. If we introduce non-linearities in between the affine layers, this is no longer the case, and we can build much more powerful models. There are a few core non-linearities. \\(\\tanh(x), \\sigma(x), \\text{ReLU}(x)\\) are the most common. You are probably wondering: \u201cwhy these functions? I can think of plenty of other non-linearities.\u201d The reason for this is that they have gradients that are easy to compute, and computing gradients is essential for learning. For example \\[\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x)) \\] A quick note: although you may have learned some neural networks in your intro to AI class where \\(\\sigma(x)\\) was the default non-linearity, typically people shy away from it in practice. This is because the gradient vanishes very quickly as the absolute value of the argument grows. Small gradients means it is hard to learn. Most people default to tanh or ReLU. # In pytorch, most non-linearities are in torch.functional (we have it imported as F) # Note that non-linearites typically don\u0027t have parameters like affine maps do. # That is, they don\u0027t have weights that are updated during training. data = torch.randn(2, 2) print(data) print(F.relu(data)) tensor([[-0.5404, -2.2102], [ 2.1130, -0.0040]]) tensor([[0.0000, 0.0000], [2.1130, 0.0000]]) Softmax and Probabilities# The function \\(\\text{Softmax}(x)\\) is also just a non-linearity, but it is special in that it usually is the last operation done in a network. This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. Let \\(x\\) be a vector of real numbers (positive, negative, whatever, there are no constraints). Then the i\u2019th component of \\(\\text{Softmax}(x)\\) is \\[\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} \\] It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1. You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant. # Softmax is also in torch.nn.functional data = torch.randn(5) print(data) print(F.softmax(data, dim=0)) print(F.softmax(data, dim=0).sum()) # Sums to 1 because it is a distribution! print(F.log_softmax(data, dim=0)) # theres also log_softmax tensor([ 1.3800, -1.3505, 0.3455, 0.5046, 1.8213]) tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584]) tensor(1.) tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801]) Objective Functions# The objective function is the function that your network is being trained to minimize (in which case it is often called a loss function or cost function). This proceeds by first choosing a training instance, running it through your neural network, and then computing the loss of the output. The parameters of the model are then updated by taking the derivative of the loss function. Intuitively, if your model is completely confident in its answer, and its answer is wrong, your loss will be high. If it is very confident in its answer, and its answer is correct, the loss will be low. The idea behind minimizing the loss function on your training examples is that your network will hopefully generalize well and have small loss on unseen examples in your dev set, test set, or in production. An example loss function is the negative log likelihood loss, which is a very common objective for multi-class classification. For supervised multi-class classification, this means training the network to minimize the negative log probability of the correct output (or equivalently, maximize the log probability of the correct output). Optimization and Training# So what we can compute a loss function for an instance? What do we do with that? We saw earlier that Tensors know how to compute gradients with respect to the things that were used to compute it. Well, since our loss is an Tensor, we can compute gradients with respect to all of the parameters used to compute it! Then we can perform standard gradient updates. Let \\(\\theta\\) be our parameters, \\(L(\\theta)\\) the loss function, and \\(\\eta\\) a positive learning rate. Then: \\[\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta) \\] There are a huge collection of algorithms and active research in attempting to do something more than just this vanilla gradient update. Many attempt to vary the learning rate based on what is happening at train time. You don\u2019t need to worry about what specifically these algorithms are doing unless you are really interested. Torch provides many in the torch.optim package, and they are all completely transparent. Using the simplest gradient update is the same as the more complicated algorithms. Trying different update algorithms and different parameters for the update algorithms (like different initial learning rates) is important in optimizing your network\u2019s performance. Often, just replacing vanilla SGD with an optimizer like Adam or RMSProp will boost performance noticably. Creating Network Components in PyTorch# Before we move on to our focus on NLP, lets do an annotated example of building a network in PyTorch using only affine maps and non-linearities. We will also see how to compute a loss function, using PyTorch\u2019s built in negative log likelihood, and update parameters by backpropagation. All network components should inherit from nn.Module and override the forward() method. That is about it, as far as the boilerplate is concerned. Inheriting from nn.Module provides functionality to your component. For example, it makes it keep track of its trainable parameters, you can swap it between CPU and GPU with the .to(device) method, where device can be a CPU device torch.device(\"cpu\") or CUDA device torch.device(\"cuda:0\"). Let\u2019s write an annotated example of a network that takes in a sparse bag-of-words representation and outputs a probability distribution over two labels: \u201cEnglish\u201d and \u201cSpanish\u201d. This model is just logistic regression. Example: Logistic Regression Bag-of-Words classifier# Our model will map a sparse BoW representation to log probabilities over labels. We assign each word in the vocab an index. For example, say our entire vocab is two words \u201chello\u201d and \u201cworld\u201d, with indices 0 and 1 respectively. The BoW vector for the sentence \u201chello hello hello hello\u201d is \\[\\left[ 4, 0 \\right] \\] For \u201chello world world hello\u201d, it is \\[\\left[ 2, 2 \\right] \\] etc. In general, it is \\[\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right] \\] Denote this BOW vector as \\(x\\). The output of our network is: \\[\\log \\text{Softmax}(Ax + b) \\] That is, we pass the input through an affine map and then do log softmax. data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"), (\"Give it to me\".split(), \"ENGLISH\"), (\"No creo que sea una buena idea\".split(), \"SPANISH\"), (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")] test_data = [(\"Yo creo que si\".split(), \"SPANISH\"), (\"it is lost on me\".split(), \"ENGLISH\")] # word_to_ix maps each word in the vocab to a unique integer, which will be its # index into the Bag of words vector word_to_ix = {} for sent, _ in data + test_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) VOCAB_SIZE = len(word_to_ix) NUM_LABELS = 2 class BoWClassifier(nn.Module): # inheriting from nn.Module! def __init__(self, num_labels, vocab_size): # calls the init function of nn.Module. Dont get confused by syntax, # just always do it in an nn.Module super(BoWClassifier, self).__init__() # Define the parameters that you will need. In this case, we need A and b, # the parameters of the affine mapping. # Torch defines nn.Linear(), which provides the affine map. # Make sure you understand why the input dimension is vocab_size # and the output is num_labels! self.linear = nn.Linear(vocab_size, num_labels) # NOTE! The non-linearity log softmax does not have parameters! So we don\u0027t need # to worry about that here def forward(self, bow_vec): # Pass the input through the linear layer, # then pass that through log_softmax. # Many non-linearities and other functions are in torch.nn.functional return F.log_softmax(self.linear(bow_vec), dim=1) def make_bow_vector(sentence, word_to_ix): vec = torch.zeros(len(word_to_ix)) for word in sentence: vec[word_to_ix[word]] += 1 return vec.view(1, -1) def make_target(label, label_to_ix): return torch.LongTensor([label_to_ix[label]]) model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) # the model knows its parameters. The first output below is A, the second is b. # Whenever you assign a component to a class variable in the __init__ function # of a module, which was done with the line # self.linear = nn.Linear(...) # Then through some Python magic from the PyTorch devs, your module # (in this case, BoWClassifier) will store knowledge of the nn.Linear\u0027s parameters for param in model.parameters(): print(param) # To run the model, pass in a BoW vector # Here we don\u0027t need to train, so the code is wrapped in torch.no_grad() with torch.no_grad(): sample = data[0] bow_vector = make_bow_vector(sample[0], word_to_ix) log_probs = model(bow_vector) print(log_probs) {\u0027me\u0027: 0, \u0027gusta\u0027: 1, \u0027comer\u0027: 2, \u0027en\u0027: 3, \u0027la\u0027: 4, \u0027cafeteria\u0027: 5, \u0027Give\u0027: 6, \u0027it\u0027: 7, \u0027to\u0027: 8, \u0027No\u0027: 9, \u0027creo\u0027: 10, \u0027que\u0027: 11, \u0027sea\u0027: 12, \u0027una\u0027: 13, \u0027buena\u0027: 14, \u0027idea\u0027: 15, \u0027is\u0027: 16, \u0027not\u0027: 17, \u0027a\u0027: 18, \u0027good\u0027: 19, \u0027get\u0027: 20, \u0027lost\u0027: 21, \u0027at\u0027: 22, \u0027Yo\u0027: 23, \u0027si\u0027: 24, \u0027on\u0027: 25} Parameter containing: tensor([[ 0.1194, 0.0609, -0.1268, 0.1274, 0.1191, 0.1739, -0.1099, -0.0323, -0.0038, 0.0286, -0.1488, -0.1392, 0.1067, -0.0460, 0.0958, 0.0112, 0.0644, 0.0431, 0.0713, 0.0972, -0.1816, 0.0987, -0.1379, -0.1480, 0.0119, -0.0334], [ 0.1152, -0.1136, -0.1743, 0.1427, -0.0291, 0.1103, 0.0630, -0.1471, 0.0394, 0.0471, -0.1313, -0.0931, 0.0669, 0.0351, -0.0834, -0.0594, 0.1796, -0.0363, 0.1106, 0.0849, -0.1268, -0.1668, 0.1882, 0.0102, 0.1344, 0.0406]], requires_grad=True) Parameter containing: tensor([0.0631, 0.1465], requires_grad=True) tensor([[-0.5378, -0.8771]]) Which of the above values corresponds to the log probability of ENGLISH, and which to SPANISH? We never defined it, but we need to if we want to train the thing. label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1} So lets train! To do this, we pass instances through to get log probabilities, compute a loss function, compute the gradient of the loss function, and then update the parameters with a gradient step. Loss functions are provided by Torch in the nn package. nn.NLLLoss() is the negative log likelihood loss we want. It also defines optimization functions in torch.optim. Here, we will just use SGD. Note that the input to NLLLoss is a vector of log probabilities, and a target label. It doesn\u2019t compute the log probabilities for us. This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you. # Run on test data before we train, just to see a before-and-after with torch.no_grad(): for instance, label in test_data: bow_vec = make_bow_vector(instance, word_to_ix) log_probs = model(bow_vec) print(log_probs) # Print the matrix column corresponding to \"creo\" print(next(model.parameters())[:, word_to_ix[\"creo\"]]) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # Usually you want to pass over the training data several times. # 100 is much bigger than on a real data set, but real datasets have more than # two instances. Usually, somewhere between 5 and 30 epochs is reasonable. for epoch in range(100): for instance, label in data: # Step 1. Remember that PyTorch accumulates gradients. # We need to clear them out before each instance model.zero_grad() # Step 2. Make our BOW vector and also we must wrap the target in a # Tensor as an integer. For example, if the target is SPANISH, then # we wrap the integer 0. The loss function then knows that the 0th # element of the log probabilities is the log probability # corresponding to SPANISH bow_vec = make_bow_vector(instance, word_to_ix) target = make_target(label, label_to_ix) # Step 3. Run our forward pass. log_probs = model(bow_vec) # Step 4. Compute the loss, gradients, and update the parameters by # calling optimizer.step() loss = loss_function(log_probs, target) loss.backward() optimizer.step() with torch.no_grad(): for instance, label in test_data: bow_vec = make_bow_vector(instance, word_to_ix) log_probs = model(bow_vec) print(log_probs) # Index corresponding to Spanish goes up, English goes down! print(next(model.parameters())[:, word_to_ix[\"creo\"]]) tensor([[-0.9297, -0.5020]]) tensor([[-0.6388, -0.7506]]) tensor([-0.1488, -0.1313], grad_fn=\u003cSelectBackward0\u003e) tensor([[-0.2093, -1.6669]]) tensor([[-2.5330, -0.0828]]) tensor([ 0.2803, -0.5605], grad_fn=\u003cSelectBackward0\u003e) We got the right answer! You can see that the log probability for Spanish is much higher in the first example, and the log probability for English is much higher in the second for the test data, as it should be. Now you see how to make a PyTorch component, pass some data through it and do gradient updates. We are ready to dig deeper into what deep NLP has to offer. Total running time of the script: (0 minutes 0.276 seconds) Download Jupyter notebook: deep_learning_tutorial.ipynb Download Python source code: deep_learning_tutorial.py Download zipped: deep_learning_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/nlp/deep_learning_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>