

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to PyTorch &mdash; PyTorch Tutorials  documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  
    <link rel="stylesheet" href="../../_static/css/pytorch_theme.css" type="text/css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="PyTorch Tutorials  documentation" href="../../index.html"/>
        <link rel="up" title="Deep Learning for NLP with Pytorch" href="../deep_learning_nlp_tutorial.html"/>
        <link rel="next" title="Deep Learning with PyTorch" href="deep_learning_tutorial.html"/>
        <link rel="prev" title="Deep Learning for NLP with Pytorch" href="../deep_learning_nlp_tutorial.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> PyTorch Tutorials
          

          
            
            <img src="../../_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../blitz/tensor_tutorial.html">What is PyTorch?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#getting-started">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#tensors">Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#operations">Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#numpy-bridge">Numpy Bridge</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#converting-torch-tensor-to-numpy-array">Converting torch Tensor to numpy Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#converting-numpy-array-to-torch-tensor">Converting numpy Array to torch Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/autograd_tutorial.html">Autograd: automatic differentiation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/autograd_tutorial.html#variable">Variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/autograd_tutorial.html#gradients">Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/neural_networks_tutorial.html">Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#define-the-network">Define the network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#loss-function">Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#backprop">Backprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#update-the-weights">Update the weights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/cifar10_tutorial.html">Training a classifier</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#what-about-data">What about data?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#training-an-image-classifier">Training an image classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#loading-and-normalizing-cifar10">1. Loading and normalizing CIFAR10</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#define-a-convolution-neural-network">2. Define a Convolution Neural Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer">3. Define a Loss function and optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#train-the-network">4. Train the network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#test-the-network-on-the-test-data">5. Test the network on the test data</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#training-on-gpu">Training on GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#where-do-i-go-next">Where do I go next?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../former_torchies_tutorial.html">PyTorch for former Torch users</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/tensor_tutorial.html">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#inplace-out-of-place">Inplace / Out-of-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#zero-indexing">Zero Indexing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#no-camel-casing">No camel casing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#numpy-bridge">Numpy Bridge</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#converting-torch-tensor-to-numpy-array">Converting torch Tensor to numpy Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#converting-numpy-array-to-torch-tensor">Converting numpy Array to torch Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/autograd_tutorial.html">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/autograd_tutorial.html#variable">Variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/autograd_tutorial.html#gradients">Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/nn_tutorial.html">nn package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#example-1-convnet">Example 1: ConvNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#forward-and-backward-function-hooks">Forward and Backward Function Hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#example-2-recurrent-net">Example 2: Recurrent Net</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html">Multi-GPU examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html#dataparallel">DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html#part-of-the-model-on-cpu-and-part-on-the-gpu">Part of the model on CPU and part on the GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_with_examples.html">Learning PyTorch with Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#warm-up-numpy">Warm-up: numpy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-tensors">PyTorch: Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#autograd">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-variables-and-autograd">PyTorch: Variables and autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-defining-new-autograd-functions">PyTorch: Defining new autograd functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#tensorflow-static-graphs">TensorFlow: Static Graphs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#nn-module"><cite>nn</cite> module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-nn">PyTorch: nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-optim">PyTorch: optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-custom-nn-modules">PyTorch: Custom nn Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-control-flow-weight-sharing">PyTorch: Control Flow + Weight Sharing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id1">Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id2">Autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id3"><cite>nn</cite> module</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../transfer_learning_tutorial.html">Transfer Learning tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#load-data">Load Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#visualize-a-few-images">Visualize a few images</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#training-the-model">Training the model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#learning-rate-scheduler">Learning rate scheduler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#visualizing-the-model-predictions">Visualizing the model predictions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#finetuning-the-convnet">Finetuning the convnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#train-and-evaluate">Train and evaluate</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor">ConvNet as fixed feature extractor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#id1">Train and evaluate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Introduction to PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-torch-s-tensor-library">Introduction to Torch&#8217;s tensor library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#creating-tensors">Creating Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operations-with-tensors">Operations with Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reshaping-tensors">Reshaping Tensors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#computation-graphs-and-automatic-differentiation">Computation Graphs and Automatic Differentiation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="deep_learning_tutorial.html">Deep Learning with PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#affine-maps">Affine Maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#non-linearities">Non-Linearities</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#softmax-and-probabilities">Softmax and Probabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#objective-functions">Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#optimization-and-training">Optimization and Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#creating-network-components-in-pytorch">Creating Network Components in Pytorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier">Example: Logistic Regression Bag-of-Words classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="word_embeddings_tutorial.html">Word Embeddings: Encoding Lexical Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#getting-dense-word-embeddings">Getting Dense Word Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#word-embeddings-in-pytorch">Word Embeddings in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#an-example-n-gram-language-modeling">An Example: N-Gram Language Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words">Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sequence_models_tutorial.html">Sequence Models and Long-Short Term Memory Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#lstm-s-in-pytorch">LSTM&#8217;s in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging">Example: An LSTM for Part-of-Speech Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#exercise-augmenting-the-lstm-part-of-speech-tagger-with-character-level-features">Exercise: Augmenting the LSTM part-of-speech tagger with character-level features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="advanced_tutorial.html">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#dyanmic-versus-static-deep-learning-toolkits">Dyanmic versus Static Deep Learning Toolkits</a></li>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#preparing-the-data">Preparing the Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#turning-names-into-tensors">Turning Names into Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#plotting-the-results">Plotting the Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#evaluating-the-results">Evaluating the Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#running-on-user-input">Running on User Input</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#preparing-the-data">Preparing the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#plotting-the-losses">Plotting the Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#sampling-the-network">Sampling the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#loading-data-files">Loading data files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model">The Seq2Seq Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#the-encoder">The Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#the-decoder">The Decoder</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#simple-decoder">Simple Decoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#attention-decoder">Attention Decoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#preparing-training-data">Preparing Training Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#training-the-model">Training the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#plotting-results">Plotting results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#training-and-evaluating">Training and Evaluating</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#visualizing-attention">Visualizing Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#replay-memory">Replay Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#dqn-algorithm">DQN algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#q-network">Q-network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#input-extraction">Input extraction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#hyperparameters-and-utilities">Hyperparameters and utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#training-loop">Training loop</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/neural_style_tutorial.html">Neural Transfer with PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#neural-what">Neural what?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#how-does-it-work">How does it work?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#ok-how-does-it-work">OK. How does it work?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#pytorch-implementation">PyTorch implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#packages">Packages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#cuda">Cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#load-images">Load images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#display-images">Display images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#content-loss">Content loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#style-loss">Style loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#load-the-neural-network">Load the neural network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#input-image">Input image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#gradient-descent">Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html#parameter-less-example">Parameter-less example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html#parametrized-example">Parametrized example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/c_extension.html">Custom C extensions for pytorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/c_extension.html#step-1-prepare-your-c-code">Step 1. prepare your C code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/c_extension.html#step-2-include-it-in-your-python-code">Step 2: Include it in your Python code</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">PyTorch Tutorials</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a> &raquo;</li>
      
    <li>Introduction to PyTorch</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/beginner/nlp/pytorch_tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="introduction-to-pytorch">
<span id="sphx-glr-beginner-nlp-pytorch-tutorial-py"></span><h1>Introduction to PyTorch<a class="headerlink" href="#introduction-to-pytorch" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-to-torch-s-tensor-library">
<h2>Introduction to Torch&#8217;s tensor library<a class="headerlink" href="#introduction-to-torch-s-tensor-library" title="Permalink to this headline">¶</a></h2>
<p>All of deep learning is computations on tensors, which are
generalizations of a matrix that can be indexed in more than 2
dimensions. We will see exactly what this means in-depth later. First,
lets look what we can do with tensors.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Author: Robert Guthrie</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span> <span class="kn">as</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="creating-tensors">
<h3>Creating Tensors<a class="headerlink" href="#creating-tensors" title="Permalink to this headline">¶</a></h3>
<p>Tensors can be created from Python lists with the torch.Tensor()
function.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Create a torch.Tensor object with the given data.  It is a 1D vector</span>
<span class="n">V_data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">V_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>

<span class="c1"># Creates a matrix</span>
<span class="n">M_data</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">M_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Create a 3D tensor of size 2x2x2.</span>
<span class="n">T_data</span> <span class="o">=</span> <span class="p">[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span>
          <span class="p">[[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]]]</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">T_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span>
 <span class="mi">2</span>
 <span class="mi">3</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>


 <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>
 <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x3</span><span class="p">]</span>


<span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
  <span class="mi">1</span>  <span class="mi">2</span>
  <span class="mi">3</span>  <span class="mi">4</span>

<span class="p">(</span><span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
  <span class="mi">5</span>  <span class="mi">6</span>
  <span class="mi">7</span>  <span class="mi">8</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x2x2</span><span class="p">]</span>
</pre></div>
</div>
<p>What is a 3D tensor anyway? Think about it like this. If you have a
vector, indexing into the vector gives you a scalar. If you have a
matrix, indexing into the matrix gives you a vector. If you have a 3D
tensor, then indexing into the tensor gives you a matrix!</p>
<p>A note on terminology:
when I say &#8220;tensor&#8221; in this tutorial, it refers
to any torch.Tensor object. Matrices and vectors are special cases of
torch.Tensors, where their dimension is 1 and 2 respectively. When I am
talking about 3D tensors, I will explicitly use the term &#8220;3D tensor&#8221;.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Index into V and get a scalar</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Index into M and get a vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Index into T and get a matrix</span>
<span class="k">print</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="mf">1.0</span>

 <span class="mi">1</span>
 <span class="mi">2</span>
 <span class="mi">3</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>


 <span class="mi">1</span>  <span class="mi">2</span>
 <span class="mi">3</span>  <span class="mi">4</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x2</span><span class="p">]</span>
</pre></div>
</div>
<p>You can also create tensors of other datatypes. The default, as you can
see, is Float. To create a tensor of integer types, try
torch.LongTensor(). Check the documentation for more data types, but
Float and Long will be the most common.</p>
<p>You can create a tensor with random data and the supplied dimensionality
with torch.randn()</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
 <span class="o">-</span><span class="mf">2.9718</span>  <span class="mf">1.7070</span> <span class="o">-</span><span class="mf">0.4305</span> <span class="o">-</span><span class="mf">2.2820</span>  <span class="mf">0.5237</span>
  <span class="mf">0.0004</span> <span class="o">-</span><span class="mf">1.2039</span>  <span class="mf">3.5283</span>  <span class="mf">0.4434</span>  <span class="mf">0.5848</span>
  <span class="mf">0.8407</span>  <span class="mf">0.5510</span>  <span class="mf">0.3863</span>  <span class="mf">0.9124</span> <span class="o">-</span><span class="mf">0.8410</span>
  <span class="mf">1.2282</span> <span class="o">-</span><span class="mf">1.8661</span>  <span class="mf">1.4146</span> <span class="o">-</span><span class="mf">1.8781</span> <span class="o">-</span><span class="mf">0.4674</span>

<span class="p">(</span><span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
 <span class="o">-</span><span class="mf">0.7576</span>  <span class="mf">0.4215</span> <span class="o">-</span><span class="mf">0.4827</span> <span class="o">-</span><span class="mf">1.1198</span>  <span class="mf">0.3056</span>
  <span class="mf">1.0386</span>  <span class="mf">0.5206</span> <span class="o">-</span><span class="mf">0.5006</span>  <span class="mf">1.2182</span>  <span class="mf">0.2117</span>
 <span class="o">-</span><span class="mf">1.0613</span> <span class="o">-</span><span class="mf">1.9441</span> <span class="o">-</span><span class="mf">0.9596</span>  <span class="mf">0.5489</span> <span class="o">-</span><span class="mf">0.9901</span>
 <span class="o">-</span><span class="mf">0.3826</span>  <span class="mf">1.5037</span>  <span class="mf">1.8267</span>  <span class="mf">0.5561</span>  <span class="mf">1.6445</span>

<span class="p">(</span><span class="mi">2</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
  <span class="mf">0.4973</span> <span class="o">-</span><span class="mf">1.5067</span>  <span class="mf">1.7661</span> <span class="o">-</span><span class="mf">0.3569</span> <span class="o">-</span><span class="mf">0.1713</span>
  <span class="mf">0.4068</span> <span class="o">-</span><span class="mf">0.4284</span> <span class="o">-</span><span class="mf">1.1299</span>  <span class="mf">1.4274</span> <span class="o">-</span><span class="mf">1.4027</span>
  <span class="mf">1.4825</span> <span class="o">-</span><span class="mf">1.1559</span>  <span class="mf">1.6190</span>  <span class="mf">0.9581</span>  <span class="mf">0.7747</span>
  <span class="mf">0.1940</span>  <span class="mf">0.1687</span>  <span class="mf">0.3061</span>  <span class="mf">1.0743</span> <span class="o">-</span><span class="mf">1.0327</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="n">x4x5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="operations-with-tensors">
<h3>Operations with Tensors<a class="headerlink" href="#operations-with-tensors" title="Permalink to this headline">¶</a></h3>
<p>You can operate on tensors in the ways you would expect.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="mi">5</span>
 <span class="mi">7</span>
 <span class="mi">9</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference external" href="http://pytorch.org/docs/torch.html">the documentation</a> for a
complete list of the massive number of operations available to you. They
expand beyond just mathematical operations.</p>
<p>One helpful operation that we will make use of later is concatenation.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># By default, it concatenates along the first axis (concatenates rows)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">z_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_1</span><span class="p">,</span> <span class="n">y_1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">z_1</span><span class="p">)</span>

<span class="c1"># Concatenate columns:</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="c1"># second arg specifies which axis to concat along</span>
<span class="n">z_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_2</span><span class="p">,</span> <span class="n">y_2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z_2</span><span class="p">)</span>

<span class="c1"># If your tensors are not compatible, torch will complain.  Uncomment to see the error</span>
<span class="c1"># torch.cat([x_1, x_2])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="mf">1.0930</span>  <span class="mf">0.7769</span> <span class="o">-</span><span class="mf">1.3128</span>  <span class="mf">0.7099</span>  <span class="mf">0.9944</span>
<span class="o">-</span><span class="mf">0.2694</span> <span class="o">-</span><span class="mf">0.6491</span> <span class="o">-</span><span class="mf">0.1373</span> <span class="o">-</span><span class="mf">0.2954</span> <span class="o">-</span><span class="mf">0.7725</span>
<span class="o">-</span><span class="mf">0.2215</span>  <span class="mf">0.5074</span> <span class="o">-</span><span class="mf">0.6794</span> <span class="o">-</span><span class="mf">1.6115</span>  <span class="mf">0.5230</span>
<span class="o">-</span><span class="mf">0.8890</span>  <span class="mf">0.2620</span>  <span class="mf">0.0302</span>  <span class="mf">0.0013</span> <span class="o">-</span><span class="mf">1.3987</span>
 <span class="mf">1.4666</span> <span class="o">-</span><span class="mf">0.1028</span> <span class="o">-</span><span class="mf">0.0097</span> <span class="o">-</span><span class="mf">0.8420</span> <span class="o">-</span><span class="mf">0.2067</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">5</span><span class="n">x5</span><span class="p">]</span>


 <span class="mf">1.0672</span>  <span class="mf">0.1732</span> <span class="o">-</span><span class="mf">0.6873</span>  <span class="mf">0.3620</span>  <span class="mf">0.3776</span> <span class="o">-</span><span class="mf">0.2443</span> <span class="o">-</span><span class="mf">0.5850</span>  <span class="mf">2.0812</span>
 <span class="mf">0.3111</span>  <span class="mf">0.2358</span> <span class="o">-</span><span class="mf">1.0658</span> <span class="o">-</span><span class="mf">0.1186</span>  <span class="mf">0.4903</span>  <span class="mf">0.8349</span>  <span class="mf">0.8894</span>  <span class="mf">0.4148</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x8</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="reshaping-tensors">
<h3>Reshaping Tensors<a class="headerlink" href="#reshaping-tensors" title="Permalink to this headline">¶</a></h3>
<p>Use the .view() method to reshape a tensor. This method receives heavy
use, because many neural network components expect their inputs to have
a certain shape. Often you will need to reshape before passing your data
to the component.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>  <span class="c1"># Reshape to 2 rows, 12 columns</span>
<span class="c1"># Same as above.  If one of the dimensions is -1, its size can be inferred</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
  <span class="mf">0.0507</span> <span class="o">-</span><span class="mf">0.9644</span> <span class="o">-</span><span class="mf">2.0111</span>  <span class="mf">0.5245</span>
  <span class="mf">2.1332</span> <span class="o">-</span><span class="mf">0.0822</span>  <span class="mf">0.8388</span> <span class="o">-</span><span class="mf">1.3233</span>
  <span class="mf">0.0701</span>  <span class="mf">1.2200</span>  <span class="mf">0.4251</span> <span class="o">-</span><span class="mf">1.2328</span>

<span class="p">(</span><span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span>
 <span class="o">-</span><span class="mf">0.6195</span>  <span class="mf">1.5133</span>  <span class="mf">1.9954</span> <span class="o">-</span><span class="mf">0.6585</span>
 <span class="o">-</span><span class="mf">0.4139</span> <span class="o">-</span><span class="mf">0.2250</span> <span class="o">-</span><span class="mf">0.6890</span>  <span class="mf">0.9882</span>
  <span class="mf">0.7404</span> <span class="o">-</span><span class="mf">2.0990</span>  <span class="mf">1.2582</span> <span class="o">-</span><span class="mf">0.3990</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x3x4</span><span class="p">]</span>



<span class="n">Columns</span> <span class="mi">0</span> <span class="n">to</span> <span class="mi">9</span>
 <span class="mf">0.0507</span> <span class="o">-</span><span class="mf">0.9644</span> <span class="o">-</span><span class="mf">2.0111</span>  <span class="mf">0.5245</span>  <span class="mf">2.1332</span> <span class="o">-</span><span class="mf">0.0822</span>  <span class="mf">0.8388</span> <span class="o">-</span><span class="mf">1.3233</span>  <span class="mf">0.0701</span>  <span class="mf">1.2200</span>
<span class="o">-</span><span class="mf">0.6195</span>  <span class="mf">1.5133</span>  <span class="mf">1.9954</span> <span class="o">-</span><span class="mf">0.6585</span> <span class="o">-</span><span class="mf">0.4139</span> <span class="o">-</span><span class="mf">0.2250</span> <span class="o">-</span><span class="mf">0.6890</span>  <span class="mf">0.9882</span>  <span class="mf">0.7404</span> <span class="o">-</span><span class="mf">2.0990</span>

<span class="n">Columns</span> <span class="mi">10</span> <span class="n">to</span> <span class="mi">11</span>
 <span class="mf">0.4251</span> <span class="o">-</span><span class="mf">1.2328</span>
 <span class="mf">1.2582</span> <span class="o">-</span><span class="mf">0.3990</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x12</span><span class="p">]</span>



<span class="n">Columns</span> <span class="mi">0</span> <span class="n">to</span> <span class="mi">9</span>
 <span class="mf">0.0507</span> <span class="o">-</span><span class="mf">0.9644</span> <span class="o">-</span><span class="mf">2.0111</span>  <span class="mf">0.5245</span>  <span class="mf">2.1332</span> <span class="o">-</span><span class="mf">0.0822</span>  <span class="mf">0.8388</span> <span class="o">-</span><span class="mf">1.3233</span>  <span class="mf">0.0701</span>  <span class="mf">1.2200</span>
<span class="o">-</span><span class="mf">0.6195</span>  <span class="mf">1.5133</span>  <span class="mf">1.9954</span> <span class="o">-</span><span class="mf">0.6585</span> <span class="o">-</span><span class="mf">0.4139</span> <span class="o">-</span><span class="mf">0.2250</span> <span class="o">-</span><span class="mf">0.6890</span>  <span class="mf">0.9882</span>  <span class="mf">0.7404</span> <span class="o">-</span><span class="mf">2.0990</span>

<span class="n">Columns</span> <span class="mi">10</span> <span class="n">to</span> <span class="mi">11</span>
 <span class="mf">0.4251</span> <span class="o">-</span><span class="mf">1.2328</span>
 <span class="mf">1.2582</span> <span class="o">-</span><span class="mf">0.3990</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x12</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="computation-graphs-and-automatic-differentiation">
<h2>Computation Graphs and Automatic Differentiation<a class="headerlink" href="#computation-graphs-and-automatic-differentiation" title="Permalink to this headline">¶</a></h2>
<p>The concept of a computation graph is essential to efficient deep
learning programming, because it allows you to not have to write the
back propagation gradients yourself. A computation graph is simply a
specification of how your data is combined to give you the output. Since
the graph totally specifies what parameters were involved with which
operations, it contains enough information to compute derivatives. This
probably sounds vague, so lets see what is going on using the
fundamental class of Pytorch: autograd.Variable.</p>
<p>First, think from a programmers perspective. What is stored in the
torch.Tensor objects we were creating above? Obviously the data and the
shape, and maybe a few other things. But when we added two tensors
together, we got an output tensor. All this output tensor knows is its
data and shape. It has no idea that it was the sum of two other tensors
(it could have been read in from a file, it could be the result of some
other operation, etc.)</p>
<p>The Variable class keeps track of how it was created. Lets see it in
action.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Variables wrap tensor objects</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># You can access the data with the .data attribute</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># You can also do all the same operations you did with tensors with Variables.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># BUT z knows something extra.</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span>
 <span class="mi">2</span>
 <span class="mi">3</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>


 <span class="mi">5</span>
 <span class="mi">7</span>
 <span class="mi">9</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>

<span class="o">&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">basic_ops</span><span class="o">.</span><span class="n">Add</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f2163a509e8</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>So Variables know what created them. z knows that it wasn&#8217;t read in from
a file, it wasn&#8217;t the result of a multiplication or exponential or
whatever. And if you keep following z.creator, you will find yourself at
x and y.</p>
<p>But how does that help us compute a gradient?</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Lets sum up all the entries in z</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
 <span class="mi">21</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">1</span><span class="p">]</span>

<span class="o">&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">reduce</span><span class="o">.</span><span class="n">Sum</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f2121863048</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>So now, what is the derivative of this sum with respect to the first
component of x? In math, we want</p>
<div class="math">
\[\frac{\partial s}{\partial x_0}\]</div>
<p>Well, s knows that it was created as a sum of the tensor z. z knows
that it was the sum x + y. So</p>
<div class="math">
\[s = \overbrace{x_0 + y_0}^\text{$z_0$} + \overbrace{x_1 + y_1}^\text{$z_1$} + \overbrace{x_2 + y_2}^\text{$z_2$}\]</div>
<p>And so s contains enough information to determine that the derivative
we want is 1!</p>
<p>Of course this glosses over the challenge of how to actually compute
that derivative. The point here is that s is carrying along enough
information that it is possible to compute it. In reality, the
developers of Pytorch program the sum() and + operations to know how to
compute their gradients, and run the back propagation algorithm. An
in-depth discussion of that algorithm is beyond the scope of this
tutorial.</p>
<p>Lets have Pytorch compute the gradient, and see that we were right:
(note if you run this block multiple times, the gradient will increment.
That is because Pytorch <em>accumulates</em> the gradient into the .grad
property, since for many models this is very convenient.)</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># calling .backward() on any variable will run backprop, starting from it.</span>
<span class="n">s</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>Understanding what is going on in the block below is crucial for being a
successful programmer in deep learning.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># These are Tensor types, and backprop would not be possible</span>

<span class="n">var_x</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">var_y</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># var_z contains enough information to compute gradients, as we saw above</span>
<span class="n">var_z</span> <span class="o">=</span> <span class="n">var_x</span> <span class="o">+</span> <span class="n">var_y</span>
<span class="k">print</span><span class="p">(</span><span class="n">var_z</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>

<span class="n">var_z_data</span> <span class="o">=</span> <span class="n">var_z</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># Get the wrapped Tensor object out of var_z...</span>
<span class="c1"># Re-wrap the tensor in a new variable</span>
<span class="n">new_var_z</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">var_z_data</span><span class="p">)</span>

<span class="c1"># ... does new_var_z have information to backprop to x and y?</span>
<span class="c1"># NO!</span>
<span class="k">print</span><span class="p">(</span><span class="n">new_var_z</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
<span class="c1"># And how could it?  We yanked the tensor out of var_z (that is</span>
<span class="c1"># what var_z.data is).  This tensor doesn&#39;t know anything about</span>
<span class="c1"># how it was computed.  We pass it into new_var_z, and this is all the</span>
<span class="c1"># information new_var_z gets.  If var_z_data doesn&#39;t know how it was</span>
<span class="c1"># computed, theres no way new_var_z will.</span>
<span class="c1"># In essence, we have broken the variable away from its past history</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">basic_ops</span><span class="o">.</span><span class="n">Add</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f2121863828</span><span class="o">&gt;</span>
<span class="kc">None</span>
</pre></div>
</div>
<p>Here is the basic, extremely important rule for computing with
autograd.Variables (note this is more general than Pytorch. There is an
equivalent object in every major deep learning toolkit):</p>
<p><strong>If you want the error from your loss function to backpropogate to a
component of your network, you MUST NOT break the Variable chain from
that component to your loss Variable. If you do, the loss will have no
idea your component exists, and its parameters can&#8217;t be updated.</strong></p>
<p>I say this in bold, because this error can creep up on you in very
subtle ways (I will show some such ways below), and it will not cause
your code to crash or complain, so you must be careful.</p>
<p><strong>Total running time of the script:</strong> ( 0 minutes  0.003 seconds)</p>
<div class="sphx-glr-footer docutils container">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/pytorch_tutorial.py" download=""><code class="xref download docutils literal"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">pytorch_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/pytorch_tutorial.ipynb" download=""><code class="xref download docutils literal"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">pytorch_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="http://sphinx-gallery.readthedocs.io">Generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="deep_learning_tutorial.html" class="btn btn-neutral float-right" title="Deep Learning with PyTorch" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../deep_learning_nlp_tutorial.html" class="btn btn-neutral" title="Deep Learning for NLP with Pytorch" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>