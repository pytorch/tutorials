
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Learn how to use Mosaic for PyTorch GPU memory profiling. Capture and analyze memory snapshots, identify memory savings from activation checkpointing, debug OOM errors, and integrate memory analysis into training pipelines." name="description"/>
<meta content="PyTorch, Mosaic, memory profiling, GPU memory, CUDA, activation checkpointing, OOM debugging, deep learning, memory optimization, memory snapshots, distributed training, LLaMA, transformer models" name="keywords"/>
<meta content="2026-01-27T21:12:55+00:00" property="article:modified_time"/>
<title>Mosaic: Memory Profiling for PyTorch — PyTorch Tutorials 2.10.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=36fba2ff" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=a8d6e986"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'beginner/mosaic_memory_profiling_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/beginner/mosaic_memory_profiling_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../recipes_index.html" rel="next" title="Recipes"/>
<link href="../intermediate/realtime_rpi.html" rel="prev" title="Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jan 27, 2026" name="docbuild:last-update"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.10.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jan 27, 2026" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
<span class="dropdown-title">RAY</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
<span class="dropdown-title">Brand Guidelines</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started/locally">Get Started</a>
</li>
<li>
<a href="https://docs.pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.10.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="hyperparameter_tuning_tutorial.html">Hyperparameter tuning using Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/realtime_rpi.html">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Mosaic: Memory Profiling for PyTorch</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../ecosystem.html">Ecosystem</a></li>
<li aria-current="page" class="breadcrumb-item active">Mosaic:...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../ecosystem.html" itemprop="item"/>
<meta content="Ecosystem" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Mosaic: Memory Profiling for PyTorch" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">beginner/mosaic_memory_profiling_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-mosaic-memory-profiling-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="mosaic-memory-profiling-for-pytorch">
<span id="sphx-glr-beginner-mosaic-memory-profiling-tutorial-py"></span><h1>Mosaic: Memory Profiling for PyTorch<a class="headerlink" href="#mosaic-memory-profiling-for-pytorch" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/basilwong">Basil Wong</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How to capture and analyze PyTorch memory snapshots</p></li>
<li><p class="sd-card-text">Identify memory savings from activation checkpointing</p></li>
<li><p class="sd-card-text">Debug unexpected memory usage from abandoned code</p></li>
<li><p class="sd-card-text">Integrate memory analysis into training pipelines</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v2.0.0 or later</p></li>
<li><p class="sd-card-text">CUDA-capable GPU</p></li>
<li><p class="sd-card-text">Basic understanding of PyTorch training loops</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>This tutorial demonstrates how to use <a class="reference external" href="https://github.com/facebookresearch/mosaic">Mosaic</a>, a post-processing memory
snapshot analysis tool for PyTorch. Mosaic helps analyze GPU memory usage in
distributed deep learning, providing detailed insights into memory allocations,
peak usage, and memory imbalances across parallel workers.</p>
<p>Mosaic was instrumental in debugging OOM issues during the
<a class="reference external" href="https://ai.meta.com/blog/meta-llama-3-1/">405B LLaMA training</a>
and is now open source.</p>
</section>
<section id="introduction-to-mosaic">
<h1>Introduction to Mosaic<a class="headerlink" href="#introduction-to-mosaic" title="Link to this heading">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>In distributed deep learning, understanding GPU memory usage is critical
for optimizing training efficiency and debugging Out-of-Memory (OOM) errors.
Mosaic is a post-analysis tool for memory usage designed to work with
large-scale jobs. It helps analyze PyTorch memory snapshots captured during
the execution of PyTorch training jobs, providing detailed insights into
memory allocations, peak usage, and memory imbalances across parallel workers.</p>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h2>
<p>Clone the mosaic repository and install from the mosaic directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/facebookresearch/mosaic
<span class="nb">cd</span><span class="w"> </span>mosaic
python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate
pip3<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip3<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
<p>Alternatively, install directly via pip:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/facebookresearch/mosaic.git
</pre></div>
</div>
</section>
<section id="simple-usage-examples">
<h2>Simple Usage Examples<a class="headerlink" href="#simple-usage-examples" title="Link to this heading">#</a></h2>
<p><strong>1. Peak Memory Usage Analysis</strong></p>
<p>When addressing memory problems like OOM errors, focusing on peak memory
usage is crucial. The <code class="docutils literal notranslate"><span class="pre">mosaic_get_memory_usage_peak</span></code> command presents a
stack trace of the memory allocations that contributed to the peak memory
usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_usage_peak<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>snapshot&gt;
</pre></div>
</div>
<p><strong>2. Categorical Memory Profiling</strong></p>
<p>Mosaic classifies allocations into categories (activation, backward,
optimizer, etc.):</p>
<ul class="simple">
<li><p><strong>Activation Memory:</strong> Tensors saved for backward pass</p></li>
<li><p><strong>Gradient Memory:</strong> Gradients computed during backpropagation</p></li>
<li><p><strong>Optimizer State:</strong> Adam/SGD momentum and variance buffers</p></li>
<li><p><strong>Parameter Memory:</strong> Model weights</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--out-path<span class="w"> </span>&lt;html&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>categories
</pre></div>
</div>
<p>An example HTML output looks like:</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-no-allocation-ordering.png"><img alt="Mosaic categorical memory profiling without allocation ordering" src="../_images/mosaic-categorical-memory-profiling-no-allocation-ordering.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Categorical memory profiling showing memory breakdown by type
(activation, gradient, optimizer, etc.)</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To maintain allocation order for the categories, add <code class="docutils literal notranslate"><span class="pre">--preserve-allocation-order</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--out-path<span class="w"> </span>&lt;html&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>categories<span class="w"> </span>--preserve-allocation-order
</pre></div>
</div>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-allocation-ordering.png"><img alt="Mosaic categorical memory profiling with allocation ordering preserved" src="../_images/mosaic-categorical-memory-profiling-allocation-ordering.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Categorical profiling with <code class="docutils literal notranslate"><span class="pre">--preserve-allocation-order</span></code> shows memory
allocations in chronological order</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>3. Custom Dictionary Profiling</strong></p>
<p>For targeted analysis via regex pattern matching:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--profile<span class="w"> </span>custom<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--custom-profile<span class="w"> </span><span class="s1">'{"ncclx": "ncclx"}'</span>
</pre></div>
</div>
<p>This is invaluable for tracking specific kernels, optimizers, or custom code patterns:</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-ncclx.png"><img alt="Mosaic custom dictionary profiling with ncclx pattern" src="../_images/mosaic-categorical-memory-profiling-ncclx.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Custom profiling with regex patterns to track specific operations like
NCCL communications</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="dependencies-and-imports">
<h1>Dependencies and Imports<a class="headerlink" href="#dependencies-and-imports" title="Link to this heading">#</a></h1>
<p>Let’s set up the required dependencies and imports for this tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>

<span class="c1"># Fix for sphinx-gallery environment where __main__.__file__ may not exist</span>
<span class="c1"># This is needed for transformers library compatibility</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">],</span> <span class="s2">"__file__"</span><span class="p">):</span>
    <span class="c1"># Use this file's path as a fallback, or a dummy path if __file__ is not available</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">]</span><span class="o">.</span><span class="vm">__file__</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
        <span class="c1"># __file__ not available, use transformers modeling file as fallback</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">transformers.modeling_utils</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">]</span><span class="o">.</span><span class="vm">__file__</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="vm">__file__</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">Dataset</span></a>

<span class="c1"># Install dependencies if needed</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Config</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">CausalLMOutputWithCrossAttentions</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span>
        <span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span> <span class="s2">"-m"</span><span class="p">,</span> <span class="s2">"pip"</span><span class="p">,</span> <span class="s2">"install"</span><span class="p">,</span> <span class="s2">"-q"</span><span class="p">,</span> <span class="s2">"transformers"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Config</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">CausalLMOutputWithCrossAttentions</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">mosaic.libmosaic.analyzer.memory_abstract</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryAbstract</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">"-m"</span><span class="p">,</span>
            <span class="s2">"pip"</span><span class="p">,</span>
            <span class="s2">"install"</span><span class="p">,</span>
            <span class="s2">"-q"</span><span class="p">,</span>
            <span class="s2">"git+https://github.com/facebookresearch/mosaic.git"</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">mosaic.libmosaic.analyzer.memory_abstract</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryAbstract</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"CUDA available: </span><span class="si">{</span><a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU: </span><span class="si">{</span><a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>PyTorch version: 2.10.0+cu128
CUDA available: True
GPU: NVIDIA A10G
</pre></div>
</div>
</section>
<section id="shared-utilities">
<h1>Shared Utilities<a class="headerlink" href="#shared-utilities" title="Link to this heading">#</a></h1>
<p>These helper classes and functions are used throughout the tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RandomTokenDataset</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">Dataset</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Generates random token sequences for training.</span>

<span class="sd">    This dataset creates random input sequences suitable for language model</span>
<span class="sd">    training, simulating real training data without requiring actual text.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">num_samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator" title="torch.Generator"><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span></a><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>  <span class="c1"># noqa: ARG002</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,))</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span> <span class="s2">"labels"</span><span class="p">:</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clone</span><span class="p">()}</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">capture_memory_snapshot</span><span class="p">(</span><span class="n">output_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Context manager to capture and save PyTorch CUDA memory snapshots.</span>

<span class="sd">    This captures all GPU memory allocations during the context and saves</span>
<span class="sd">    them to a pickle file for later analysis with Mosaic.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_path: Path to save the memory snapshot pickle file.</span>
<span class="sd">    """</span>
    <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._record_memory_history" title="torch.cuda.memory._record_memory_history"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_record_memory_history</span></a><span class="p">(</span><span class="n">max_entries</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._snapshot" title="torch.cuda.memory._snapshot"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_snapshot</span></a><span class="p">()</span>
        <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._record_memory_history" title="torch.cuda.memory._record_memory_history"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_record_memory_history</span></a><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">snapshot</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"✓ Memory snapshot saved to </span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="case-1-understanding-memory-differences-with-activation-checkpointing">
<h1>Case 1: Understanding Memory Differences with Activation Checkpointing<a class="headerlink" href="#case-1-understanding-memory-differences-with-activation-checkpointing" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to analyze and compare GPU
memory usage between different model configurations.</p>
<p><strong>What we’ll do:</strong></p>
<ol class="arabic simple">
<li><p>Train GPT-2 and capture a memory snapshot (baseline)</p></li>
<li><p>Enable activation checkpointing and train again (modified)</p></li>
<li><p>Use Mosaic to identify exactly where memory savings occur</p></li>
</ol>
<section id="training-function-for-activation-checkpointing-comparison">
<h2>Training Function for Activation Checkpointing Comparison<a class="headerlink" href="#training-function-for-activation-checkpointing-comparison" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_ac</span><span class="p">(</span>
    <span class="n">activation_checkpointing</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""Run training loop and capture memory snapshot.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_checkpointing: Whether to enable gradient checkpointing.</span>
<span class="sd">        snapshot_path: Path to save the memory snapshot.</span>
<span class="sd">        batch_size: Training batch size.</span>
<span class="sd">        seq_length: Sequence length for input tokens.</span>
<span class="sd">        num_steps: Number of training steps to run.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Peak GPU memory usage in GB.</span>
<span class="sd">    """</span>
    <span class="c1"># Clear any previous memory</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="c1"># Load model</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loading GPT-2 (activation_checkpointing=</span><span class="si">{</span><span class="n">activation_checkpointing</span><span class="si">}</span><span class="s2">)..."</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">activation_checkpointing</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Activation checkpointing is ENABLED"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Activation checkpointing is DISABLED"</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># Create dataset and dataloader</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">RandomTokenDataset</span></a><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Setup optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="c1"># Training loop with memory capture</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"✓ Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory_gb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="c1"># Cleanup</span>
    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory_gb</span>
</pre></div>
</div>
</section>
<section id="run-baseline-training-without-activation-checkpointing">
<h2>Run Baseline Training (Without Activation Checkpointing)<a class="headerlink" href="#run-baseline-training-without-activation-checkpointing" title="Link to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial requires a CUDA-capable GPU. If you’re running in
Google Colab, make sure to select a GPU runtime:
Runtime → Change runtime type → Hardware accelerator → GPU</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: No CUDA GPU detected!"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">This tutorial requires a CUDA-capable GPU for memory profiling."</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">If you're running in Google Colab:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  1. Go to Runtime → Change runtime type"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  2. Set Hardware accelerator to 'GPU'"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  3. Click 'Save' and re-run the notebook"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Skipping GPU memory profiling examples..."</span><span class="p">)</span>
    <span class="n">HAS_CUDA</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">HAS_CUDA</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Check if Mosaic CLI is available</span>
<span class="n">HAS_MOSAIC_CLI</span> <span class="o">=</span> <span class="n">shutil</span><span class="o">.</span><span class="n">which</span><span class="p">(</span><span class="s2">"mosaic_get_memory_profile"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Note: Mosaic CLI not found. Install Mosaic to generate HTML profiles."</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"      pip install git+https://github.com/facebookresearch/mosaic.git"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"BASELINE: Training WITHOUT Activation Checkpointing"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">baseline_memory</span> <span class="o">=</span> <span class="n">run_training_ac</span><span class="p">(</span>
        <span class="n">activation_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"snapshot_baseline.pickle"</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
BASELINE: Training WITHOUT Activation Checkpointing
============================================================
Loading GPT-2 (activation_checkpointing=False)...
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.

Loading weights:   0%|          | 0/148 [00:00&lt;?, ?it/s]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 14169.95it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 759.01it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 1199.40it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 1058.10it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 978.68it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 914.66it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1082.96it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1028.65it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1167.35it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1077.17it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1144.63it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1098.08it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 987.56it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 911.98it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 822.23it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 801.80it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 766.71it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 721.44it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 764.02it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 714.47it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 704.09it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 678.12it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 675.50it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 637.57it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 664.54it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 633.41it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 650.71it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 628.15it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 644.51it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 633.77it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 656.94it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 629.53it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 611.64it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 602.20it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 627.66it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 610.60it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 628.41it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 610.21it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 619.79it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 609.44it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 621.04it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 588.82it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 595.77it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 585.30it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 596.41it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 590.36it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 601.38it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 593.11it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 603.28it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 600.39it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 609.02it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 606.32it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 614.02it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 608.75it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 624.94it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 609.70it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 618.28it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 614.85it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 632.38it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 629.55it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 646.71it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 643.84it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 660.90it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 658.38it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 675.79it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 673.20it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 690.32it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 687.66it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 704.59it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 701.94it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 718.68it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 715.84it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 732.33it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 729.47it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 745.56it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 742.48it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 757.52it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 754.80it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 770.69it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 767.81it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 783.56it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 780.70it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 796.33it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 793.51it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 808.90it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 806.08it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 821.32it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 818.52it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 833.54it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 830.64it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 845.44it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 842.19it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 856.02it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 853.12it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 867.60it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 864.32it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 878.13it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 875.35it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 889.37it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 886.40it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 900.49it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 897.50it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 911.36it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 908.22it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 921.90it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 918.96it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 932.69it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 929.73it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 943.28it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 940.37it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 953.50it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 950.56it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 963.96it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 960.85it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 974.02it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 971.27it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 984.29it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 981.03it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 992.91it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 989.81it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1002.42it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 999.36it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1011.94it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1008.67it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1021.40it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1018.86it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1031.51it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1028.31it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1040.11it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1036.93it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1049.25it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1046.07it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1058.19it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1055.01it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1067.05it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1063.86it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1075.62it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1072.49it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1084.09it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1080.61it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1091.43it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1087.96it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1099.40it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1096.28it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1107.74it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1104.66it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1116.00it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1112.87it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1123.96it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1120.97it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1132.09it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1128.75it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1139.29it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1136.16it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1147.16it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1144.06it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1154.96it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1151.84it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1162.72it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1159.58it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1170.31it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1167.23it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1177.82it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1174.61it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1185.10it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1182.05it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1192.20it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1189.39it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1199.51it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1196.15it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1206.10it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1202.20it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1212.15it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1209.10it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1219.14it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1216.09it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1226.09it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1222.98it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1232.94it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1229.49it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1238.63it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1235.14it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1244.13it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1240.66it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1249.47it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1245.96it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1254.58it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1251.51it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1260.98it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1257.85it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1267.30it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1264.19it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1273.54it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1270.37it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1279.64it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1276.55it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1285.78it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1282.67it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1291.74it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1288.59it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1297.59it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1294.82it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1303.71it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1300.40it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1308.92it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1305.87it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1314.84it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1311.45it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1319.88it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1316.90it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1325.56it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1322.53it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1331.21it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1328.23it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1336.72it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1333.71it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1342.26it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1339.32it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1347.25it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1344.14it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1352.53it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1349.55it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1357.14it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1353.33it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1360.75it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1357.43it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1365.81it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1362.77it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1371.08it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1368.10it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1376.38it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1373.07it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1380.53it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1377.17it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1384.39it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1381.05it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1388.37it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1385.48it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1393.30it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1390.46it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1398.42it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1395.16it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1403.02it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1399.99it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1407.67it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1404.77it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1412.66it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1409.64it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1417.44it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1414.46it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1422.21it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1419.22it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1426.99it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1424.02it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1431.68it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1428.72it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1436.31it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1433.38it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1440.92it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1437.86it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1445.28it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1442.21it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1449.58it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1446.60it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1453.52it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1450.49it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1457.74it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1454.72it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1461.92it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1458.59it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1465.06it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1462.02it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1469.21it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1466.19it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1473.36it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1470.34it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1477.46it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1474.35it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1481.01it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1477.90it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1484.96it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1481.72it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1487.85it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1484.47it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1490.58it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1487.33it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1493.55it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1490.61it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1497.41it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1494.40it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1501.27it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1498.29it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1505.17it/s, Materializing param=transformer.wpe.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1502.20it/s, Materializing param=transformer.wpe.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1509.02it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1506.00it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1502.29it/s, Materializing param=transformer.wte.weight]
GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
Activation checkpointing is DISABLED
Running 5 training steps...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
  Step 1/5, Loss: 12.2491
  Step 2/5, Loss: 12.0704
  Step 3/5, Loss: 11.9368
  Step 4/5, Loss: 11.7790
  Step 5/5, Loss: 11.8115
✓ Memory snapshot saved to snapshot_baseline.pickle
✓ Peak GPU memory: 5.12 GB
</pre></div>
</div>
</section>
<section id="run-modified-training-with-activation-checkpointing">
<h2>Run Modified Training (With Activation Checkpointing)<a class="headerlink" href="#run-modified-training-with-activation-checkpointing" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MODIFIED: Training WITH Activation Checkpointing"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">ac_memory</span> <span class="o">=</span> <span class="n">run_training_ac</span><span class="p">(</span>
        <span class="n">activation_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"snapshot_with_ac.pickle"</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Summary</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MEMORY COMPARISON SUMMARY"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Baseline (no AC):     </span><span class="si">{</span><span class="n">baseline_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"With AC:              </span><span class="si">{</span><span class="n">ac_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">baseline_memory</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">saved_pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">baseline_memory</span> <span class="o">-</span> <span class="n">ac_memory</span><span class="p">)</span> <span class="o">/</span> <span class="n">baseline_memory</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Memory Saved:         </span><span class="si">{</span><span class="n">baseline_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">ac_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB (</span><span class="si">{</span><span class="n">saved_pct</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)"</span>
        <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
MODIFIED: Training WITH Activation Checkpointing
============================================================
Loading GPT-2 (activation_checkpointing=True)...

Loading weights:   0%|          | 0/148 [00:00&lt;?, ?it/s]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 45590.26it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 4619.28it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 3674.38it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 2680.07it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1417.63it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1292.41it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 692.19it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 668.12it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 786.19it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 744.25it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 842.23it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 799.14it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 859.61it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 757.33it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 819.56it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 764.08it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 809.99it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 778.79it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 762.70it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 737.80it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 769.92it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 759.29it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 778.90it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 744.03it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 785.17it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 727.43it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 732.06it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 683.37it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 706.79it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 690.48it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 711.15it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 693.77it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 725.14it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 705.65it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 735.91it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 730.59it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 748.50it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 740.88it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 768.73it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 757.55it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 785.15it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 756.60it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 760.85it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 756.16it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 774.96it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 770.20it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 764.34it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 759.80it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 772.91it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 767.95it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 778.86it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 774.63it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 787.55it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 782.32it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 797.24it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 777.30it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 787.94it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 777.60it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 789.02it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 777.04it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 795.08it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 785.82it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 803.60it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 800.03it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 811.88it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 795.37it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 812.14it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 808.31it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 808.29it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 804.73it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 815.70it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 811.89it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 823.83it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 820.46it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 835.30it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 831.94it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 846.82it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 843.56it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 844.97it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 834.19it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 827.23it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 824.19it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 818.91it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 815.94it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 815.82it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 812.32it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 822.11it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 810.69it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 821.34it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 818.36it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 825.80it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 823.06it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 836.62it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 834.13it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 848.36it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 845.85it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 860.14it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 857.76it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 872.03it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 869.67it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 883.84it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 881.38it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 895.44it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 893.03it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 906.96it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 904.57it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 918.34it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 915.93it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 929.67it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 927.26it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 940.48it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 937.99it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 951.46it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 949.00it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 962.18it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 959.70it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 972.93it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 970.40it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 983.08it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 980.49it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 993.47it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 990.97it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1003.78it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1001.25it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1013.97it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1011.44it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1024.12it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1021.60it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1033.67it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1031.12it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1043.61it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1041.05it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1053.49it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1050.96it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1063.20it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1060.65it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1072.66it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1070.10it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1082.15it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1079.58it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1091.54it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1088.96it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1100.33it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1097.57it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1109.27it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1106.66it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1118.27it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1115.67it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1127.17it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1124.53it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1135.88it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1133.24it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1144.64it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1142.01it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1153.42it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1150.82it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1162.15it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1159.56it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1170.57it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1167.62it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1178.62it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1175.98it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1186.98it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1184.34it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1195.11it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1192.44it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1203.14it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1200.46it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1211.23it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1208.56it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1219.18it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1216.53it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1227.11it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1223.81it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1234.25it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1231.29it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1241.10it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1237.87it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1247.53it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1244.08it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1253.03it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1249.88it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1260.02it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1257.33it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1267.45it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1264.78it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1274.77it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1272.09it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1282.03it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1279.38it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1289.25it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1286.59it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1296.34it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1293.36it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1302.98it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1300.25it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1309.89it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1307.20it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1316.74it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1314.04it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1323.41it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1320.72it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1330.25it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1327.56it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1337.03it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1334.35it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1343.75it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1340.82it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1349.89it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1347.16it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1356.42it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1353.69it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1362.84it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1360.13it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1369.25it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1366.56it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1375.60it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1372.88it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1381.85it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1379.17it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1388.13it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1385.47it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1394.11it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1391.18it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1399.96it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1397.30it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1405.71it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1403.01it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1411.82it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1409.13it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1417.81it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1415.13it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1423.77it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1421.10it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1429.64it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1426.96it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1435.50it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1432.83it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1441.28it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1438.37it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1446.52it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1443.80it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1452.12it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1449.21it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1457.45it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1454.73it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1462.95it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1460.29it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1468.45it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1465.77it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1474.04it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1471.37it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1479.59it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1476.94it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1485.08it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1482.43it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1489.69it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1486.71it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1494.00it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1491.23it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1498.52it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1495.72it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1502.91it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1500.15it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1507.27it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1504.51it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1511.76it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1508.95it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1516.19it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1513.46it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1520.57it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1517.79it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1524.87it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1522.13it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1529.25it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1526.51it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1533.65it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1530.90it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1537.55it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1534.85it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1541.89it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1539.17it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1546.01it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1543.29it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1550.11it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1547.39it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1554.02it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1551.27it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1558.02it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1555.33it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1562.28it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1559.60it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1566.43it/s, Materializing param=transformer.wpe.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1563.77it/s, Materializing param=transformer.wpe.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1570.63it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1567.99it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1563.87it/s, Materializing param=transformer.wte.weight]
GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
Activation checkpointing is ENABLED
Running 5 training steps...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  Step 1/5, Loss: 12.3067
  Step 2/5, Loss: 12.1271
  Step 3/5, Loss: 11.9482
  Step 4/5, Loss: 11.8362
  Step 5/5, Loss: 11.7708
✓ Memory snapshot saved to snapshot_with_ac.pickle
✓ Peak GPU memory: 3.04 GB

============================================================
MEMORY COMPARISON SUMMARY
============================================================
Baseline (no AC):     5.12 GB
With AC:              3.04 GB
Memory Saved:         2.08 GB (40.6%)
</pre></div>
</div>
</section>
<section id="generate-categorical-memory-profiles-with-mosaic">
<h2>Generate Categorical Memory Profiles with Mosaic<a class="headerlink" href="#generate-categorical-memory-profiles-with-mosaic" title="Link to this heading">#</a></h2>
<p>Use Mosaic to generate HTML profiles for both snapshots.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Categorical Memory Profiling"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Generate HTML profiles using subprocess</span>
    <span class="n">result1</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">"mosaic_get_memory_profile"</span><span class="p">,</span>
            <span class="s2">"--snapshot"</span><span class="p">,</span>
            <span class="s2">"snapshot_baseline.pickle"</span><span class="p">,</span>
            <span class="s2">"--out-path"</span><span class="p">,</span>
            <span class="s2">"profile_baseline.html"</span><span class="p">,</span>
            <span class="s2">"--profile"</span><span class="p">,</span>
            <span class="s2">"categories"</span><span class="p">,</span>
            <span class="s2">"--preserve-allocation-order"</span><span class="p">,</span>
            <span class="s2">"--plotter_sampling_rate"</span><span class="p">,</span>
            <span class="s2">"20"</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">()</span>

    <span class="n">result2</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">"mosaic_get_memory_profile"</span><span class="p">,</span>
            <span class="s2">"--snapshot"</span><span class="p">,</span>
            <span class="s2">"snapshot_with_ac.pickle"</span><span class="p">,</span>
            <span class="s2">"--out-path"</span><span class="p">,</span>
            <span class="s2">"profile_with_ac.html"</span><span class="p">,</span>
            <span class="s2">"--profile"</span><span class="p">,</span>
            <span class="s2">"categories"</span><span class="p">,</span>
            <span class="s2">"--preserve-allocation-order"</span><span class="p">,</span>
            <span class="s2">"--plotter_sampling_rate"</span><span class="p">,</span>
            <span class="s2">"20"</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">result1</span><span class="o">.</span><span class="n">returncode</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">result2</span><span class="o">.</span><span class="n">returncode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Generated profile_baseline.html"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Generated profile_with_ac.html"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Download these files to view the interactive memory profiles."</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Note: Mosaic profile generation encountered issues."</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"This may happen if running in an environment without full Mosaic support."</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
MOSAIC: Categorical Memory Profiling
============================================================


Note: Mosaic profile generation encountered issues.
This may happen if running in an environment without full Mosaic support.
</pre></div>
</div>
</section>
<section id="results-interpretation-activation-checkpointing">
<h2>Results Interpretation: Activation Checkpointing<a class="headerlink" href="#results-interpretation-activation-checkpointing" title="Link to this heading">#</a></h2>
<section id="what-we-observed">
<h3>What We Observed<a class="headerlink" href="#what-we-observed" title="Link to this heading">#</a></h3>
<p>Based on the Mosaic categorical profiling results:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id4">
<caption><span class="caption-text">Memory Comparison Results</span><a class="headerlink" href="#id4" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Baseline</p></th>
<th class="head"><p>With Activation Checkpointing</p></th>
<th class="head"><p>Difference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Total Peak Memory</strong></p></td>
<td><p><strong>4.62 GB</strong></p></td>
<td><p><strong>2.55 GB</strong></p></td>
<td><p><strong>2.07 GB (45% reduction)</strong></p></td>
</tr>
<tr class="row-odd"><td><p>Activation Memory</p></td>
<td><p>2.93 GB</p></td>
<td><p>872.79 MB</p></td>
<td><p><strong>2.08 GB saved (71% reduction)</strong></p></td>
</tr>
<tr class="row-even"><td><p>Backward/Gradient Memory</p></td>
<td><p>793.39 MB</p></td>
<td><p>785.27 MB</p></td>
<td><p>8 MB (minimal change)</p></td>
</tr>
<tr class="row-odd"><td><p>Optimizer State</p></td>
<td><p>949.4 MB</p></td>
<td><p>949.4 MB</p></td>
<td><p>No change</p></td>
</tr>
<tr class="row-even"><td><p>Unknown</p></td>
<td><p>32 KB</p></td>
<td><p>32 KB</p></td>
<td><p>No change</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="key-insights">
<h3>Key Insights<a class="headerlink" href="#key-insights" title="Link to this heading">#</a></h3>
<p><strong>Primary Finding:</strong> Activation memory dropped from <strong>2.93 GB → 872 MB</strong>
(71% reduction), which accounts for nearly all the total memory savings.</p>
</section>
<section id="why-does-this-happen">
<h3>Why Does This Happen?<a class="headerlink" href="#why-does-this-happen" title="Link to this heading">#</a></h3>
<p><strong>Activation checkpointing</strong> is a memory optimization technique that:</p>
<ol class="arabic simple">
<li><p><strong>Without AC (Baseline):</strong> All intermediate activations from the forward
pass are stored in memory for use during backpropagation. GPT-2 has 12
transformer layers, each storing multiple activations (attention outputs,
MLP outputs, etc.). For batch_size=4, seq_length=512, this adds up quickly.</p></li>
<li><p><strong>With AC (Optimized):</strong> Only activations at checkpoint boundaries are
stored; intermediate activations are recomputed during the backward pass.
This dramatically reduces activation memory (71% in our case) while other
memory categories remain unchanged.</p></li>
</ol>
</section>
<section id="how-mosaic-helped">
<h3>How Mosaic Helped<a class="headerlink" href="#how-mosaic-helped" title="Link to this heading">#</a></h3>
<p>Mosaic’s categorical profiling immediately identified:</p>
<ul class="simple">
<li><p>Activation memory is the category with the largest difference (2.08 GB saved)</p></li>
<li><p>Backward/Gradient memory stayed nearly constant (793 MB → 785 MB)</p></li>
<li><p>Optimizer state remained unchanged (949 MB) - expected since model
parameters don’t change</p></li>
</ul>
<p><strong>Without Mosaic:</strong> You would need to manually instrument your code, track
allocations, and categorize them yourself.</p>
<p><strong>With Mosaic:</strong> You get instant categorical breakdowns with exact numbers,
making it trivial to identify/quantify memory optimizations.</p>
</section>
</section>
</section>
<section id="case-2-debugging-unexpected-memory-usage">
<h1>Case 2: Debugging Unexpected Memory Usage<a class="headerlink" href="#case-2-debugging-unexpected-memory-usage" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to debug when your model is
using more memory than expected and you’re not sure why.</p>
<p><strong>What we’ll do:</strong></p>
<ol class="arabic simple">
<li><p>Train GPT-2 and capture a memory snapshot.</p></li>
<li><p>Train GPT-2 with a bug that introduces additional memory and capture
a memory snapshot.</p></li>
<li><p>Use Mosaic to identify potential culprits introducing additional memory.</p></li>
</ol>
<section id="the-buggy-model">
<h2>The Buggy Model<a class="headerlink" href="#the-buggy-model" title="Link to this heading">#</a></h2>
<p>This model has <strong>abandoned debug code</strong> that creates unnecessary GPU memory
overhead. Someone added projection layers to “analyze hidden states” during
debugging, but forgot to remove them before training.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GPT2WithDebugOverhead</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""GPT2 with abandoned 'feature analysis' code that bloats peak memory."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># BUG: Large projection layers from an abandoned experiment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span></a><span class="p">(</span>
            <span class="p">[</span>
                <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="n">debug_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  [DEBUG] Added </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="si">}</span><span class="s2"> debug projection layers"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  [DEBUG] Extra parameters: </span><span class="si">{</span><span class="n">debug_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Run normal GPT-2 forward with hidden states</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># BUG: Project all hidden states through debug layers</span>
        <span class="n">projected</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_layer_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">proj</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">proj_hidden</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
            <span class="n">projected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proj_hidden</span><span class="p">)</span>

        <span class="c1"># Tie to loss so gradients flow through</span>
        <span class="n">debug_regularization</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">projected</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-10</span>

        <span class="k">return</span> <span class="n">CausalLMOutputWithCrossAttentions</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span> <span class="o">+</span> <span class="n">debug_regularization</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-functions-for-debug-comparison">
<h2>Training Functions for Debug Comparison<a class="headerlink" href="#training-functions-for-debug-comparison" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_clean</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training with the normal model."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Loading clean model (no debug overhead)..."</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">RandomTokenDataset</span></a><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Running training (should contain no debug overhead)..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory</span>


<span class="k">def</span><span class="w"> </span><span class="nf">run_training_with_bug</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training with the buggy model."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Loading buggy model with debug overhead..."</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">GPT2Config</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2WithDebugOverhead</span></a><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Load pretrained weights</span>
    <span class="n">pretrained</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">pretrained</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">RandomTokenDataset</span></a><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Running training (WITH debug overhead bug)..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory</span>
</pre></div>
</div>
</section>
<section id="run-training-for-baseline-clean-model">
<h2>Run Training for Baseline (Clean Model)<a class="headerlink" href="#run-training-for-baseline-clean-model" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training with baseline model"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">baseline_memory_debug</span> <span class="o">=</span> <span class="n">run_training_clean</span><span class="p">(</span>
        <span class="s2">"snapshot_debug_baseline.pickle"</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
Training with baseline model
============================================================
Loading clean model (no debug overhead)...

Loading weights:   0%|          | 0/148 [00:00&lt;?, ?it/s]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 59074.70it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 5041.23it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 2150.93it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 1125.84it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1313.32it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1208.97it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1035.89it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 984.81it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1127.80it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1082.07it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1215.33it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1171.43it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 1274.75it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 1235.07it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 1168.37it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 1138.06it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 1178.54it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 1151.65it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 1225.90it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 1200.36it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 1274.97it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 1250.20it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 1313.63it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 1288.94it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 1219.30it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 1199.03it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 1233.15it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 1214.01it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 1264.06it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 1245.71it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 1288.18it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 1270.33it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 1300.18it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 1282.41it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 1294.92it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 1225.23it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 1162.11it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 1149.27it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 1128.78it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 1077.13it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 1082.37it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 1051.20it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 1061.20it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 1051.89it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 1051.93it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 1033.64it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 1051.25it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 1008.75it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 1017.61it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 983.47it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 970.96it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 948.02it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 947.25it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 915.00it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 939.13it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 912.46it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 935.52it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 929.72it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 950.77it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 945.10it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 967.21it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 961.35it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 981.95it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 976.24it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 997.82it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 992.17it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 972.72it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 956.61it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 967.16it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 960.57it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 973.65it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 968.48it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 973.69it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 968.77it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 955.55it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 946.48it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 959.85it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 955.00it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 968.95it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 964.72it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 979.45it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 974.45it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 988.28it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 983.69it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 998.51it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 993.93it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 1010.56it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 1006.43it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 1017.71it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 1013.28it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 1027.39it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 1023.28it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 1022.50it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 1018.43it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 1014.94it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 1004.39it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 1004.24it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 1000.40it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 1000.56it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 996.85it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 1002.09it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 998.39it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 987.66it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 983.87it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 995.65it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 978.61it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 989.86it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 986.60it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 996.61it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 993.33it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 1004.42it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 1001.15it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 1001.06it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 997.81it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1005.35it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1001.97it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1010.66it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1007.12it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1010.70it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1007.38it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1017.76it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1014.56it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1023.24it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1020.14it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1029.65it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1020.13it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1030.99it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1011.24it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1021.36it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1016.00it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1024.33it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1014.36it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1024.33it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1016.96it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1011.18it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1006.70it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1015.62it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1011.83it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1022.77it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1020.20it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1030.93it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1028.27it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1038.93it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1035.94it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1046.37it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1043.69it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1053.96it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1050.65it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1060.83it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1058.07it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1068.28it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1065.65it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1075.84it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1073.18it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1083.25it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1080.53it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1090.60it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1087.65it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1097.54it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1094.67it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1104.36it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1101.60it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1111.28it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1108.37it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1117.97it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1115.10it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1124.31it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1121.49it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1130.91it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1127.80it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1136.77it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1134.10it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1143.50it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1140.75it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1149.90it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1147.30it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1156.45it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1153.69it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1163.03it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1160.16it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1169.40it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1166.74it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1175.82it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1173.21it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1182.34it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1179.66it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1188.65it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1185.93it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1194.88it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1192.23it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1201.14it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1198.47it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1206.53it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1203.63it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1212.04it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1209.30it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1217.83it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1214.96it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1223.51it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1220.67it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1229.05it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1226.23it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1234.51it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1231.73it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1240.19it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1237.55it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1246.03it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1243.44it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1251.93it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1249.37it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1257.78it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1255.21it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1263.39it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1260.84it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1269.01it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1266.47it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1274.45it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1271.62it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1279.51it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1276.92it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1284.71it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1282.07it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1289.85it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1287.31it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1295.14it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1292.79it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1300.71it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1298.38it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1306.26it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1303.95it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1311.89it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1309.33it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1317.21it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1314.67it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1322.28it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1319.71it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1327.11it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1324.56it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1332.11it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1329.27it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1336.63it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1334.26it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1341.87it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1339.20it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1346.83it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1344.53it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1351.79it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1349.45it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1356.85it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1354.55it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1361.76it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1359.21it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1366.43it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1363.88it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1371.01it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1368.37it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1375.38it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1372.74it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1380.11it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1377.45it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1384.65it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1382.11it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1389.00it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1386.59it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1393.70it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1391.38it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1398.43it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1396.07it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1402.90it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1400.56it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1407.39it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1405.04it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1411.83it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1409.50it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1416.34it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1413.70it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1420.45it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1418.16it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1424.88it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1422.58it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1429.22it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1426.71it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1433.30it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1430.78it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1437.35it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1434.82it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1441.34it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1438.84it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.wpe.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.wpe.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1445.22it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1448.19it/s, Materializing param=transformer.wte.weight]
GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
Running training (should contain no debug overhead)...
  Step 1, Loss: 12.2905
  Step 2, Loss: 12.0938
  Step 3, Loss: 12.0060
✓ Memory snapshot saved to snapshot_debug_baseline.pickle
Peak GPU memory: 5.13 GB
</pre></div>
</div>
</section>
<section id="run-training-with-the-bug">
<h2>Run Training WITH the Bug<a class="headerlink" href="#run-training-with-the-bug" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training with debug projection overhead (BUG)"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">buggy_memory</span> <span class="o">=</span> <span class="n">run_training_with_bug</span><span class="p">(</span><span class="s2">"snapshot_with_bug.pickle"</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># Handle transformers version compatibility issues</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Note: Skipping buggy model demo due to transformers compatibility: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">buggy_memory</span> <span class="o">=</span> <span class="n">baseline_memory_debug</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
Training with debug projection overhead (BUG)
============================================================
Loading buggy model with debug overhead...
Note: Skipping buggy model demo due to transformers compatibility: GPT2Model does not support setting experts implementation.
</pre></div>
</div>
</section>
<section id="use-mosaic-to-find-the-problem">
<h2>Use Mosaic to Find the Problem<a class="headerlink" href="#use-mosaic-to-find-the-problem" title="Link to this heading">#</a></h2>
<p>Analyze both snapshots to identify the source of extra memory usage.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Analyzing the Baseline Snapshot"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span><span class="s2">"mosaic_get_memory_usage_peak"</span><span class="p">,</span> <span class="s2">"--snapshot"</span><span class="p">,</span> <span class="s2">"snapshot_debug_baseline.pickle"</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Analyzing the Buggy Snapshot"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span><span class="s2">"mosaic_get_memory_usage_peak"</span><span class="p">,</span> <span class="s2">"--snapshot"</span><span class="p">,</span> <span class="s2">"snapshot_with_bug.pickle"</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
MOSAIC: Analyzing the Baseline Snapshot
============================================================

============================================================
MOSAIC: Analyzing the Buggy Snapshot
============================================================
</pre></div>
</div>
</section>
<section id="analyzing-the-mosaic-output">
<h2>Analyzing The Mosaic Output<a class="headerlink" href="#analyzing-the-mosaic-output" title="Link to this heading">#</a></h2>
<p>When you run Mosaic’s peak memory analysis, it shows stack traces for each
memory allocation. Let’s look at how to find abandoned or unnecessary code
that’s bloating the memory.</p>
<section id="optimizer-state-allocations-delta">
<h3>1. Optimizer State Allocations Delta<a class="headerlink" href="#optimizer-state-allocations-delta" title="Link to this heading">#</a></h3>
<p>In the buggy snapshot output, we can see that the first two stack traces
represent the <strong>optimizer state allocations</strong> (like <code class="docutils literal notranslate"><span class="pre">zeros_like</span></code> for Adam
optimizer state). See <code class="docutils literal notranslate"><span class="pre">torch/optim/adam.py</span></code> in the stack trace.</p>
<p>In the snapshot of the buggy model we can see around a total of 0.21 GB
more memory:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id5">
<caption><span class="caption-text">Optimizer State Comparison</span><a class="headerlink" href="#id5" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Version</p></th>
<th class="head"><p>Stack Trace Position</p></th>
<th class="head"><p>Calls</p></th>
<th class="head"><p>Memory (per trace)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Buggy model</p></td>
<td><p>1st and 2nd</p></td>
<td><p>172 calls</p></td>
<td><p>0.569 GB + 0.569 GB</p></td>
</tr>
<tr class="row-odd"><td><p>Baseline</p></td>
<td><p>2nd and 3rd</p></td>
<td><p>148 calls</p></td>
<td><p>0.464 GB + 0.464 GB</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this tells us:</strong> The optimizer is tracking more tensors! This is your
first clue that there are extra parameters or tensors in the computation graph.</p>
</section>
<section id="additional-activation-allocations">
<h3>2. Additional Activation Allocations<a class="headerlink" href="#additional-activation-allocations" title="Link to this heading">#</a></h3>
<p>The buggy version shows <strong>extra allocations</strong> that don’t appear in the
baseline model. Scrolling down the Mosaic output of the buggy model we can
see additional stack traces which contain:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch::autograd::Engine::evaluate_function</span></code>: We’re in the backward pass</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AddmmBackward0::apply</span></code>: Computing gradients for an addmm operation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">empty_cuda</span></code> at the bottom: Allocating a new CUDA tensor to store
the gradient</p></li>
</ol>
<ul class="simple">
<li><p>0.176 GB from matrix multiply gradients (<code class="docutils literal notranslate"><span class="pre">AddmmBackward0</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_mat1_backward</span></code>)</p></li>
</ul>
</section>
<section id="memory-total-explanation">
<h3>Memory Total Explanation<a class="headerlink" href="#memory-total-explanation" title="Link to this heading">#</a></h3>
<p><strong>Total Peak Dynamic Memory Usage:</strong> This is the peak memory that changes
during execution, measured relative to the starting point of the snapshot.
It tracks memory allocations that occur during the traced execution timeline.</p>
<p><strong>Total Static Memory Usage:</strong> This is the “starting memory” or baseline
memory that exists before tracing begins. It’s estimated by the PyTorch
visualizer and remains constant throughout the snapshot (doesn’t come with
stack traces).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the snapshots you may observe differences in total <em>static</em> memory
usage, which accounts for the remaining difference.</p>
</div>
<p><strong>Total Overall Peak Memory Usage:</strong> Dynamic + Static</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"COMPARISON"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Baseline (clean model):           </span><span class="si">{</span><span class="n">baseline_memory_debug</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"With bug (debug projections):     </span><span class="si">{</span><span class="n">buggy_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">"Extra memory from bug:            </span><span class="si">{</span><span class="n">buggy_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">baseline_memory_debug</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
COMPARISON
============================================================
Baseline (clean model):           5.13 GB
With bug (debug projections):     5.13 GB
Extra memory from bug:            0.00 GB
</pre></div>
</div>
</section>
</section>
</section>
<section id="case-3-integrating-memory-analysis-into-your-training-pipeline">
<h1>Case 3: Integrating Memory Analysis into Your Training Pipeline<a class="headerlink" href="#case-3-integrating-memory-analysis-into-your-training-pipeline" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to automatically capture memory
snapshots during training, get structured memory breakdown data for
monitoring/dashboards, and build automated memory monitoring for large-scale
training using Mosaic <strong>programmatically</strong> (as a Python dependency).</p>
<p>Mosaic integrates memory analysis directly into your training pipeline.</p>
<section id="training-with-automatic-memory-capture">
<h2>Training with Automatic Memory Capture<a class="headerlink" href="#training-with-automatic-memory-capture" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_with_memory_capture</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"training_snapshot.pickle"</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""Run training and automatically capture memory snapshot."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">RandomTokenDataset</span></a><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps with memory capture..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"✓ PyTorch reported peak memory: </span><span class="si">{</span><span class="n">peak_memory_gb</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">snapshot_path</span>


<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"CASE 3: Pipeline Integration"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">pipeline_snapshot_path</span> <span class="o">=</span> <span class="n">run_training_with_memory_capture</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
CASE 3: Pipeline Integration
============================================================

Loading weights:   0%|          | 0/148 [00:00&lt;?, ?it/s]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 51150.05it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 1284.23it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 1776.87it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 1526.59it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1723.92it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 1496.54it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1697.07it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1576.95it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1359.67it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1109.96it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1155.88it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1054.73it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 1060.35it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 944.91it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 980.72it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 892.36it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 964.55it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 941.72it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 955.10it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 890.55it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 949.88it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 934.67it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 970.70it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 945.48it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 963.53it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 950.16it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 978.46it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 960.82it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 996.71it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 964.59it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 986.10it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 955.60it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 994.95it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 984.36it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 1015.82it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 966.21it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 994.05it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 984.48it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 1014.83it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 1005.45it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 1036.86it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 1027.02it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 1040.07it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 1004.30it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 990.69it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 976.75it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 998.61it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 977.27it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 994.00it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 973.15it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 979.23it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 970.82it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 990.93it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 982.94it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 1004.21it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 982.33it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 942.96it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 922.93it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 931.03it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 915.38it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 920.82it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 900.94it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 914.76it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 909.10it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 923.30it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 912.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 932.10it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 904.43it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 909.18it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 898.74it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 911.25it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 907.11it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 917.70it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 912.06it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 912.12it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 894.22it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 907.30it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 898.18it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 904.77it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 888.26it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 902.05it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 892.02it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 892.62it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 880.85it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 890.99it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 881.93it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 886.43it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 876.28it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 864.79it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 854.44it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 869.69it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 867.09it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 882.48it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 879.91it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 895.05it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 891.95it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 906.96it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 904.13it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 918.92it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 915.98it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 930.67it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 928.04it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 942.71it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 939.82it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 954.15it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 951.46it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 965.88it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 963.25it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 977.57it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 974.92it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 989.06it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 986.44it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 1000.53it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 997.83it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1011.71it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1008.99it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1022.74it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1019.70it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1033.32it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1030.57it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1044.04it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1040.53it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1053.63it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1050.80it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1064.02it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1061.26it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1074.52it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1071.41it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1084.12it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1081.10it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1093.65it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1090.53it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1103.12it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1100.01it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1112.64it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1109.87it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1122.36it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1119.36it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1131.59it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1128.50it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1140.77it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1137.92it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1150.10it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1147.28it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1159.47it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1156.32it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1168.23it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1165.03it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1176.74it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1173.81it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1185.40it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1182.42it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1194.19it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1191.08it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1202.39it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1199.42it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1211.07it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1208.02it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1219.43it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1216.39it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1227.85it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1225.01it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1236.45it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1233.29it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1244.41it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1241.50it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1252.50it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1249.33it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1260.25it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1257.09it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1267.93it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1264.78it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1275.59it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1272.41it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1282.57it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1279.62it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1290.36it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1287.20it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1297.54it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1294.58it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1304.86it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1301.78it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1312.27it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1309.15it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1319.44it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1316.51it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1326.78it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1323.64it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1333.84it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1330.98it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1341.17it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1338.31it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1348.41it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1345.56it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1355.56it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1352.66it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1362.65it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1359.76it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1369.67it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1366.89it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1376.66it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1373.73it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1383.39it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1380.47it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1390.24it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1387.38it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1397.15it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1394.29it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1403.98it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1401.09it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1410.61it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1407.73it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1417.23it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1414.36it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1423.48it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1420.28it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1429.23it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1426.00it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1435.07it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1431.87it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1440.66it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1437.44it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1446.49it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1443.53it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1452.37it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1449.20it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1458.15it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1455.10it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1463.54it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1460.38it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1469.22it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1466.39it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1475.35it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1472.53it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1481.47it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1478.33it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1487.00it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1484.14it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1492.85it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1489.61it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1498.16it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1495.34it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1503.88it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1501.05it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1509.55it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1506.71it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1514.63it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1511.68it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1520.08it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1517.25it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1525.58it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1522.72it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1531.07it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1528.24it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1536.27it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1533.31it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1541.54it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1538.45it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1546.55it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1543.46it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1551.48it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1548.65it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1556.48it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1553.50it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1561.48it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1558.38it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1566.19it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1563.12it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1570.80it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1567.69it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1575.47it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1572.58it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1580.44it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1577.65it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1585.35it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1582.15it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1589.84it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1587.08it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1594.85it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1592.09it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1599.79it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1596.74it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1603.86it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1601.03it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1608.62it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1605.69it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1613.15it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1610.34it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1617.47it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1614.19it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1621.60it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1618.86it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1626.52it/s, Materializing param=transformer.wpe.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1623.77it/s, Materializing param=transformer.wpe.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1631.21it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1628.22it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1624.21it/s, Materializing param=transformer.wte.weight]
GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
Running 5 training steps with memory capture...
  Step 1/5, Loss: 12.3107
  Step 2/5, Loss: 12.1208
  Step 3/5, Loss: 11.8964
  Step 4/5, Loss: 11.8080
  Step 5/5, Loss: 11.6887
✓ Memory snapshot saved to training_snapshot.pickle
✓ PyTorch reported peak memory: 5.126 GB
</pre></div>
</div>
</section>
<section id="mosaic-memory-analysis-via-python-api">
<h2>Mosaic Memory Analysis via Python API<a class="headerlink" href="#mosaic-memory-analysis-via-python-api" title="Link to this heading">#</a></h2>
<p>Instead of using CLI commands, we can use Mosaic’s Python API directly
for programmatic integration.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC MEMORY ANALYSIS (via Python API)"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Load and analyze the memory snapshot</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">pipeline_snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>

    <span class="c1"># Analyze peak memory usage</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="c1"># Get results</span>
    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak dynamic memory: </span><span class="si">{</span><span class="n">dynamic_peak</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Static memory: </span><span class="si">{</span><span class="n">static_memory</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Overall peak memory: </span><span class="si">{</span><span class="n">overall_peak</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"✓ Analysis complete using Mosaic Python API"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
MOSAIC MEMORY ANALYSIS (via Python API)
============================================================
Peak dynamic memory: 4.620 GiB
Static memory: 0.495 GiB
Overall peak memory: 5.115 GiB
✓ Analysis complete using Mosaic Python API
</pre></div>
</div>
</section>
<section id="reusable-memory-analysis-function">
<h2>Reusable Memory Analysis Function<a class="headerlink" href="#reusable-memory-analysis-function" title="Link to this heading">#</a></h2>
<p>Create a reusable function for analyzing training memory snapshots.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">analyze_training_memory</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Analyze a memory snapshot using Mosaic's Python API.</span>

<span class="sd">    Returns a structured dictionary with memory breakdown.</span>

<span class="sd">    Args:</span>
<span class="sd">        snapshot_path: Path to the memory snapshot pickle file.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary containing memory analysis results.</span>
<span class="sd">    """</span>
    <span class="c1"># Load snapshot</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>

    <span class="c1"># Analyze peak memory</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="c1"># Extract results</span>
    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"snapshot_path"</span><span class="p">:</span> <span class="n">snapshot_path</span><span class="p">,</span>
        <span class="s2">"dynamic_peak_memory_bytes"</span><span class="p">:</span> <span class="n">dynamic_peak</span><span class="p">,</span>
        <span class="s2">"static_memory_bytes"</span><span class="p">:</span> <span class="n">static_memory</span><span class="p">,</span>
        <span class="s2">"overall_peak_memory_bytes"</span><span class="p">:</span> <span class="n">overall_peak</span><span class="p">,</span>
        <span class="s2">"dynamic_peak_memory_gib"</span><span class="p">:</span> <span class="n">dynamic_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="s2">"static_memory_gib"</span><span class="p">:</span> <span class="n">static_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="s2">"overall_peak_memory_gib"</span><span class="p">:</span> <span class="n">overall_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
    <span class="p">}</span>


<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">analysis</span> <span class="o">=</span> <span class="n">analyze_training_memory</span><span class="p">(</span><span class="n">pipeline_snapshot_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Memory Analysis Result:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">analysis</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Memory Analysis Result:
  snapshot_path: training_snapshot.pickle
  dynamic_peak_memory_bytes: 4960794632
  static_memory_bytes: 531589120
  overall_peak_memory_bytes: 5492383752
  dynamic_peak_memory_gib: 4.620100028812885
  static_memory_gib: 0.49508094787597656
  overall_peak_memory_gib: 5.115180976688862
</pre></div>
</div>
</section>
<section id="complete-training-pipeline-with-memory-monitoring">
<h2>Complete Training Pipeline with Memory Monitoring<a class="headerlink" href="#complete-training-pipeline-with-memory-monitoring" title="Link to this heading">#</a></h2>
<p>This demonstrates a production-ready training pipeline with integrated
Mosaic memory monitoring that can be used in CI/CD, monitoring dashboards,
or capacity planning.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_pipeline_with_memory_monitoring</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"pipeline_snapshot.pickle"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Complete training pipeline with integrated Mosaic memory monitoring.</span>

<span class="sd">    Can be integrated into CI/CD, monitoring dashboards, or capacity planning.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_name: HuggingFace model name to use.</span>
<span class="sd">        batch_size: Training batch size.</span>
<span class="sd">        seq_length: Sequence length for input tokens.</span>
<span class="sd">        num_steps: Number of training steps.</span>
<span class="sd">        snapshot_path: Path to save the memory snapshot.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary containing training and memory analysis report.</span>
<span class="sd">    """</span>
    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

    <span class="c1"># Setup</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loading model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GPT2LMHeadModel</span></a><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="c1"># Training with memory capture</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps..."</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">pytorch_peak_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

    <span class="c1"># Mosaic analysis using Python API</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Analyzing memory with Mosaic..."</span><span class="p">)</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="n">report</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"model"</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
        <span class="s2">"config"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">"seq_length"</span><span class="p">:</span> <span class="n">seq_length</span><span class="p">,</span>
            <span class="s2">"num_steps"</span><span class="p">:</span> <span class="n">num_steps</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">"pytorch_peak_memory_gb"</span><span class="p">:</span> <span class="n">pytorch_peak_gb</span><span class="p">,</span>
        <span class="s2">"mosaic_analysis"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"dynamic_peak_gib"</span><span class="p">:</span> <span class="n">dynamic_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
            <span class="s2">"static_memory_gib"</span><span class="p">:</span> <span class="n">static_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
            <span class="s2">"overall_peak_gib"</span><span class="p">:</span> <span class="n">overall_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">"snapshot_path"</span><span class="p">:</span> <span class="n">snapshot_path</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">report</span>


<span class="c1"># Run the pipeline</span>
<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">training_pipeline_with_memory_monitoring</span><span class="p">(</span>
        <span class="s2">"gpt2"</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"PIPELINE REPORT"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'model'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Config: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'config'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"PyTorch Peak Memory: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'pytorch_peak_memory_gb'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mosaic Dynamic Peak: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'mosaic_analysis'</span><span class="p">][</span><span class="s1">'dynamic_peak_gib'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mosaic Overall Peak: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'mosaic_analysis'</span><span class="p">][</span><span class="s1">'overall_peak_gib'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Loading model: gpt2

Loading weights:   0%|          | 0/148 [00:00&lt;?, ?it/s]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 57456.22it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|          | 1/148 [00:00&lt;00:00, 5084.00it/s, Materializing param=transformer.h.0.attn.c_attn.bias]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 4398.85it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   1%|▏         | 2/148 [00:00&lt;00:00, 3019.66it/s, Materializing param=transformer.h.0.attn.c_attn.weight]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 3106.12it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   2%|▏         | 3/148 [00:00&lt;00:00, 2639.59it/s, Materializing param=transformer.h.0.attn.c_proj.bias]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1711.44it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 4/148 [00:00&lt;00:00, 1557.19it/s, Materializing param=transformer.h.0.attn.c_proj.weight]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1711.96it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   3%|▎         | 5/148 [00:00&lt;00:00, 1617.17it/s, Materializing param=transformer.h.0.ln_1.bias]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1578.19it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   4%|▍         | 6/148 [00:00&lt;00:00, 1510.46it/s, Materializing param=transformer.h.0.ln_1.weight]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 1573.09it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▍         | 7/148 [00:00&lt;00:00, 1504.41it/s, Materializing param=transformer.h.0.ln_2.bias]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 1230.68it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   5%|▌         | 8/148 [00:00&lt;00:00, 1148.34it/s, Materializing param=transformer.h.0.ln_2.weight]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 1171.34it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   6%|▌         | 9/148 [00:00&lt;00:00, 1112.74it/s, Materializing param=transformer.h.0.mlp.c_fc.bias]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 996.44it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 10/148 [00:00&lt;00:00, 976.99it/s, Materializing param=transformer.h.0.mlp.c_fc.weight]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 920.72it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   7%|▋         | 11/148 [00:00&lt;00:00, 903.71it/s, Materializing param=transformer.h.0.mlp.c_proj.bias]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 885.76it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   8%|▊         | 12/148 [00:00&lt;00:00, 853.19it/s, Materializing param=transformer.h.0.mlp.c_proj.weight]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 883.60it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 13/148 [00:00&lt;00:00, 853.30it/s, Materializing param=transformer.h.1.attn.c_attn.bias]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 844.85it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:   9%|▉         | 14/148 [00:00&lt;00:00, 835.00it/s, Materializing param=transformer.h.1.attn.c_attn.weight]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 873.10it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  10%|█         | 15/148 [00:00&lt;00:00, 832.61it/s, Materializing param=transformer.h.1.attn.c_proj.bias]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 856.19it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█         | 16/148 [00:00&lt;00:00, 824.17it/s, Materializing param=transformer.h.1.attn.c_proj.weight]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 814.65it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  11%|█▏        | 17/148 [00:00&lt;00:00, 796.56it/s, Materializing param=transformer.h.1.ln_1.bias]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 831.09it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  12%|█▏        | 18/148 [00:00&lt;00:00, 816.26it/s, Materializing param=transformer.h.1.ln_1.weight]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 832.72it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  13%|█▎        | 19/148 [00:00&lt;00:00, 820.09it/s, Materializing param=transformer.h.1.ln_2.bias]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 805.80it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▎        | 20/148 [00:00&lt;00:00, 799.26it/s, Materializing param=transformer.h.1.ln_2.weight]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 821.61it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  14%|█▍        | 21/148 [00:00&lt;00:00, 815.68it/s, Materializing param=transformer.h.1.mlp.c_fc.bias]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 829.88it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  15%|█▍        | 22/148 [00:00&lt;00:00, 809.65it/s, Materializing param=transformer.h.1.mlp.c_fc.weight]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 832.53it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 23/148 [00:00&lt;00:00, 794.87it/s, Materializing param=transformer.h.1.mlp.c_proj.bias]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 817.61it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  16%|█▌        | 24/148 [00:00&lt;00:00, 812.32it/s, Materializing param=transformer.h.1.mlp.c_proj.weight]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 829.31it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  17%|█▋        | 25/148 [00:00&lt;00:00, 787.07it/s, Materializing param=transformer.h.2.attn.c_attn.bias]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 803.65it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 26/148 [00:00&lt;00:00, 798.51it/s, Materializing param=transformer.h.2.attn.c_attn.weight]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 767.99it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  18%|█▊        | 27/148 [00:00&lt;00:00, 763.70it/s, Materializing param=transformer.h.2.attn.c_proj.bias]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 770.17it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  19%|█▉        | 28/148 [00:00&lt;00:00, 750.59it/s, Materializing param=transformer.h.2.attn.c_proj.weight]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 770.37it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|█▉        | 29/148 [00:00&lt;00:00, 759.82it/s, Materializing param=transformer.h.2.ln_1.bias]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 774.50it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  20%|██        | 30/148 [00:00&lt;00:00, 770.72it/s, Materializing param=transformer.h.2.ln_1.weight]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 777.56it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  21%|██        | 31/148 [00:00&lt;00:00, 760.02it/s, Materializing param=transformer.h.2.ln_2.bias]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 778.14it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 32/148 [00:00&lt;00:00, 769.61it/s, Materializing param=transformer.h.2.ln_2.weight]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 776.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  22%|██▏       | 33/148 [00:00&lt;00:00, 765.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 769.48it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  23%|██▎       | 34/148 [00:00&lt;00:00, 748.06it/s, Materializing param=transformer.h.2.mlp.c_fc.weight]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 749.06it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▎       | 35/148 [00:00&lt;00:00, 734.85it/s, Materializing param=transformer.h.2.mlp.c_proj.bias]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 747.38it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  24%|██▍       | 36/148 [00:00&lt;00:00, 737.93it/s, Materializing param=transformer.h.2.mlp.c_proj.weight]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 753.71it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  25%|██▌       | 37/148 [00:00&lt;00:00, 750.66it/s, Materializing param=transformer.h.3.attn.c_attn.bias]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 766.33it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▌       | 38/148 [00:00&lt;00:00, 763.25it/s, Materializing param=transformer.h.3.attn.c_attn.weight]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 778.75it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  26%|██▋       | 39/148 [00:00&lt;00:00, 775.65it/s, Materializing param=transformer.h.3.attn.c_proj.bias]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 791.05it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  27%|██▋       | 40/148 [00:00&lt;00:00, 787.98it/s, Materializing param=transformer.h.3.attn.c_proj.weight]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 803.19it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 41/148 [00:00&lt;00:00, 800.09it/s, Materializing param=transformer.h.3.ln_1.bias]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 814.81it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  28%|██▊       | 42/148 [00:00&lt;00:00, 811.73it/s, Materializing param=transformer.h.3.ln_1.weight]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 826.58it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  29%|██▉       | 43/148 [00:00&lt;00:00, 823.52it/s, Materializing param=transformer.h.3.ln_2.bias]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 838.19it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|██▉       | 44/148 [00:00&lt;00:00, 834.84it/s, Materializing param=transformer.h.3.ln_2.weight]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 849.16it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  30%|███       | 45/148 [00:00&lt;00:00, 846.04it/s, Materializing param=transformer.h.3.mlp.c_fc.bias]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 859.57it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  31%|███       | 46/148 [00:00&lt;00:00, 856.93it/s, Materializing param=transformer.h.3.mlp.c_fc.weight]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 872.18it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 47/148 [00:00&lt;00:00, 869.71it/s, Materializing param=transformer.h.3.mlp.c_proj.bias]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 884.88it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  32%|███▏      | 48/148 [00:00&lt;00:00, 882.32it/s, Materializing param=transformer.h.3.mlp.c_proj.weight]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 897.39it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  33%|███▎      | 49/148 [00:00&lt;00:00, 894.91it/s, Materializing param=transformer.h.4.attn.c_attn.bias]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 909.82it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 50/148 [00:00&lt;00:00, 907.30it/s, Materializing param=transformer.h.4.attn.c_attn.weight]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 922.07it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  34%|███▍      | 51/148 [00:00&lt;00:00, 919.51it/s, Materializing param=transformer.h.4.attn.c_proj.bias]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 934.11it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  35%|███▌      | 52/148 [00:00&lt;00:00, 931.57it/s, Materializing param=transformer.h.4.attn.c_proj.weight]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 946.11it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▌      | 53/148 [00:00&lt;00:00, 943.50it/s, Materializing param=transformer.h.4.ln_1.bias]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 957.86it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  36%|███▋      | 54/148 [00:00&lt;00:00, 955.10it/s, Materializing param=transformer.h.4.ln_1.weight]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 969.34it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  37%|███▋      | 55/148 [00:00&lt;00:00, 966.74it/s, Materializing param=transformer.h.4.ln_2.bias]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 980.88it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  38%|███▊      | 56/148 [00:00&lt;00:00, 978.32it/s, Materializing param=transformer.h.4.ln_2.weight]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 991.89it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▊      | 57/148 [00:00&lt;00:00, 989.23it/s, Materializing param=transformer.h.4.mlp.c_fc.bias]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1003.07it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  39%|███▉      | 58/148 [00:00&lt;00:00, 1000.44it/s, Materializing param=transformer.h.4.mlp.c_fc.weight]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1014.02it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  40%|███▉      | 59/148 [00:00&lt;00:00, 1010.89it/s, Materializing param=transformer.h.4.mlp.c_proj.bias]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1024.22it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 60/148 [00:00&lt;00:00, 1021.54it/s, Materializing param=transformer.h.4.mlp.c_proj.weight]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1034.99it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  41%|████      | 61/148 [00:00&lt;00:00, 1032.30it/s, Materializing param=transformer.h.5.attn.c_attn.bias]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1045.31it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  42%|████▏     | 62/148 [00:00&lt;00:00, 1042.54it/s, Materializing param=transformer.h.5.attn.c_attn.weight]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1055.72it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 63/148 [00:00&lt;00:00, 1053.02it/s, Materializing param=transformer.h.5.attn.c_proj.bias]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1066.13it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  43%|████▎     | 64/148 [00:00&lt;00:00, 1063.45it/s, Materializing param=transformer.h.5.attn.c_proj.weight]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1076.37it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  44%|████▍     | 65/148 [00:00&lt;00:00, 1073.47it/s, Materializing param=transformer.h.5.ln_1.bias]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1086.49it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▍     | 66/148 [00:00&lt;00:00, 1083.78it/s, Materializing param=transformer.h.5.ln_1.weight]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1096.58it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  45%|████▌     | 67/148 [00:00&lt;00:00, 1093.86it/s, Materializing param=transformer.h.5.ln_2.bias]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1106.10it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  46%|████▌     | 68/148 [00:00&lt;00:00, 1103.38it/s, Materializing param=transformer.h.5.ln_2.weight]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1116.03it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 69/148 [00:00&lt;00:00, 1113.30it/s, Materializing param=transformer.h.5.mlp.c_fc.bias]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1125.79it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  47%|████▋     | 70/148 [00:00&lt;00:00, 1123.05it/s, Materializing param=transformer.h.5.mlp.c_fc.weight]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1135.33it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  48%|████▊     | 71/148 [00:00&lt;00:00, 1132.57it/s, Materializing param=transformer.h.5.mlp.c_proj.bias]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1144.82it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▊     | 72/148 [00:00&lt;00:00, 1142.03it/s, Materializing param=transformer.h.5.mlp.c_proj.weight]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1154.15it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  49%|████▉     | 73/148 [00:00&lt;00:00, 1151.36it/s, Materializing param=transformer.h.6.attn.c_attn.bias]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1163.34it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  50%|█████     | 74/148 [00:00&lt;00:00, 1160.28it/s, Materializing param=transformer.h.6.attn.c_attn.weight]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1172.13it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████     | 75/148 [00:00&lt;00:00, 1169.34it/s, Materializing param=transformer.h.6.attn.c_proj.bias]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1181.31it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  51%|█████▏    | 76/148 [00:00&lt;00:00, 1178.41it/s, Materializing param=transformer.h.6.attn.c_proj.weight]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1189.92it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  52%|█████▏    | 77/148 [00:00&lt;00:00, 1186.84it/s, Materializing param=transformer.h.6.ln_1.bias]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1198.42it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 78/148 [00:00&lt;00:00, 1195.65it/s, Materializing param=transformer.h.6.ln_1.weight]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1207.41it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  53%|█████▎    | 79/148 [00:00&lt;00:00, 1204.34it/s, Materializing param=transformer.h.6.ln_2.bias]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1215.79it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  54%|█████▍    | 80/148 [00:00&lt;00:00, 1212.78it/s, Materializing param=transformer.h.6.ln_2.weight]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1223.86it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▍    | 81/148 [00:00&lt;00:00, 1220.69it/s, Materializing param=transformer.h.6.mlp.c_fc.bias]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1231.66it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  55%|█████▌    | 82/148 [00:00&lt;00:00, 1228.71it/s, Materializing param=transformer.h.6.mlp.c_fc.weight]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1239.93it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  56%|█████▌    | 83/148 [00:00&lt;00:00, 1237.13it/s, Materializing param=transformer.h.6.mlp.c_proj.bias]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1247.78it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 84/148 [00:00&lt;00:00, 1244.57it/s, Materializing param=transformer.h.6.mlp.c_proj.weight]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1255.48it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  57%|█████▋    | 85/148 [00:00&lt;00:00, 1252.62it/s, Materializing param=transformer.h.7.attn.c_attn.bias]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1263.27it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  58%|█████▊    | 86/148 [00:00&lt;00:00, 1260.31it/s, Materializing param=transformer.h.7.attn.c_attn.weight]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1271.10it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 87/148 [00:00&lt;00:00, 1267.76it/s, Materializing param=transformer.h.7.attn.c_proj.bias]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1278.35it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  59%|█████▉    | 88/148 [00:00&lt;00:00, 1275.20it/s, Materializing param=transformer.h.7.attn.c_proj.weight]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1285.48it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  60%|██████    | 89/148 [00:00&lt;00:00, 1282.30it/s, Materializing param=transformer.h.7.ln_1.bias]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1292.88it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████    | 90/148 [00:00&lt;00:00, 1290.04it/s, Materializing param=transformer.h.7.ln_1.weight]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1300.63it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  61%|██████▏   | 91/148 [00:00&lt;00:00, 1297.50it/s, Materializing param=transformer.h.7.ln_2.bias]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1307.83it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  62%|██████▏   | 92/148 [00:00&lt;00:00, 1304.99it/s, Materializing param=transformer.h.7.ln_2.weight]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1315.38it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  63%|██████▎   | 93/148 [00:00&lt;00:00, 1312.54it/s, Materializing param=transformer.h.7.mlp.c_fc.bias]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1322.94it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▎   | 94/148 [00:00&lt;00:00, 1320.10it/s, Materializing param=transformer.h.7.mlp.c_fc.weight]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1329.83it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  64%|██████▍   | 95/148 [00:00&lt;00:00, 1326.91it/s, Materializing param=transformer.h.7.mlp.c_proj.bias]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1336.69it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  65%|██████▍   | 96/148 [00:00&lt;00:00, 1333.84it/s, Materializing param=transformer.h.7.mlp.c_proj.weight]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1343.88it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 97/148 [00:00&lt;00:00, 1340.63it/s, Materializing param=transformer.h.8.attn.c_attn.bias]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1350.38it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  66%|██████▌   | 98/148 [00:00&lt;00:00, 1347.48it/s, Materializing param=transformer.h.8.attn.c_attn.weight]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1357.36it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  67%|██████▋   | 99/148 [00:00&lt;00:00, 1354.51it/s, Materializing param=transformer.h.8.attn.c_proj.bias]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1364.34it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 100/148 [00:00&lt;00:00, 1361.16it/s, Materializing param=transformer.h.8.attn.c_proj.weight]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1370.72it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  68%|██████▊   | 101/148 [00:00&lt;00:00, 1367.59it/s, Materializing param=transformer.h.8.ln_1.bias]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1377.30it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  69%|██████▉   | 102/148 [00:00&lt;00:00, 1374.45it/s, Materializing param=transformer.h.8.ln_1.weight]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1384.21it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|██████▉   | 103/148 [00:00&lt;00:00, 1381.13it/s, Materializing param=transformer.h.8.ln_2.bias]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1390.71it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  70%|███████   | 104/148 [00:00&lt;00:00, 1387.67it/s, Materializing param=transformer.h.8.ln_2.weight]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1397.21it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  71%|███████   | 105/148 [00:00&lt;00:00, 1394.37it/s, Materializing param=transformer.h.8.mlp.c_fc.bias]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1403.78it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 106/148 [00:00&lt;00:00, 1400.94it/s, Materializing param=transformer.h.8.mlp.c_fc.weight]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1410.38it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  72%|███████▏  | 107/148 [00:00&lt;00:00, 1407.57it/s, Materializing param=transformer.h.8.mlp.c_proj.bias]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1416.95it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  73%|███████▎  | 108/148 [00:00&lt;00:00, 1413.97it/s, Materializing param=transformer.h.8.mlp.c_proj.weight]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1422.92it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▎  | 109/148 [00:00&lt;00:00, 1420.04it/s, Materializing param=transformer.h.9.attn.c_attn.bias]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1429.29it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  74%|███████▍  | 110/148 [00:00&lt;00:00, 1426.48it/s, Materializing param=transformer.h.9.attn.c_attn.weight]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1435.69it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  75%|███████▌  | 111/148 [00:00&lt;00:00, 1432.90it/s, Materializing param=transformer.h.9.attn.c_proj.bias]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1442.05it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▌  | 112/148 [00:00&lt;00:00, 1439.23it/s, Materializing param=transformer.h.9.attn.c_proj.weight]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1448.30it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  76%|███████▋  | 113/148 [00:00&lt;00:00, 1445.46it/s, Materializing param=transformer.h.9.ln_1.bias]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1454.54it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  77%|███████▋  | 114/148 [00:00&lt;00:00, 1451.74it/s, Materializing param=transformer.h.9.ln_1.weight]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1460.74it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 115/148 [00:00&lt;00:00, 1457.94it/s, Materializing param=transformer.h.9.ln_2.bias]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1466.87it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  78%|███████▊  | 116/148 [00:00&lt;00:00, 1463.84it/s, Materializing param=transformer.h.9.ln_2.weight]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1472.54it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  79%|███████▉  | 117/148 [00:00&lt;00:00, 1469.72it/s, Materializing param=transformer.h.9.mlp.c_fc.bias]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1478.56it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|███████▉  | 118/148 [00:00&lt;00:00, 1475.50it/s, Materializing param=transformer.h.9.mlp.c_fc.weight]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1484.14it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  80%|████████  | 119/148 [00:00&lt;00:00, 1481.28it/s, Materializing param=transformer.h.9.mlp.c_proj.bias]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1489.78it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  81%|████████  | 120/148 [00:00&lt;00:00, 1486.92it/s, Materializing param=transformer.h.9.mlp.c_proj.weight]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1495.54it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 121/148 [00:00&lt;00:00, 1492.83it/s, Materializing param=transformer.h.10.attn.c_attn.bias]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1501.41it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  82%|████████▏ | 122/148 [00:00&lt;00:00, 1497.93it/s, Materializing param=transformer.h.10.attn.c_attn.weight]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1506.34it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  83%|████████▎ | 123/148 [00:00&lt;00:00, 1503.50it/s, Materializing param=transformer.h.10.attn.c_proj.bias]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1511.96it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 124/148 [00:00&lt;00:00, 1509.14it/s, Materializing param=transformer.h.10.attn.c_proj.weight]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1517.55it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  84%|████████▍ | 125/148 [00:00&lt;00:00, 1514.65it/s, Materializing param=transformer.h.10.ln_1.bias]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1523.11it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  85%|████████▌ | 126/148 [00:00&lt;00:00, 1520.33it/s, Materializing param=transformer.h.10.ln_1.weight]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1528.73it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▌ | 127/148 [00:00&lt;00:00, 1525.94it/s, Materializing param=transformer.h.10.ln_2.bias]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1534.31it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  86%|████████▋ | 128/148 [00:00&lt;00:00, 1531.55it/s, Materializing param=transformer.h.10.ln_2.weight]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1539.82it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  87%|████████▋ | 129/148 [00:00&lt;00:00, 1537.04it/s, Materializing param=transformer.h.10.mlp.c_fc.bias]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1545.23it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  88%|████████▊ | 130/148 [00:00&lt;00:00, 1542.41it/s, Materializing param=transformer.h.10.mlp.c_fc.weight]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1550.54it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▊ | 131/148 [00:00&lt;00:00, 1547.50it/s, Materializing param=transformer.h.10.mlp.c_proj.bias]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1555.55it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  89%|████████▉ | 132/148 [00:00&lt;00:00, 1552.72it/s, Materializing param=transformer.h.10.mlp.c_proj.weight]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1560.66it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  90%|████████▉ | 133/148 [00:00&lt;00:00, 1557.84it/s, Materializing param=transformer.h.11.attn.c_attn.bias]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1565.91it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 134/148 [00:00&lt;00:00, 1563.10it/s, Materializing param=transformer.h.11.attn.c_attn.weight]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1571.01it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  91%|█████████ | 135/148 [00:00&lt;00:00, 1568.19it/s, Materializing param=transformer.h.11.attn.c_proj.bias]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1576.06it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  92%|█████████▏| 136/148 [00:00&lt;00:00, 1573.24it/s, Materializing param=transformer.h.11.attn.c_proj.weight]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1581.07it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 137/148 [00:00&lt;00:00, 1578.31it/s, Materializing param=transformer.h.11.ln_1.bias]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1586.19it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  93%|█████████▎| 138/148 [00:00&lt;00:00, 1583.43it/s, Materializing param=transformer.h.11.ln_1.weight]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1591.28it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  94%|█████████▍| 139/148 [00:00&lt;00:00, 1588.49it/s, Materializing param=transformer.h.11.ln_2.bias]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1596.28it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▍| 140/148 [00:00&lt;00:00, 1593.50it/s, Materializing param=transformer.h.11.ln_2.weight]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1601.24it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  95%|█████████▌| 141/148 [00:00&lt;00:00, 1598.50it/s, Materializing param=transformer.h.11.mlp.c_fc.bias]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1606.17it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  96%|█████████▌| 142/148 [00:00&lt;00:00, 1603.16it/s, Materializing param=transformer.h.11.mlp.c_fc.weight]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1610.72it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 143/148 [00:00&lt;00:00, 1607.94it/s, Materializing param=transformer.h.11.mlp.c_proj.bias]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1615.48it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  97%|█████████▋| 144/148 [00:00&lt;00:00, 1612.69it/s, Materializing param=transformer.h.11.mlp.c_proj.weight]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1620.06it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  98%|█████████▊| 145/148 [00:00&lt;00:00, 1617.31it/s, Materializing param=transformer.ln_f.bias]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1624.90it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▊| 146/148 [00:00&lt;00:00, 1622.17it/s, Materializing param=transformer.ln_f.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1629.75it/s, Materializing param=transformer.wpe.weight]
Loading weights:  99%|█████████▉| 147/148 [00:00&lt;00:00, 1627.01it/s, Materializing param=transformer.wpe.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1634.50it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1631.74it/s, Materializing param=transformer.wte.weight]
Loading weights: 100%|██████████| 148/148 [00:00&lt;00:00, 1627.90it/s, Materializing param=transformer.wte.weight]
GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
WARNING:transformers.modeling_utils:GPT2LMHeadModel LOAD REPORT from: gpt2
Key                  | Status     |  |
---------------------+------------+--+-
h.{0...11}.attn.bias | UNEXPECTED |  |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
Running 5 training steps...
  Step 1/5, Loss: 12.2789
  Step 2/5, Loss: 12.0998
  Step 3/5, Loss: 11.9411
  Step 4/5, Loss: 11.8859
  Step 5/5, Loss: 11.7158
✓ Memory snapshot saved to pipeline_snapshot.pickle
Analyzing memory with Mosaic...

============================================================
PIPELINE REPORT
============================================================
Model: gpt2
Config: {'batch_size': 4, 'seq_length': 512, 'num_steps': 5}
PyTorch Peak Memory: 5.126 GB
Mosaic Dynamic Peak: 4.620 GiB
Mosaic Overall Peak: 5.115 GiB
</pre></div>
</div>
</section>
<section id="ci-cd-and-dashboard-integration-patterns">
<h2>CI/CD and Dashboard Integration Patterns<a class="headerlink" href="#ci-cd-and-dashboard-integration-patterns" title="Link to this heading">#</a></h2>
<p>These patterns show how to integrate Mosaic analysis into automated
workflows.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</pre></div>
</div>
<section id="pattern-1-ci-cd-memory-regression-testing">
<h3>Pattern 1: CI/CD Memory Regression Testing<a class="headerlink" href="#pattern-1-ci-cd-memory-regression-testing" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">check_memory_regression</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">threshold_gib</span><span class="o">=</span><span class="mf">5.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Check if memory usage exceeds threshold for CI/CD pipelines.</span>

<span class="sd">    Args:</span>
<span class="sd">        report: Memory analysis report from training_pipeline_with_memory_monitoring.</span>
<span class="sd">        threshold_gib: Maximum allowed memory in GiB.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If memory exceeds threshold.</span>
<span class="sd">    """</span>
    <span class="n">peak</span> <span class="o">=</span> <span class="n">report</span><span class="p">[</span><span class="s2">"mosaic_analysis"</span><span class="p">][</span><span class="s2">"overall_peak_gib"</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">peak</span> <span class="o">&lt;</span> <span class="n">threshold_gib</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"Memory regression! </span><span class="si">{</span><span class="n">peak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GiB &gt; </span><span class="si">{</span><span class="n">threshold_gib</span><span class="si">}</span><span class="s2"> GiB"</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Memory check passed: </span><span class="si">{</span><span class="n">peak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GiB &lt; </span><span class="si">{</span><span class="n">threshold_gib</span><span class="si">}</span><span class="s2"> GiB threshold"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pattern-2-export-to-json-for-dashboards">
<h3>Pattern 2: Export to JSON for Dashboards<a class="headerlink" href="#pattern-2-export-to-json-for-dashboards" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">check_memory_regression</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">threshold_gib</span><span class="o">=</span><span class="mf">8.0</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"memory_report.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Memory report exported to memory_report.json"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Memory check passed: 5.12 GiB &lt; 8.0 GiB threshold
Memory report exported to memory_report.json
</pre></div>
</div>
</section>
</section>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>This tutorial demonstrated three key use cases for Mosaic memory profiling:</p>
<p><strong>Case 1: Activation Checkpointing Analysis</strong></p>
<ul class="simple">
<li><p>Used Mosaic to compare memory usage between baseline and optimized models</p></li>
<li><p>Identified that activation checkpointing reduced activation memory by 71%</p></li>
<li><p>Mosaic’s categorical profiling made it trivial to pinpoint memory savings</p></li>
</ul>
<p><strong>Case 2: Debugging Unexpected Memory Usage</strong></p>
<ul class="simple">
<li><p>Created a “buggy” model with abandoned debug code</p></li>
<li><p>Used <code class="docutils literal notranslate"><span class="pre">mosaic_get_memory_usage_peak</span></code> to identify extra allocations</p></li>
<li><p>Stack traces revealed optimizer state tracking extra parameters</p></li>
</ul>
<p><strong>Case 3: Pipeline Integration</strong></p>
<ul class="simple">
<li><p>Demonstrated programmatic usage via Mosaic’s Python API</p></li>
<li><p>Showed integration patterns for CI/CD and dashboards with structured reports</p></li>
</ul>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/facebookresearch/mosaic">Mosaic GitHub Repository</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management">PyTorch Memory Management Documentation</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/torch_cuda_memory.html">Understanding CUDA Memory Usage</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/checkpoint.html">Activation Checkpointing in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/memory_viz">PyTorch Memory Snapshot Visualizer</a></p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 14.368 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-mosaic-memory-profiling-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6d7a706c6dae6d031b7942e5102d35dc/mosaic_memory_profiling_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">mosaic_memory_profiling_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/46e1279ff67a61c2741a365e01e6cc18/mosaic_memory_profiling_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">mosaic_memory_profiling_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6e3887911e4e0035cb8b0fe92b7fd5ff/mosaic_memory_profiling_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">mosaic_memory_profiling_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/realtime_rpi.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</p>
</div>
</a>
<a class="right-next" href="../recipes_index.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Recipes</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/realtime_rpi.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</p>
</div>
</a>
<a class="right-next" href="../recipes_index.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Recipes</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Mosaic: Memory Profiling for PyTorch</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-mosaic">Introduction to Mosaic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting Started</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-usage-examples">Simple Usage Examples</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies-and-imports">Dependencies and Imports</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-utilities">Shared Utilities</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-1-understanding-memory-differences-with-activation-checkpointing">Case 1: Understanding Memory Differences with Activation Checkpointing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-function-for-activation-checkpointing-comparison">Training Function for Activation Checkpointing Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-baseline-training-without-activation-checkpointing">Run Baseline Training (Without Activation Checkpointing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-modified-training-with-activation-checkpointing">Run Modified Training (With Activation Checkpointing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-categorical-memory-profiles-with-mosaic">Generate Categorical Memory Profiles with Mosaic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-interpretation-activation-checkpointing">Results Interpretation: Activation Checkpointing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-observed">What We Observed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-happen">Why Does This Happen?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-mosaic-helped">How Mosaic Helped</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-2-debugging-unexpected-memory-usage">Case 2: Debugging Unexpected Memory Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-buggy-model">The Buggy Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-functions-for-debug-comparison">Training Functions for Debug Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training-for-baseline-clean-model">Run Training for Baseline (Clean Model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training-with-the-bug">Run Training WITH the Bug</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-mosaic-to-find-the-problem">Use Mosaic to Find the Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-the-mosaic-output">Analyzing The Mosaic Output</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-state-allocations-delta">1. Optimizer State Allocations Delta</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-activation-allocations">2. Additional Activation Allocations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-total-explanation">Memory Total Explanation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-3-integrating-memory-analysis-into-your-training-pipeline">Case 3: Integrating Memory Analysis into Your Training Pipeline</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-automatic-memory-capture">Training with Automatic Memory Capture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mosaic-memory-analysis-via-python-api">Mosaic Memory Analysis via Python API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reusable-memory-analysis-function">Reusable Memory Analysis Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-training-pipeline-with-memory-monitoring">Complete Training Pipeline with Memory Monitoring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ci-cd-and-dashboard-integration-patterns">CI/CD and Dashboard Integration Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-1-ci-cd-memory-regression-testing">Pattern 1: CI/CD Memory Regression Testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-2-export-to-json-for-dashboards">Pattern 2: Export to JSON for Dashboards</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a></li>
</ul>
</li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Mosaic: Memory Profiling for PyTorch",
       "headline": "Mosaic: Memory Profiling for PyTorch",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/mosaic_memory_profiling_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. Mosaic: Memory Profiling for PyTorch# Author: Basil Wong What you will learn How to capture and analyze PyTorch memory snapshots Identify memory savings from activation checkpointing Debug unexpected memory usage from abandoned code Integrate memory analysis into training pipelines Prerequisites PyTorch v2.0.0 or later CUDA-capable GPU Basic understanding of PyTorch training loops This tutorial demonstrates how to use Mosaic, a post-processing memory snapshot analysis tool for PyTorch. Mosaic helps analyze GPU memory usage in distributed deep learning, providing detailed insights into memory allocations, peak usage, and memory imbalances across parallel workers. Mosaic was instrumental in debugging OOM issues during the 405B LLaMA training and is now open source. Introduction to Mosaic# Overview# In distributed deep learning, understanding GPU memory usage is critical for optimizing training efficiency and debugging Out-of-Memory (OOM) errors. Mosaic is a post-analysis tool for memory usage designed to work with large-scale jobs. It helps analyze PyTorch memory snapshots captured during the execution of PyTorch training jobs, providing detailed insights into memory allocations, peak usage, and memory imbalances across parallel workers. Getting Started# Clone the mosaic repository and install from the mosaic directory: git clone https://github.com/facebookresearch/mosaic cd mosaic python3 -m venv venv source venv/bin/activate pip3 install -r requirements.txt pip3 install -e . Alternatively, install directly via pip: pip install git+https://github.com/facebookresearch/mosaic.git Simple Usage Examples# 1. Peak Memory Usage Analysis When addressing memory problems like OOM errors, focusing on peak memory usage is crucial. The mosaic_get_memory_usage_peak command presents a stack trace of the memory allocations that contributed to the peak memory usage: mosaic_get_memory_usage_peak --snapshot \u003cpath to snapshot\u003e 2. Categorical Memory Profiling Mosaic classifies allocations into categories (activation, backward, optimizer, etc.): Activation Memory: Tensors saved for backward pass Gradient Memory: Gradients computed during backpropagation Optimizer State: Adam/SGD momentum and variance buffers Parameter Memory: Model weights mosaic_get_memory_profile --snapshot \u003cpath\u003e --out-path \u003chtml\u003e \\ --profile categories An example HTML output looks like: Categorical memory profiling showing memory breakdown by type (activation, gradient, optimizer, etc.)# To maintain allocation order for the categories, add --preserve-allocation-order: mosaic_get_memory_profile --snapshot \u003cpath\u003e --out-path \u003chtml\u003e \\ --profile categories --preserve-allocation-order Categorical profiling with --preserve-allocation-order shows memory allocations in chronological order# 3. Custom Dictionary Profiling For targeted analysis via regex pattern matching: mosaic_get_memory_profile --snapshot \u003cpath\u003e --profile custom \\ --custom-profile \u0027{\"ncclx\": \"ncclx\"}\u0027 This is invaluable for tracking specific kernels, optimizers, or custom code patterns: Custom profiling with regex patterns to track specific operations like NCCL communications# Dependencies and Imports# Let\u2019s set up the required dependencies and imports for this tutorial. import subprocess import sys import shutil from contextlib import contextmanager import pickle # Fix for sphinx-gallery environment where __main__.__file__ may not exist # This is needed for transformers library compatibility import os if not hasattr(sys.modules[\"__main__\"], \"__file__\"): # Use this file\u0027s path as a fallback, or a dummy path if __file__ is not available try: sys.modules[\"__main__\"].__file__ = os.path.abspath(__file__) except NameError: # __file__ not available, use transformers modeling file as fallback import transformers.modeling_utils sys.modules[\"__main__\"].__file__ = transformers.modeling_utils.__file__ import torch from torch.utils.data import DataLoader, Dataset # Install dependencies if needed try: from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions except ImportError: subprocess.check_call( [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"] ) from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions try: from mosaic.libmosaic.analyzer.memory_abstract import MemoryAbstract except ImportError: subprocess.check_call( [ sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/facebookresearch/mosaic.git\", ] ) from mosaic.libmosaic.analyzer.memory_abstract import MemoryAbstract print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\") PyTorch version: 2.10.0+cu128 CUDA available: True GPU: NVIDIA A10G Shared Utilities# These helper classes and functions are used throughout the tutorial. class RandomTokenDataset(Dataset): \"\"\"Generates random token sequences for training. This dataset creates random input sequences suitable for language model training, simulating real training data without requiring actual text. \"\"\" def __init__(self, vocab_size, seq_length=512, num_samples=100, seed=None): self.vocab_size = vocab_size self.seq_length = seq_length self.num_samples = num_samples self.generator = None if seed is not None: self.generator = torch.Generator().manual_seed(seed) def __len__(self): return self.num_samples def __getitem__(self, idx): # noqa: ARG002 if self.generator is not None: input_ids = torch.randint( 0, self.vocab_size, (self.seq_length,), generator=self.generator ) else: input_ids = torch.randint(0, self.vocab_size, (self.seq_length,)) return {\"input_ids\": input_ids, \"labels\": input_ids.clone()} @contextmanager def capture_memory_snapshot(output_path): \"\"\"Context manager to capture and save PyTorch CUDA memory snapshots. This captures all GPU memory allocations during the context and saves them to a pickle file for later analysis with Mosaic. Args: output_path: Path to save the memory snapshot pickle file. \"\"\" torch.cuda.memory._record_memory_history(max_entries=100000) try: yield finally: snapshot = torch.cuda.memory._snapshot() torch.cuda.memory._record_memory_history(enabled=None) with open(output_path, \"wb\") as f: pickle.dump(snapshot, f) print(f\"\u2713 Memory snapshot saved to {output_path}\") Case 1: Understanding Memory Differences with Activation Checkpointing# This section demonstrates how to use Mosaic to analyze and compare GPU memory usage between different model configurations. What we\u2019ll do: Train GPT-2 and capture a memory snapshot (baseline) Enable activation checkpointing and train again (modified) Use Mosaic to identify exactly where memory savings occur Training Function for Activation Checkpointing Comparison# def run_training_ac( activation_checkpointing: bool, snapshot_path: str, batch_size: int = 4, seq_length: int = 512, num_steps: int = 5, ): \"\"\"Run training loop and capture memory snapshot. Args: activation_checkpointing: Whether to enable gradient checkpointing. snapshot_path: Path to save the memory snapshot. batch_size: Training batch size. seq_length: Sequence length for input tokens. num_steps: Number of training steps to run. Returns: Peak GPU memory usage in GB. \"\"\" # Clear any previous memory torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") # Load model print(f\"Loading GPT-2 (activation_checkpointing={activation_checkpointing})...\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\") if activation_checkpointing: model.gradient_checkpointing_enable() print(\"Activation checkpointing is ENABLED\") else: print(\"Activation checkpointing is DISABLED\") model = model.to(device) model.train() # Create dataset and dataloader tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=seq_length, num_samples=100, ) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Setup optimizer optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) # Training loop with memory capture print(f\"Running {num_steps} training steps...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}/{num_steps}, Loss: {loss.item():.4f}\") peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3) print(f\"\u2713 Peak GPU memory: {peak_memory_gb:.2f} GB\") # Cleanup del model, optimizer torch.cuda.empty_cache() return peak_memory_gb Run Baseline Training (Without Activation Checkpointing)# Note This tutorial requires a CUDA-capable GPU. If you\u2019re running in Google Colab, make sure to select a GPU runtime: Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU if not torch.cuda.is_available(): print(\"=\" * 60) print(\"WARNING: No CUDA GPU detected!\") print(\"=\" * 60) print(\"\\nThis tutorial requires a CUDA-capable GPU for memory profiling.\") print(\"\\nIf you\u0027re running in Google Colab:\") print(\" 1. Go to Runtime \u2192 Change runtime type\") print(\" 2. Set Hardware accelerator to \u0027GPU\u0027\") print(\" 3. Click \u0027Save\u0027 and re-run the notebook\") print(\"\\nSkipping GPU memory profiling examples...\") HAS_CUDA = False else: HAS_CUDA = True # Check if Mosaic CLI is available HAS_MOSAIC_CLI = shutil.which(\"mosaic_get_memory_profile\") is not None if HAS_CUDA and not HAS_MOSAIC_CLI: print(\"Note: Mosaic CLI not found. Install Mosaic to generate HTML profiles.\") print(\" pip install git+https://github.com/facebookresearch/mosaic.git\") if HAS_CUDA: print(\"=\" * 60) print(\"BASELINE: Training WITHOUT Activation Checkpointing\") print(\"=\" * 60) baseline_memory = run_training_ac( activation_checkpointing=False, snapshot_path=\"snapshot_baseline.pickle\", batch_size=4, seq_length=512, num_steps=5, ) ============================================================ BASELINE: Training WITHOUT Activation Checkpointing ============================================================ Loading GPT-2 (activation_checkpointing=False)... Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads. Loading weights: 0%| | 0/148 [00:00\u003c?, ?it/s] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 14169.95it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 759.01it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 1199.40it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 1058.10it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 978.68it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 914.66it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1082.96it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1028.65it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1167.35it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1077.17it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1144.63it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1098.08it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 987.56it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 911.98it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 822.23it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 801.80it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 766.71it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 721.44it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 764.02it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 714.47it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 704.09it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 678.12it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 675.50it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 637.57it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 664.54it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 633.41it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 650.71it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 628.15it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 644.51it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 633.77it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 656.94it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 629.53it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 611.64it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 602.20it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 627.66it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 610.60it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 628.41it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 610.21it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 619.79it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 609.44it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 621.04it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 588.82it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 595.77it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 585.30it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 596.41it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 590.36it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 601.38it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 593.11it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 603.28it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 600.39it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 609.02it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 606.32it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 614.02it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 608.75it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 624.94it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 609.70it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 618.28it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 614.85it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 632.38it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 629.55it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 646.71it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 643.84it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 660.90it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 658.38it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 675.79it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 673.20it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 690.32it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 687.66it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 704.59it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 701.94it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 718.68it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 715.84it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 732.33it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 729.47it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 745.56it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 742.48it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 757.52it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 754.80it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 770.69it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 767.81it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 783.56it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 780.70it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 796.33it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 793.51it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 808.90it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 806.08it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 821.32it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 818.52it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 833.54it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 830.64it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 845.44it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 842.19it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 856.02it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 853.12it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 867.60it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 864.32it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 878.13it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 875.35it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 889.37it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 886.40it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 900.49it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 897.50it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 911.36it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 908.22it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 921.90it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 918.96it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 932.69it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 929.73it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 943.28it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 940.37it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 953.50it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 950.56it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 963.96it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 960.85it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 974.02it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 971.27it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 984.29it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 981.03it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 992.91it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 989.81it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1002.42it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 999.36it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1011.94it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1008.67it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1021.40it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1018.86it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1031.51it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1028.31it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1040.11it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1036.93it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1049.25it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1046.07it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1058.19it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1055.01it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1067.05it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1063.86it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1075.62it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1072.49it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1084.09it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1080.61it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1091.43it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1087.96it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1099.40it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1096.28it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1107.74it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1104.66it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1116.00it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1112.87it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1123.96it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1120.97it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1132.09it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1128.75it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1139.29it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1136.16it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1147.16it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1144.06it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1154.96it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1151.84it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1162.72it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1159.58it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1170.31it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1167.23it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1177.82it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1174.61it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1185.10it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1182.05it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1192.20it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1189.39it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1199.51it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1196.15it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1206.10it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1202.20it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1212.15it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1209.10it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1219.14it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1216.09it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1226.09it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1222.98it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1232.94it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1229.49it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1238.63it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1235.14it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1244.13it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1240.66it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1249.47it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1245.96it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1254.58it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1251.51it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1260.98it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1257.85it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1267.30it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1264.19it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1273.54it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1270.37it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1279.64it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1276.55it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1285.78it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1282.67it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1291.74it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1288.59it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1297.59it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1294.82it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1303.71it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1300.40it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1308.92it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1305.87it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1314.84it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1311.45it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1319.88it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1316.90it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1325.56it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1322.53it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1331.21it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1328.23it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1336.72it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1333.71it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1342.26it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1339.32it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1347.25it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1344.14it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1352.53it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1349.55it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1357.14it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1353.33it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1360.75it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1357.43it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1365.81it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1362.77it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1371.08it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1368.10it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1376.38it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1373.07it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1380.53it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1377.17it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1384.39it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1381.05it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1388.37it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1385.48it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1393.30it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1390.46it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1398.42it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1395.16it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1403.02it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1399.99it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1407.67it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1404.77it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1412.66it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1409.64it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1417.44it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1414.46it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1422.21it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1419.22it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1426.99it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1424.02it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1431.68it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1428.72it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1436.31it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1433.38it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1440.92it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1437.86it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1445.28it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1442.21it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1449.58it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1446.60it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1453.52it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1450.49it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1457.74it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1454.72it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1461.92it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1458.59it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1465.06it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1462.02it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1469.21it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1466.19it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1473.36it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1470.34it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1477.46it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1474.35it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1481.01it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1477.90it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1484.96it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1481.72it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1487.85it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1484.47it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1490.58it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1487.33it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1493.55it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1490.61it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1497.41it/s, Materializing param=transformer.ln_f.bias] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1494.40it/s, Materializing param=transformer.ln_f.bias] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1501.27it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1498.29it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1505.17it/s, Materializing param=transformer.wpe.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1502.20it/s, Materializing param=transformer.wpe.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1509.02it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1506.00it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1502.29it/s, Materializing param=transformer.wte.weight] GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. Activation checkpointing is DISABLED Running 5 training steps... `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`. Step 1/5, Loss: 12.2491 Step 2/5, Loss: 12.0704 Step 3/5, Loss: 11.9368 Step 4/5, Loss: 11.7790 Step 5/5, Loss: 11.8115 \u2713 Memory snapshot saved to snapshot_baseline.pickle \u2713 Peak GPU memory: 5.12 GB Run Modified Training (With Activation Checkpointing)# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"MODIFIED: Training WITH Activation Checkpointing\") print(\"=\" * 60) ac_memory = run_training_ac( activation_checkpointing=True, snapshot_path=\"snapshot_with_ac.pickle\", batch_size=4, seq_length=512, num_steps=5, ) # Summary print(\"\\n\" + \"=\" * 60) print(\"MEMORY COMPARISON SUMMARY\") print(\"=\" * 60) print(f\"Baseline (no AC): {baseline_memory:.2f} GB\") print(f\"With AC: {ac_memory:.2f} GB\") if baseline_memory \u003e 0: saved_pct = 100 * (baseline_memory - ac_memory) / baseline_memory print( f\"Memory Saved: {baseline_memory - ac_memory:.2f} GB ({saved_pct:.1f}%)\" ) ============================================================ MODIFIED: Training WITH Activation Checkpointing ============================================================ Loading GPT-2 (activation_checkpointing=True)... Loading weights: 0%| | 0/148 [00:00\u003c?, ?it/s] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 45590.26it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 4619.28it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 3674.38it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 2680.07it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1417.63it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1292.41it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 692.19it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 668.12it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 786.19it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 744.25it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 842.23it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 799.14it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 859.61it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 757.33it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 819.56it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 764.08it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 809.99it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 778.79it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 762.70it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 737.80it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 769.92it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 759.29it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 778.90it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 744.03it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 785.17it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 727.43it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 732.06it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 683.37it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 706.79it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 690.48it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 711.15it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 693.77it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 725.14it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 705.65it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 735.91it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 730.59it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 748.50it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 740.88it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 768.73it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 757.55it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 785.15it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 756.60it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 760.85it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 756.16it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 774.96it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 770.20it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 764.34it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 759.80it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 772.91it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 767.95it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 778.86it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 774.63it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 787.55it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 782.32it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 797.24it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 777.30it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 787.94it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 777.60it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 789.02it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 777.04it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 795.08it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 785.82it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 803.60it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 800.03it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 811.88it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 795.37it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 812.14it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 808.31it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 808.29it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 804.73it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 815.70it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 811.89it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 823.83it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 820.46it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 835.30it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 831.94it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 846.82it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 843.56it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 844.97it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 834.19it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 827.23it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 824.19it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 818.91it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 815.94it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 815.82it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 812.32it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 822.11it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 810.69it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 821.34it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 818.36it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 825.80it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 823.06it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 836.62it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 834.13it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 848.36it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 845.85it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 860.14it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 857.76it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 872.03it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 869.67it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 883.84it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 881.38it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 895.44it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 893.03it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 906.96it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 904.57it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 918.34it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 915.93it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 929.67it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 927.26it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 940.48it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 937.99it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 951.46it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 949.00it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 962.18it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 959.70it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 972.93it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 970.40it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 983.08it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 980.49it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 993.47it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 990.97it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1003.78it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1001.25it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1013.97it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1011.44it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1024.12it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1021.60it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1033.67it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1031.12it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1043.61it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1041.05it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1053.49it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1050.96it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1063.20it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1060.65it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1072.66it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1070.10it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1082.15it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1079.58it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1091.54it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1088.96it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1100.33it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1097.57it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1109.27it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1106.66it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1118.27it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1115.67it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1127.17it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1124.53it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1135.88it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1133.24it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1144.64it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1142.01it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1153.42it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1150.82it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1162.15it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1159.56it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1170.57it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1167.62it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1178.62it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1175.98it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1186.98it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1184.34it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1195.11it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1192.44it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1203.14it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1200.46it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1211.23it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1208.56it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1219.18it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1216.53it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1227.11it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1223.81it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1234.25it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1231.29it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1241.10it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1237.87it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1247.53it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1244.08it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1253.03it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1249.88it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1260.02it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1257.33it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1267.45it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1264.78it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1274.77it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1272.09it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1282.03it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1279.38it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1289.25it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1286.59it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1296.34it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1293.36it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1302.98it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1300.25it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1309.89it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1307.20it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1316.74it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1314.04it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1323.41it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1320.72it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1330.25it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1327.56it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1337.03it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1334.35it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1343.75it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1340.82it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1349.89it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1347.16it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1356.42it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1353.69it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1362.84it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1360.13it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1369.25it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1366.56it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1375.60it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1372.88it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1381.85it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1379.17it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1388.13it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1385.47it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1394.11it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1391.18it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1399.96it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1397.30it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1405.71it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1403.01it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1411.82it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1409.13it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1417.81it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1415.13it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1423.77it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1421.10it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1429.64it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1426.96it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1435.50it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1432.83it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1441.28it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1438.37it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1446.52it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1443.80it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1452.12it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1449.21it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1457.45it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1454.73it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1462.95it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1460.29it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1468.45it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1465.77it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1474.04it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1471.37it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1479.59it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1476.94it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1485.08it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1482.43it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1489.69it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1486.71it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1494.00it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1491.23it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1498.52it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1495.72it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1502.91it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1500.15it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1507.27it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1504.51it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1511.76it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1508.95it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1516.19it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1513.46it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1520.57it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1517.79it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1524.87it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1522.13it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1529.25it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1526.51it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1533.65it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1530.90it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1537.55it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1534.85it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1541.89it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1539.17it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1546.01it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1543.29it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1550.11it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1547.39it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1554.02it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1551.27it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1558.02it/s, Materializing param=transformer.ln_f.bias] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1555.33it/s, Materializing param=transformer.ln_f.bias] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1562.28it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1559.60it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1566.43it/s, Materializing param=transformer.wpe.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1563.77it/s, Materializing param=transformer.wpe.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1570.63it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1567.99it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1563.87it/s, Materializing param=transformer.wte.weight] GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. Activation checkpointing is ENABLED Running 5 training steps... `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`... Step 1/5, Loss: 12.3067 Step 2/5, Loss: 12.1271 Step 3/5, Loss: 11.9482 Step 4/5, Loss: 11.8362 Step 5/5, Loss: 11.7708 \u2713 Memory snapshot saved to snapshot_with_ac.pickle \u2713 Peak GPU memory: 3.04 GB ============================================================ MEMORY COMPARISON SUMMARY ============================================================ Baseline (no AC): 5.12 GB With AC: 3.04 GB Memory Saved: 2.08 GB (40.6%) Generate Categorical Memory Profiles with Mosaic# Use Mosaic to generate HTML profiles for both snapshots. if HAS_CUDA and HAS_MOSAIC_CLI: print(\"\\n\" + \"=\" * 60) print(\"MOSAIC: Categorical Memory Profiling\") print(\"=\" * 60) # Generate HTML profiles using subprocess result1 = subprocess.run( [ \"mosaic_get_memory_profile\", \"--snapshot\", \"snapshot_baseline.pickle\", \"--out-path\", \"profile_baseline.html\", \"--profile\", \"categories\", \"--preserve-allocation-order\", \"--plotter_sampling_rate\", \"20\", ], ) print() result2 = subprocess.run( [ \"mosaic_get_memory_profile\", \"--snapshot\", \"snapshot_with_ac.pickle\", \"--out-path\", \"profile_with_ac.html\", \"--profile\", \"categories\", \"--preserve-allocation-order\", \"--plotter_sampling_rate\", \"20\", ], ) if result1.returncode == 0 and result2.returncode == 0: print(\"\\nGenerated profile_baseline.html\") print(\"Generated profile_with_ac.html\") print(\"\\nDownload these files to view the interactive memory profiles.\") else: print(\"\\nNote: Mosaic profile generation encountered issues.\") print(\"This may happen if running in an environment without full Mosaic support.\") ============================================================ MOSAIC: Categorical Memory Profiling ============================================================ Note: Mosaic profile generation encountered issues. This may happen if running in an environment without full Mosaic support. Results Interpretation: Activation Checkpointing# What We Observed# Based on the Mosaic categorical profiling results: Memory Comparison Results# Metric Baseline With Activation Checkpointing Difference Total Peak Memory 4.62 GB 2.55 GB 2.07 GB (45% reduction) Activation Memory 2.93 GB 872.79 MB 2.08 GB saved (71% reduction) Backward/Gradient Memory 793.39 MB 785.27 MB 8 MB (minimal change) Optimizer State 949.4 MB 949.4 MB No change Unknown 32 KB 32 KB No change Key Insights# Primary Finding: Activation memory dropped from 2.93 GB \u2192 872 MB (71% reduction), which accounts for nearly all the total memory savings. Why Does This Happen?# Activation checkpointing is a memory optimization technique that: Without AC (Baseline): All intermediate activations from the forward pass are stored in memory for use during backpropagation. GPT-2 has 12 transformer layers, each storing multiple activations (attention outputs, MLP outputs, etc.). For batch_size=4, seq_length=512, this adds up quickly. With AC (Optimized): Only activations at checkpoint boundaries are stored; intermediate activations are recomputed during the backward pass. This dramatically reduces activation memory (71% in our case) while other memory categories remain unchanged. How Mosaic Helped# Mosaic\u2019s categorical profiling immediately identified: Activation memory is the category with the largest difference (2.08 GB saved) Backward/Gradient memory stayed nearly constant (793 MB \u2192 785 MB) Optimizer state remained unchanged (949 MB) - expected since model parameters don\u2019t change Without Mosaic: You would need to manually instrument your code, track allocations, and categorize them yourself. With Mosaic: You get instant categorical breakdowns with exact numbers, making it trivial to identify/quantify memory optimizations. Case 2: Debugging Unexpected Memory Usage# This section demonstrates how to use Mosaic to debug when your model is using more memory than expected and you\u2019re not sure why. What we\u2019ll do: Train GPT-2 and capture a memory snapshot. Train GPT-2 with a bug that introduces additional memory and capture a memory snapshot. Use Mosaic to identify potential culprits introducing additional memory. The Buggy Model# This model has abandoned debug code that creates unnecessary GPU memory overhead. Someone added projection layers to \u201canalyze hidden states\u201d during debugging, but forgot to remove them before training. class GPT2WithDebugOverhead(GPT2LMHeadModel): \"\"\"GPT2 with abandoned \u0027feature analysis\u0027 code that bloats peak memory.\"\"\" def __init__(self, config): super().__init__(config) # BUG: Large projection layers from an abandoned experiment self.debug_projections = torch.nn.ModuleList( [ torch.nn.Linear(config.n_embd, config.n_embd * 4) for _ in range(config.n_layer) ] ) debug_params = sum(p.numel() for p in self.debug_projections.parameters()) print(f\" [DEBUG] Added {config.n_layer} debug projection layers\") print(f\" [DEBUG] Extra parameters: {debug_params:,}\") def forward(self, input_ids=None, labels=None, **kwargs): # Run normal GPT-2 forward with hidden states outputs = super().forward( input_ids=input_ids, labels=labels, output_hidden_states=True, **kwargs, ) # BUG: Project all hidden states through debug layers projected = [] for _layer_idx, (hidden, proj) in enumerate( zip(outputs.hidden_states[1:], self.debug_projections) ): proj_hidden = proj(hidden) projected.append(proj_hidden) # Tie to loss so gradients flow through debug_regularization = sum(p.mean() for p in projected) * 1e-10 return CausalLMOutputWithCrossAttentions( loss=outputs.loss + debug_regularization, logits=outputs.logits, ) Training Functions for Debug Comparison# def run_training_clean(snapshot_path, num_steps=3): \"\"\"Training with the normal model.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") print(\"Loading clean model (no debug overhead)...\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=512, seed=42 ) dataloader = DataLoader(dataset, batch_size=4, shuffle=False) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(\"Running training (should contain no debug overhead)...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}, Loss: {loss.item():.4f}\") peak_memory = torch.cuda.max_memory_allocated() / 1024**3 print(f\"Peak GPU memory: {peak_memory:.2f} GB\") del model, optimizer torch.cuda.empty_cache() return peak_memory def run_training_with_bug(snapshot_path, num_steps=3): \"\"\"Training with the buggy model.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") print(\"Loading buggy model with debug overhead...\") config = GPT2Config.from_pretrained(\"gpt2\") model = GPT2WithDebugOverhead(config).to(device) # Load pretrained weights pretrained = GPT2LMHeadModel.from_pretrained(\"gpt2\") model.load_state_dict(pretrained.state_dict(), strict=False) del pretrained torch.cuda.empty_cache() model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=512, seed=42 ) dataloader = DataLoader(dataset, batch_size=4, shuffle=False) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(\"Running training (WITH debug overhead bug)...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}, Loss: {loss.item():.4f}\") peak_memory = torch.cuda.max_memory_allocated() / 1024**3 print(f\"Peak GPU memory: {peak_memory:.2f} GB\") del model, optimizer torch.cuda.empty_cache() return peak_memory Run Training for Baseline (Clean Model)# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"Training with baseline model\") print(\"=\" * 60) baseline_memory_debug = run_training_clean( \"snapshot_debug_baseline.pickle\", num_steps=3 ) ============================================================ Training with baseline model ============================================================ Loading clean model (no debug overhead)... Loading weights: 0%| | 0/148 [00:00\u003c?, ?it/s] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 59074.70it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 5041.23it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 2150.93it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 1125.84it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1313.32it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1208.97it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1035.89it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 984.81it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1127.80it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1082.07it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1215.33it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1171.43it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 1274.75it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 1235.07it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 1168.37it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 1138.06it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 1178.54it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 1151.65it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 1225.90it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 1200.36it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 1274.97it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 1250.20it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 1313.63it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 1288.94it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 1219.30it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 1199.03it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 1233.15it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 1214.01it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 1264.06it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 1245.71it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 1288.18it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 1270.33it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 1300.18it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 1282.41it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 1294.92it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 1225.23it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 1162.11it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 1149.27it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 1128.78it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 1077.13it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 1082.37it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 1051.20it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 1061.20it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 1051.89it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 1051.93it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 1033.64it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 1051.25it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 1008.75it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 1017.61it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 983.47it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 970.96it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 948.02it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 947.25it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 915.00it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 939.13it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 912.46it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 935.52it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 929.72it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 950.77it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 945.10it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 967.21it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 961.35it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 981.95it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 976.24it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 997.82it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 992.17it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 972.72it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 956.61it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 967.16it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 960.57it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 973.65it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 968.48it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 973.69it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 968.77it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 955.55it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 946.48it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 959.85it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 955.00it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 968.95it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 964.72it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 979.45it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 974.45it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 988.28it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 983.69it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 998.51it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 993.93it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 1010.56it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 1006.43it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 1017.71it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 1013.28it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 1027.39it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 1023.28it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 1022.50it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 1018.43it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 1014.94it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 1004.39it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 1004.24it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 1000.40it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 1000.56it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 996.85it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 1002.09it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 998.39it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 987.66it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 983.87it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 995.65it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 978.61it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 989.86it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 986.60it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 996.61it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 993.33it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 1004.42it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 1001.15it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 1001.06it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 997.81it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1005.35it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1001.97it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1010.66it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1007.12it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1010.70it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1007.38it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1017.76it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1014.56it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1023.24it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1020.14it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1029.65it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1020.13it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1030.99it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1011.24it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1021.36it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1016.00it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1024.33it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1014.36it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1024.33it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1016.96it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1011.18it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1006.70it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1015.62it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1011.83it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1022.77it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1020.20it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1030.93it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1028.27it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1038.93it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1035.94it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1046.37it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1043.69it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1053.96it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1050.65it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1060.83it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1058.07it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1068.28it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1065.65it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1075.84it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1073.18it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1083.25it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1080.53it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1090.60it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1087.65it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1097.54it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1094.67it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1104.36it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1101.60it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1111.28it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1108.37it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1117.97it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1115.10it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1124.31it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1121.49it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1130.91it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1127.80it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1136.77it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1134.10it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1143.50it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1140.75it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1149.90it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1147.30it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1156.45it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1153.69it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1163.03it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1160.16it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1169.40it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1166.74it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1175.82it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1173.21it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1182.34it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1179.66it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1188.65it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1185.93it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1194.88it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1192.23it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1201.14it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1198.47it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1206.53it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1203.63it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1212.04it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1209.30it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1217.83it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1214.96it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1223.51it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1220.67it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1229.05it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1226.23it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1234.51it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1231.73it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1240.19it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1237.55it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1246.03it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1243.44it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1251.93it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1249.37it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1257.78it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1255.21it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1263.39it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1260.84it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1269.01it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1266.47it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1274.45it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1271.62it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1279.51it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1276.92it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1284.71it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1282.07it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1289.85it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1287.31it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1295.14it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1292.79it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1300.71it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1298.38it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1306.26it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1303.95it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1311.89it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1309.33it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1317.21it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1314.67it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1322.28it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1319.71it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1327.11it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1324.56it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1332.11it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1329.27it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1336.63it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1334.26it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1341.87it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1339.20it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1346.83it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1344.53it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1351.79it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1349.45it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1356.85it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1354.55it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1361.76it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1359.21it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1366.43it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1363.88it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1371.01it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1368.37it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1375.38it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1372.74it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1380.11it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1377.45it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1384.65it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1382.11it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1389.00it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1386.59it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1393.70it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1391.38it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1398.43it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1396.07it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1402.90it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1400.56it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1407.39it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1405.04it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1411.83it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1409.50it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1416.34it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1413.70it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1420.45it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1418.16it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1424.88it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1422.58it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1429.22it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1426.71it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1433.30it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1430.78it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1437.35it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1434.82it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1441.34it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1438.84it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.ln_f.bias] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.ln_f.bias] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.wpe.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.wpe.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1445.22it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1448.19it/s, Materializing param=transformer.wte.weight] GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. Running training (should contain no debug overhead)... Step 1, Loss: 12.2905 Step 2, Loss: 12.0938 Step 3, Loss: 12.0060 \u2713 Memory snapshot saved to snapshot_debug_baseline.pickle Peak GPU memory: 5.13 GB Run Training WITH the Bug# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"Training with debug projection overhead (BUG)\") print(\"=\" * 60) try: buggy_memory = run_training_with_bug(\"snapshot_with_bug.pickle\", num_steps=3) except (AttributeError, ValueError) as e: # Handle transformers version compatibility issues print(f\"Note: Skipping buggy model demo due to transformers compatibility: {e}\") buggy_memory = baseline_memory_debug ============================================================ Training with debug projection overhead (BUG) ============================================================ Loading buggy model with debug overhead... Note: Skipping buggy model demo due to transformers compatibility: GPT2Model does not support setting experts implementation. Use Mosaic to Find the Problem# Analyze both snapshots to identify the source of extra memory usage. if HAS_CUDA and HAS_MOSAIC_CLI: print(\"\\n\" + \"=\" * 60) print(\"MOSAIC: Analyzing the Baseline Snapshot\") print(\"=\" * 60) subprocess.run( [\"mosaic_get_memory_usage_peak\", \"--snapshot\", \"snapshot_debug_baseline.pickle\"], ) print(\"\\n\" + \"=\" * 60) print(\"MOSAIC: Analyzing the Buggy Snapshot\") print(\"=\" * 60) subprocess.run( [\"mosaic_get_memory_usage_peak\", \"--snapshot\", \"snapshot_with_bug.pickle\"], ) ============================================================ MOSAIC: Analyzing the Baseline Snapshot ============================================================ ============================================================ MOSAIC: Analyzing the Buggy Snapshot ============================================================ Analyzing The Mosaic Output# When you run Mosaic\u2019s peak memory analysis, it shows stack traces for each memory allocation. Let\u2019s look at how to find abandoned or unnecessary code that\u2019s bloating the memory. 1. Optimizer State Allocations Delta# In the buggy snapshot output, we can see that the first two stack traces represent the optimizer state allocations (like zeros_like for Adam optimizer state). See torch/optim/adam.py in the stack trace. In the snapshot of the buggy model we can see around a total of 0.21 GB more memory: Optimizer State Comparison# Version Stack Trace Position Calls Memory (per trace) Buggy model 1st and 2nd 172 calls 0.569 GB + 0.569 GB Baseline 2nd and 3rd 148 calls 0.464 GB + 0.464 GB What this tells us: The optimizer is tracking more tensors! This is your first clue that there are extra parameters or tensors in the computation graph. 2. Additional Activation Allocations# The buggy version shows extra allocations that don\u2019t appear in the baseline model. Scrolling down the Mosaic output of the buggy model we can see additional stack traces which contain: torch::autograd::Engine::evaluate_function: We\u2019re in the backward pass AddmmBackward0::apply: Computing gradients for an addmm operation empty_cuda at the bottom: Allocating a new CUDA tensor to store the gradient 0.176 GB from matrix multiply gradients (AddmmBackward0, mm_mat1_backward) Memory Total Explanation# Total Peak Dynamic Memory Usage: This is the peak memory that changes during execution, measured relative to the starting point of the snapshot. It tracks memory allocations that occur during the traced execution timeline. Total Static Memory Usage: This is the \u201cstarting memory\u201d or baseline memory that exists before tracing begins. It\u2019s estimated by the PyTorch visualizer and remains constant throughout the snapshot (doesn\u2019t come with stack traces). Note In the snapshots you may observe differences in total static memory usage, which accounts for the remaining difference. Total Overall Peak Memory Usage: Dynamic + Static if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"COMPARISON\") print(\"=\" * 60) print(f\"Baseline (clean model): {baseline_memory_debug:.2f} GB\") print(f\"With bug (debug projections): {buggy_memory:.2f} GB\") print( f\"Extra memory from bug: {buggy_memory - baseline_memory_debug:.2f} GB\" ) ============================================================ COMPARISON ============================================================ Baseline (clean model): 5.13 GB With bug (debug projections): 5.13 GB Extra memory from bug: 0.00 GB Case 3: Integrating Memory Analysis into Your Training Pipeline# This section demonstrates how to use Mosaic to automatically capture memory snapshots during training, get structured memory breakdown data for monitoring/dashboards, and build automated memory monitoring for large-scale training using Mosaic programmatically (as a Python dependency). Mosaic integrates memory analysis directly into your training pipeline. Training with Automatic Memory Capture# def run_training_with_memory_capture( batch_size=4, seq_length=512, num_steps=5, snapshot_path=\"training_snapshot.pickle\", ): \"\"\"Run training and automatically capture memory snapshot.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset(tokenizer.vocab_size, seq_length) dataloader = DataLoader(dataset, batch_size=batch_size) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(f\"Running {num_steps} training steps with memory capture...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) outputs.loss.backward() optimizer.step() print(f\" Step {step + 1}/{num_steps}, Loss: {outputs.loss.item():.4f}\") peak_memory_gb = torch.cuda.max_memory_allocated() / 1024**3 print(f\"\u2713 PyTorch reported peak memory: {peak_memory_gb:.3f} GB\") del model, optimizer torch.cuda.empty_cache() return snapshot_path if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"CASE 3: Pipeline Integration\") print(\"=\" * 60) pipeline_snapshot_path = run_training_with_memory_capture(batch_size=4, seq_length=512) ============================================================ CASE 3: Pipeline Integration ============================================================ Loading weights: 0%| | 0/148 [00:00\u003c?, ?it/s] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 51150.05it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 1284.23it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 1776.87it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 1526.59it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1723.92it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 1496.54it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1697.07it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1576.95it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1359.67it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1109.96it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1155.88it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1054.73it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 1060.35it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 944.91it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 980.72it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 892.36it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 964.55it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 941.72it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 955.10it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 890.55it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 949.88it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 934.67it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 970.70it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 945.48it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 963.53it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 950.16it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 978.46it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 960.82it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 996.71it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 964.59it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 986.10it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 955.60it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 994.95it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 984.36it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 1015.82it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 966.21it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 994.05it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 984.48it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 1014.83it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 1005.45it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 1036.86it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 1027.02it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 1040.07it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 1004.30it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 990.69it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 976.75it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 998.61it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 977.27it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 994.00it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 973.15it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 979.23it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 970.82it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 990.93it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 982.94it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 1004.21it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 982.33it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 942.96it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 922.93it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 931.03it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 915.38it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 920.82it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 900.94it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 914.76it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 909.10it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 923.30it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 912.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 932.10it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 904.43it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 909.18it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 898.74it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 911.25it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 907.11it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 917.70it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 912.06it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 912.12it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 894.22it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 907.30it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 898.18it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 904.77it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 888.26it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 902.05it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 892.02it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 892.62it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 880.85it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 890.99it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 881.93it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 886.43it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 876.28it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 864.79it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 854.44it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 869.69it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 867.09it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 882.48it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 879.91it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 895.05it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 891.95it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 906.96it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 904.13it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 918.92it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 915.98it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 930.67it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 928.04it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 942.71it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 939.82it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 954.15it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 951.46it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 965.88it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 963.25it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 977.57it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 974.92it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 989.06it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 986.44it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 1000.53it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 997.83it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1011.71it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1008.99it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1022.74it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1019.70it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1033.32it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1030.57it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1044.04it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1040.53it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1053.63it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1050.80it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1064.02it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1061.26it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1074.52it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1071.41it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1084.12it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1081.10it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1093.65it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1090.53it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1103.12it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1100.01it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1112.64it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1109.87it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1122.36it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1119.36it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1131.59it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1128.50it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1140.77it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1137.92it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1150.10it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1147.28it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1159.47it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1156.32it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1168.23it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1165.03it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1176.74it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1173.81it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1185.40it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1182.42it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1194.19it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1191.08it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1202.39it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1199.42it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1211.07it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1208.02it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1219.43it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1216.39it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1227.85it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1225.01it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1236.45it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1233.29it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1244.41it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1241.50it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1252.50it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1249.33it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1260.25it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1257.09it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1267.93it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1264.78it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1275.59it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1272.41it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1282.57it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1279.62it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1290.36it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1287.20it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1297.54it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1294.58it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1304.86it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1301.78it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1312.27it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1309.15it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1319.44it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1316.51it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1326.78it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1323.64it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1333.84it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1330.98it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1341.17it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1338.31it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1348.41it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1345.56it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1355.56it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1352.66it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1362.65it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1359.76it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1369.67it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1366.89it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1376.66it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1373.73it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1383.39it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1380.47it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1390.24it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1387.38it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1397.15it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1394.29it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1403.98it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1401.09it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1410.61it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1407.73it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1417.23it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1414.36it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1423.48it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1420.28it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1429.23it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1426.00it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1435.07it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1431.87it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1440.66it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1437.44it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1446.49it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1443.53it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1452.37it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1449.20it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1458.15it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1455.10it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1463.54it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1460.38it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1469.22it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1466.39it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1475.35it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1472.53it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1481.47it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1478.33it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1487.00it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1484.14it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1492.85it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1489.61it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1498.16it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1495.34it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1503.88it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1501.05it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1509.55it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1506.71it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1514.63it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1511.68it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1520.08it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1517.25it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1525.58it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1522.72it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1531.07it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1528.24it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1536.27it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1533.31it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1541.54it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1538.45it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1546.55it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1543.46it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1551.48it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1548.65it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1556.48it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1553.50it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1561.48it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1558.38it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1566.19it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1563.12it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1570.80it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1567.69it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1575.47it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1572.58it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1580.44it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1577.65it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1585.35it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1582.15it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1589.84it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1587.08it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1594.85it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1592.09it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1599.79it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1596.74it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1603.86it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1601.03it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1608.62it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1605.69it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1613.15it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1610.34it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1617.47it/s, Materializing param=transformer.ln_f.bias] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1614.19it/s, Materializing param=transformer.ln_f.bias] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1621.60it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1618.86it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1626.52it/s, Materializing param=transformer.wpe.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1623.77it/s, Materializing param=transformer.wpe.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1631.21it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1628.22it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1624.21it/s, Materializing param=transformer.wte.weight] GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. Running 5 training steps with memory capture... Step 1/5, Loss: 12.3107 Step 2/5, Loss: 12.1208 Step 3/5, Loss: 11.8964 Step 4/5, Loss: 11.8080 Step 5/5, Loss: 11.6887 \u2713 Memory snapshot saved to training_snapshot.pickle \u2713 PyTorch reported peak memory: 5.126 GB Mosaic Memory Analysis via Python API# Instead of using CLI commands, we can use Mosaic\u2019s Python API directly for programmatic integration. if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"MOSAIC MEMORY ANALYSIS (via Python API)\") print(\"=\" * 60) # Load and analyze the memory snapshot memory_abstract = MemoryAbstract(memory_snapshot_file=pipeline_snapshot_path) memory_abstract.load_memory_snapshot() # Analyze peak memory usage memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") # Get results dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory print(f\"Peak dynamic memory: {dynamic_peak / 1024**3:.3f} GiB\") print(f\"Static memory: {static_memory / 1024**3:.3f} GiB\") print(f\"Overall peak memory: {overall_peak / 1024**3:.3f} GiB\") print(\"\u2713 Analysis complete using Mosaic Python API\") ============================================================ MOSAIC MEMORY ANALYSIS (via Python API) ============================================================ Peak dynamic memory: 4.620 GiB Static memory: 0.495 GiB Overall peak memory: 5.115 GiB \u2713 Analysis complete using Mosaic Python API Reusable Memory Analysis Function# Create a reusable function for analyzing training memory snapshots. def analyze_training_memory(snapshot_path): \"\"\"Analyze a memory snapshot using Mosaic\u0027s Python API. Returns a structured dictionary with memory breakdown. Args: snapshot_path: Path to the memory snapshot pickle file. Returns: Dictionary containing memory analysis results. \"\"\" # Load snapshot memory_abstract = MemoryAbstract(memory_snapshot_file=snapshot_path) memory_abstract.load_memory_snapshot() # Analyze peak memory memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") # Extract results dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory return { \"snapshot_path\": snapshot_path, \"dynamic_peak_memory_bytes\": dynamic_peak, \"static_memory_bytes\": static_memory, \"overall_peak_memory_bytes\": overall_peak, \"dynamic_peak_memory_gib\": dynamic_peak / 1024**3, \"static_memory_gib\": static_memory / 1024**3, \"overall_peak_memory_gib\": overall_peak / 1024**3, } if HAS_CUDA: analysis = analyze_training_memory(pipeline_snapshot_path) print(\"\\nMemory Analysis Result:\") for key, value in analysis.items(): print(f\" {key}: {value}\") Memory Analysis Result: snapshot_path: training_snapshot.pickle dynamic_peak_memory_bytes: 4960794632 static_memory_bytes: 531589120 overall_peak_memory_bytes: 5492383752 dynamic_peak_memory_gib: 4.620100028812885 static_memory_gib: 0.49508094787597656 overall_peak_memory_gib: 5.115180976688862 Complete Training Pipeline with Memory Monitoring# This demonstrates a production-ready training pipeline with integrated Mosaic memory monitoring that can be used in CI/CD, monitoring dashboards, or capacity planning. def training_pipeline_with_memory_monitoring( model_name: str, batch_size: int, seq_length: int, num_steps: int = 5, snapshot_path: str = \"pipeline_snapshot.pickle\", ) -\u003e dict: \"\"\"Complete training pipeline with integrated Mosaic memory monitoring. Can be integrated into CI/CD, monitoring dashboards, or capacity planning. Args: model_name: HuggingFace model name to use. batch_size: Training batch size. seq_length: Sequence length for input tokens. num_steps: Number of training steps. snapshot_path: Path to save the memory snapshot. Returns: Dictionary containing training and memory analysis report. \"\"\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Setup print(f\"Loading model: {model_name}\") model = GPT2LMHeadModel.from_pretrained(model_name).to(device) model.train() optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) tokenizer = GPT2Tokenizer.from_pretrained(model_name) torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() # Training with memory capture print(f\"Running {num_steps} training steps...\") with capture_memory_snapshot(snapshot_path): for step in range(num_steps): input_ids = torch.randint( 0, tokenizer.vocab_size, (batch_size, seq_length) ).to(device) outputs = model(input_ids=input_ids, labels=input_ids) outputs.loss.backward() optimizer.step() optimizer.zero_grad() print(f\" Step {step + 1}/{num_steps}, Loss: {outputs.loss.item():.4f}\") pytorch_peak_gb = torch.cuda.max_memory_allocated() / 1024**3 # Mosaic analysis using Python API print(\"Analyzing memory with Mosaic...\") memory_abstract = MemoryAbstract(memory_snapshot_file=snapshot_path) memory_abstract.load_memory_snapshot() memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory report = { \"model\": model_name, \"config\": { \"batch_size\": batch_size, \"seq_length\": seq_length, \"num_steps\": num_steps, }, \"pytorch_peak_memory_gb\": pytorch_peak_gb, \"mosaic_analysis\": { \"dynamic_peak_gib\": dynamic_peak / 1024**3, \"static_memory_gib\": static_memory / 1024**3, \"overall_peak_gib\": overall_peak / 1024**3, }, \"snapshot_path\": snapshot_path, } del model, optimizer torch.cuda.empty_cache() return report # Run the pipeline if HAS_CUDA: report = training_pipeline_with_memory_monitoring( \"gpt2\", batch_size=4, seq_length=512, num_steps=5 ) print(\"\\n\" + \"=\" * 60) print(\"PIPELINE REPORT\") print(\"=\" * 60) print(f\"Model: {report[\u0027model\u0027]}\") print(f\"Config: {report[\u0027config\u0027]}\") print(f\"PyTorch Peak Memory: {report[\u0027pytorch_peak_memory_gb\u0027]:.3f} GB\") print(f\"Mosaic Dynamic Peak: {report[\u0027mosaic_analysis\u0027][\u0027dynamic_peak_gib\u0027]:.3f} GiB\") print(f\"Mosaic Overall Peak: {report[\u0027mosaic_analysis\u0027][\u0027overall_peak_gib\u0027]:.3f} GiB\") Loading model: gpt2 Loading weights: 0%| | 0/148 [00:00\u003c?, ?it/s] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 57456.22it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%| | 1/148 [00:00\u003c00:00, 5084.00it/s, Materializing param=transformer.h.0.attn.c_attn.bias] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 4398.85it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 1%|\u258f | 2/148 [00:00\u003c00:00, 3019.66it/s, Materializing param=transformer.h.0.attn.c_attn.weight] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 3106.12it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 2%|\u258f | 3/148 [00:00\u003c00:00, 2639.59it/s, Materializing param=transformer.h.0.attn.c_proj.bias] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1711.44it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 4/148 [00:00\u003c00:00, 1557.19it/s, Materializing param=transformer.h.0.attn.c_proj.weight] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1711.96it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 3%|\u258e | 5/148 [00:00\u003c00:00, 1617.17it/s, Materializing param=transformer.h.0.ln_1.bias] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1578.19it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 4%|\u258d | 6/148 [00:00\u003c00:00, 1510.46it/s, Materializing param=transformer.h.0.ln_1.weight] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 1573.09it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258d | 7/148 [00:00\u003c00:00, 1504.41it/s, Materializing param=transformer.h.0.ln_2.bias] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 1230.68it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 5%|\u258c | 8/148 [00:00\u003c00:00, 1148.34it/s, Materializing param=transformer.h.0.ln_2.weight] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 1171.34it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 6%|\u258c | 9/148 [00:00\u003c00:00, 1112.74it/s, Materializing param=transformer.h.0.mlp.c_fc.bias] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 996.44it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 10/148 [00:00\u003c00:00, 976.99it/s, Materializing param=transformer.h.0.mlp.c_fc.weight] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 920.72it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 7%|\u258b | 11/148 [00:00\u003c00:00, 903.71it/s, Materializing param=transformer.h.0.mlp.c_proj.bias] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 885.76it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 8%|\u258a | 12/148 [00:00\u003c00:00, 853.19it/s, Materializing param=transformer.h.0.mlp.c_proj.weight] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 883.60it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 13/148 [00:00\u003c00:00, 853.30it/s, Materializing param=transformer.h.1.attn.c_attn.bias] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 844.85it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 9%|\u2589 | 14/148 [00:00\u003c00:00, 835.00it/s, Materializing param=transformer.h.1.attn.c_attn.weight] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 873.10it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 10%|\u2588 | 15/148 [00:00\u003c00:00, 832.61it/s, Materializing param=transformer.h.1.attn.c_proj.bias] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 856.19it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588 | 16/148 [00:00\u003c00:00, 824.17it/s, Materializing param=transformer.h.1.attn.c_proj.weight] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 814.65it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 11%|\u2588\u258f | 17/148 [00:00\u003c00:00, 796.56it/s, Materializing param=transformer.h.1.ln_1.bias] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 831.09it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 12%|\u2588\u258f | 18/148 [00:00\u003c00:00, 816.26it/s, Materializing param=transformer.h.1.ln_1.weight] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 832.72it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 13%|\u2588\u258e | 19/148 [00:00\u003c00:00, 820.09it/s, Materializing param=transformer.h.1.ln_2.bias] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 805.80it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258e | 20/148 [00:00\u003c00:00, 799.26it/s, Materializing param=transformer.h.1.ln_2.weight] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 821.61it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 14%|\u2588\u258d | 21/148 [00:00\u003c00:00, 815.68it/s, Materializing param=transformer.h.1.mlp.c_fc.bias] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 829.88it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 15%|\u2588\u258d | 22/148 [00:00\u003c00:00, 809.65it/s, Materializing param=transformer.h.1.mlp.c_fc.weight] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 832.53it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 23/148 [00:00\u003c00:00, 794.87it/s, Materializing param=transformer.h.1.mlp.c_proj.bias] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 817.61it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 16%|\u2588\u258c | 24/148 [00:00\u003c00:00, 812.32it/s, Materializing param=transformer.h.1.mlp.c_proj.weight] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 829.31it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 17%|\u2588\u258b | 25/148 [00:00\u003c00:00, 787.07it/s, Materializing param=transformer.h.2.attn.c_attn.bias] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 803.65it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 26/148 [00:00\u003c00:00, 798.51it/s, Materializing param=transformer.h.2.attn.c_attn.weight] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 767.99it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 18%|\u2588\u258a | 27/148 [00:00\u003c00:00, 763.70it/s, Materializing param=transformer.h.2.attn.c_proj.bias] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 770.17it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 19%|\u2588\u2589 | 28/148 [00:00\u003c00:00, 750.59it/s, Materializing param=transformer.h.2.attn.c_proj.weight] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 770.37it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2589 | 29/148 [00:00\u003c00:00, 759.82it/s, Materializing param=transformer.h.2.ln_1.bias] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 774.50it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 20%|\u2588\u2588 | 30/148 [00:00\u003c00:00, 770.72it/s, Materializing param=transformer.h.2.ln_1.weight] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 777.56it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 21%|\u2588\u2588 | 31/148 [00:00\u003c00:00, 760.02it/s, Materializing param=transformer.h.2.ln_2.bias] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 778.14it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 32/148 [00:00\u003c00:00, 769.61it/s, Materializing param=transformer.h.2.ln_2.weight] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 776.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 22%|\u2588\u2588\u258f | 33/148 [00:00\u003c00:00, 765.51it/s, Materializing param=transformer.h.2.mlp.c_fc.bias] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 769.48it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 23%|\u2588\u2588\u258e | 34/148 [00:00\u003c00:00, 748.06it/s, Materializing param=transformer.h.2.mlp.c_fc.weight] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 749.06it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258e | 35/148 [00:00\u003c00:00, 734.85it/s, Materializing param=transformer.h.2.mlp.c_proj.bias] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 747.38it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 24%|\u2588\u2588\u258d | 36/148 [00:00\u003c00:00, 737.93it/s, Materializing param=transformer.h.2.mlp.c_proj.weight] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 753.71it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 25%|\u2588\u2588\u258c | 37/148 [00:00\u003c00:00, 750.66it/s, Materializing param=transformer.h.3.attn.c_attn.bias] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 766.33it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258c | 38/148 [00:00\u003c00:00, 763.25it/s, Materializing param=transformer.h.3.attn.c_attn.weight] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 778.75it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 26%|\u2588\u2588\u258b | 39/148 [00:00\u003c00:00, 775.65it/s, Materializing param=transformer.h.3.attn.c_proj.bias] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 791.05it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 27%|\u2588\u2588\u258b | 40/148 [00:00\u003c00:00, 787.98it/s, Materializing param=transformer.h.3.attn.c_proj.weight] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 803.19it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 41/148 [00:00\u003c00:00, 800.09it/s, Materializing param=transformer.h.3.ln_1.bias] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 814.81it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 28%|\u2588\u2588\u258a | 42/148 [00:00\u003c00:00, 811.73it/s, Materializing param=transformer.h.3.ln_1.weight] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 826.58it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 29%|\u2588\u2588\u2589 | 43/148 [00:00\u003c00:00, 823.52it/s, Materializing param=transformer.h.3.ln_2.bias] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 838.19it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2589 | 44/148 [00:00\u003c00:00, 834.84it/s, Materializing param=transformer.h.3.ln_2.weight] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 849.16it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 30%|\u2588\u2588\u2588 | 45/148 [00:00\u003c00:00, 846.04it/s, Materializing param=transformer.h.3.mlp.c_fc.bias] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 859.57it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 31%|\u2588\u2588\u2588 | 46/148 [00:00\u003c00:00, 856.93it/s, Materializing param=transformer.h.3.mlp.c_fc.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 872.18it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 47/148 [00:00\u003c00:00, 869.71it/s, Materializing param=transformer.h.3.mlp.c_proj.bias] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 884.88it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 32%|\u2588\u2588\u2588\u258f | 48/148 [00:00\u003c00:00, 882.32it/s, Materializing param=transformer.h.3.mlp.c_proj.weight] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 897.39it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 33%|\u2588\u2588\u2588\u258e | 49/148 [00:00\u003c00:00, 894.91it/s, Materializing param=transformer.h.4.attn.c_attn.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 909.82it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 50/148 [00:00\u003c00:00, 907.30it/s, Materializing param=transformer.h.4.attn.c_attn.weight] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 922.07it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 34%|\u2588\u2588\u2588\u258d | 51/148 [00:00\u003c00:00, 919.51it/s, Materializing param=transformer.h.4.attn.c_proj.bias] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 934.11it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 35%|\u2588\u2588\u2588\u258c | 52/148 [00:00\u003c00:00, 931.57it/s, Materializing param=transformer.h.4.attn.c_proj.weight] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 946.11it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258c | 53/148 [00:00\u003c00:00, 943.50it/s, Materializing param=transformer.h.4.ln_1.bias] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 957.86it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 36%|\u2588\u2588\u2588\u258b | 54/148 [00:00\u003c00:00, 955.10it/s, Materializing param=transformer.h.4.ln_1.weight] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 969.34it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 37%|\u2588\u2588\u2588\u258b | 55/148 [00:00\u003c00:00, 966.74it/s, Materializing param=transformer.h.4.ln_2.bias] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 980.88it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 38%|\u2588\u2588\u2588\u258a | 56/148 [00:00\u003c00:00, 978.32it/s, Materializing param=transformer.h.4.ln_2.weight] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 991.89it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u258a | 57/148 [00:00\u003c00:00, 989.23it/s, Materializing param=transformer.h.4.mlp.c_fc.bias] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1003.07it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 39%|\u2588\u2588\u2588\u2589 | 58/148 [00:00\u003c00:00, 1000.44it/s, Materializing param=transformer.h.4.mlp.c_fc.weight] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1014.02it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 40%|\u2588\u2588\u2588\u2589 | 59/148 [00:00\u003c00:00, 1010.89it/s, Materializing param=transformer.h.4.mlp.c_proj.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1024.22it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 60/148 [00:00\u003c00:00, 1021.54it/s, Materializing param=transformer.h.4.mlp.c_proj.weight] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1034.99it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 41%|\u2588\u2588\u2588\u2588 | 61/148 [00:00\u003c00:00, 1032.30it/s, Materializing param=transformer.h.5.attn.c_attn.bias] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1045.31it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 42%|\u2588\u2588\u2588\u2588\u258f | 62/148 [00:00\u003c00:00, 1042.54it/s, Materializing param=transformer.h.5.attn.c_attn.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1055.72it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 63/148 [00:00\u003c00:00, 1053.02it/s, Materializing param=transformer.h.5.attn.c_proj.bias] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1066.13it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 43%|\u2588\u2588\u2588\u2588\u258e | 64/148 [00:00\u003c00:00, 1063.45it/s, Materializing param=transformer.h.5.attn.c_proj.weight] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1076.37it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 44%|\u2588\u2588\u2588\u2588\u258d | 65/148 [00:00\u003c00:00, 1073.47it/s, Materializing param=transformer.h.5.ln_1.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1086.49it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258d | 66/148 [00:00\u003c00:00, 1083.78it/s, Materializing param=transformer.h.5.ln_1.weight] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1096.58it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 45%|\u2588\u2588\u2588\u2588\u258c | 67/148 [00:00\u003c00:00, 1093.86it/s, Materializing param=transformer.h.5.ln_2.bias] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1106.10it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 46%|\u2588\u2588\u2588\u2588\u258c | 68/148 [00:00\u003c00:00, 1103.38it/s, Materializing param=transformer.h.5.ln_2.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1116.03it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 69/148 [00:00\u003c00:00, 1113.30it/s, Materializing param=transformer.h.5.mlp.c_fc.bias] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1125.79it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 47%|\u2588\u2588\u2588\u2588\u258b | 70/148 [00:00\u003c00:00, 1123.05it/s, Materializing param=transformer.h.5.mlp.c_fc.weight] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1135.33it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 48%|\u2588\u2588\u2588\u2588\u258a | 71/148 [00:00\u003c00:00, 1132.57it/s, Materializing param=transformer.h.5.mlp.c_proj.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1144.82it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u258a | 72/148 [00:00\u003c00:00, 1142.03it/s, Materializing param=transformer.h.5.mlp.c_proj.weight] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1154.15it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 49%|\u2588\u2588\u2588\u2588\u2589 | 73/148 [00:00\u003c00:00, 1151.36it/s, Materializing param=transformer.h.6.attn.c_attn.bias] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1163.34it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 50%|\u2588\u2588\u2588\u2588\u2588 | 74/148 [00:00\u003c00:00, 1160.28it/s, Materializing param=transformer.h.6.attn.c_attn.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1172.13it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588 | 75/148 [00:00\u003c00:00, 1169.34it/s, Materializing param=transformer.h.6.attn.c_proj.bias] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1181.31it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 76/148 [00:00\u003c00:00, 1178.41it/s, Materializing param=transformer.h.6.attn.c_proj.weight] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1189.92it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 77/148 [00:00\u003c00:00, 1186.84it/s, Materializing param=transformer.h.6.ln_1.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1198.42it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 78/148 [00:00\u003c00:00, 1195.65it/s, Materializing param=transformer.h.6.ln_1.weight] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1207.41it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 79/148 [00:00\u003c00:00, 1204.34it/s, Materializing param=transformer.h.6.ln_2.bias] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1215.79it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 80/148 [00:00\u003c00:00, 1212.78it/s, Materializing param=transformer.h.6.ln_2.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1223.86it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258d | 81/148 [00:00\u003c00:00, 1220.69it/s, Materializing param=transformer.h.6.mlp.c_fc.bias] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1231.66it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 55%|\u2588\u2588\u2588\u2588\u2588\u258c | 82/148 [00:00\u003c00:00, 1228.71it/s, Materializing param=transformer.h.6.mlp.c_fc.weight] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1239.93it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 83/148 [00:00\u003c00:00, 1237.13it/s, Materializing param=transformer.h.6.mlp.c_proj.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1247.78it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 84/148 [00:00\u003c00:00, 1244.57it/s, Materializing param=transformer.h.6.mlp.c_proj.weight] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1255.48it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 85/148 [00:00\u003c00:00, 1252.62it/s, Materializing param=transformer.h.7.attn.c_attn.bias] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1263.27it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 86/148 [00:00\u003c00:00, 1260.31it/s, Materializing param=transformer.h.7.attn.c_attn.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1271.10it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 87/148 [00:00\u003c00:00, 1267.76it/s, Materializing param=transformer.h.7.attn.c_proj.bias] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1278.35it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 88/148 [00:00\u003c00:00, 1275.20it/s, Materializing param=transformer.h.7.attn.c_proj.weight] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1285.48it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 89/148 [00:00\u003c00:00, 1282.30it/s, Materializing param=transformer.h.7.ln_1.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1292.88it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 90/148 [00:00\u003c00:00, 1290.04it/s, Materializing param=transformer.h.7.ln_1.weight] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1300.63it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91/148 [00:00\u003c00:00, 1297.50it/s, Materializing param=transformer.h.7.ln_2.bias] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1307.83it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 92/148 [00:00\u003c00:00, 1304.99it/s, Materializing param=transformer.h.7.ln_2.weight] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1315.38it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 93/148 [00:00\u003c00:00, 1312.54it/s, Materializing param=transformer.h.7.mlp.c_fc.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1322.94it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 94/148 [00:00\u003c00:00, 1320.10it/s, Materializing param=transformer.h.7.mlp.c_fc.weight] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1329.83it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 95/148 [00:00\u003c00:00, 1326.91it/s, Materializing param=transformer.h.7.mlp.c_proj.bias] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1336.69it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 96/148 [00:00\u003c00:00, 1333.84it/s, Materializing param=transformer.h.7.mlp.c_proj.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1343.88it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/148 [00:00\u003c00:00, 1340.63it/s, Materializing param=transformer.h.8.attn.c_attn.bias] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1350.38it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 98/148 [00:00\u003c00:00, 1347.48it/s, Materializing param=transformer.h.8.attn.c_attn.weight] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1357.36it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/148 [00:00\u003c00:00, 1354.51it/s, Materializing param=transformer.h.8.attn.c_proj.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1364.34it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 100/148 [00:00\u003c00:00, 1361.16it/s, Materializing param=transformer.h.8.attn.c_proj.weight] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1370.72it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 101/148 [00:00\u003c00:00, 1367.59it/s, Materializing param=transformer.h.8.ln_1.bias] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1377.30it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 102/148 [00:00\u003c00:00, 1374.45it/s, Materializing param=transformer.h.8.ln_1.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1384.21it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 103/148 [00:00\u003c00:00, 1381.13it/s, Materializing param=transformer.h.8.ln_2.bias] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1390.71it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 104/148 [00:00\u003c00:00, 1387.67it/s, Materializing param=transformer.h.8.ln_2.weight] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1397.21it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 105/148 [00:00\u003c00:00, 1394.37it/s, Materializing param=transformer.h.8.mlp.c_fc.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1403.78it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 106/148 [00:00\u003c00:00, 1400.94it/s, Materializing param=transformer.h.8.mlp.c_fc.weight] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1410.38it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 107/148 [00:00\u003c00:00, 1407.57it/s, Materializing param=transformer.h.8.mlp.c_proj.bias] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1416.95it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108/148 [00:00\u003c00:00, 1413.97it/s, Materializing param=transformer.h.8.mlp.c_proj.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1422.92it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 109/148 [00:00\u003c00:00, 1420.04it/s, Materializing param=transformer.h.9.attn.c_attn.bias] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1429.29it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110/148 [00:00\u003c00:00, 1426.48it/s, Materializing param=transformer.h.9.attn.c_attn.weight] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1435.69it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 111/148 [00:00\u003c00:00, 1432.90it/s, Materializing param=transformer.h.9.attn.c_proj.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1442.05it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112/148 [00:00\u003c00:00, 1439.23it/s, Materializing param=transformer.h.9.attn.c_proj.weight] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1448.30it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113/148 [00:00\u003c00:00, 1445.46it/s, Materializing param=transformer.h.9.ln_1.bias] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1454.54it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 114/148 [00:00\u003c00:00, 1451.74it/s, Materializing param=transformer.h.9.ln_1.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1460.74it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 115/148 [00:00\u003c00:00, 1457.94it/s, Materializing param=transformer.h.9.ln_2.bias] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1466.87it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116/148 [00:00\u003c00:00, 1463.84it/s, Materializing param=transformer.h.9.ln_2.weight] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1472.54it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 117/148 [00:00\u003c00:00, 1469.72it/s, Materializing param=transformer.h.9.mlp.c_fc.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1478.56it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 118/148 [00:00\u003c00:00, 1475.50it/s, Materializing param=transformer.h.9.mlp.c_fc.weight] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1484.14it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119/148 [00:00\u003c00:00, 1481.28it/s, Materializing param=transformer.h.9.mlp.c_proj.bias] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1489.78it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 120/148 [00:00\u003c00:00, 1486.92it/s, Materializing param=transformer.h.9.mlp.c_proj.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1495.54it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 121/148 [00:00\u003c00:00, 1492.83it/s, Materializing param=transformer.h.10.attn.c_attn.bias] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1501.41it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122/148 [00:00\u003c00:00, 1497.93it/s, Materializing param=transformer.h.10.attn.c_attn.weight] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1506.34it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 123/148 [00:00\u003c00:00, 1503.50it/s, Materializing param=transformer.h.10.attn.c_proj.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1511.96it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 124/148 [00:00\u003c00:00, 1509.14it/s, Materializing param=transformer.h.10.attn.c_proj.weight] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1517.55it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 125/148 [00:00\u003c00:00, 1514.65it/s, Materializing param=transformer.h.10.ln_1.bias] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1523.11it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 126/148 [00:00\u003c00:00, 1520.33it/s, Materializing param=transformer.h.10.ln_1.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1528.73it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 127/148 [00:00\u003c00:00, 1525.94it/s, Materializing param=transformer.h.10.ln_2.bias] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1534.31it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 128/148 [00:00\u003c00:00, 1531.55it/s, Materializing param=transformer.h.10.ln_2.weight] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1539.82it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 129/148 [00:00\u003c00:00, 1537.04it/s, Materializing param=transformer.h.10.mlp.c_fc.bias] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1545.23it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 130/148 [00:00\u003c00:00, 1542.41it/s, Materializing param=transformer.h.10.mlp.c_fc.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1550.54it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 131/148 [00:00\u003c00:00, 1547.50it/s, Materializing param=transformer.h.10.mlp.c_proj.bias] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1555.55it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 132/148 [00:00\u003c00:00, 1552.72it/s, Materializing param=transformer.h.10.mlp.c_proj.weight] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1560.66it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 133/148 [00:00\u003c00:00, 1557.84it/s, Materializing param=transformer.h.11.attn.c_attn.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1565.91it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 134/148 [00:00\u003c00:00, 1563.10it/s, Materializing param=transformer.h.11.attn.c_attn.weight] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1571.01it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 135/148 [00:00\u003c00:00, 1568.19it/s, Materializing param=transformer.h.11.attn.c_proj.bias] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1576.06it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 136/148 [00:00\u003c00:00, 1573.24it/s, Materializing param=transformer.h.11.attn.c_proj.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1581.07it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 137/148 [00:00\u003c00:00, 1578.31it/s, Materializing param=transformer.h.11.ln_1.bias] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1586.19it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 138/148 [00:00\u003c00:00, 1583.43it/s, Materializing param=transformer.h.11.ln_1.weight] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1591.28it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 139/148 [00:00\u003c00:00, 1588.49it/s, Materializing param=transformer.h.11.ln_2.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1596.28it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 140/148 [00:00\u003c00:00, 1593.50it/s, Materializing param=transformer.h.11.ln_2.weight] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1601.24it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 141/148 [00:00\u003c00:00, 1598.50it/s, Materializing param=transformer.h.11.mlp.c_fc.bias] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1606.17it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 142/148 [00:00\u003c00:00, 1603.16it/s, Materializing param=transformer.h.11.mlp.c_fc.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1610.72it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 143/148 [00:00\u003c00:00, 1607.94it/s, Materializing param=transformer.h.11.mlp.c_proj.bias] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1615.48it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 144/148 [00:00\u003c00:00, 1612.69it/s, Materializing param=transformer.h.11.mlp.c_proj.weight] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1620.06it/s, Materializing param=transformer.ln_f.bias] Loading weights: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 145/148 [00:00\u003c00:00, 1617.31it/s, Materializing param=transformer.ln_f.bias] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1624.90it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 146/148 [00:00\u003c00:00, 1622.17it/s, Materializing param=transformer.ln_f.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1629.75it/s, Materializing param=transformer.wpe.weight] Loading weights: 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 147/148 [00:00\u003c00:00, 1627.01it/s, Materializing param=transformer.wpe.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1634.50it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1631.74it/s, Materializing param=transformer.wte.weight] Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00\u003c00:00, 1627.90it/s, Materializing param=transformer.wte.weight] GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. WARNING:transformers.modeling_utils:GPT2LMHeadModel LOAD REPORT from: gpt2 Key | Status | | ---------------------+------------+--+- h.{0...11}.attn.bias | UNEXPECTED | | Notes: - UNEXPECTED :can be ignored when loading from different task/architecture; not ok if you expect identical arch. Running 5 training steps... Step 1/5, Loss: 12.2789 Step 2/5, Loss: 12.0998 Step 3/5, Loss: 11.9411 Step 4/5, Loss: 11.8859 Step 5/5, Loss: 11.7158 \u2713 Memory snapshot saved to pipeline_snapshot.pickle Analyzing memory with Mosaic... ============================================================ PIPELINE REPORT ============================================================ Model: gpt2 Config: {\u0027batch_size\u0027: 4, \u0027seq_length\u0027: 512, \u0027num_steps\u0027: 5} PyTorch Peak Memory: 5.126 GB Mosaic Dynamic Peak: 4.620 GiB Mosaic Overall Peak: 5.115 GiB CI/CD and Dashboard Integration Patterns# These patterns show how to integrate Mosaic analysis into automated workflows. import json Pattern 1: CI/CD Memory Regression Testing# def check_memory_regression(report, threshold_gib=5.0): \"\"\"Check if memory usage exceeds threshold for CI/CD pipelines. Args: report: Memory analysis report from training_pipeline_with_memory_monitoring. threshold_gib: Maximum allowed memory in GiB. Raises: AssertionError: If memory exceeds threshold. \"\"\" peak = report[\"mosaic_analysis\"][\"overall_peak_gib\"] assert peak \u003c threshold_gib, ( f\"Memory regression! {peak:.2f} GiB \u003e {threshold_gib} GiB\" ) print(f\"Memory check passed: {peak:.2f} GiB \u003c {threshold_gib} GiB threshold\") Pattern 2: Export to JSON for Dashboards# if HAS_CUDA: check_memory_regression(report, threshold_gib=8.0) with open(\"memory_report.json\", \"w\") as f: json.dump(report, f, indent=2, default=str) print(\"Memory report exported to memory_report.json\") Memory check passed: 5.12 GiB \u003c 8.0 GiB threshold Memory report exported to memory_report.json Conclusion# This tutorial demonstrated three key use cases for Mosaic memory profiling: Case 1: Activation Checkpointing Analysis Used Mosaic to compare memory usage between baseline and optimized models Identified that activation checkpointing reduced activation memory by 71% Mosaic\u2019s categorical profiling made it trivial to pinpoint memory savings Case 2: Debugging Unexpected Memory Usage Created a \u201cbuggy\u201d model with abandoned debug code Used mosaic_get_memory_usage_peak to identify extra allocations Stack traces revealed optimizer state tracking extra parameters Case 3: Pipeline Integration Demonstrated programmatic usage via Mosaic\u2019s Python API Showed integration patterns for CI/CD and dashboards with structured reports Further Reading# Mosaic GitHub Repository PyTorch Memory Management Documentation Understanding CUDA Memory Usage Activation Checkpointing in PyTorch PyTorch Memory Snapshot Visualizer Total running time of the script: (0 minutes 14.368 seconds) Download Jupyter notebook: mosaic_memory_profiling_tutorial.ipynb Download Python source code: mosaic_memory_profiling_tutorial.py Download zipped: mosaic_memory_profiling_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/mosaic_memory_profiling_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>