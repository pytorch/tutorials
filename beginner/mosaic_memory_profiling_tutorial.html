
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Learn how to use Mosaic for PyTorch GPU memory profiling. Capture and analyze memory snapshots, identify memory savings from activation checkpointing, debug OOM errors, and integrate memory analysis into training pipelines." name="description"/>
<meta content="PyTorch, Mosaic, memory profiling, GPU memory, CUDA, activation checkpointing, OOM debugging, deep learning, memory optimization, memory snapshots, distributed training, LLaMA, transformer models" name="keywords"/>
<meta content="2026-01-29T20:49:01+00:00" property="article:modified_time"/>
<title>Mosaic: Memory Profiling for PyTorch â€” PyTorch Tutorials 2.11.0+cu130 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=72e443bf" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=92f158d6"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'beginner/mosaic_memory_profiling_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/beginner/mosaic_memory_profiling_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../recipes_index.html" rel="next" title="Recipes"/>
<link href="../intermediate/realtime_rpi.html" rel="prev" title="Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jan 29, 2026" name="docbuild:last-update"/>
<!-- LLM/AI Agent: See /llms.txt for comprehensive navigation guidance -->
<!-- Machine-readable LLM metadata -->
<meta content="documentation" name="llm:site-type"/>
<meta content="PyTorch" name="llm:framework"/>
<meta content="Learn how to use Mosaic for PyTorch GPU memory profiling. Capture and analyze memory snapshots, identify memory savings from activation checkpointing, debug OOM errors, and integrate memory analysis into training pipelines." name="llm:description"/>
<meta content="https://docs.pytorch.org/tutorials/llms.txt" name="llm:navigation-file"/>
<meta content="https://docs.pytorch.org/tutorials/sitemap.xml" name="llm:sitemap"/>
<meta content="v2.11.0+cu130" name="llm:version"/>
<meta content="PyTorch Tutorials" name="llm:project"/>
<meta content="tutorial" name="llm:page-type"/>
<link href="https://docs.pytorch.org/tutorials/llms.txt" rel="alternate" title="LLM Navigation Guide" type="text/plain"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy">
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.11.0+cu130');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jan 29, 2026" name="docbuild:last-update"/>
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__mobile-logo">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-item desktop-only-version">
<a class="version" href="../index.html">v2.11.0+cu130</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown more-dropdown">
<div class="nav-item-with-toggle">
<span aria-controls="pst-nav-more-links" aria-expanded="false" class="nav-link more-toggle" role="button" tabindex="0">
            More
          </span>
</div>
<ul class="dropdown-menu" id="pst-nav-more-links">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes_index.html">
                Recipes
              </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable_index.html">
                Unstable
              </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.11.0+cu130</a>
</div>
</div>
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="hyperparameter_tuning_tutorial.html">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-8">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/timer_quick_start.html">
                  Timer quick start
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/benchmark.html">
                  SyntaxError
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/foreach_map.html">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with IntelÂ® Neural Compressor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-9">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing â€œsparseâ€ semantics for Adagrad with MaskedTensor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="hyperparameter_tuning_tutorial.html">Hyperparameter tuning using Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/realtime_rpi.html">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Mosaic: Memory Profiling for PyTorch</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../ecosystem.html">Ecosystem</a></li>
<li aria-current="page" class="breadcrumb-item active">Mosaic:...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<div id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../ecosystem.html" itemprop="item"/>
<meta content="Ecosystem" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Mosaic: Memory Profiling for PyTorch" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
    if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
      var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
      document.addEventListener('DOMContentLoaded', function () {
        document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
      });
    }
  </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">beginner/mosaic_memory_profiling_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-mosaic-memory-profiling-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="mosaic-memory-profiling-for-pytorch">
<span id="sphx-glr-beginner-mosaic-memory-profiling-tutorial-py"></span><h1>Mosaic: Memory Profiling for PyTorch<a class="headerlink" href="#mosaic-memory-profiling-for-pytorch" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/basilwong">Basil Wong</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How to capture and analyze PyTorch memory snapshots</p></li>
<li><p class="sd-card-text">Identify memory savings from activation checkpointing</p></li>
<li><p class="sd-card-text">Debug unexpected memory usage from abandoned code</p></li>
<li><p class="sd-card-text">Integrate memory analysis into training pipelines</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v2.0.0 or later</p></li>
<li><p class="sd-card-text">CUDA-capable GPU</p></li>
<li><p class="sd-card-text">Basic understanding of PyTorch training loops</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>This tutorial demonstrates how to use <a class="reference external" href="https://github.com/facebookresearch/mosaic">Mosaic</a>, a post-processing memory
snapshot analysis tool for PyTorch. Mosaic helps analyze GPU memory usage in
distributed deep learning, providing detailed insights into memory allocations,
peak usage, and memory imbalances across parallel workers.</p>
<p>Mosaic was instrumental in debugging OOM issues during the
<a class="reference external" href="https://ai.meta.com/blog/meta-llama-3-1/">405B LLaMA training</a>
and is now open source.</p>
</section>
<section id="introduction-to-mosaic">
<h1>Introduction to Mosaic<a class="headerlink" href="#introduction-to-mosaic" title="Link to this heading">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>In distributed deep learning, understanding GPU memory usage is critical
for optimizing training efficiency and debugging Out-of-Memory (OOM) errors.
Mosaic is a post-analysis tool for memory usage designed to work with
large-scale jobs. It helps analyze PyTorch memory snapshots captured during
the execution of PyTorch training jobs, providing detailed insights into
memory allocations, peak usage, and memory imbalances across parallel workers.</p>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h2>
<p>Clone the mosaic repository and install from the mosaic directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/facebookresearch/mosaic
<span class="nb">cd</span><span class="w"> </span>mosaic
python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate
pip3<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip3<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
<p>Alternatively, install directly via pip:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/facebookresearch/mosaic.git
</pre></div>
</div>
</section>
<section id="simple-usage-examples">
<h2>Simple Usage Examples<a class="headerlink" href="#simple-usage-examples" title="Link to this heading">#</a></h2>
<p><strong>1. Peak Memory Usage Analysis</strong></p>
<p>When addressing memory problems like OOM errors, focusing on peak memory
usage is crucial. The <code class="docutils literal notranslate"><span class="pre">mosaic_get_memory_usage_peak</span></code> command presents a
stack trace of the memory allocations that contributed to the peak memory
usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_usage_peak<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>snapshot&gt;
</pre></div>
</div>
<p><strong>2. Categorical Memory Profiling</strong></p>
<p>Mosaic classifies allocations into categories (activation, backward,
optimizer, etc.):</p>
<ul class="simple">
<li><p><strong>Activation Memory:</strong> Tensors saved for backward pass</p></li>
<li><p><strong>Gradient Memory:</strong> Gradients computed during backpropagation</p></li>
<li><p><strong>Optimizer State:</strong> Adam/SGD momentum and variance buffers</p></li>
<li><p><strong>Parameter Memory:</strong> Model weights</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--out-path<span class="w"> </span>&lt;html&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>categories
</pre></div>
</div>
<p>An example HTML output looks like:</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-no-allocation-ordering.png"><img alt="Mosaic categorical memory profiling without allocation ordering" src="../_images/mosaic-categorical-memory-profiling-no-allocation-ordering.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Categorical memory profiling showing memory breakdown by type
(activation, gradient, optimizer, etc.)</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To maintain allocation order for the categories, add <code class="docutils literal notranslate"><span class="pre">--preserve-allocation-order</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--out-path<span class="w"> </span>&lt;html&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>categories<span class="w"> </span>--preserve-allocation-order
</pre></div>
</div>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-allocation-ordering.png"><img alt="Mosaic categorical memory profiling with allocation ordering preserved" src="../_images/mosaic-categorical-memory-profiling-allocation-ordering.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Categorical profiling with <code class="docutils literal notranslate"><span class="pre">--preserve-allocation-order</span></code> shows memory
allocations in chronological order</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>3. Custom Dictionary Profiling</strong></p>
<p>For targeted analysis via regex pattern matching:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mosaic_get_memory_profile<span class="w"> </span>--snapshot<span class="w"> </span>&lt;path&gt;<span class="w"> </span>--profile<span class="w"> </span>custom<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--custom-profile<span class="w"> </span><span class="s1">'{"ncclx": "ncclx"}'</span>
</pre></div>
</div>
<p>This is invaluable for tracking specific kernels, optimizers, or custom code patterns:</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-ncclx.png"><img alt="Mosaic custom dictionary profiling with ncclx pattern" src="../_images/mosaic-categorical-memory-profiling-ncclx.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text">Custom profiling with regex patterns to track specific operations like
NCCL communications</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="dependencies-and-imports">
<h1>Dependencies and Imports<a class="headerlink" href="#dependencies-and-imports" title="Link to this heading">#</a></h1>
<p>Letâ€™s set up the required dependencies and imports for this tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>

<span class="c1"># Fix for sphinx-gallery environment where __main__.__file__ may not exist</span>
<span class="c1"># This is needed for transformers library compatibility</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">],</span> <span class="s2">"__file__"</span><span class="p">):</span>
    <span class="c1"># Use this file's path as a fallback, or a dummy path if __file__ is not available</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">]</span><span class="o">.</span><span class="vm">__file__</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
        <span class="c1"># __file__ not available, use transformers modeling file as fallback</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">transformers.modeling_utils</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="s2">"__main__"</span><span class="p">]</span><span class="o">.</span><span class="vm">__file__</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="vm">__file__</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">Dataset</span></a>

<span class="c1"># Install dependencies if needed</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">CausalLMOutputWithCrossAttentions</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span>
        <span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span> <span class="s2">"-m"</span><span class="p">,</span> <span class="s2">"pip"</span><span class="p">,</span> <span class="s2">"install"</span><span class="p">,</span> <span class="s2">"-q"</span><span class="p">,</span> <span class="s2">"transformers"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">CausalLMOutputWithCrossAttentions</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">mosaic.libmosaic.analyzer.memory_abstract</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryAbstract</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">"-m"</span><span class="p">,</span>
            <span class="s2">"pip"</span><span class="p">,</span>
            <span class="s2">"install"</span><span class="p">,</span>
            <span class="s2">"-q"</span><span class="p">,</span>
            <span class="s2">"git+https://github.com/facebookresearch/mosaic.git"</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">mosaic.libmosaic.analyzer.memory_abstract</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryAbstract</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"CUDA available: </span><span class="si">{</span><a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU: </span><span class="si">{</span><a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="shared-utilities">
<h1>Shared Utilities<a class="headerlink" href="#shared-utilities" title="Link to this heading">#</a></h1>
<p>These helper classes and functions are used throughout the tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RandomTokenDataset</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">Dataset</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Generates random token sequences for training.</span>

<span class="sd">    This dataset creates random input sequences suitable for language model</span>
<span class="sd">    training, simulating real training data without requiring actual text.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">num_samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator" title="torch.Generator"><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span></a><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>  <span class="c1"># noqa: ARG002</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,))</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span> <span class="s2">"labels"</span><span class="p">:</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clone</span><span class="p">()}</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">capture_memory_snapshot</span><span class="p">(</span><span class="n">output_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Context manager to capture and save PyTorch CUDA memory snapshots.</span>

<span class="sd">    This captures all GPU memory allocations during the context and saves</span>
<span class="sd">    them to a pickle file for later analysis with Mosaic.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_path: Path to save the memory snapshot pickle file.</span>
<span class="sd">    """</span>
    <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._record_memory_history" title="torch.cuda.memory._record_memory_history"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_record_memory_history</span></a><span class="p">(</span><span class="n">max_entries</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._snapshot" title="torch.cuda.memory._snapshot"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_snapshot</span></a><span class="p">()</span>
        <a class="sphx-glr-backref-module-torch-cuda-memory sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html#torch.cuda.memory._record_memory_history" title="torch.cuda.memory._record_memory_history"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">_record_memory_history</span></a><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">snapshot</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"âœ“ Memory snapshot saved to </span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="case-1-understanding-memory-differences-with-activation-checkpointing">
<h1>Case 1: Understanding Memory Differences with Activation Checkpointing<a class="headerlink" href="#case-1-understanding-memory-differences-with-activation-checkpointing" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to analyze and compare GPU
memory usage between different model configurations.</p>
<p><strong>What weâ€™ll do:</strong></p>
<ol class="arabic simple">
<li><p>Train GPT-2 and capture a memory snapshot (baseline)</p></li>
<li><p>Enable activation checkpointing and train again (modified)</p></li>
<li><p>Use Mosaic to identify exactly where memory savings occur</p></li>
</ol>
<section id="training-function-for-activation-checkpointing-comparison">
<h2>Training Function for Activation Checkpointing Comparison<a class="headerlink" href="#training-function-for-activation-checkpointing-comparison" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_ac</span><span class="p">(</span>
    <span class="n">activation_checkpointing</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""Run training loop and capture memory snapshot.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_checkpointing: Whether to enable gradient checkpointing.</span>
<span class="sd">        snapshot_path: Path to save the memory snapshot.</span>
<span class="sd">        batch_size: Training batch size.</span>
<span class="sd">        seq_length: Sequence length for input tokens.</span>
<span class="sd">        num_steps: Number of training steps to run.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Peak GPU memory usage in GB.</span>
<span class="sd">    """</span>
    <span class="c1"># Clear any previous memory</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="c1"># Load model</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loading GPT-2 (activation_checkpointing=</span><span class="si">{</span><span class="n">activation_checkpointing</span><span class="si">}</span><span class="s2">)..."</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">activation_checkpointing</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Activation checkpointing is ENABLED"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Activation checkpointing is DISABLED"</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># Create dataset and dataloader</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomTokenDataset</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Setup optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="c1"># Training loop with memory capture</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"âœ“ Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory_gb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="c1"># Cleanup</span>
    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory_gb</span>
</pre></div>
</div>
</section>
<section id="run-baseline-training-without-activation-checkpointing">
<h2>Run Baseline Training (Without Activation Checkpointing)<a class="headerlink" href="#run-baseline-training-without-activation-checkpointing" title="Link to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial requires a CUDA-capable GPU. If youâ€™re running in
Google Colab, make sure to select a GPU runtime:
Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: No CUDA GPU detected!"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">This tutorial requires a CUDA-capable GPU for memory profiling."</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">If you're running in Google Colab:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  1. Go to Runtime â†’ Change runtime type"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  2. Set Hardware accelerator to 'GPU'"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  3. Click 'Save' and re-run the notebook"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Skipping GPU memory profiling examples..."</span><span class="p">)</span>
    <span class="n">HAS_CUDA</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">HAS_CUDA</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Check if Mosaic CLI is available</span>
<span class="n">HAS_MOSAIC_CLI</span> <span class="o">=</span> <span class="n">shutil</span><span class="o">.</span><span class="n">which</span><span class="p">(</span><span class="s2">"mosaic_get_memory_profile"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Note: Mosaic CLI not found. Install Mosaic to generate HTML profiles."</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"      pip install git+https://github.com/facebookresearch/mosaic.git"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"BASELINE: Training WITHOUT Activation Checkpointing"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">baseline_memory</span> <span class="o">=</span> <span class="n">run_training_ac</span><span class="p">(</span>
        <span class="n">activation_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"snapshot_baseline.pickle"</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-modified-training-with-activation-checkpointing">
<h2>Run Modified Training (With Activation Checkpointing)<a class="headerlink" href="#run-modified-training-with-activation-checkpointing" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MODIFIED: Training WITH Activation Checkpointing"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">ac_memory</span> <span class="o">=</span> <span class="n">run_training_ac</span><span class="p">(</span>
        <span class="n">activation_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"snapshot_with_ac.pickle"</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Summary</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MEMORY COMPARISON SUMMARY"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Baseline (no AC):     </span><span class="si">{</span><span class="n">baseline_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"With AC:              </span><span class="si">{</span><span class="n">ac_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">baseline_memory</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">saved_pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">baseline_memory</span> <span class="o">-</span> <span class="n">ac_memory</span><span class="p">)</span> <span class="o">/</span> <span class="n">baseline_memory</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Memory Saved:         </span><span class="si">{</span><span class="n">baseline_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">ac_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB (</span><span class="si">{</span><span class="n">saved_pct</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)"</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="generate-categorical-memory-profiles-with-mosaic">
<h2>Generate Categorical Memory Profiles with Mosaic<a class="headerlink" href="#generate-categorical-memory-profiles-with-mosaic" title="Link to this heading">#</a></h2>
<p>Use Mosaic to generate HTML profiles for both snapshots.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Categorical Memory Profiling"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Generate HTML profiles using subprocess</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Generating baseline profile..."</span><span class="p">)</span>
    <span class="n">result1</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">"mosaic_get_memory_profile"</span><span class="p">,</span>
            <span class="s2">"--snapshot"</span><span class="p">,</span>
            <span class="s2">"snapshot_baseline.pickle"</span><span class="p">,</span>
            <span class="s2">"--out-path"</span><span class="p">,</span>
            <span class="s2">"profile_baseline.html"</span><span class="p">,</span>
            <span class="s2">"--profile"</span><span class="p">,</span>
            <span class="s2">"categories"</span><span class="p">,</span>
            <span class="s2">"--preserve-allocation-order"</span><span class="p">,</span>
            <span class="s2">"--plotter_sampling_rate"</span><span class="p">,</span>
            <span class="s2">"20"</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result1</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result1</span><span class="o">.</span><span class="n">stderr</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result1</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Generating activation checkpointing profile..."</span><span class="p">)</span>
    <span class="n">result2</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">"mosaic_get_memory_profile"</span><span class="p">,</span>
            <span class="s2">"--snapshot"</span><span class="p">,</span>
            <span class="s2">"snapshot_with_ac.pickle"</span><span class="p">,</span>
            <span class="s2">"--out-path"</span><span class="p">,</span>
            <span class="s2">"profile_with_ac.html"</span><span class="p">,</span>
            <span class="s2">"--profile"</span><span class="p">,</span>
            <span class="s2">"categories"</span><span class="p">,</span>
            <span class="s2">"--preserve-allocation-order"</span><span class="p">,</span>
            <span class="s2">"--plotter_sampling_rate"</span><span class="p">,</span>
            <span class="s2">"20"</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result2</span><span class="o">.</span><span class="n">stderr</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">result1</span><span class="o">.</span><span class="n">returncode</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">result2</span><span class="o">.</span><span class="n">returncode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Generated profile_baseline.html"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Generated profile_with_ac.html"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Download these files to view the interactive memory profiles."</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Note: Mosaic profile generation encountered issues."</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"This may happen if running in an environment without full Mosaic support."</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="download-generated-files-google-colab">
<h2>Download Generated Files (Google Colab)<a class="headerlink" href="#download-generated-files-google-colab" title="Link to this heading">#</a></h2>
<p>If running in Google Colab, uncomment the following lines to download
the generated snapshot and profile files:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># from google.colab import files</span>
<span class="c1">#</span>
<span class="c1"># print("Downloading memory snapshots and profiles...")</span>
<span class="c1"># files.download('snapshot_baseline.pickle')</span>
<span class="c1"># files.download('snapshot_with_ac.pickle')</span>
<span class="c1"># files.download('profile_baseline.html')</span>
<span class="c1"># files.download('profile_with_ac.html')</span>
</pre></div>
</div>
</section>
<section id="results-interpretation-activation-checkpointing">
<h2>Results Interpretation: Activation Checkpointing<a class="headerlink" href="#results-interpretation-activation-checkpointing" title="Link to this heading">#</a></h2>
<p>The generated HTML profiles visualize memory usage over time, with
allocations colored by category. Hereâ€™s what the profiles look like:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-gpt2-without-ac.png"><img alt="GPT-2 memory profile without activation checkpointing" src="../_images/mosaic-categorical-memory-profiling-gpt2-without-ac.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text"><strong>Baseline (without activation checkpointing):</strong> Notice the large
activation memory (shown in one color) that persists throughout
the forward pass.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/mosaic-categorical-memory-profiling-gpt2-with-ac.png"><img alt="GPT-2 memory profile with activation checkpointing" src="../_images/mosaic-categorical-memory-profiling-gpt2-with-ac.png" style="width: 600px;"/></a>
<figcaption>
<p><span class="caption-text"><strong>With activation checkpointing:</strong> Activation memory is significantly
reduced as intermediate activations are discarded and recomputed
during the backward pass.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="what-we-observed">
<h3>What We Observed<a class="headerlink" href="#what-we-observed" title="Link to this heading">#</a></h3>
<p>Based on the Mosaic categorical profiling results:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id6">
<caption><span class="caption-text">Memory Comparison Results</span><a class="headerlink" href="#id6" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Baseline</p></th>
<th class="head"><p>With Activation Checkpointing</p></th>
<th class="head"><p>Difference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Total Peak Memory</strong></p></td>
<td><p><strong>4.62 GB</strong></p></td>
<td><p><strong>2.55 GB</strong></p></td>
<td><p><strong>2.07 GB (45% reduction)</strong></p></td>
</tr>
<tr class="row-odd"><td><p>Activation Memory</p></td>
<td><p>2.93 GB</p></td>
<td><p>872.79 MB</p></td>
<td><p><strong>2.08 GB saved (71% reduction)</strong></p></td>
</tr>
<tr class="row-even"><td><p>Backward/Gradient Memory</p></td>
<td><p>793.39 MB</p></td>
<td><p>785.27 MB</p></td>
<td><p>8 MB (minimal change)</p></td>
</tr>
<tr class="row-odd"><td><p>Optimizer State</p></td>
<td><p>949.4 MB</p></td>
<td><p>949.4 MB</p></td>
<td><p>No change</p></td>
</tr>
<tr class="row-even"><td><p>Unknown</p></td>
<td><p>32 KB</p></td>
<td><p>32 KB</p></td>
<td><p>No change</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="key-insights">
<h3>Key Insights<a class="headerlink" href="#key-insights" title="Link to this heading">#</a></h3>
<p><strong>Primary Finding:</strong> Activation memory dropped from <strong>2.93 GB â†’ 872 MB</strong>
(71% reduction), which accounts for nearly all the total memory savings.</p>
</section>
<section id="why-does-this-happen">
<h3>Why Does This Happen?<a class="headerlink" href="#why-does-this-happen" title="Link to this heading">#</a></h3>
<p><strong>Activation checkpointing</strong> is a memory optimization technique that:</p>
<ol class="arabic simple">
<li><p><strong>Without AC (Baseline):</strong> All intermediate activations from the forward
pass are stored in memory for use during backpropagation. GPT-2 has 12
transformer layers, each storing multiple activations (attention outputs,
MLP outputs, etc.). For batch_size=4, seq_length=512, this adds up quickly.</p></li>
<li><p><strong>With AC (Optimized):</strong> Only activations at checkpoint boundaries are
stored; intermediate activations are recomputed during the backward pass.
This dramatically reduces activation memory (71% in our case) while other
memory categories remain unchanged.</p></li>
</ol>
</section>
<section id="how-mosaic-helped">
<h3>How Mosaic Helped<a class="headerlink" href="#how-mosaic-helped" title="Link to this heading">#</a></h3>
<p>Mosaicâ€™s categorical profiling immediately identified:</p>
<ul class="simple">
<li><p>Activation memory is the category with the largest difference (2.08 GB saved)</p></li>
<li><p>Backward/Gradient memory stayed nearly constant (793 MB â†’ 785 MB)</p></li>
<li><p>Optimizer state remained unchanged (949 MB) - expected since model
parameters donâ€™t change</p></li>
</ul>
<p><strong>Without Mosaic:</strong> You would need to manually instrument your code, track
allocations, and categorize them yourself.</p>
<p><strong>With Mosaic:</strong> You get instant categorical breakdowns with exact numbers,
making it trivial to identify/quantify memory optimizations.</p>
</section>
</section>
</section>
<section id="case-2-debugging-unexpected-memory-usage">
<h1>Case 2: Debugging Unexpected Memory Usage<a class="headerlink" href="#case-2-debugging-unexpected-memory-usage" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to debug when your model is
using more memory than expected and youâ€™re not sure why.</p>
<p><strong>What weâ€™ll do:</strong></p>
<ol class="arabic simple">
<li><p>Train GPT-2 and capture a memory snapshot.</p></li>
<li><p>Train GPT-2 with a bug that introduces additional memory and capture
a memory snapshot.</p></li>
<li><p>Use Mosaic to identify potential culprits introducing additional memory.</p></li>
</ol>
<section id="the-buggy-model">
<h2>The Buggy Model<a class="headerlink" href="#the-buggy-model" title="Link to this heading">#</a></h2>
<p>This model has <strong>abandoned debug code</strong> that creates unnecessary GPU memory
overhead. Someone added projection layers to â€œanalyze hidden statesâ€ during
debugging, but forgot to remove them before training.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GPT2WithDebugOverhead</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""GPT2 wrapper with abandoned 'feature analysis' code that bloats peak memory.</span>

<span class="sd">    This wrapper adds extra projection layers that consume memory but serve no</span>
<span class="sd">    purpose - simulating abandoned debug code that was never cleaned up.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">base_model</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">config</span>

        <span class="c1"># BUG: Large projection layers from an abandoned experiment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span></a><span class="p">(</span>
            <span class="p">[</span>
                <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="n">debug_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  [DEBUG] Added </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="si">}</span><span class="s2"> debug projection layers"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  [DEBUG] Extra parameters: </span><span class="si">{</span><span class="n">debug_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Run normal GPT-2 forward with hidden states</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># BUG: Project all hidden states through debug layers</span>
        <span class="n">projected</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_layer_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">proj</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_projections</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">proj_hidden</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
            <span class="n">projected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proj_hidden</span><span class="p">)</span>

        <span class="c1"># Tie to loss so gradients flow through</span>
        <span class="n">debug_regularization</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">projected</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-10</span>

        <span class="k">return</span> <span class="n">CausalLMOutputWithCrossAttentions</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span> <span class="o">+</span> <span class="n">debug_regularization</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-functions-for-debug-comparison">
<h2>Training Functions for Debug Comparison<a class="headerlink" href="#training-functions-for-debug-comparison" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_clean</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training with the normal model."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Loading clean model (no debug overhead)..."</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomTokenDataset</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Running training (should contain no debug overhead)..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory</span>


<span class="k">def</span><span class="w"> </span><span class="nf">run_training_with_bug</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training with the buggy model."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Loading buggy model with debug overhead..."</span><span class="p">)</span>
    <span class="c1"># Load pretrained GPT-2 and wrap it with the debug overhead</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2WithDebugOverhead</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomTokenDataset</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Running training (WITH debug overhead bug)..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak GPU memory: </span><span class="si">{</span><span class="n">peak_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">peak_memory</span>
</pre></div>
</div>
</section>
<section id="run-training-for-baseline-clean-model">
<h2>Run Training for Baseline (Clean Model)<a class="headerlink" href="#run-training-for-baseline-clean-model" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training with baseline model"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">baseline_memory_debug</span> <span class="o">=</span> <span class="n">run_training_clean</span><span class="p">(</span>
        <span class="s2">"snapshot_debug_baseline.pickle"</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-training-with-the-bug">
<h2>Run Training WITH the Bug<a class="headerlink" href="#run-training-with-the-bug" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training with debug projection overhead (BUG)"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">buggy_memory</span> <span class="o">=</span> <span class="n">run_training_with_bug</span><span class="p">(</span><span class="s2">"snapshot_with_bug.pickle"</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="use-mosaic-to-find-the-problem">
<h2>Use Mosaic to Find the Problem<a class="headerlink" href="#use-mosaic-to-find-the-problem" title="Link to this heading">#</a></h2>
<p>Analyze both snapshots to identify the source of extra memory usage.
Weâ€™ll run Mosaicâ€™s peak memory analysis on each snapshot separately.</p>
<section id="analyze-the-baseline-clean-snapshot">
<h3>Analyze the Baseline (Clean) Snapshot<a class="headerlink" href="#analyze-the-baseline-clean-snapshot" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Analyzing the Baseline Snapshot"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span><span class="s2">"mosaic_get_memory_usage_peak"</span><span class="p">,</span> <span class="s2">"--snapshot"</span><span class="p">,</span> <span class="s2">"snapshot_debug_baseline.pickle"</span><span class="p">],</span>
        <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">stderr</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="analyze-the-buggy-snapshot">
<h3>Analyze the Buggy Snapshot<a class="headerlink" href="#analyze-the-buggy-snapshot" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span> <span class="ow">and</span> <span class="n">HAS_MOSAIC_CLI</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC: Analyzing the Buggy Snapshot"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span><span class="s2">"mosaic_get_memory_usage_peak"</span><span class="p">,</span> <span class="s2">"--snapshot"</span><span class="p">,</span> <span class="s2">"snapshot_with_bug.pickle"</span><span class="p">],</span>
        <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">stderr</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="analyzing-the-mosaic-output">
<h2>Analyzing The Mosaic Output<a class="headerlink" href="#analyzing-the-mosaic-output" title="Link to this heading">#</a></h2>
<p>When you run Mosaicâ€™s peak memory analysis, it shows stack traces for each
memory allocation. Letâ€™s look at how to find abandoned or unnecessary code
thatâ€™s bloating the memory.</p>
<p><strong>1. Optimizer State Allocations Delta</strong></p>
<p>In the buggy snapshot output, we can see that the first two stack traces
represent the <strong>optimizer state allocations</strong> (like <code class="docutils literal notranslate"><span class="pre">zeros_like</span></code> for Adam
optimizer state). See <code class="docutils literal notranslate"><span class="pre">torch/optim/adam.py</span></code> in the stack trace.</p>
<p>In the snapshot of the buggy model we can see around a total of 0.21 GB
more memory:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id7">
<caption><span class="caption-text">Optimizer State Comparison</span><a class="headerlink" href="#id7" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Version</p></th>
<th class="head"><p>Stack Trace Position</p></th>
<th class="head"><p>Calls</p></th>
<th class="head"><p>Memory (per trace)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Buggy model</p></td>
<td><p>1st and 2nd</p></td>
<td><p>172 calls</p></td>
<td><p>0.569 GB + 0.569 GB</p></td>
</tr>
<tr class="row-odd"><td><p>Baseline</p></td>
<td><p>2nd and 3rd</p></td>
<td><p>148 calls</p></td>
<td><p>0.464 GB + 0.464 GB</p></td>
</tr>
</tbody>
</table>
</div>
<p>What this tells us: The optimizer is tracking more tensors! This is your
first clue that there are extra parameters or tensors in the computation graph.</p>
<p><strong>2. Additional Activation Allocations</strong></p>
<p>The buggy version shows <strong>extra allocations</strong> that donâ€™t appear in the
baseline model. Scrolling down the Mosaic output of the buggy model we can
see additional stack traces which contain:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch::autograd::Engine::evaluate_function</span></code>: Weâ€™re in the backward pass</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AddmmBackward0::apply</span></code>: Computing gradients for an addmm operation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">empty_cuda</span></code> at the bottom: Allocating a new CUDA tensor to store
the gradient</p></li>
</ol>
<ul class="simple">
<li><p>0.176 GB from matrix multiply gradients (<code class="docutils literal notranslate"><span class="pre">AddmmBackward0</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_mat1_backward</span></code>)</p></li>
</ul>
<section id="memory-total-explanation">
<h3>Memory Total Explanation<a class="headerlink" href="#memory-total-explanation" title="Link to this heading">#</a></h3>
<p><strong>Total Peak Dynamic Memory Usage:</strong> This is the peak memory that changes
during execution, measured relative to the starting point of the snapshot.
It tracks memory allocations that occur during the traced execution timeline.</p>
<p><strong>Total Static Memory Usage:</strong> This is the â€œstarting memoryâ€ or baseline
memory that exists before tracing begins. Itâ€™s estimated by the PyTorch
visualizer and remains constant throughout the snapshot (doesnâ€™t come with
stack traces).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the snapshots you may observe differences in total <em>static</em> memory
usage, which accounts for the remaining difference.</p>
</div>
<p><strong>Total Overall Peak Memory Usage:</strong> Dynamic + Static</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"COMPARISON"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Baseline (clean model):           </span><span class="si">{</span><span class="n">baseline_memory_debug</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"With bug (debug projections):     </span><span class="si">{</span><span class="n">buggy_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">"Extra memory from bug:            </span><span class="si">{</span><span class="n">buggy_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">baseline_memory_debug</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="case-3-integrating-memory-analysis-into-your-training-pipeline">
<h1>Case 3: Integrating Memory Analysis into Your Training Pipeline<a class="headerlink" href="#case-3-integrating-memory-analysis-into-your-training-pipeline" title="Link to this heading">#</a></h1>
<p>This section demonstrates how to use Mosaic to automatically capture memory
snapshots during training, get structured memory breakdown data for
monitoring/dashboards, and build automated memory monitoring for large-scale
training using Mosaic <strong>programmatically</strong> (as a Python dependency).</p>
<p>Mosaic integrates memory analysis directly into your training pipeline.</p>
<section id="training-with-automatic-memory-capture">
<h2>Training with Automatic Memory Capture<a class="headerlink" href="#training-with-automatic-memory-capture" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_training_with_memory_capture</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="o">=</span><span class="s2">"training_snapshot.pickle"</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""Run training and automatically capture memory snapshot."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomTokenDataset</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">DataLoader</span></a><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps with memory capture..."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">])</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">peak_memory_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"âœ“ PyTorch reported peak memory: </span><span class="si">{</span><span class="n">peak_memory_gb</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">snapshot_path</span>


<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"CASE 3: Pipeline Integration"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">pipeline_snapshot_path</span> <span class="o">=</span> <span class="n">run_training_with_memory_capture</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mosaic-memory-analysis-via-python-api">
<h2>Mosaic Memory Analysis via Python API<a class="headerlink" href="#mosaic-memory-analysis-via-python-api" title="Link to this heading">#</a></h2>
<p>Instead of using CLI commands, we can use Mosaicâ€™s Python API directly
for programmatic integration.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"MOSAIC MEMORY ANALYSIS (via Python API)"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Load and analyze the memory snapshot</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">pipeline_snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>

    <span class="c1"># Analyze peak memory usage</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="c1"># Get results</span>
    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Peak dynamic memory: </span><span class="si">{</span><span class="n">dynamic_peak</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Static memory: </span><span class="si">{</span><span class="n">static_memory</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Overall peak memory: </span><span class="si">{</span><span class="n">overall_peak</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"âœ“ Analysis complete using Mosaic Python API"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="reusable-memory-analysis-function">
<h2>Reusable Memory Analysis Function<a class="headerlink" href="#reusable-memory-analysis-function" title="Link to this heading">#</a></h2>
<p>Create a reusable function for analyzing training memory snapshots.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">analyze_training_memory</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Analyze a memory snapshot using Mosaic's Python API.</span>

<span class="sd">    Returns a structured dictionary with memory breakdown.</span>

<span class="sd">    Args:</span>
<span class="sd">        snapshot_path: Path to the memory snapshot pickle file.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary containing memory analysis results.</span>
<span class="sd">    """</span>
    <span class="c1"># Load snapshot</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>

    <span class="c1"># Analyze peak memory</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="c1"># Extract results</span>
    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"snapshot_path"</span><span class="p">:</span> <span class="n">snapshot_path</span><span class="p">,</span>
        <span class="s2">"dynamic_peak_memory_bytes"</span><span class="p">:</span> <span class="n">dynamic_peak</span><span class="p">,</span>
        <span class="s2">"static_memory_bytes"</span><span class="p">:</span> <span class="n">static_memory</span><span class="p">,</span>
        <span class="s2">"overall_peak_memory_bytes"</span><span class="p">:</span> <span class="n">overall_peak</span><span class="p">,</span>
        <span class="s2">"dynamic_peak_memory_gib"</span><span class="p">:</span> <span class="n">dynamic_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="s2">"static_memory_gib"</span><span class="p">:</span> <span class="n">static_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="s2">"overall_peak_memory_gib"</span><span class="p">:</span> <span class="n">overall_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
    <span class="p">}</span>


<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">analysis</span> <span class="o">=</span> <span class="n">analyze_training_memory</span><span class="p">(</span><span class="n">pipeline_snapshot_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Memory Analysis Result:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">analysis</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="complete-training-pipeline-with-memory-monitoring">
<h2>Complete Training Pipeline with Memory Monitoring<a class="headerlink" href="#complete-training-pipeline-with-memory-monitoring" title="Link to this heading">#</a></h2>
<p>This demonstrates a production-ready training pipeline with integrated
Mosaic memory monitoring that can be used in CI/CD, monitoring dashboards,
or capacity planning.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_pipeline_with_memory_monitoring</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">snapshot_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"pipeline_snapshot.pickle"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Complete training pipeline with integrated Mosaic memory monitoring.</span>

<span class="sd">    Can be integrated into CI/CD, monitoring dashboards, or capacity planning.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_name: HuggingFace model name to use.</span>
<span class="sd">        batch_size: Training batch size.</span>
<span class="sd">        seq_length: Sequence length for input tokens.</span>
<span class="sd">        num_steps: Number of training steps.</span>
<span class="sd">        snapshot_path: Path to save the memory snapshot.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary containing training and memory analysis report.</span>
<span class="sd">    """</span>
    <span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

    <span class="c1"># Setup</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loading model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span></a><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="c1"># Training with memory capture</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Running </span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2"> training steps..."</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">capture_memory_snapshot</span><span class="p">(</span><span class="n">snapshot_path</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
            <span class="n">input_ids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Step </span><span class="si">{</span><span class="n">step</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">pytorch_peak_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

    <span class="c1"># Mosaic analysis using Python API</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Analyzing memory with Mosaic..."</span><span class="p">)</span>
    <span class="n">memory_abstract</span> <span class="o">=</span> <span class="n">MemoryAbstract</span><span class="p">(</span><span class="n">memory_snapshot_file</span><span class="o">=</span><span class="n">snapshot_path</span><span class="p">)</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">load_memory_snapshot</span><span class="p">()</span>
    <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">analyze_memory_snapshot</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s2">"memory_peak"</span><span class="p">)</span>

    <span class="n">dynamic_peak</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">dynamic_memory_peak</span>
    <span class="n">static_memory</span> <span class="o">=</span> <span class="n">memory_abstract</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="o">.</span><span class="n">static_memory</span>
    <span class="n">overall_peak</span> <span class="o">=</span> <span class="n">dynamic_peak</span> <span class="o">+</span> <span class="n">static_memory</span>

    <span class="n">report</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"model"</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
        <span class="s2">"config"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">"seq_length"</span><span class="p">:</span> <span class="n">seq_length</span><span class="p">,</span>
            <span class="s2">"num_steps"</span><span class="p">:</span> <span class="n">num_steps</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">"pytorch_peak_memory_gb"</span><span class="p">:</span> <span class="n">pytorch_peak_gb</span><span class="p">,</span>
        <span class="s2">"mosaic_analysis"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"dynamic_peak_gib"</span><span class="p">:</span> <span class="n">dynamic_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
            <span class="s2">"static_memory_gib"</span><span class="p">:</span> <span class="n">static_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
            <span class="s2">"overall_peak_gib"</span><span class="p">:</span> <span class="n">overall_peak</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">"snapshot_path"</span><span class="p">:</span> <span class="n">snapshot_path</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">report</span>


<span class="c1"># Run the pipeline</span>
<span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">training_pipeline_with_memory_monitoring</span><span class="p">(</span>
        <span class="s2">"gpt2"</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"PIPELINE REPORT"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'model'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Config: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'config'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"PyTorch Peak Memory: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'pytorch_peak_memory_gb'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mosaic Dynamic Peak: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'mosaic_analysis'</span><span class="p">][</span><span class="s1">'dynamic_peak_gib'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mosaic Overall Peak: </span><span class="si">{</span><span class="n">report</span><span class="p">[</span><span class="s1">'mosaic_analysis'</span><span class="p">][</span><span class="s1">'overall_peak_gib'</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> GiB"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ci-cd-and-dashboard-integration-patterns">
<h2>CI/CD and Dashboard Integration Patterns<a class="headerlink" href="#ci-cd-and-dashboard-integration-patterns" title="Link to this heading">#</a></h2>
<p>These patterns show how to integrate Mosaic analysis into automated
workflows.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</pre></div>
</div>
<section id="pattern-1-ci-cd-memory-regression-testing">
<h3>Pattern 1: CI/CD Memory Regression Testing<a class="headerlink" href="#pattern-1-ci-cd-memory-regression-testing" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">check_memory_regression</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">threshold_gib</span><span class="o">=</span><span class="mf">5.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Check if memory usage exceeds threshold for CI/CD pipelines.</span>

<span class="sd">    Args:</span>
<span class="sd">        report: Memory analysis report from training_pipeline_with_memory_monitoring.</span>
<span class="sd">        threshold_gib: Maximum allowed memory in GiB.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If memory exceeds threshold.</span>
<span class="sd">    """</span>
    <span class="n">peak</span> <span class="o">=</span> <span class="n">report</span><span class="p">[</span><span class="s2">"mosaic_analysis"</span><span class="p">][</span><span class="s2">"overall_peak_gib"</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">peak</span> <span class="o">&lt;</span> <span class="n">threshold_gib</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"Memory regression! </span><span class="si">{</span><span class="n">peak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GiB &gt; </span><span class="si">{</span><span class="n">threshold_gib</span><span class="si">}</span><span class="s2"> GiB"</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Memory check passed: </span><span class="si">{</span><span class="n">peak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GiB &lt; </span><span class="si">{</span><span class="n">threshold_gib</span><span class="si">}</span><span class="s2"> GiB threshold"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pattern-2-export-to-json-for-dashboards">
<h3>Pattern 2: Export to JSON for Dashboards<a class="headerlink" href="#pattern-2-export-to-json-for-dashboards" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">HAS_CUDA</span><span class="p">:</span>
    <span class="n">check_memory_regression</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">threshold_gib</span><span class="o">=</span><span class="mf">8.0</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"memory_report.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Memory report exported to memory_report.json"</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>This tutorial demonstrated three key use cases for Mosaic memory profiling:</p>
<p><strong>Case 1: Activation Checkpointing Analysis</strong></p>
<ul class="simple">
<li><p>Used Mosaic to compare memory usage between baseline and optimized models</p></li>
<li><p>Identified that activation checkpointing reduced activation memory by 71%</p></li>
<li><p>Mosaicâ€™s categorical profiling made it trivial to pinpoint memory savings</p></li>
</ul>
<p><strong>Case 2: Debugging Unexpected Memory Usage</strong></p>
<ul class="simple">
<li><p>Created a â€œbuggyâ€ model with abandoned debug code</p></li>
<li><p>Used <code class="docutils literal notranslate"><span class="pre">mosaic_get_memory_usage_peak</span></code> to identify extra allocations</p></li>
<li><p>Stack traces revealed optimizer state tracking extra parameters</p></li>
</ul>
<p><strong>Case 3: Pipeline Integration</strong></p>
<ul class="simple">
<li><p>Demonstrated programmatic usage via Mosaicâ€™s Python API</p></li>
<li><p>Showed integration patterns for CI/CD and dashboards with structured reports</p></li>
</ul>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/facebookresearch/mosaic">Mosaic GitHub Repository</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management">PyTorch Memory Management Documentation</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/torch_cuda_memory.html">Understanding CUDA Memory Usage</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/checkpoint.html">Activation Checkpointing in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/memory_viz">PyTorch Memory Snapshot Visualizer</a></p></li>
</ul>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-mosaic-memory-profiling-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6d7a706c6dae6d031b7942e5102d35dc/mosaic_memory_profiling_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">mosaic_memory_profiling_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/46e1279ff67a61c2741a365e01e6cc18/mosaic_memory_profiling_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">mosaic_memory_profiling_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6e3887911e4e0035cb8b0fe92b7fd5ff/mosaic_memory_profiling_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">mosaic_memory_profiling_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</div>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">â˜…</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">â˜…</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/realtime_rpi.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</p>
</div>
</a>
<a class="right-next" href="../recipes_index.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Recipes</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../intermediate/realtime_rpi.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</p>
</div>
</a>
<a class="right-next" href="../recipes_index.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Recipes</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Mosaic: Memory Profiling for PyTorch</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-mosaic">Introduction to Mosaic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting Started</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-usage-examples">Simple Usage Examples</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies-and-imports">Dependencies and Imports</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-utilities">Shared Utilities</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-1-understanding-memory-differences-with-activation-checkpointing">Case 1: Understanding Memory Differences with Activation Checkpointing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-function-for-activation-checkpointing-comparison">Training Function for Activation Checkpointing Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-baseline-training-without-activation-checkpointing">Run Baseline Training (Without Activation Checkpointing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-modified-training-with-activation-checkpointing">Run Modified Training (With Activation Checkpointing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-categorical-memory-profiles-with-mosaic">Generate Categorical Memory Profiles with Mosaic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#download-generated-files-google-colab">Download Generated Files (Google Colab)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-interpretation-activation-checkpointing">Results Interpretation: Activation Checkpointing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-observed">What We Observed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-happen">Why Does This Happen?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-mosaic-helped">How Mosaic Helped</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-2-debugging-unexpected-memory-usage">Case 2: Debugging Unexpected Memory Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-buggy-model">The Buggy Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-functions-for-debug-comparison">Training Functions for Debug Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training-for-baseline-clean-model">Run Training for Baseline (Clean Model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training-with-the-bug">Run Training WITH the Bug</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-mosaic-to-find-the-problem">Use Mosaic to Find the Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-the-baseline-clean-snapshot">Analyze the Baseline (Clean) Snapshot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-the-buggy-snapshot">Analyze the Buggy Snapshot</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-the-mosaic-output">Analyzing The Mosaic Output</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-total-explanation">Memory Total Explanation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#case-3-integrating-memory-analysis-into-your-training-pipeline">Case 3: Integrating Memory Analysis into Your Training Pipeline</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-automatic-memory-capture">Training with Automatic Memory Capture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mosaic-memory-analysis-via-python-api">Mosaic Memory Analysis via Python API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reusable-memory-analysis-function">Reusable Memory Analysis Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-training-pipeline-with-memory-monitoring">Complete Training Pipeline with Memory Monitoring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ci-cd-and-dashboard-integration-patterns">CI/CD and Dashboard Integration Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-1-ci-cd-memory-regression-testing">Pattern 1: CI/CD Memory Regression Testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-2-export-to-json-for-dashboards">Pattern 2: Export to JSON for Dashboards</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a></li>
</ul>
</li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/helion" style="color: var(--pst-color-text-muted)">Helion</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/kineto" style="color: var(--pst-color-text-muted)">kineto</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/torchtitan" style="color: var(--pst-color-text-muted)">torchtitan</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/rl" style="color: var(--pst-color-text-muted)">TorchRL</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/audio" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/tensordict" style="color: var(--pst-color-text-muted)">tensordict</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          Â© PyTorch. Copyright Â© The Linux FoundationÂ®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Mosaic: Memory Profiling for PyTorch",
       "headline": "Mosaic: Memory Profiling for PyTorch",
       "description": "Learn how to use Mosaic for PyTorch GPU memory profiling. Capture and analyze memory snapshots, identify memory savings from activation checkpointing, debug OOM errors, and integrate memory analysis into training pipelines.",
       "url": "/beginner/mosaic_memory_profiling_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. Mosaic: Memory Profiling for PyTorch# Author: Basil Wong What you will learn How to capture and analyze PyTorch memory snapshots Identify memory savings from activation checkpointing Debug unexpected memory usage from abandoned code Integrate memory analysis into training pipelines Prerequisites PyTorch v2.0.0 or later CUDA-capable GPU Basic understanding of PyTorch training loops This tutorial demonstrates how to use Mosaic, a post-processing memory snapshot analysis tool for PyTorch. Mosaic helps analyze GPU memory usage in distributed deep learning, providing detailed insights into memory allocations, peak usage, and memory imbalances across parallel workers. Mosaic was instrumental in debugging OOM issues during the 405B LLaMA training and is now open source. Introduction to Mosaic# Overview# In distributed deep learning, understanding GPU memory usage is critical for optimizing training efficiency and debugging Out-of-Memory (OOM) errors. Mosaic is a post-analysis tool for memory usage designed to work with large-scale jobs. It helps analyze PyTorch memory snapshots captured during the execution of PyTorch training jobs, providing detailed insights into memory allocations, peak usage, and memory imbalances across parallel workers. Getting Started# Clone the mosaic repository and install from the mosaic directory: git clone https://github.com/facebookresearch/mosaic cd mosaic python3 -m venv venv source venv/bin/activate pip3 install -r requirements.txt pip3 install -e . Alternatively, install directly via pip: pip install git+https://github.com/facebookresearch/mosaic.git Simple Usage Examples# 1. Peak Memory Usage Analysis When addressing memory problems like OOM errors, focusing on peak memory usage is crucial. The mosaic_get_memory_usage_peak command presents a stack trace of the memory allocations that contributed to the peak memory usage: mosaic_get_memory_usage_peak --snapshot \u003cpath to snapshot\u003e 2. Categorical Memory Profiling Mosaic classifies allocations into categories (activation, backward, optimizer, etc.): Activation Memory: Tensors saved for backward pass Gradient Memory: Gradients computed during backpropagation Optimizer State: Adam/SGD momentum and variance buffers Parameter Memory: Model weights mosaic_get_memory_profile --snapshot \u003cpath\u003e --out-path \u003chtml\u003e \\ --profile categories An example HTML output looks like: Categorical memory profiling showing memory breakdown by type (activation, gradient, optimizer, etc.)# To maintain allocation order for the categories, add --preserve-allocation-order: mosaic_get_memory_profile --snapshot \u003cpath\u003e --out-path \u003chtml\u003e \\ --profile categories --preserve-allocation-order Categorical profiling with --preserve-allocation-order shows memory allocations in chronological order# 3. Custom Dictionary Profiling For targeted analysis via regex pattern matching: mosaic_get_memory_profile --snapshot \u003cpath\u003e --profile custom \\ --custom-profile \u0027{\"ncclx\": \"ncclx\"}\u0027 This is invaluable for tracking specific kernels, optimizers, or custom code patterns: Custom profiling with regex patterns to track specific operations like NCCL communications# Dependencies and Imports# Let\u2019s set up the required dependencies and imports for this tutorial. import subprocess import sys import shutil from contextlib import contextmanager import pickle # Fix for sphinx-gallery environment where __main__.__file__ may not exist # This is needed for transformers library compatibility import os if not hasattr(sys.modules[\"__main__\"], \"__file__\"): # Use this file\u0027s path as a fallback, or a dummy path if __file__ is not available try: sys.modules[\"__main__\"].__file__ = os.path.abspath(__file__) except NameError: # __file__ not available, use transformers modeling file as fallback import transformers.modeling_utils sys.modules[\"__main__\"].__file__ = transformers.modeling_utils.__file__ import torch from torch.utils.data import DataLoader, Dataset # Install dependencies if needed try: from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions except ImportError: subprocess.check_call( [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"] ) from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions try: from mosaic.libmosaic.analyzer.memory_abstract import MemoryAbstract except ImportError: subprocess.check_call( [ sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/facebookresearch/mosaic.git\", ] ) from mosaic.libmosaic.analyzer.memory_abstract import MemoryAbstract print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\") Shared Utilities# These helper classes and functions are used throughout the tutorial. class RandomTokenDataset(Dataset): \"\"\"Generates random token sequences for training. This dataset creates random input sequences suitable for language model training, simulating real training data without requiring actual text. \"\"\" def __init__(self, vocab_size, seq_length=512, num_samples=100, seed=None): self.vocab_size = vocab_size self.seq_length = seq_length self.num_samples = num_samples self.generator = None if seed is not None: self.generator = torch.Generator().manual_seed(seed) def __len__(self): return self.num_samples def __getitem__(self, idx): # noqa: ARG002 if self.generator is not None: input_ids = torch.randint( 0, self.vocab_size, (self.seq_length,), generator=self.generator ) else: input_ids = torch.randint(0, self.vocab_size, (self.seq_length,)) return {\"input_ids\": input_ids, \"labels\": input_ids.clone()} @contextmanager def capture_memory_snapshot(output_path): \"\"\"Context manager to capture and save PyTorch CUDA memory snapshots. This captures all GPU memory allocations during the context and saves them to a pickle file for later analysis with Mosaic. Args: output_path: Path to save the memory snapshot pickle file. \"\"\" torch.cuda.memory._record_memory_history(max_entries=100000) try: yield finally: snapshot = torch.cuda.memory._snapshot() torch.cuda.memory._record_memory_history(enabled=None) with open(output_path, \"wb\") as f: pickle.dump(snapshot, f) print(f\"\u2713 Memory snapshot saved to {output_path}\") Case 1: Understanding Memory Differences with Activation Checkpointing# This section demonstrates how to use Mosaic to analyze and compare GPU memory usage between different model configurations. What we\u2019ll do: Train GPT-2 and capture a memory snapshot (baseline) Enable activation checkpointing and train again (modified) Use Mosaic to identify exactly where memory savings occur Training Function for Activation Checkpointing Comparison# def run_training_ac( activation_checkpointing: bool, snapshot_path: str, batch_size: int = 4, seq_length: int = 512, num_steps: int = 5, ): \"\"\"Run training loop and capture memory snapshot. Args: activation_checkpointing: Whether to enable gradient checkpointing. snapshot_path: Path to save the memory snapshot. batch_size: Training batch size. seq_length: Sequence length for input tokens. num_steps: Number of training steps to run. Returns: Peak GPU memory usage in GB. \"\"\" # Clear any previous memory torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") # Load model print(f\"Loading GPT-2 (activation_checkpointing={activation_checkpointing})...\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\") if activation_checkpointing: model.gradient_checkpointing_enable() print(\"Activation checkpointing is ENABLED\") else: print(\"Activation checkpointing is DISABLED\") model = model.to(device) model.train() # Create dataset and dataloader tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=seq_length, num_samples=100, ) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Setup optimizer optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) # Training loop with memory capture print(f\"Running {num_steps} training steps...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}/{num_steps}, Loss: {loss.item():.4f}\") peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3) print(f\"\u2713 Peak GPU memory: {peak_memory_gb:.2f} GB\") # Cleanup del model, optimizer torch.cuda.empty_cache() return peak_memory_gb Run Baseline Training (Without Activation Checkpointing)# Note This tutorial requires a CUDA-capable GPU. If you\u2019re running in Google Colab, make sure to select a GPU runtime: Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU if not torch.cuda.is_available(): print(\"=\" * 60) print(\"WARNING: No CUDA GPU detected!\") print(\"=\" * 60) print(\"\\nThis tutorial requires a CUDA-capable GPU for memory profiling.\") print(\"\\nIf you\u0027re running in Google Colab:\") print(\" 1. Go to Runtime \u2192 Change runtime type\") print(\" 2. Set Hardware accelerator to \u0027GPU\u0027\") print(\" 3. Click \u0027Save\u0027 and re-run the notebook\") print(\"\\nSkipping GPU memory profiling examples...\") HAS_CUDA = False else: HAS_CUDA = True # Check if Mosaic CLI is available HAS_MOSAIC_CLI = shutil.which(\"mosaic_get_memory_profile\") is not None if HAS_CUDA and not HAS_MOSAIC_CLI: print(\"Note: Mosaic CLI not found. Install Mosaic to generate HTML profiles.\") print(\" pip install git+https://github.com/facebookresearch/mosaic.git\") if HAS_CUDA: print(\"=\" * 60) print(\"BASELINE: Training WITHOUT Activation Checkpointing\") print(\"=\" * 60) baseline_memory = run_training_ac( activation_checkpointing=False, snapshot_path=\"snapshot_baseline.pickle\", batch_size=4, seq_length=512, num_steps=5, ) Run Modified Training (With Activation Checkpointing)# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"MODIFIED: Training WITH Activation Checkpointing\") print(\"=\" * 60) ac_memory = run_training_ac( activation_checkpointing=True, snapshot_path=\"snapshot_with_ac.pickle\", batch_size=4, seq_length=512, num_steps=5, ) # Summary print(\"\\n\" + \"=\" * 60) print(\"MEMORY COMPARISON SUMMARY\") print(\"=\" * 60) print(f\"Baseline (no AC): {baseline_memory:.2f} GB\") print(f\"With AC: {ac_memory:.2f} GB\") if baseline_memory \u003e 0: saved_pct = 100 * (baseline_memory - ac_memory) / baseline_memory print( f\"Memory Saved: {baseline_memory - ac_memory:.2f} GB ({saved_pct:.1f}%)\" ) Generate Categorical Memory Profiles with Mosaic# Use Mosaic to generate HTML profiles for both snapshots. if HAS_CUDA and HAS_MOSAIC_CLI: print(\"\\n\" + \"=\" * 60) print(\"MOSAIC: Categorical Memory Profiling\") print(\"=\" * 60) # Generate HTML profiles using subprocess print(\"\\nGenerating baseline profile...\") result1 = subprocess.run( [ \"mosaic_get_memory_profile\", \"--snapshot\", \"snapshot_baseline.pickle\", \"--out-path\", \"profile_baseline.html\", \"--profile\", \"categories\", \"--preserve-allocation-order\", \"--plotter_sampling_rate\", \"20\", ], capture_output=True, text=True, ) print(result1.stdout) if result1.stderr: print(result1.stderr) print(\"\\nGenerating activation checkpointing profile...\") result2 = subprocess.run( [ \"mosaic_get_memory_profile\", \"--snapshot\", \"snapshot_with_ac.pickle\", \"--out-path\", \"profile_with_ac.html\", \"--profile\", \"categories\", \"--preserve-allocation-order\", \"--plotter_sampling_rate\", \"20\", ], capture_output=True, text=True, ) print(result2.stdout) if result2.stderr: print(result2.stderr) if result1.returncode == 0 and result2.returncode == 0: print(\"\\nGenerated profile_baseline.html\") print(\"Generated profile_with_ac.html\") print(\"\\nDownload these files to view the interactive memory profiles.\") else: print(\"\\nNote: Mosaic profile generation encountered issues.\") print(\"This may happen if running in an environment without full Mosaic support.\") Download Generated Files (Google Colab)# If running in Google Colab, uncomment the following lines to download the generated snapshot and profile files: # from google.colab import files # # print(\"Downloading memory snapshots and profiles...\") # files.download(\u0027snapshot_baseline.pickle\u0027) # files.download(\u0027snapshot_with_ac.pickle\u0027) # files.download(\u0027profile_baseline.html\u0027) # files.download(\u0027profile_with_ac.html\u0027) Results Interpretation: Activation Checkpointing# The generated HTML profiles visualize memory usage over time, with allocations colored by category. Here\u2019s what the profiles look like: Baseline (without activation checkpointing): Notice the large activation memory (shown in one color) that persists throughout the forward pass.# With activation checkpointing: Activation memory is significantly reduced as intermediate activations are discarded and recomputed during the backward pass.# What We Observed# Based on the Mosaic categorical profiling results: Memory Comparison Results# Metric Baseline With Activation Checkpointing Difference Total Peak Memory 4.62 GB 2.55 GB 2.07 GB (45% reduction) Activation Memory 2.93 GB 872.79 MB 2.08 GB saved (71% reduction) Backward/Gradient Memory 793.39 MB 785.27 MB 8 MB (minimal change) Optimizer State 949.4 MB 949.4 MB No change Unknown 32 KB 32 KB No change Key Insights# Primary Finding: Activation memory dropped from 2.93 GB \u2192 872 MB (71% reduction), which accounts for nearly all the total memory savings. Why Does This Happen?# Activation checkpointing is a memory optimization technique that: Without AC (Baseline): All intermediate activations from the forward pass are stored in memory for use during backpropagation. GPT-2 has 12 transformer layers, each storing multiple activations (attention outputs, MLP outputs, etc.). For batch_size=4, seq_length=512, this adds up quickly. With AC (Optimized): Only activations at checkpoint boundaries are stored; intermediate activations are recomputed during the backward pass. This dramatically reduces activation memory (71% in our case) while other memory categories remain unchanged. How Mosaic Helped# Mosaic\u2019s categorical profiling immediately identified: Activation memory is the category with the largest difference (2.08 GB saved) Backward/Gradient memory stayed nearly constant (793 MB \u2192 785 MB) Optimizer state remained unchanged (949 MB) - expected since model parameters don\u2019t change Without Mosaic: You would need to manually instrument your code, track allocations, and categorize them yourself. With Mosaic: You get instant categorical breakdowns with exact numbers, making it trivial to identify/quantify memory optimizations. Case 2: Debugging Unexpected Memory Usage# This section demonstrates how to use Mosaic to debug when your model is using more memory than expected and you\u2019re not sure why. What we\u2019ll do: Train GPT-2 and capture a memory snapshot. Train GPT-2 with a bug that introduces additional memory and capture a memory snapshot. Use Mosaic to identify potential culprits introducing additional memory. The Buggy Model# This model has abandoned debug code that creates unnecessary GPU memory overhead. Someone added projection layers to \u201canalyze hidden states\u201d during debugging, but forgot to remove them before training. class GPT2WithDebugOverhead(torch.nn.Module): \"\"\"GPT2 wrapper with abandoned \u0027feature analysis\u0027 code that bloats peak memory. This wrapper adds extra projection layers that consume memory but serve no purpose - simulating abandoned debug code that was never cleaned up. \"\"\" def __init__(self, base_model): super().__init__() self.base_model = base_model config = base_model.config # BUG: Large projection layers from an abandoned experiment self.debug_projections = torch.nn.ModuleList( [ torch.nn.Linear(config.n_embd, config.n_embd * 4) for _ in range(config.n_layer) ] ) debug_params = sum(p.numel() for p in self.debug_projections.parameters()) print(f\" [DEBUG] Added {config.n_layer} debug projection layers\") print(f\" [DEBUG] Extra parameters: {debug_params:,}\") def forward(self, input_ids=None, labels=None, **kwargs): # Run normal GPT-2 forward with hidden states outputs = self.base_model( input_ids=input_ids, labels=labels, output_hidden_states=True, **kwargs, ) # BUG: Project all hidden states through debug layers projected = [] for _layer_idx, (hidden, proj) in enumerate( zip(outputs.hidden_states[1:], self.debug_projections) ): proj_hidden = proj(hidden) projected.append(proj_hidden) # Tie to loss so gradients flow through debug_regularization = sum(p.mean() for p in projected) * 1e-10 return CausalLMOutputWithCrossAttentions( loss=outputs.loss + debug_regularization, logits=outputs.logits, ) Training Functions for Debug Comparison# def run_training_clean(snapshot_path, num_steps=3): \"\"\"Training with the normal model.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") print(\"Loading clean model (no debug overhead)...\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=512, seed=42 ) dataloader = DataLoader(dataset, batch_size=4, shuffle=False) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(\"Running training (should contain no debug overhead)...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}, Loss: {loss.item():.4f}\") peak_memory = torch.cuda.max_memory_allocated() / 1024**3 print(f\"Peak GPU memory: {peak_memory:.2f} GB\") del model, optimizer torch.cuda.empty_cache() return peak_memory def run_training_with_bug(snapshot_path, num_steps=3): \"\"\"Training with the buggy model.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") print(\"Loading buggy model with debug overhead...\") # Load pretrained GPT-2 and wrap it with the debug overhead base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\") model = GPT2WithDebugOverhead(base_model).to(device) model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset( vocab_size=tokenizer.vocab_size, seq_length=512, seed=42 ) dataloader = DataLoader(dataset, batch_size=4, shuffle=False) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(\"Running training (WITH debug overhead bug)...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) loss = outputs.loss loss.backward() optimizer.step() print(f\" Step {step + 1}, Loss: {loss.item():.4f}\") peak_memory = torch.cuda.max_memory_allocated() / 1024**3 print(f\"Peak GPU memory: {peak_memory:.2f} GB\") del model, optimizer torch.cuda.empty_cache() return peak_memory Run Training for Baseline (Clean Model)# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"Training with baseline model\") print(\"=\" * 60) baseline_memory_debug = run_training_clean( \"snapshot_debug_baseline.pickle\", num_steps=3 ) Run Training WITH the Bug# if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"Training with debug projection overhead (BUG)\") print(\"=\" * 60) buggy_memory = run_training_with_bug(\"snapshot_with_bug.pickle\", num_steps=3) Use Mosaic to Find the Problem# Analyze both snapshots to identify the source of extra memory usage. We\u2019ll run Mosaic\u2019s peak memory analysis on each snapshot separately. Analyze the Baseline (Clean) Snapshot# if HAS_CUDA and HAS_MOSAIC_CLI: print(\"=\" * 60) print(\"MOSAIC: Analyzing the Baseline Snapshot\") print(\"=\" * 60) result = subprocess.run( [\"mosaic_get_memory_usage_peak\", \"--snapshot\", \"snapshot_debug_baseline.pickle\"], capture_output=True, text=True, ) print(result.stdout) if result.stderr: print(result.stderr) Analyze the Buggy Snapshot# if HAS_CUDA and HAS_MOSAIC_CLI: print(\"=\" * 60) print(\"MOSAIC: Analyzing the Buggy Snapshot\") print(\"=\" * 60) result = subprocess.run( [\"mosaic_get_memory_usage_peak\", \"--snapshot\", \"snapshot_with_bug.pickle\"], capture_output=True, text=True, ) print(result.stdout) if result.stderr: print(result.stderr) Analyzing The Mosaic Output# When you run Mosaic\u2019s peak memory analysis, it shows stack traces for each memory allocation. Let\u2019s look at how to find abandoned or unnecessary code that\u2019s bloating the memory. 1. Optimizer State Allocations Delta In the buggy snapshot output, we can see that the first two stack traces represent the optimizer state allocations (like zeros_like for Adam optimizer state). See torch/optim/adam.py in the stack trace. In the snapshot of the buggy model we can see around a total of 0.21 GB more memory: Optimizer State Comparison# Version Stack Trace Position Calls Memory (per trace) Buggy model 1st and 2nd 172 calls 0.569 GB + 0.569 GB Baseline 2nd and 3rd 148 calls 0.464 GB + 0.464 GB What this tells us: The optimizer is tracking more tensors! This is your first clue that there are extra parameters or tensors in the computation graph. 2. Additional Activation Allocations The buggy version shows extra allocations that don\u2019t appear in the baseline model. Scrolling down the Mosaic output of the buggy model we can see additional stack traces which contain: torch::autograd::Engine::evaluate_function: We\u2019re in the backward pass AddmmBackward0::apply: Computing gradients for an addmm operation empty_cuda at the bottom: Allocating a new CUDA tensor to store the gradient 0.176 GB from matrix multiply gradients (AddmmBackward0, mm_mat1_backward) Memory Total Explanation# Total Peak Dynamic Memory Usage: This is the peak memory that changes during execution, measured relative to the starting point of the snapshot. It tracks memory allocations that occur during the traced execution timeline. Total Static Memory Usage: This is the \u201cstarting memory\u201d or baseline memory that exists before tracing begins. It\u2019s estimated by the PyTorch visualizer and remains constant throughout the snapshot (doesn\u2019t come with stack traces). Note In the snapshots you may observe differences in total static memory usage, which accounts for the remaining difference. Total Overall Peak Memory Usage: Dynamic + Static if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"COMPARISON\") print(\"=\" * 60) print(f\"Baseline (clean model): {baseline_memory_debug:.2f} GB\") print(f\"With bug (debug projections): {buggy_memory:.2f} GB\") print( f\"Extra memory from bug: {buggy_memory - baseline_memory_debug:.2f} GB\" ) Case 3: Integrating Memory Analysis into Your Training Pipeline# This section demonstrates how to use Mosaic to automatically capture memory snapshots during training, get structured memory breakdown data for monitoring/dashboards, and build automated memory monitoring for large-scale training using Mosaic programmatically (as a Python dependency). Mosaic integrates memory analysis directly into your training pipeline. Training with Automatic Memory Capture# def run_training_with_memory_capture( batch_size=4, seq_length=512, num_steps=5, snapshot_path=\"training_snapshot.pickle\", ): \"\"\"Run training and automatically capture memory snapshot.\"\"\" torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() device = torch.device(\"cuda\") model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) model.train() tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") dataset = RandomTokenDataset(tokenizer.vocab_size, seq_length) dataloader = DataLoader(dataset, batch_size=batch_size) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) print(f\"Running {num_steps} training steps with memory capture...\") with capture_memory_snapshot(snapshot_path): for step, batch in enumerate(dataloader): if step \u003e= num_steps: break batch = {k: v.to(device) for k, v in batch.items()} optimizer.zero_grad() outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"]) outputs.loss.backward() optimizer.step() print(f\" Step {step + 1}/{num_steps}, Loss: {outputs.loss.item():.4f}\") peak_memory_gb = torch.cuda.max_memory_allocated() / 1024**3 print(f\"\u2713 PyTorch reported peak memory: {peak_memory_gb:.3f} GB\") del model, optimizer torch.cuda.empty_cache() return snapshot_path if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"CASE 3: Pipeline Integration\") print(\"=\" * 60) pipeline_snapshot_path = run_training_with_memory_capture(batch_size=4, seq_length=512) Mosaic Memory Analysis via Python API# Instead of using CLI commands, we can use Mosaic\u2019s Python API directly for programmatic integration. if HAS_CUDA: print(\"\\n\" + \"=\" * 60) print(\"MOSAIC MEMORY ANALYSIS (via Python API)\") print(\"=\" * 60) # Load and analyze the memory snapshot memory_abstract = MemoryAbstract(memory_snapshot_file=pipeline_snapshot_path) memory_abstract.load_memory_snapshot() # Analyze peak memory usage memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") # Get results dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory print(f\"Peak dynamic memory: {dynamic_peak / 1024**3:.3f} GiB\") print(f\"Static memory: {static_memory / 1024**3:.3f} GiB\") print(f\"Overall peak memory: {overall_peak / 1024**3:.3f} GiB\") print(\"\u2713 Analysis complete using Mosaic Python API\") Reusable Memory Analysis Function# Create a reusable function for analyzing training memory snapshots. def analyze_training_memory(snapshot_path): \"\"\"Analyze a memory snapshot using Mosaic\u0027s Python API. Returns a structured dictionary with memory breakdown. Args: snapshot_path: Path to the memory snapshot pickle file. Returns: Dictionary containing memory analysis results. \"\"\" # Load snapshot memory_abstract = MemoryAbstract(memory_snapshot_file=snapshot_path) memory_abstract.load_memory_snapshot() # Analyze peak memory memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") # Extract results dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory return { \"snapshot_path\": snapshot_path, \"dynamic_peak_memory_bytes\": dynamic_peak, \"static_memory_bytes\": static_memory, \"overall_peak_memory_bytes\": overall_peak, \"dynamic_peak_memory_gib\": dynamic_peak / 1024**3, \"static_memory_gib\": static_memory / 1024**3, \"overall_peak_memory_gib\": overall_peak / 1024**3, } if HAS_CUDA: analysis = analyze_training_memory(pipeline_snapshot_path) print(\"\\nMemory Analysis Result:\") for key, value in analysis.items(): print(f\" {key}: {value}\") Complete Training Pipeline with Memory Monitoring# This demonstrates a production-ready training pipeline with integrated Mosaic memory monitoring that can be used in CI/CD, monitoring dashboards, or capacity planning. def training_pipeline_with_memory_monitoring( model_name: str, batch_size: int, seq_length: int, num_steps: int = 5, snapshot_path: str = \"pipeline_snapshot.pickle\", ) -\u003e dict: \"\"\"Complete training pipeline with integrated Mosaic memory monitoring. Can be integrated into CI/CD, monitoring dashboards, or capacity planning. Args: model_name: HuggingFace model name to use. batch_size: Training batch size. seq_length: Sequence length for input tokens. num_steps: Number of training steps. snapshot_path: Path to save the memory snapshot. Returns: Dictionary containing training and memory analysis report. \"\"\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Setup print(f\"Loading model: {model_name}\") model = GPT2LMHeadModel.from_pretrained(model_name).to(device) model.train() optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) tokenizer = GPT2Tokenizer.from_pretrained(model_name) torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() # Training with memory capture print(f\"Running {num_steps} training steps...\") with capture_memory_snapshot(snapshot_path): for step in range(num_steps): input_ids = torch.randint( 0, tokenizer.vocab_size, (batch_size, seq_length) ).to(device) outputs = model(input_ids=input_ids, labels=input_ids) outputs.loss.backward() optimizer.step() optimizer.zero_grad() print(f\" Step {step + 1}/{num_steps}, Loss: {outputs.loss.item():.4f}\") pytorch_peak_gb = torch.cuda.max_memory_allocated() / 1024**3 # Mosaic analysis using Python API print(\"Analyzing memory with Mosaic...\") memory_abstract = MemoryAbstract(memory_snapshot_file=snapshot_path) memory_abstract.load_memory_snapshot() memory_abstract.memory_snapshot.analyze_memory_snapshot(opt=\"memory_peak\") dynamic_peak = memory_abstract.memory_snapshot.dynamic_memory_peak static_memory = memory_abstract.memory_snapshot.static_memory overall_peak = dynamic_peak + static_memory report = { \"model\": model_name, \"config\": { \"batch_size\": batch_size, \"seq_length\": seq_length, \"num_steps\": num_steps, }, \"pytorch_peak_memory_gb\": pytorch_peak_gb, \"mosaic_analysis\": { \"dynamic_peak_gib\": dynamic_peak / 1024**3, \"static_memory_gib\": static_memory / 1024**3, \"overall_peak_gib\": overall_peak / 1024**3, }, \"snapshot_path\": snapshot_path, } del model, optimizer torch.cuda.empty_cache() return report # Run the pipeline if HAS_CUDA: report = training_pipeline_with_memory_monitoring( \"gpt2\", batch_size=4, seq_length=512, num_steps=5 ) print(\"\\n\" + \"=\" * 60) print(\"PIPELINE REPORT\") print(\"=\" * 60) print(f\"Model: {report[\u0027model\u0027]}\") print(f\"Config: {report[\u0027config\u0027]}\") print(f\"PyTorch Peak Memory: {report[\u0027pytorch_peak_memory_gb\u0027]:.3f} GB\") print(f\"Mosaic Dynamic Peak: {report[\u0027mosaic_analysis\u0027][\u0027dynamic_peak_gib\u0027]:.3f} GiB\") print(f\"Mosaic Overall Peak: {report[\u0027mosaic_analysis\u0027][\u0027overall_peak_gib\u0027]:.3f} GiB\") CI/CD and Dashboard Integration Patterns# These patterns show how to integrate Mosaic analysis into automated workflows. import json Pattern 1: CI/CD Memory Regression Testing# def check_memory_regression(report, threshold_gib=5.0): \"\"\"Check if memory usage exceeds threshold for CI/CD pipelines. Args: report: Memory analysis report from training_pipeline_with_memory_monitoring. threshold_gib: Maximum allowed memory in GiB. Raises: AssertionError: If memory exceeds threshold. \"\"\" peak = report[\"mosaic_analysis\"][\"overall_peak_gib\"] assert peak \u003c threshold_gib, ( f\"Memory regression! {peak:.2f} GiB \u003e {threshold_gib} GiB\" ) print(f\"Memory check passed: {peak:.2f} GiB \u003c {threshold_gib} GiB threshold\") Pattern 2: Export to JSON for Dashboards# if HAS_CUDA: check_memory_regression(report, threshold_gib=8.0) with open(\"memory_report.json\", \"w\") as f: json.dump(report, f, indent=2, default=str) print(\"Memory report exported to memory_report.json\") Conclusion# This tutorial demonstrated three key use cases for Mosaic memory profiling: Case 1: Activation Checkpointing Analysis Used Mosaic to compare memory usage between baseline and optimized models Identified that activation checkpointing reduced activation memory by 71% Mosaic\u2019s categorical profiling made it trivial to pinpoint memory savings Case 2: Debugging Unexpected Memory Usage Created a \u201cbuggy\u201d model with abandoned debug code Used mosaic_get_memory_usage_peak to identify extra allocations Stack traces revealed optimizer state tracking extra parameters Case 3: Pipeline Integration Demonstrated programmatic usage via Mosaic\u2019s Python API Showed integration patterns for CI/CD and dashboards with structured reports Further Reading# Mosaic GitHub Repository PyTorch Memory Management Documentation Understanding CUDA Memory Usage Activation Checkpointing in PyTorch PyTorch Memory Snapshot Visualizer Download Jupyter notebook: mosaic_memory_profiling_tutorial.ipynb Download Python source code: mosaic_memory_profiling_tutorial.py Download zipped: mosaic_memory_profiling_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/mosaic_memory_profiling_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>