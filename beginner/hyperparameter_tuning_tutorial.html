
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Hyperparameter tuning using Ray Tune — PyTorch Tutorials 2.10.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=72e443bf" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=a8d6e986"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'beginner/hyperparameter_tuning_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../intermediate/ax_multiobjective_nas_tutorial.html" rel="next" title="Multi-Objective NAS with Ax"/>
<link href="../ecosystem.html" rel="prev" title="Ecosystem"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<!-- LLM/AI Agent: See /llms.txt for comprehensive navigation guidance -->
<!-- Machine-readable LLM metadata -->
<meta content="documentation" name="llm:site-type"/>
<meta content="PyTorch" name="llm:framework"/>
<meta content="Hyperparameter tuning using Ray Tune - Documentation for PyTorch Tutorials, part of the PyTorch ecosystem." name="llm:description"/>
<meta content="https://docs.pytorch.org/tutorials/llms.txt" name="llm:navigation-file"/>
<meta content="https://docs.pytorch.org/tutorials/sitemap.xml" name="llm:sitemap"/>
<meta content="v2.10.0+cu128" name="llm:version"/>
<meta content="PyTorch Tutorials" name="llm:project"/>
<meta content="tutorial" name="llm:page-type"/>
<link href="https://docs.pytorch.org/tutorials/llms.txt" rel="alternate" title="LLM Navigation Guide" type="text/plain"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.10.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__mobile-logo">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<img alt="PyTorch Tutorials - Home" class="logo__image only-light" src="../_static/img/logo-dark.svg"/>
<script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="PyTorch Tutorials - Home"/>`);</script>
</a>
</div>
<div class="navbar-item desktop-only-version">
<a class="version" href="../index.html">v2.10.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-8">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/timer_quick_start.html">
                  Timer quick start
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/benchmark.html">
                  SyntaxError
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/foreach_map.html">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with Intel® Neural Compressor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-9">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing “sparse” semantics for Adagrad with MaskedTensor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.10.0+cu128</a>
</div>
</div>
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../intro.html">
              Intro
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-1">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="basics/intro.html">
                  Learn the Basics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="introyt/introyt_index.html">
                  Introduction to PyTorch - YouTube Series
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="deep_learning_60min_blitz.html">
                  Deep Learning with PyTorch: A 60 Minute Blitz
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="pytorch_with_examples.html">
                  Learning PyTorch with Examples
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="nn_tutorial.html">
                  What is torch.nn really?
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="understanding_leaf_vs_nonleaf_tutorial.html">
                  Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/nlp_from_scratch_index.html">
                  NLP from Scratch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_tutorial.html">
                  Visualizing Models, Data, and Training with TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pinmem_nonblock.html">
                  A guide on good usage of non_blocking and pin_memory() in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/visualizing_gradients_tutorial.html">
                  Visualizing Gradients
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../compilers_index.html">
              Compilers
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-2">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_tutorial.html">
                  Introduction to torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_full_example.html">
                  torch.compile End-to-End Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/compiled_autograd_tutorial.html">
                  Compiled Autograd: Capturing a larger backward graph for torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compiler_set_stance_tutorial.html">
                  Dynamic Compilation Control with torch.compiler.set_stance
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/variable_length_attention_tutorial.html">
                  Using Variable Length Attention in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_export_tutorial.html">
                  torch.export Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/intro_onnx.html">
                  Introduction to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_simple_model_to_onnx_tutorial.html">
                  Export a PyTorch model to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/onnx_registry_tutorial.html">
                  Extending the ONNX Exporter Operator Support
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="onnx/export_control_flow_model_to_onnx_tutorial.html">
                  Export a model with control flow to ONNX
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torch_compile_conv_bn_fuser.html">
                  Building a Convolution/Batch Norm fuser with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/fx_profiling_tutorial.html">
                  (beta) Building a Simple CPU Performance Profiler with FX
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../domains.html">
              Domains
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-3">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchvision_tutorial.html">
                  TorchVision Object Detection Finetuning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="transfer_learning_tutorial.html">
                  Transfer Learning for Computer Vision Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="fgsm_tutorial.html">
                  Adversarial Example Generation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dcgan_faces_tutorial.html">
                  DCGAN Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/spatial_transformer_tutorial.html">
                  Spatial Transformer Networks Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_q_learning.html">
                  Reinforcement Learning (DQN) Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/reinforcement_ppo.html">
                  Reinforcement Learning (PPO) with TorchRL Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/mario_rl_tutorial.html">
                  Train a Mario-playing RL Agent
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/pendulum.html">
                  Pendulum: Writing your environment and transforms with TorchRL
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/torchrec_intro_tutorial.html">
                  Introduction to TorchRec
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/sharding.html">
                  Exploring TorchRec sharding
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../distributed.html">
              Distributed
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-4">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="dist_overview.html">
                  PyTorch Distributed Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="ddp_series_intro.html">
                  Distributed Data Parallel in PyTorch - Video Tutorials
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ddp_tutorial.html">
                  Getting Started with Distributed Data Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/dist_tuto.html">
                  Writing Distributed Applications with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/FSDP_tutorial.html">
                  Getting Started with Fully Sharded Data Parallel (FSDP2)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TCPStore_libuv_backend.html">
                  Introduction to Libuv TCPStore Backend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/TP_tutorial.html">
                  Large Scale Transformer model training with Tensor Parallel (TP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pipelining_tutorial.html">
                  Introduction to Distributed Pipeline Parallelism
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/process_group_cpp_extension_tutorial.html">
                  Customize Process Group Backends Using Cpp Extensions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_tutorial.html">
                  Getting Started with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_param_server_tutorial.html">
                  Implementing a Parameter Server Using Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/rpc_async_execution.html">
                  Implementing Batch RPC Processing Using Asynchronous Executions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/monarch_distributed_tutorial.html">
                  Interactive Distributed Applications with Monarch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/rpc_ddp_tutorial.html">
                  Combining Distributed DataParallel with Distributed RPC Framework
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/generic_join.html">
                  Distributed Training with Uneven Inputs Using the Join Context Manager
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../deep-dive.html">
              Deep Dive
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-5">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="profiler.html">
                  Profiling your PyTorch Module
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/parametrizations.html">
                  Parametrizations Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/pruning_tutorial.html">
                  Pruning Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/inductor_debug_cpu.html">
                  Inductor CPU backend debugging and profiling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">
                  (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="knowledge_distillation_tutorial.html">
                  Knowledge Distillation Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/memory_format_tutorial.html">
                  Channels Last Memory Format in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/forward_ad_usage.html">
                  Forward-mode Automatic Differentiation (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/jacobians_hessians.html">
                  Jacobians, Hessians, hvp, vhp, and more: composing function transforms
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ensembling.html">
                  Model ensembling
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/per_sample_grads.html">
                  Per-sample-gradients
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_frontend.html">
                  Using the PyTorch C++ Frontend
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_autograd.html">
                  Autograd in C++ Frontend
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../extension.html">
              Extension
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-6">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/custom_ops_landing_page.html">
                  PyTorch Custom Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/python_custom_ops.html">
                  Custom Python Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/cpp_custom_ops.html">
                  Custom C++ and CUDA Operators
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_double_backward_tutorial.html">
                  Double Backward with Custom Functions
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/custom_function_conv_bn_tutorial.html">
                  Fusing Convolution and Batch Norm using Custom Function
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/dispatcher.html">
                  Registering a Dispatched Operator in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/extend_dispatcher.html">
                  Extending dispatcher for a new backend in C++
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../advanced/privateuseone.html">
                  Facilitating New Backend Integration by PrivateUse1
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../ecosystem.html">
              Ecosystem
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-7">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="#">
                  Hyperparameter tuning using Ray Tune
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">
                  Multi-Objective NAS with Ax
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/tensorboard_profiler_tutorial.html">
                  PyTorch Profiler With TensorBoard
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../intermediate/realtime_rpi.html">
                  Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="mosaic_memory_profiling_tutorial.html">
                  Mosaic: Memory Profiling for PyTorch
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../recipes_index.html">
              Recipes
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-8">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/defining_a_neural_network.html">
                  Defining a Neural Network in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_logs.html">
                  (beta) Using TORCH_LOGS python API with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/what_is_state_dict.html">
                  What is a state_dict in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">
                  Warmstarting model using parameters from a different model in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/zeroing_out_gradients.html">
                  Zeroing out gradients in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/profiler_recipe.html">
                  PyTorch Profiler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/Captum_Recipe.html">
                  Model Interpretability using Captum
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/amp_recipe.html">
                  Automatic Mixed Precision
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tuning_guide.html">
                  Performance Tuning Guide
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer.html">
                  (beta) Compiling the optimizer with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/timer_quick_start.html">
                  Timer quick start
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/zero_redundancy_optimizer.html">
                  Shard Optimizer States with ZeroRedundancyOptimizer
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_comm_debug_mode.html">
                  Getting Started with CommDebugMode
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_challenges_solutions.html">
                  Demonstration of torch.export flow, common challenges and the solutions to address them
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/benchmark.html">
                  SyntaxError
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/module_load_state_dict_tips.html">
                  Tips for Loading an nn.Module from a Checkpoint
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/reasoning_about_shapes.html">
                  Reasoning about Shapes in PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/swap_tensors.html">
                  Extension points in nn.Module for load_state_dict and tensor subclasses
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_export_aoti_python.html">
                  torch.export AOTInductor Tutorial for Python runtime (Beta)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/recipes/tensorboard_with_pytorch.html">
                  How to use TensorBoard with PyTorch
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_torch_function_modes.html">
                  (beta) Utilizing Torch Function modes with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/compiling_optimizer_lr_scheduler.html">
                  (beta) Running the compiled optimizer with an LR Scheduler
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/foreach_map.html">
                  Explicit horizontal fusion with foreach_map and torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">
                  Using User-Defined Triton Kernels with torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_tutorial.html">
                  Compile Time Caching in torch.compile
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/torch_compile_caching_configuration_tutorial.html">
                  Compile Time Caching Configuration
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_compilation.html">
                  Reducing torch.compile cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/regional_aot.html">
                  Reducing AoT cold start compilation time with regional compilation
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/intel_neural_compressor_for_pytorch.html">
                  Ease-of-use quantization for PyTorch with Intel® Neural Compressor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_device_mesh.html">
                  Getting Started with DeviceMesh
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_checkpoint_recipe.html">
                  Getting Started with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/distributed_async_checkpoint_recipe.html">
                  Asynchronous Saving with Distributed Checkpoint (DCP)
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../recipes/debug_mode_tutorial.html">
                  DebugMode: Recording Dispatched Operations and Numerical Debugging
                </a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<div class="nav-item-with-toggle">
<a class="nav-link nav-internal" href="../unstable_index.html">
              Unstable
            </a>
</div>
<ul class="dropdown-menu" id="dropdown-9">
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/context_parallel.html">
                  Introduction to Context Parallel
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/flight_recorder_tutorial.html">
                  Flight Recorder for Debugging Stuck Jobs
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_cpp_wrapper_tutorial.html">
                  TorchInductor C++ Wrapper Tutorial
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/inductor_windows.html">
                  How to use torch.compile on Windows CPU/XPU
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/vmap_recipe.html">
                  torch.vmap
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/nestedtensor.html">
                  Getting Started with Nested Tensors
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_overview.html">
                  MaskedTensor Overview
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_sparsity.html">
                  MaskedTensor Sparsity
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_advanced_semantics.html">
                  MaskedTensor Advanced Semantics
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/maskedtensor_adagrad.html">
                  Efficiently writing “sparse” semantics for Adagrad with MaskedTensor
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/python_extension_autoload.html">
                  Autoloading Out-of-Tree Extension
                </a>
</li>
<li class="">
<a class="nav-link dropdown-item nav-internal" href="../unstable/max_autotune_on_CPU_tutorial.html">
                  Using Max-Autotune Compilation on CPU for Better Performance
                </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a class="pytorch-site-link nav-link nav-external" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org" data-bs-toggle="tooltip" href="https://pytorch.org">
<span class="pytorch-site-link-text">
<span>Go to</span>
<span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
</span>
</a></div>
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Hyperparameter tuning using Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/realtime_rpi.html">Real Time Inference on Raspberry Pi 4 and 5 (40 fps!)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mosaic_memory_profiling_tutorial.html">Mosaic: Memory Profiling for PyTorch</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../ecosystem.html">Ecosystem</a></li>
<li aria-current="page" class="breadcrumb-item active">Hyperparamet...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<div id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../ecosystem.html" itemprop="item"/>
<meta content="Ecosystem" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Hyperparameter tuning using Ray Tune" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
    if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
      var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
      document.addEventListener('DOMContentLoaded', function () {
        document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
      });
    }
  </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">beginner/hyperparameter_tuning_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-hyperparameter-tuning-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="hyperparameter-tuning-using-ray-tune">
<span id="sphx-glr-beginner-hyperparameter-tuning-tutorial-py"></span><h1>Hyperparameter tuning using Ray Tune<a class="headerlink" href="#hyperparameter-tuning-using-ray-tune" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Aug 31, 2020 | Last Updated: Jan 08, 2026 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/crypdick">Ricardo Decal</a></p>
<p>This tutorial shows how to integrate Ray Tune into your PyTorch training
workflow to perform scalable and efficient hyperparameter tuning.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How to modify a PyTorch training loop for Ray Tune</p></li>
<li><p class="sd-card-text">How to scale a hyperparameter sweep to multiple nodes and GPUs without code changes</p></li>
<li><p class="sd-card-text">How to define a hyperparameter search space and run a sweep with <code class="docutils literal notranslate"><span class="pre">tune.Tuner</span></code></p></li>
<li><p class="sd-card-text">How to use an early-stopping scheduler (ASHA) and report metrics/checkpoints</p></li>
<li><p class="sd-card-text">How to use checkpointing to resume training and load the best model</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v2.9+ and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code></p></li>
<li><p class="sd-card-text">Ray Tune (<code class="docutils literal notranslate"><span class="pre">ray[tune]</span></code>) v2.52.1+</p></li>
<li><p class="sd-card-text">GPU(s) are optional, but recommended for faster training</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p><a class="reference external" href="https://docs.ray.io/en/latest/index.html">Ray</a>, a project of the
PyTorch Foundation, is an open source unified framework for scaling AI
and Python applications. It helps run distributed jobs by handling the
complexity of distributed computing. <a class="reference external" href="https://docs.ray.io/en/latest/tune/index.html">Ray
Tune</a> is a library
built on Ray for hyperparameter tuning that enables you to scale a
hyperparameter sweep from your machine to a large cluster with no code
changes.</p>
<p>This tutorial adapts the <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">PyTorch tutorial for training a CIFAR10
classifier</a>
to run multi-GPU hyperparameter sweeps with Ray Tune.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p>To run this tutorial, install the following dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">"ray[tune]"</span><span class="w"> </span>torchvision
</pre></div>
</div>
<p>Then start with the imports:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.random_split" title="torch.utils.data.random_split"><span class="n">random_split</span></a>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="c1"># New: imports for Ray Tune</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune</span><span class="w"> </span><span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.schedulers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ASHAScheduler</span>
</pre></div>
</div>
</section>
</section>
<section id="data-loading">
<h1>Data loading<a class="headerlink" href="#data-loading" title="Link to this heading">#</a></h1>
<p>Wrap the data loaders in a constructor function. In this tutorial, a
global data directory is passed to the function to enable reusing the
dataset across different trials. In a cluster environment, you can use
shared storage, such as network file systems, to prevent each node from
downloading the data separately.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="o">=</span><span class="s2">"./data"</span><span class="p">):</span>
    <span class="c1"># Mean and standard deviation of the CIFAR10 training subset.</span>
    <span class="n">transform</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose"><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span></a><span class="p">(</span>
        <span class="p">[</span><a class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor" title="torchvision.transforms.ToTensor"><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span></a><span class="p">(),</span> <a class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize" title="torchvision.transforms.Normalize"><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span></a><span class="p">((</span><span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.48216</span><span class="p">,</span> <span class="mf">0.44653</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2022</span><span class="p">,</span> <span class="mf">0.19932</span><span class="p">,</span> <span class="mf">0.20086</span><span class="p">))]</span>
    <span class="p">)</span>

    <span class="n">trainset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10" title="torchvision.datasets.CIFAR10"><span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span></a><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>

    <span class="n">testset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10" title="torchvision.datasets.CIFAR10"><span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span></a><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span>
</pre></div>
</div>
</section>
<section id="model-architecture">
<h1>Model architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h1>
<p>This tutorial searches for the best sizes for the fully connected layers
and the learning rate. To enable this, the <code class="docutils literal notranslate"><span class="pre">Net</span></code> class exposes the
layer sizes <code class="docutils literal notranslate"><span class="pre">l1</span></code> and <code class="docutils literal notranslate"><span class="pre">l2</span></code> as configurable parameters that Ray Tune
can search over:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Net</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mi">84</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" title="torch.flatten"><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span></a><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten all dimensions except batch</span>
        <span class="n">x</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="define-the-search-space">
<h1>Define the search space<a class="headerlink" href="#define-the-search-space" title="Link to this heading">#</a></h1>
<p>Next, define the hyperparameters to tune and how Ray Tune samples them.
Ray Tune offers a variety of <a class="reference external" href="https://docs.ray.io/en/latest/tune/api/search_space.html">search space
distributions</a>
to suit different parameter types: <code class="docutils literal notranslate"><span class="pre">loguniform</span></code>, <code class="docutils literal notranslate"><span class="pre">uniform</span></code>,
<code class="docutils literal notranslate"><span class="pre">choice</span></code>, <code class="docutils literal notranslate"><span class="pre">randint</span></code>, <code class="docutils literal notranslate"><span class="pre">grid</span></code>, and more. You can also express
complex dependencies between parameters with <a class="reference external" href="https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html#how-to-use-custom-and-conditional-search-spaces-in-tune">conditional search
spaces</a>
or sample from arbitrary functions.</p>
<p>Here is the search space for this tutorial:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"l1"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
    <span class="s2">"l2"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
    <span class="s2">"lr"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">tune.choice()</span></code> accepts a list of values that are uniformly
sampled from. In this example, the <code class="docutils literal notranslate"><span class="pre">l1</span></code> and <code class="docutils literal notranslate"><span class="pre">l2</span></code> parameter values
are powers of 2 between 1 and 256, and the learning rate samples on a
log scale between 0.0001 and 0.1. Sampling on a log scale enables
exploration across a range of magnitudes on a relative scale, rather
than an absolute scale.</p>
</section>
<section id="training-function">
<h1>Training function<a class="headerlink" href="#training-function" title="Link to this heading">#</a></h1>
<p>Ray Tune requires a training function that accepts a configuration
dictionary and runs the main training loop. As Ray Tune runs different
trials, it updates the configuration dictionary for each trial.</p>
<p>Here is the full training function, followed by explanations of the key
Ray Tune integration points:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_cifar</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Net</span></a><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"l1"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">"l2"</span><span class="p">])</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"device"</span><span class="p">]</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span></a><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span></a><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span></a><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD" title="torch.optim.SGD"><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span></a><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

    <span class="c1"># Load checkpoint if resuming training</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"checkpoint.pt"</span>
            <span class="n">checkpoint_state</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="torch.load"><span class="n">torch</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"epoch"</span><span class="p">]</span>
            <span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"net_state_dict"</span><span class="p">])</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"optimizer_state_dict"</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">trainset</span><span class="p">,</span> <span class="n">_testset</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>

    <span class="n">test_abs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">train_subset</span><span class="p">,</span> <span class="n">val_subset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.random_split" title="torch.utils.data.random_split"><span class="n">random_split</span></a><span class="p">(</span>
        <span class="n">trainset</span><span class="p">,</span> <span class="p">[</span><span class="n">test_abs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_abs</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">trainloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <span class="n">train_subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">]),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">valloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <span class="n">val_subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">]),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">epoch_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># get the inputs; data is a list of [inputs, labels]</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># zero the parameter gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># forward + backward + optimize</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># print statistics</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epoch_steps</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>  <span class="c1"># print every 2000 mini-batches</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">"[</span><span class="si">%d</span><span class="s2">, </span><span class="si">%5d</span><span class="s2">] loss: </span><span class="si">%.3f</span><span class="s2">"</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">epoch_steps</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Validation loss</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">val_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.max.html#torch.max" title="torch.max"><span class="n">torch</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="n">val_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Save checkpoint and report metrics</span>
        <span class="n">checkpoint_data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"epoch"</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s2">"net_state_dict"</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">"optimizer_state_dict"</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"checkpoint.pt"</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.save.html#torch.save" title="torch.save"><span class="n">torch</span><span class="o">.</span><span class="n">save</span></a><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
            <span class="n">tune</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">"loss"</span><span class="p">:</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="n">val_steps</span><span class="p">,</span> <span class="s2">"accuracy"</span><span class="p">:</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">},</span>
                <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Finished Training"</span><span class="p">)</span>
</pre></div>
</div>
<section id="key-integration-points">
<h2>Key integration points<a class="headerlink" href="#key-integration-points" title="Link to this heading">#</a></h2>
<section id="using-hyperparameters-from-the-configuration-dictionary">
<h3>Using hyperparameters from the configuration dictionary<a class="headerlink" href="#using-hyperparameters-from-the-configuration-dictionary" title="Link to this heading">#</a></h3>
<p>Ray Tune updates the <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary with the hyperparameters for
each trial. In this example, the model architecture and optimizer
receive the hyperparameters from the <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Net</span></a><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"l1"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">"l2"</span><span class="p">])</span>
<span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD" title="torch.optim.SGD"><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span></a><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="reporting-metrics-and-saving-checkpoints">
<h3>Reporting metrics and saving checkpoints<a class="headerlink" href="#reporting-metrics-and-saving-checkpoints" title="Link to this heading">#</a></h3>
<p>The most important integration is communicating with Ray Tune. Ray Tune
uses the validation metrics to determine the best hyperparameter
configuration and to stop underperforming trials early, saving
resources.</p>
<p>Checkpointing enables you to later load the trained models, resume
hyperparameter searches, and provides fault tolerance. It’s also
required for some Ray Tune schedulers like <a class="reference external" href="https://docs.ray.io/en/latest/tune/examples/pbt_guide.html">Population Based
Training</a>
that pause and resume trials during the search.</p>
<p>This code from the training function loads model and optimizer state at
the start if a checkpoint exists:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
<span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"checkpoint.pt"</span>
        <span class="n">checkpoint_state</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="torch.load"><span class="n">torch</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"epoch"</span><span class="p">]</span>
        <span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"net_state_dict"</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">"optimizer_state_dict"</span><span class="p">])</span>
</pre></div>
</div>
<p>At the end of each epoch, save a checkpoint and report the validation
metrics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint_data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"epoch"</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
    <span class="s2">"net_state_dict"</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">"optimizer_state_dict"</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
    <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"checkpoint.pt"</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.save.html#torch.save" title="torch.save"><span class="n">torch</span><span class="o">.</span><span class="n">save</span></a><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">"loss"</span><span class="p">:</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="n">val_steps</span><span class="p">,</span> <span class="s2">"accuracy"</span><span class="p">:</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">},</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Ray Tune checkpointing supports local file systems, cloud storage, and
distributed file systems. For more information, see the <a class="reference external" href="https://docs.ray.io/en/latest/tune/tutorials/tune-storage.html">Ray Tune
storage
documentation</a>.</p>
</section>
<section id="multi-gpu-support">
<h3>Multi-GPU support<a class="headerlink" href="#multi-gpu-support" title="Link to this heading">#</a></h3>
<p>Image classification models can be greatly accelerated by using GPUs.
The training function supports multi-GPU training by wrapping the model
in <code class="docutils literal notranslate"><span class="pre">nn.DataParallel</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span></a><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">net</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span></a><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p>This training function supports training on CPUs, a single GPU, multiple GPUs, or
multiple nodes without code changes. Ray Tune automatically distributes the trials
across the nodes according to the available resources. Ray Tune also supports <a class="reference external" href="https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html#fractional-accelerators">fractional
GPUs</a>
so that one GPU can be shared among multiple trials, provided that the
models, optimizers, and data batches fit into the GPU memory.</p>
</section>
<section id="validation-split">
<h3>Validation split<a class="headerlink" href="#validation-split" title="Link to this heading">#</a></h3>
<p>The original CIFAR10 dataset only has train and test subsets. This is
sufficient for training a single model, however for hyperparameter
tuning a validation subset is required. The training function creates a
validation subset by reserving 20% of the training subset. The test
subset is used to evaluate the best model’s generalization error after
the search completes.</p>
</section>
</section>
</section>
<section id="evaluation-function">
<h1>Evaluation function<a class="headerlink" href="#evaluation-function" title="Link to this heading">#</a></h1>
<p>After finding the optimal hyperparameters, test the model on a held-out
test set to estimate the generalization error:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">_trainset</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>

    <span class="n">testloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
            <span class="n">image_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">image_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">image_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">image_batch</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.max.html#torch.max" title="torch.max"><span class="n">torch</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
</pre></div>
</div>
</section>
<section id="configure-and-run-ray-tune">
<h1>Configure and run Ray Tune<a class="headerlink" href="#configure-and-run-ray-tune" title="Link to this heading">#</a></h1>
<p>With the training and evaluation functions defined, configure Ray Tune
to run the hyperparameter search.</p>
<section id="scheduler-for-early-stopping">
<h2>Scheduler for early stopping<a class="headerlink" href="#scheduler-for-early-stopping" title="Link to this heading">#</a></h2>
<p>Ray Tune provides schedulers to improve the efficiency of the
hyperparameter search by detecting underperforming trials and stopping
them early. The <code class="docutils literal notranslate"><span class="pre">ASHAScheduler</span></code> uses the Asynchronous Successive
Halving Algorithm (ASHA) to aggressively terminate low-performing
trials:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ASHAScheduler</span><span class="p">(</span>
    <span class="n">max_t</span><span class="o">=</span><span class="n">max_num_epochs</span><span class="p">,</span>
    <span class="n">grace_period</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Ray Tune also provides <a class="reference external" href="https://docs.ray.io/en/latest/tune/api/suggestion.html">advanced search
algorithms</a>
to smartly pick the next set of hyperparameters based on previous
results, instead of relying only on random or grid search. Examples
include
<a class="reference external" href="https://docs.ray.io/en/latest/tune/api/suggestion.html#optuna">Optuna</a>
and
<a class="reference external" href="https://docs.ray.io/en/latest/tune/api/suggestion.html#bayesopt">BayesOpt</a>.</p>
</section>
<section id="resource-allocation">
<h2>Resource allocation<a class="headerlink" href="#resource-allocation" title="Link to this heading">#</a></h2>
<p>Tell Ray Tune what resources to allocate for each trial by passing a
<code class="docutils literal notranslate"><span class="pre">resources</span></code> dictionary to <code class="docutils literal notranslate"><span class="pre">tune.with_resources</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
    <span class="n">partial</span><span class="p">(</span><span class="n">train_cifar</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">),</span>
    <span class="n">resources</span><span class="o">=</span><span class="p">{</span><span class="s2">"cpu"</span><span class="p">:</span> <span class="n">cpus_per_trial</span><span class="p">,</span> <span class="s2">"gpu"</span><span class="p">:</span> <span class="n">gpus_per_trial</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Ray Tune automatically manages the placement of these trials and ensures
that the trials run in isolation, so you don’t need to manually assign
GPUs to processes.</p>
<p>For example, if you are running this experiment on a cluster of 20
machines, each with 8 GPUs, you can set <code class="docutils literal notranslate"><span class="pre">gpus_per_trial</span> <span class="pre">=</span> <span class="pre">0.5</span></code> to
schedule two concurrent trials per GPU. This configuration runs 320
trials in parallel across the cluster.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To run this tutorial without GPUs, set <code class="docutils literal notranslate"><span class="pre">gpus_per_trial=0</span></code>
and expect significantly longer runtimes.</p>
<p>To avoid long runtimes during development, start with a small number
of trials and epochs.</p>
</div>
</section>
<section id="creating-the-tuner">
<h2>Creating the Tuner<a class="headerlink" href="#creating-the-tuner" title="Link to this heading">#</a></h2>
<p>The Ray Tune API is modular and composable. Pass your configuration to
the <code class="docutils literal notranslate"><span class="pre">tune.Tuner</span></code> class to create a tuner object, then run
<code class="docutils literal notranslate"><span class="pre">tuner.fit()</span></code> to start training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">train_cifar</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">),</span>
        <span class="n">resources</span><span class="o">=</span><span class="p">{</span><span class="s2">"cpu"</span><span class="p">:</span> <span class="n">cpus_per_trial</span><span class="p">,</span> <span class="s2">"gpu"</span><span class="p">:</span> <span class="n">gpus_per_trial</span><span class="p">}</span>
    <span class="p">),</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">"loss"</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_trials</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>After training completes, retrieve the best performing trial, load its
checkpoint, and evaluate on the test set.</p>
</section>
<section id="putting-it-all-together">
<h2>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">num_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gpus_per_trial</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cpus_per_trial</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Starting hyperparameter tuning."</span><span class="p">)</span>
    <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">include_dashboard</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">"./data"</span><span class="p">)</span>
    <span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>  <span class="c1"># Pre-download the dataset</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"l1"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
        <span class="s2">"l2"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
        <span class="s2">"lr"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span>
        <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span>
        <span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">ASHAScheduler</span><span class="p">(</span>
        <span class="n">max_t</span><span class="o">=</span><span class="n">max_num_epochs</span><span class="p">,</span>
        <span class="n">grace_period</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
        <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">train_cifar</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">),</span>
            <span class="n">resources</span><span class="o">=</span><span class="p">{</span><span class="s2">"cpu"</span><span class="p">:</span> <span class="n">cpus_per_trial</span><span class="p">,</span> <span class="s2">"gpu"</span><span class="p">:</span> <span class="n">gpus_per_trial</span><span class="p">}</span>
        <span class="p">),</span>
        <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span>
            <span class="n">metric</span><span class="o">=</span><span class="s2">"loss"</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">"min"</span><span class="p">,</span>
            <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_trials</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

    <span class="n">best_result</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">get_best_result</span><span class="p">(</span><span class="s2">"loss"</span><span class="p">,</span> <span class="s2">"min"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best trial config: </span><span class="si">{</span><span class="n">best_result</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best trial final validation loss: </span><span class="si">{</span><span class="n">best_result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best trial final validation accuracy: </span><span class="si">{</span><span class="n">best_result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">best_trained_model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Net</span></a><span class="p">(</span><span class="n">best_result</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"l1"</span><span class="p">],</span> <span class="n">best_result</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"l2"</span><span class="p">])</span>
    <span class="n">best_trained_model</span> <span class="o">=</span> <span class="n">best_trained_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gpus_per_trial</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">best_trained_model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span></a><span class="p">(</span><span class="n">best_trained_model</span><span class="p">)</span>

    <span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">best_result</span><span class="o">.</span><span class="n">checkpoint</span>
    <span class="k">with</span> <span class="n">best_checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"checkpoint.pt"</span>
        <span class="n">best_checkpoint_data</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="torch.load"><span class="n">torch</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="n">best_trained_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_checkpoint_data</span><span class="p">[</span><span class="s2">"net_state_dict"</span><span class="p">])</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_accuracy</span><span class="p">(</span><span class="n">best_trained_model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best trial test set accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="c1"># Set the number of trials, epochs, and GPUs per trial here:</span>
    <span class="n">main</span><span class="p">(</span><span class="n">num_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gpus_per_trial</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Starting hyperparameter tuning.
2026-02-19 00:21:27,068 WARNING services.py:2137 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147471360 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
2026-02-19 00:21:27,235 INFO worker.py:2023 -- Started a local Ray instance.
/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py:2062: FutureWarning:

Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0


  0%|          | 0.00/170M [00:00&lt;?, ?B/s]
  0%|          | 590k/170M [00:00&lt;00:28, 5.88MB/s]
  5%|▍         | 8.32M/170M [00:00&lt;00:03, 47.8MB/s]
 11%|█▏        | 19.5M/170M [00:00&lt;00:01, 76.8MB/s]
 18%|█▊        | 30.6M/170M [00:00&lt;00:01, 90.5MB/s]
 25%|██▍       | 41.8M/170M [00:00&lt;00:01, 98.1MB/s]
 31%|███▏      | 53.5M/170M [00:00&lt;00:01, 104MB/s]
 38%|███▊      | 65.2M/170M [00:00&lt;00:00, 109MB/s]
 45%|████▌     | 76.9M/170M [00:00&lt;00:00, 111MB/s]
 52%|█████▏    | 88.6M/170M [00:00&lt;00:00, 113MB/s]
 59%|█████▉    | 100M/170M [00:01&lt;00:00, 114MB/s]
 66%|██████▌   | 112M/170M [00:01&lt;00:00, 115MB/s]
 73%|███████▎  | 124M/170M [00:01&lt;00:00, 116MB/s]
 79%|███████▉  | 135M/170M [00:01&lt;00:00, 116MB/s]
 86%|████████▋ | 147M/170M [00:01&lt;00:00, 116MB/s]
 93%|█████████▎| 159M/170M [00:01&lt;00:00, 116MB/s]
100%|█████████▉| 170M/170M [00:01&lt;00:00, 116MB/s]
100%|██████████| 170M/170M [00:01&lt;00:00, 106MB/s]
╭────────────────────────────────────────────────────────────────────╮
│ Configuration for experiment     train_cifar_2026-02-19_00-21-32   │
├────────────────────────────────────────────────────────────────────┤
│ Search algorithm                 BasicVariantGenerator             │
│ Scheduler                        AsyncHyperBandScheduler           │
│ Number of trials                 10                                │
╰────────────────────────────────────────────────────────────────────╯

View detailed results here: /var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2026-02-19_00-21-25_906106_3915/artifacts/2026-02-19_00-21-32/train_cifar_2026-02-19_00-21-32/driver_artifacts`

Trial status: 10 PENDING
Current time: 2026-02-19 00:21:33. Total running time: 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
╭───────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size │
├───────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   PENDING     256     16   0.0127436                4 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4 │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16 │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2 │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4 │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2 │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16 │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8 │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4 │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4 │
╰───────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00000 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00000 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ device                                      cuda │
│ l1                                           256 │
│ l2                                            16 │
│ lr                                       0.01274 │
╰──────────────────────────────────────────────────╯
(func pid=5038) [1,  2000] loss: 2.229
(func pid=5038) [1,  4000] loss: 1.111
(func pid=5038) [1,  6000] loss: 0.742
(pid=gcs_server) [2026-02-19 00:21:56,163 E 3920 3920] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(raylet) [2026-02-19 00:21:57,169 E 4060 4060] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(bundle_reservation_check_func pid=4127) [2026-02-19 00:21:57,776 E 4127 4343] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=5038) [1,  8000] loss: 0.569

Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:22:03. Total running time: 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
╭───────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size │
├───────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4 │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16 │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2 │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4 │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2 │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16 │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8 │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4 │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4 │
╰───────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [2026-02-19 00:22:03,749 E 5038 5073] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 [repeated 14x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
(func pid=5038) [1, 10000] loss: 0.460
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000000)
(func pid=5038) [2,  2000] loss: 2.286
(func pid=5038) [2,  4000] loss: 1.155
(func pid=5038) [2,  6000] loss: 0.770
(func pid=5038) [2,  8000] loss: 0.577
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:22:33. Total running time: 1min 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.2855792963981627 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        1             33.694   2.28558       0.1153 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [2, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000001)
(func pid=5038) [3,  2000] loss: 2.309
(func pid=5038) [3,  4000] loss: 1.155
(func pid=5038) [3,  6000] loss: 0.770
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:23:03. Total running time: 1min 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.311649545574188 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        2             65.489   2.31165       0.1004 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [3,  8000] loss: 0.577
(func pid=5038) [3, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000002)
(func pid=5038) [4,  2000] loss: 2.311
(func pid=5038) [4,  4000] loss: 1.155
(func pid=5038) [4,  6000] loss: 0.770
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:23:33. Total running time: 2min 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.3093930989265443 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        3            96.8854   2.30939       0.1011 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [4,  8000] loss: 0.577
(func pid=5038) [4, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000003)
(func pid=5038) [5,  2000] loss: 2.310
(func pid=5038) [5,  4000] loss: 1.155
(func pid=5038) [5,  6000] loss: 0.770
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:24:03. Total running time: 2min 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.3065259016036985 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        4            128.365   2.30653       0.0989 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [5,  8000] loss: 0.578
(func pid=5038) [5, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000004)
(func pid=5038) [6,  2000] loss: 2.310
(func pid=5038) [6,  4000] loss: 1.155
(func pid=5038) [6,  6000] loss: 0.770
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:24:33. Total running time: 3min 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.3228442967414855 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        5            159.733   2.32284       0.1011 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [6,  8000] loss: 0.577
(func pid=5038) [6, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000005)
(func pid=5038) [7,  2000] loss: 2.309
(func pid=5038) [7,  4000] loss: 1.156
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:25:03. Total running time: 3min 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.3060613892555235 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        6            191.092   2.30606       0.0973 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [7,  6000] loss: 0.770
(func pid=5038) [7,  8000] loss: 0.578
(func pid=5038) [7, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000006)
(func pid=5038) [8,  2000] loss: 2.310
(func pid=5038) [8,  4000] loss: 1.155
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:25:33. Total running time: 4min 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.3189553537368774 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        7            222.355   2.31896       0.0973 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [8,  6000] loss: 0.770
(func pid=5038) [8,  8000] loss: 0.577
(func pid=5038) [8, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000007)
(func pid=5038) [9,  2000] loss: 2.310
(func pid=5038) [9,  4000] loss: 1.155
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:26:03. Total running time: 4min 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.310531533908844 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        8            253.395   2.31053       0.0973 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [9,  6000] loss: 0.770
(func pid=5038) [9,  8000] loss: 0.577
(func pid=5038) [9, 10000] loss: 0.462
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000008)
(func pid=5038) [10,  2000] loss: 2.310
(func pid=5038) [10,  4000] loss: 1.155
Trial status: 1 RUNNING | 9 PENDING
Current time: 2026-02-19 00:26:33. Total running time: 5min 0s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.306463604450226 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   RUNNING     256     16   0.0127436                4        9            284.858   2.30646       0.0977 │
│ train_cifar_f19bc_00001   PENDING       1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00002   PENDING      16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING      64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING      16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING      32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING      64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING       2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING       4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING       2     32   0.000418759              4                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5038) [10,  6000] loss: 0.770
(func pid=5038) [10,  8000] loss: 0.577
(func pid=5038) [10, 10000] loss: 0.462

Trial train_cifar_f19bc_00000 completed after 10 iterations at 2026-02-19 00:26:53. Total running time: 5min 20s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00000 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000009 │
│ time_this_iter_s                                  31.45353 │
│ time_total_s                                     316.31201 │
│ training_iteration                                      10 │
│ accuracy                                            0.0977 │
│ loss                                               2.30515 │
╰────────────────────────────────────────────────────────────╯
(func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000009)

Trial train_cifar_f19bc_00001 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00001 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ device                                      cuda │
│ l1                                             1 │
│ l2                                            16 │
│ lr                                       0.01513 │
╰──────────────────────────────────────────────────╯

Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2026-02-19 00:27:03. Total running time: 5min 30s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.305153544998169 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00001   RUNNING         1     16   0.0151318                4                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10            316.312   2.30515       0.0977 │
│ train_cifar_f19bc_00002   PENDING        16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING        64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=5779) [1,  2000] loss: 2.314
(func pid=5779) [1,  4000] loss: 1.156
(func pid=5779) [1,  6000] loss: 0.771
(func pid=5779) [1,  8000] loss: 0.578
(func pid=5779) [2026-02-19 00:27:24,637 E 5779 5814] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=5779) [1, 10000] loss: 0.462

Trial train_cifar_f19bc_00001 completed after 1 iterations at 2026-02-19 00:27:31. Total running time: 5min 58s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  33.60916 │
│ time_total_s                                      33.60916 │
│ training_iteration                                       1 │
│ accuracy                                            0.0996 │
│ loss                                               2.30601 │
╰────────────────────────────────────────────────────────────╯
(func pid=5779) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00001_1_batch_size=4,l1=1,l2=16,lr=0.0151_2026-02-19_00-21-32/checkpoint_000000)

Trial status: 2 TERMINATED | 8 PENDING
Current time: 2026-02-19 00:27:33. Total running time: 6min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.305153544998169 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   PENDING        16     64   0.0411267               16                                                    │
│ train_cifar_f19bc_00003   PENDING        64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00002 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00002 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ device                                      cuda │
│ l1                                            16 │
│ l2                                            64 │
│ lr                                       0.04113 │
╰──────────────────────────────────────────────────╯
(func pid=5912) [1,  2000] loss: 2.249

Trial train_cifar_f19bc_00002 completed after 1 iterations at 2026-02-19 00:27:46. Total running time: 6min 13s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00002 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  11.04507 │
│ time_total_s                                      11.04507 │
│ training_iteration                                       1 │
│ accuracy                                            0.0997 │
│ loss                                               2.30548 │
╰────────────────────────────────────────────────────────────╯
(func pid=5912) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00002_2_batch_size=16,l1=16,l2=64,lr=0.0411_2026-02-19_00-21-32/checkpoint_000000)

Trial train_cifar_f19bc_00003 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00003 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     2 │
│ device                                      cuda │
│ l1                                            64 │
│ l2                                            16 │
│ lr                                       0.00209 │
╰──────────────────────────────────────────────────╯
(func pid=6043) [1,  2000] loss: 2.173
(func pid=6043) [1,  4000] loss: 0.983

Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:28:03. Total running time: 6min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.305153544998169 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [1,  6000] loss: 0.617
(func pid=6043) [1,  8000] loss: 0.466
(func pid=6043) [2026-02-19 00:28:17,631 E 6043 6078] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=6043) [1, 10000] loss: 0.363
(func pid=6043) [1, 12000] loss: 0.302
(func pid=6043) [1, 14000] loss: 0.261
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:28:34. Total running time: 7min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00000 with loss=2.305153544998169 and params={'l1': 256, 'l2': 16, 'lr': 0.012743624317955983, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [1, 16000] loss: 0.226
(func pid=6043) [1, 18000] loss: 0.198
(func pid=6043) [1, 20000] loss: 0.181
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000000)
(func pid=6043) [2,  2000] loss: 1.750
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:29:04. Total running time: 7min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7880410368874669 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        1            63.7784   1.78804       0.3669 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [2,  4000] loss: 0.875
(func pid=6043) [2,  6000] loss: 0.578
(func pid=6043) [2,  8000] loss: 0.446
(func pid=6043) [2, 10000] loss: 0.351
(func pid=6043) [2, 12000] loss: 0.298
(func pid=6043) [2, 14000] loss: 0.256
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:29:34. Total running time: 8min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7880410368874669 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        1            63.7784   1.78804       0.3669 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [2, 16000] loss: 0.220
(func pid=6043) [2, 18000] loss: 0.197
(func pid=6043) [2, 20000] loss: 0.180
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000001)
(func pid=6043) [3,  2000] loss: 1.743
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:30:04. Total running time: 8min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7180798332419247 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        2           125.663    1.71808       0.3757 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [3,  4000] loss: 0.868
(func pid=6043) [3,  6000] loss: 0.593
(func pid=6043) [3,  8000] loss: 0.439
(func pid=6043) [3, 10000] loss: 0.360
(func pid=6043) [3, 12000] loss: 0.291
(func pid=6043) [3, 14000] loss: 0.252
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:30:34. Total running time: 9min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7180798332419247 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        2           125.663    1.71808       0.3757 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [3, 16000] loss: 0.221
(func pid=6043) [3, 18000] loss: 0.200
(func pid=6043) [3, 20000] loss: 0.182
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000002)
(func pid=6043) [4,  2000] loss: 1.751
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:31:04. Total running time: 9min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7497133266493679 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        3           187.743    1.74971       0.3626 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [4,  4000] loss: 0.885
(func pid=6043) [4,  6000] loss: 0.589
(func pid=6043) [4,  8000] loss: 0.452
(func pid=6043) [4, 10000] loss: 0.363
(func pid=6043) [4, 12000] loss: 0.302
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:31:34. Total running time: 10min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.7497133266493679 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        3           187.743    1.74971       0.3626 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [4, 14000] loss: 0.258
(func pid=6043) [4, 16000] loss: 0.223
(func pid=6043) [4, 18000] loss: 0.199
(func pid=6043) [4, 20000] loss: 0.180
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000003)
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:32:04. Total running time: 10min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.8110783561166375 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        4           249.872    1.81108       0.3437 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [5,  2000] loss: 1.830
(func pid=6043) [5,  4000] loss: 0.905
(func pid=6043) [5,  6000] loss: 0.609
(func pid=6043) [5,  8000] loss: 0.443
(func pid=6043) [5, 10000] loss: 0.355
(func pid=6043) [5, 12000] loss: 0.292
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:32:34. Total running time: 11min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.8110783561166375 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        4           249.872    1.81108       0.3437 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [5, 14000] loss: 0.258
(func pid=6043) [5, 16000] loss: 0.224
(func pid=6043) [5, 18000] loss: 0.198
(func pid=6043) [5, 20000] loss: 0.181
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000004)
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:33:04. Total running time: 11min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.759163848245889 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        5           311.983    1.75916       0.3694 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [6,  2000] loss: 1.818
(func pid=6043) [6,  4000] loss: 0.908
(func pid=6043) [6,  6000] loss: 0.607
(func pid=6043) [6,  8000] loss: 0.457
(func pid=6043) [6, 10000] loss: 0.363
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:33:34. Total running time: 12min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.759163848245889 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        5           311.983    1.75916       0.3694 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [6, 12000] loss: 0.307
(func pid=6043) [6, 14000] loss: 0.262
(func pid=6043) [6, 16000] loss: 0.237
(func pid=6043) [6, 18000] loss: 0.203
(func pid=6043) [6, 20000] loss: 0.187
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:34:04. Total running time: 12min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.759163848245889 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        5           311.983    1.75916       0.3694 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000005)
(func pid=6043) [7,  2000] loss: 1.836
(func pid=6043) [7,  4000] loss: 0.912
(func pid=6043) [7,  6000] loss: 0.611
(func pid=6043) [7,  8000] loss: 0.451
(func pid=6043) [7, 10000] loss: 0.361
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:34:34. Total running time: 13min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.8926031447559595 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        6           374.083    1.8926        0.3244 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [7, 12000] loss: 0.309
(func pid=6043) [7, 14000] loss: 0.261
(func pid=6043) [7, 16000] loss: 0.227
(func pid=6043) [7, 18000] loss: 0.208
(func pid=6043) [7, 20000] loss: 0.179
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:35:04. Total running time: 13min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.8926031447559595 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        6           374.083    1.8926        0.3244 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000006)
(func pid=6043) [8,  2000] loss: 1.846
(func pid=6043) [8,  4000] loss: 0.905
(func pid=6043) [8,  6000] loss: 0.624
(func pid=6043) [8,  8000] loss: 0.465
(func pid=6043) [8, 10000] loss: 0.377
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:35:34. Total running time: 14min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=2.000167779254168 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        7           436.423    2.00017       0.2984 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [8, 12000] loss: 0.309
(func pid=6043) [8, 14000] loss: 0.279
(func pid=6043) [8, 16000] loss: 0.240
(func pid=6043) [8, 18000] loss: 0.208
(func pid=6043) [8, 20000] loss: 0.191
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:36:04. Total running time: 14min 31s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=2.000167779254168 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        7           436.423    2.00017       0.2984 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000007)
(func pid=6043) [9,  2000] loss: 1.863
(func pid=6043) [9,  4000] loss: 0.929
(func pid=6043) [9,  6000] loss: 0.622
(func pid=6043) [9,  8000] loss: 0.461
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:36:34. Total running time: 15min 1s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9275328431497094 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        8           497.929    1.92753       0.3171 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [9, 10000] loss: 0.370
(func pid=6043) [9, 12000] loss: 0.325
(func pid=6043) [9, 14000] loss: 0.277
(func pid=6043) [9, 16000] loss: 0.243
(func pid=6043) [9, 18000] loss: 0.213
(func pid=6043) [9, 20000] loss: 0.185
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:37:04. Total running time: 15min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9275328431497094 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        8           497.929    1.92753       0.3171 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000008)
(func pid=6043) [10,  2000] loss: 1.848
(func pid=6043) [10,  4000] loss: 0.959
(func pid=6043) [10,  6000] loss: 0.632
(func pid=6043) [10,  8000] loss: 0.470
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:37:34. Total running time: 16min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9394293992295861 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        9           559.907    1.93943       0.295  │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6043) [10, 10000] loss: 0.369
(func pid=6043) [10, 12000] loss: 0.315
(func pid=6043) [10, 14000] loss: 0.264
(func pid=6043) [10, 16000] loss: 0.242
(func pid=6043) [10, 18000] loss: 0.212
(func pid=6043) [10, 20000] loss: 0.192
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2026-02-19 00:38:05. Total running time: 16min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9394293992295861 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00003   RUNNING        64     16   0.00209069               2        9           559.907    1.93943       0.295  │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00004   PENDING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00003 completed after 10 iterations at 2026-02-19 00:38:12. Total running time: 16min 39s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00003 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000009 │
│ time_this_iter_s                                  61.70261 │
│ time_total_s                                     621.60953 │
│ training_iteration                                      10 │
│ accuracy                                            0.3082 │
│ loss                                               1.90101 │
╰────────────────────────────────────────────────────────────╯
(func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000009)

Trial train_cifar_f19bc_00004 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00004 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ device                                      cuda │
│ l1                                            16 │
│ l2                                           128 │
│ lr                                       0.01055 │
╰──────────────────────────────────────────────────╯
(func pid=6810) [1,  2000] loss: 2.169
(func pid=6810) [1,  4000] loss: 1.065
(func pid=6810) [1,  6000] loss: 0.737

Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2026-02-19 00:38:35. Total running time: 17min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00004   RUNNING        16    128   0.0105504                4                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6810) [1,  8000] loss: 0.559
(func pid=6810) [2026-02-19 00:38:43,722 E 6810 6845] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=6810) [1, 10000] loss: 0.446
(func pid=6810) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00004_4_batch_size=4,l1=16,l2=128,lr=0.0106_2026-02-19_00-21-32/checkpoint_000000)
(func pid=6810) [2,  2000] loss: 2.260
(func pid=6810) [2,  4000] loss: 1.149
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2026-02-19 00:39:05. Total running time: 17min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00004   RUNNING        16    128   0.0105504                4        1            33.4177   2.2142        0.1507 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00005   PENDING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=6810) [2,  6000] loss: 0.755
(func pid=6810) [2,  8000] loss: 0.568
(func pid=6810) [2, 10000] loss: 0.462

Trial train_cifar_f19bc_00004 completed after 2 iterations at 2026-02-19 00:39:21. Total running time: 17min 48s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00004 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                  31.36562 │
│ time_total_s                                      64.78331 │
│ training_iteration                                       2 │
│ accuracy                                            0.1006 │
│ loss                                               2.30682 │
╰────────────────────────────────────────────────────────────╯
(func pid=6810) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00004_4_batch_size=4,l1=16,l2=128,lr=0.0106_2026-02-19_00-21-32/checkpoint_000001)

Trial train_cifar_f19bc_00005 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00005 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     2 │
│ device                                      cuda │
│ l1                                            32 │
│ l2                                             4 │
│ lr                                       0.00244 │
╰──────────────────────────────────────────────────╯
(func pid=7010) [1,  2000] loss: 2.247

Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:39:35. Total running time: 18min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [1,  4000] loss: 1.048
(func pid=7010) [1,  6000] loss: 0.680
(func pid=7010) [1,  8000] loss: 0.504
(func pid=7010) [2026-02-19 00:39:52,730 E 7010 7045] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=7010) [1, 10000] loss: 0.398
(func pid=7010) [1, 12000] loss: 0.330
(func pid=7010) [1, 14000] loss: 0.284
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:40:05. Total running time: 18min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [1, 16000] loss: 0.252
(func pid=7010) [1, 18000] loss: 0.217
(func pid=7010) [1, 20000] loss: 0.193
(func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000000)
(func pid=7010) [2,  2000] loss: 1.936
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:40:35. Total running time: 19min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        1            63.2569   1.9359        0.2656 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [2,  4000] loss: 0.957
(func pid=7010) [2,  6000] loss: 0.629
(func pid=7010) [2,  8000] loss: 0.487
(func pid=7010) [2, 10000] loss: 0.390
(func pid=7010) [2, 12000] loss: 0.324
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:41:05. Total running time: 19min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        1            63.2569   1.9359        0.2656 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [2, 14000] loss: 0.276
(func pid=7010) [2, 16000] loss: 0.246
(func pid=7010) [2, 18000] loss: 0.216
(func pid=7010) [2, 20000] loss: 0.195
(func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000001)
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:41:35. Total running time: 20min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        2           124.7      1.98254       0.2347 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [3,  2000] loss: 1.940
(func pid=7010) [3,  4000] loss: 0.984
(func pid=7010) [3,  6000] loss: 0.646
(func pid=7010) [3,  8000] loss: 0.482
(func pid=7010) [3, 10000] loss: 0.386
(func pid=7010) [3, 12000] loss: 0.323
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:42:05. Total running time: 20min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        2           124.7      1.98254       0.2347 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [3, 14000] loss: 0.269
(func pid=7010) [3, 16000] loss: 0.239
(func pid=7010) [3, 18000] loss: 0.216
(func pid=7010) [3, 20000] loss: 0.194
(func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000002)
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:42:35. Total running time: 21min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00005 with loss=1.8970869152605534 and params={'l1': 32, 'l2': 4, 'lr': 0.002438900339047484, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        3           185.479    1.89709       0.27   │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [4,  2000] loss: 1.898
(func pid=7010) [4,  4000] loss: 0.968
(func pid=7010) [4,  6000] loss: 0.639
(func pid=7010) [4,  8000] loss: 0.485
(func pid=7010) [4, 10000] loss: 0.380
(func pid=7010) [4, 12000] loss: 0.316
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2026-02-19 00:43:05. Total running time: 21min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00005 with loss=1.8970869152605534 and params={'l1': 32, 'l2': 4, 'lr': 0.002438900339047484, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00005   RUNNING        32      4   0.0024389                2        3           185.479    1.89709       0.27   │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7010) [4, 14000] loss: 0.271
(func pid=7010) [4, 16000] loss: 0.244
(func pid=7010) [4, 18000] loss: 0.217
(func pid=7010) [4, 20000] loss: 0.199

Trial train_cifar_f19bc_00005 completed after 4 iterations at 2026-02-19 00:43:32. Total running time: 21min 59s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00005 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000003 │
│ time_this_iter_s                                   61.3401 │
│ time_total_s                                     246.81928 │
│ training_iteration                                       4 │
│ accuracy                                            0.1474 │
│ loss                                                2.2912 │
╰────────────────────────────────────────────────────────────╯
(func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000003)

Trial status: 6 TERMINATED | 4 PENDING
Current time: 2026-02-19 00:43:35. Total running time: 22min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   PENDING        64    128   0.090221                16                                                    │
│ train_cifar_f19bc_00007   PENDING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00006 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00006 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ device                                      cuda │
│ l1                                            64 │
│ l2                                           128 │
│ lr                                       0.09022 │
╰──────────────────────────────────────────────────╯
(func pid=7356) [1,  2000] loss: 2.198

Trial train_cifar_f19bc_00006 completed after 1 iterations at 2026-02-19 00:43:47. Total running time: 22min 14s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00006 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  11.06867 │
│ time_total_s                                      11.06867 │
│ training_iteration                                       1 │
│ accuracy                                            0.1415 │
│ loss                                               2.25626 │
╰────────────────────────────────────────────────────────────╯
(func pid=7356) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00006_6_batch_size=16,l1=64,l2=128,lr=0.0902_2026-02-19_00-21-32/checkpoint_000000)

Trial train_cifar_f19bc_00007 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00007 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     8 │
│ device                                      cuda │
│ l1                                             2 │
│ l2                                             2 │
│ lr                                       0.02633 │
╰──────────────────────────────────────────────────╯
(func pid=7486) [1,  2000] loss: 2.311
(func pid=7486) [1,  4000] loss: 1.155

Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2026-02-19 00:44:05. Total running time: 22min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00007   RUNNING         2      2   0.0263252                8                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00008   PENDING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00007 completed after 1 iterations at 2026-02-19 00:44:10. Total running time: 22min 37s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00007 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  18.46077 │
│ time_total_s                                      18.46077 │
│ training_iteration                                       1 │
│ accuracy                                            0.0983 │
│ loss                                               2.31514 │
╰────────────────────────────────────────────────────────────╯
(func pid=7486) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00007_7_batch_size=8,l1=2,l2=2,lr=0.0263_2026-02-19_00-21-32/checkpoint_000000)

Trial train_cifar_f19bc_00008 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00008 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ device                                      cuda │
│ l1                                             4 │
│ l2                                            64 │
│ lr                                       0.01741 │
╰──────────────────────────────────────────────────╯
(func pid=7618) [1,  2000] loss: 2.314
(func pid=7618) [1,  4000] loss: 1.156
(func pid=7618) [1,  6000] loss: 0.770

Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2026-02-19 00:44:35. Total running time: 23min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00008   RUNNING         4     64   0.0174143                4                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00009   PENDING         2     32   0.000418759              4                                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7618) [1,  8000] loss: 0.578
(func pid=7618) [2026-02-19 00:44:41,753 E 7618 7653] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=7618) [1, 10000] loss: 0.463

Trial train_cifar_f19bc_00008 completed after 1 iterations at 2026-02-19 00:44:48. Total running time: 23min 15s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00008 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  33.60499 │
│ time_total_s                                      33.60499 │
│ training_iteration                                       1 │
│ accuracy                                               0.1 │
│ loss                                               2.31214 │
╰────────────────────────────────────────────────────────────╯
(func pid=7618) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00008_8_batch_size=4,l1=4,l2=64,lr=0.0174_2026-02-19_00-21-33/checkpoint_000000)

Trial train_cifar_f19bc_00009 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00009 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ device                                      cuda │
│ l1                                             2 │
│ l2                                            32 │
│ lr                                       0.00042 │
╰──────────────────────────────────────────────────╯
(func pid=7750) [1,  2000] loss: 2.273
(func pid=7750) [1,  4000] loss: 1.034

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:45:05. Total running time: 23min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={'l1': 64, 'l2': 16, 'lr': 0.0020906918730271263, 'batch_size': 2, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4                                                    │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [1,  6000] loss: 0.656
(func pid=7750) [1,  8000] loss: 0.478
(func pid=7750) [2026-02-19 00:45:19,758 E 7750 7785] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(func pid=7750) [1, 10000] loss: 0.364
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000000)
(func pid=7750) [2,  2000] loss: 1.774
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:45:35. Total running time: 24min 2s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.8082569658756256 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        1            33.7361   1.80826       0.2745 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [2,  4000] loss: 0.874
(func pid=7750) [2,  6000] loss: 0.578
(func pid=7750) [2,  8000] loss: 0.427
(func pid=7750) [2, 10000] loss: 0.341
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000001)
(func pid=7750) [3,  2000] loss: 1.657
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:46:05. Total running time: 24min 32s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.7081201287031174 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        2            64.9707   1.70812       0.308  │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [3,  4000] loss: 0.836
(func pid=7750) [3,  6000] loss: 0.548
(func pid=7750) [3,  8000] loss: 0.402
(func pid=7750) [3, 10000] loss: 0.323
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000002)
(func pid=7750) [4,  2000] loss: 1.611
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:46:35. Total running time: 25min 3s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.7130692383050918 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        3            96.6171   1.71307       0.3287 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [4,  4000] loss: 0.797
(func pid=7750) [4,  6000] loss: 0.530
(func pid=7750) [4,  8000] loss: 0.394
(func pid=7750) [4, 10000] loss: 0.314
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000003)
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:47:05. Total running time: 25min 33s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.6541795006990432 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        4           128.138    1.65418       0.3575 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [5,  2000] loss: 1.560
(func pid=7750) [5,  4000] loss: 0.791
(func pid=7750) [5,  6000] loss: 0.519
(func pid=7750) [5,  8000] loss: 0.389
(func pid=7750) [5, 10000] loss: 0.308
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000004)
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:47:36. Total running time: 26min 3s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.574478276658058 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        5           159.744    1.57448       0.3775 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [6,  2000] loss: 1.530
(func pid=7750) [6,  4000] loss: 0.768
(func pid=7750) [6,  6000] loss: 0.511
(func pid=7750) [6,  8000] loss: 0.385
(func pid=7750) [6, 10000] loss: 0.306
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000005)
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:48:06. Total running time: 26min 33s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.5453639246702193 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        6           191.206    1.54536       0.3986 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [7,  2000] loss: 1.509
(func pid=7750) [7,  4000] loss: 0.758
(func pid=7750) [7,  6000] loss: 0.507
(func pid=7750) [7,  8000] loss: 0.378
(func pid=7750) [7, 10000] loss: 0.303
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000006)
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:48:36. Total running time: 27min 3s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.539768257367611 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        7           222.796    1.53977       0.4019 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) [8,  2000] loss: 1.491
(func pid=7750) [8,  4000] loss: 0.754
(func pid=7750) [8,  6000] loss: 0.497
(func pid=7750) [8,  8000] loss: 0.375
(func pid=7750) [8, 10000] loss: 0.301
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:49:06. Total running time: 27min 33s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.539768257367611 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        7           222.796    1.53977       0.4019 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000007)
(func pid=7750) [9,  2000] loss: 1.489
(func pid=7750) [9,  4000] loss: 0.743
(func pid=7750) [9,  6000] loss: 0.491
(func pid=7750) [9,  8000] loss: 0.371
(func pid=7750) [9, 10000] loss: 0.299
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:49:36. Total running time: 28min 3s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.5236012852072716 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        8           254.392    1.5236        0.4036 │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000008)
(func pid=7750) [10,  2000] loss: 1.488
(func pid=7750) [10,  4000] loss: 0.734
(func pid=7750) [10,  6000] loss: 0.495
(func pid=7750) [10,  8000] loss: 0.369
(func pid=7750) [10, 10000] loss: 0.295
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2026-02-19 00:50:06. Total running time: 28min 33s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.5332289441108704 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00009   RUNNING         2     32   0.000418759              4        9           286.01     1.53323       0.405  │
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f19bc_00009 completed after 10 iterations at 2026-02-19 00:50:09. Total running time: 28min 37s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f19bc_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000009 │
│ time_this_iter_s                                  31.09251 │
│ time_total_s                                     317.10224 │
│ training_iteration                                      10 │
│ accuracy                                            0.4094 │
│ loss                                               1.52454 │
╰────────────────────────────────────────────────────────────╯
2026-02-19 00:50:09,949 INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32' in 0.0103s.

Trial status: 10 TERMINATED
Current time: 2026-02-19 00:50:09. Total running time: 28min 37s
Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G)
Current best trial: f19bc_00009 with loss=1.5245350178837775 and params={'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f19bc_00000   TERMINATED    256     16   0.0127436                4       10           316.312    2.30515       0.0977 │
│ train_cifar_f19bc_00001   TERMINATED      1     16   0.0151318                4        1            33.6092   2.30601       0.0996 │
│ train_cifar_f19bc_00002   TERMINATED     16     64   0.0411267               16        1            11.0451   2.30548       0.0997 │
│ train_cifar_f19bc_00003   TERMINATED     64     16   0.00209069               2       10           621.61     1.90101       0.3082 │
│ train_cifar_f19bc_00004   TERMINATED     16    128   0.0105504                4        2            64.7833   2.30682       0.1006 │
│ train_cifar_f19bc_00005   TERMINATED     32      4   0.0024389                2        4           246.819    2.2912        0.1474 │
│ train_cifar_f19bc_00006   TERMINATED     64    128   0.090221                16        1            11.0687   2.25626       0.1415 │
│ train_cifar_f19bc_00007   TERMINATED      2      2   0.0263252                8        1            18.4608   2.31514       0.0983 │
│ train_cifar_f19bc_00008   TERMINATED      4     64   0.0174143                4        1            33.605    2.31214       0.1    │
│ train_cifar_f19bc_00009   TERMINATED      2     32   0.000418759              4       10           317.102    1.52454       0.4094 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Best trial config: {'l1': 2, 'l2': 32, 'lr': 0.0004187588842664496, 'batch_size': 4, 'device': 'cuda'}
Best trial final validation loss: 1.5245350178837775
Best trial final validation accuracy: 0.4094
(func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000009)
Best trial test set accuracy: 0.4001
</pre></div>
</div>
</section>
</section>
<section id="results">
<h1>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h1>
<p>Your Ray Tune trial summary output looks something like this. The text
table summarizes the validation performance of the trials and highlights
the best hyperparameter configuration:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Number<span class="w"> </span>of<span class="w"> </span>trials:<span class="w"> </span><span class="m">10</span>/10<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>TERMINATED<span class="o">)</span>
+-----+--------------+------+------+-------------+--------+---------+------------+
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">   </span>batch_size<span class="w"> </span><span class="p">|</span><span class="w">   </span>l1<span class="w"> </span><span class="p">|</span><span class="w">   </span>l2<span class="w"> </span><span class="p">|</span><span class="w">          </span>lr<span class="w"> </span><span class="p">|</span><span class="w">   </span>iter<span class="w"> </span><span class="p">|</span><span class="w">    </span>loss<span class="w"> </span><span class="p">|</span><span class="w">   </span>accuracy<span class="w"> </span><span class="p">|</span>
<span class="p">|</span>-----+--------------+------+------+-------------+--------+---------+------------<span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000668163<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.31479<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0977<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.0331514<span class="w">   </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.31605<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0983<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000150295<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.30755<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.1023<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">           </span><span class="m">16</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.0128248<span class="w">   </span><span class="p">|</span><span class="w">     </span><span class="m">10</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.66912<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.4391<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">128</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00464561<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.7316<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.3463<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00031556<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.19409<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.1736<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">16</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00574329<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.85679<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.3368<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00325652<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.30272<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0984<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000342987<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.76044<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.292<span class="w">  </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.003734<span class="w">    </span><span class="p">|</span><span class="w">      </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.53101<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.4761<span class="w"> </span><span class="p">|</span>
+-----+--------------+------+------+-------------+--------+---------+------------+

Best<span class="w"> </span>trial<span class="w"> </span>config:<span class="w"> </span><span class="o">{</span><span class="s1">'l1'</span>:<span class="w"> </span><span class="m">64</span>,<span class="w"> </span><span class="s1">'l2'</span>:<span class="w"> </span><span class="m">32</span>,<span class="w"> </span><span class="s1">'lr'</span>:<span class="w"> </span><span class="m">0</span>.0037339984519545164,<span class="w"> </span><span class="s1">'batch_size'</span>:<span class="w"> </span><span class="m">4</span><span class="o">}</span>
Best<span class="w"> </span>trial<span class="w"> </span>final<span class="w"> </span>validation<span class="w"> </span>loss:<span class="w"> </span><span class="m">1</span>.5310075663924216
Best<span class="w"> </span>trial<span class="w"> </span>final<span class="w"> </span>validation<span class="w"> </span>accuracy:<span class="w"> </span><span class="m">0</span>.4761
Best<span class="w"> </span>trial<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nb">set</span><span class="w"> </span>accuracy:<span class="w"> </span><span class="m">0</span>.4737
</pre></div>
</div>
<p>Most trials stopped early to conserve resources. The best performing
trial achieved a validation accuracy of approximately 47%, which the
test set confirms.</p>
</section>
<section id="observability">
<h1>Observability<a class="headerlink" href="#observability" title="Link to this heading">#</a></h1>
<p>Monitoring is critical when running large-scale experiments. Ray
provides a
<a class="reference external" href="https://docs.ray.io/en/latest/ray-observability/getting-started.html">dashboard</a>
that lets you view the status of your trials, check cluster resource
use, and inspect logs in real time.</p>
<p>For debugging, Ray also offers <a class="reference external" href="https://docs.ray.io/en/latest/ray-observability/index.html">distributed debugging
tools</a>
that let you attach a debugger to running trials across the cluster.</p>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>In this tutorial, you learned how to tune the hyperparameters of a
PyTorch model using Ray Tune. You saw how to integrate Ray Tune into
your PyTorch training loop, define a search space for your
hyperparameters, use an efficient scheduler like <code class="docutils literal notranslate"><span class="pre">ASHAScheduler</span></code> to
terminate low-performing trials early, save checkpoints and report
metrics to Ray Tune, and run the hyperparameter search and analyze the
results.</p>
<p>Ray Tune makes it straightforward to scale your experiments from a
single machine to a large cluster, helping you find the best model
configuration efficiently.</p>
</section>
<section id="further-reading">
<h1>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune
documentation</a></p></li>
<li><p><a class="reference external" href="https://docs.ray.io/en/latest/tune/examples/index.html">Ray Tune
examples</a></p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (28 minutes 50.467 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-hyperparameter-tuning-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/30bcc2970bf630097b13789b5cdcea48/hyperparameter_tuning_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">hyperparameter_tuning_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b2e3bdbf14ea1e9b3a80770f0a498037/hyperparameter_tuning_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">hyperparameter_tuning_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/1e0488dfc19f08d47b44e8a248ce666e/hyperparameter_tuning_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">hyperparameter_tuning_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</article>
</div>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../ecosystem.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Ecosystem</p>
</div>
</a>
<a class="right-next" href="../intermediate/ax_multiobjective_nas_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Multi-Objective NAS with Ax</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../ecosystem.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Ecosystem</p>
</div>
</a>
<a class="right-next" href="../intermediate/ax_multiobjective_nas_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Multi-Objective NAS with Ax</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Hyperparameter tuning using Ray Tune</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loading">Data loading</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">Model architecture</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-search-space">Define the search space</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-function">Training function</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-integration-points">Key integration points</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hyperparameters-from-the-configuration-dictionary">Using hyperparameters from the configuration dictionary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reporting-metrics-and-saving-checkpoints">Reporting metrics and saving checkpoints</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-support">Multi-GPU support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-split">Validation split</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-function">Evaluation function</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-and-run-ray-tune">Configure and run Ray Tune</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scheduler-for-early-stopping">Scheduler for early stopping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resource-allocation">Resource allocation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-tuner">Creating the Tuner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting it all together</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#observability">Observability</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/helion" style="color: var(--pst-color-text-muted)">Helion</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/kineto" style="color: var(--pst-color-text-muted)">kineto</a></li>
<li><a class="nav-link nav-external" href="https://github.com/pytorch/torchtitan" style="color: var(--pst-color-text-muted)">torchtitan</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/rl" style="color: var(--pst-color-text-muted)">TorchRL</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/audio" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/tensordict" style="color: var(--pst-color-text-muted)">tensordict</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Hyperparameter tuning using Ray Tune",
       "headline": "Hyperparameter tuning using Ray Tune",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/beginner/hyperparameter_tuning_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. Hyperparameter tuning using Ray Tune# Author: Ricardo Decal This tutorial shows how to integrate Ray Tune into your PyTorch training workflow to perform scalable and efficient hyperparameter tuning. What you will learn How to modify a PyTorch training loop for Ray Tune How to scale a hyperparameter sweep to multiple nodes and GPUs without code changes How to define a hyperparameter search space and run a sweep with tune.Tuner How to use an early-stopping scheduler (ASHA) and report metrics/checkpoints How to use checkpointing to resume training and load the best model Prerequisites PyTorch v2.9+ and torchvision Ray Tune (ray[tune]) v2.52.1+ GPU(s) are optional, but recommended for faster training Ray, a project of the PyTorch Foundation, is an open source unified framework for scaling AI and Python applications. It helps run distributed jobs by handling the complexity of distributed computing. Ray Tune is a library built on Ray for hyperparameter tuning that enables you to scale a hyperparameter sweep from your machine to a large cluster with no code changes. This tutorial adapts the PyTorch tutorial for training a CIFAR10 classifier to run multi-GPU hyperparameter sweeps with Ray Tune. Setup# To run this tutorial, install the following dependencies: pip install \"ray[tune]\" torchvision Then start with the imports: from functools import partial import os import tempfile from pathlib import Path import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import random_split import torchvision import torchvision.transforms as transforms # New: imports for Ray Tune import ray from ray import tune from ray.tune import Checkpoint from ray.tune.schedulers import ASHAScheduler Data loading# Wrap the data loaders in a constructor function. In this tutorial, a global data directory is passed to the function to enable reusing the dataset across different trials. In a cluster environment, you can use shared storage, such as network file systems, to prevent each node from downloading the data separately. def load_data(data_dir=\"./data\"): # Mean and standard deviation of the CIFAR10 training subset. transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.2022, 0.19932, 0.20086))] ) trainset = torchvision.datasets.CIFAR10( root=data_dir, train=True, download=True, transform=transform ) testset = torchvision.datasets.CIFAR10( root=data_dir, train=False, download=True, transform=transform ) return trainset, testset Model architecture# This tutorial searches for the best sizes for the fully connected layers and the learning rate. To enable this, the Net class exposes the layer sizes l1 and l2 as configurable parameters that Ray Tune can search over: class Net(nn.Module): def __init__(self, l1=120, l2=84): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, l1) self.fc2 = nn.Linear(l1, l2) self.fc3 = nn.Linear(l2, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x Define the search space# Next, define the hyperparameters to tune and how Ray Tune samples them. Ray Tune offers a variety of search space distributions to suit different parameter types: loguniform, uniform, choice, randint, grid, and more. You can also express complex dependencies between parameters with conditional search spaces or sample from arbitrary functions. Here is the search space for this tutorial: config = { \"l1\": tune.choice([2**i for i in range(9)]), \"l2\": tune.choice([2**i for i in range(9)]), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]), } The tune.choice() accepts a list of values that are uniformly sampled from. In this example, the l1 and l2 parameter values are powers of 2 between 1 and 256, and the learning rate samples on a log scale between 0.0001 and 0.1. Sampling on a log scale enables exploration across a range of magnitudes on a relative scale, rather than an absolute scale. Training function# Ray Tune requires a training function that accepts a configuration dictionary and runs the main training loop. As Ray Tune runs different trials, it updates the configuration dictionary for each trial. Here is the full training function, followed by explanations of the key Ray Tune integration points: def train_cifar(config, data_dir=None): net = Net(config[\"l1\"], config[\"l2\"]) device = config[\"device\"] net = net.to(device) if torch.cuda.device_count() \u003e 1: net = nn.DataParallel(net) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) # Load checkpoint if resuming training checkpoint = tune.get_checkpoint() if checkpoint: with checkpoint.as_directory() as checkpoint_dir: checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\" checkpoint_state = torch.load(checkpoint_path) start_epoch = checkpoint_state[\"epoch\"] net.load_state_dict(checkpoint_state[\"net_state_dict\"]) optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"]) else: start_epoch = 0 trainset, _testset = load_data(data_dir) test_abs = int(len(trainset) * 0.8) train_subset, val_subset = random_split( trainset, [test_abs, len(trainset) - test_abs] ) trainloader = torch.utils.data.DataLoader( train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8 ) valloader = torch.utils.data.DataLoader( val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8 ) for epoch in range(start_epoch, 10): # loop over the dataset multiple times running_loss = 0.0 epoch_steps = 0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() epoch_steps += 1 if i % 2000 == 1999: # print every 2000 mini-batches print( \"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps) ) running_loss = 0.0 # Validation loss val_loss = 0.0 val_steps = 0 total = 0 correct = 0 for i, data in enumerate(valloader, 0): with torch.no_grad(): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) outputs = net(inputs) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() loss = criterion(outputs, labels) val_loss += loss.cpu().numpy() val_steps += 1 # Save checkpoint and report metrics checkpoint_data = { \"epoch\": epoch, \"net_state_dict\": net.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), } with tempfile.TemporaryDirectory() as checkpoint_dir: checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\" torch.save(checkpoint_data, checkpoint_path) checkpoint = Checkpoint.from_directory(checkpoint_dir) tune.report( {\"loss\": val_loss / val_steps, \"accuracy\": correct / total}, checkpoint=checkpoint, ) print(\"Finished Training\") Key integration points# Using hyperparameters from the configuration dictionary# Ray Tune updates the config dictionary with the hyperparameters for each trial. In this example, the model architecture and optimizer receive the hyperparameters from the config dictionary: net = Net(config[\"l1\"], config[\"l2\"]) optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) Reporting metrics and saving checkpoints# The most important integration is communicating with Ray Tune. Ray Tune uses the validation metrics to determine the best hyperparameter configuration and to stop underperforming trials early, saving resources. Checkpointing enables you to later load the trained models, resume hyperparameter searches, and provides fault tolerance. It\u2019s also required for some Ray Tune schedulers like Population Based Training that pause and resume trials during the search. This code from the training function loads model and optimizer state at the start if a checkpoint exists: checkpoint = tune.get_checkpoint() if checkpoint: with checkpoint.as_directory() as checkpoint_dir: checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\" checkpoint_state = torch.load(checkpoint_path) start_epoch = checkpoint_state[\"epoch\"] net.load_state_dict(checkpoint_state[\"net_state_dict\"]) optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"]) At the end of each epoch, save a checkpoint and report the validation metrics: checkpoint_data = { \"epoch\": epoch, \"net_state_dict\": net.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), } with tempfile.TemporaryDirectory() as checkpoint_dir: checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\" torch.save(checkpoint_data, checkpoint_path) checkpoint = Checkpoint.from_directory(checkpoint_dir) tune.report( {\"loss\": val_loss / val_steps, \"accuracy\": correct / total}, checkpoint=checkpoint, ) Ray Tune checkpointing supports local file systems, cloud storage, and distributed file systems. For more information, see the Ray Tune storage documentation. Multi-GPU support# Image classification models can be greatly accelerated by using GPUs. The training function supports multi-GPU training by wrapping the model in nn.DataParallel: if torch.cuda.device_count() \u003e 1: net = nn.DataParallel(net) This training function supports training on CPUs, a single GPU, multiple GPUs, or multiple nodes without code changes. Ray Tune automatically distributes the trials across the nodes according to the available resources. Ray Tune also supports fractional GPUs so that one GPU can be shared among multiple trials, provided that the models, optimizers, and data batches fit into the GPU memory. Validation split# The original CIFAR10 dataset only has train and test subsets. This is sufficient for training a single model, however for hyperparameter tuning a validation subset is required. The training function creates a validation subset by reserving 20% of the training subset. The test subset is used to evaluate the best model\u2019s generalization error after the search completes. Evaluation function# After finding the optimal hyperparameters, test the model on a held-out test set to estimate the generalization error: def test_accuracy(net, device=\"cpu\", data_dir=None): _trainset, testset = load_data(data_dir) testloader = torch.utils.data.DataLoader( testset, batch_size=4, shuffle=False, num_workers=2 ) correct = 0 total = 0 with torch.no_grad(): for data in testloader: image_batch, labels = data image_batch, labels = image_batch.to(device), labels.to(device) outputs = net(image_batch) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() return correct / total Configure and run Ray Tune# With the training and evaluation functions defined, configure Ray Tune to run the hyperparameter search. Scheduler for early stopping# Ray Tune provides schedulers to improve the efficiency of the hyperparameter search by detecting underperforming trials and stopping them early. The ASHAScheduler uses the Asynchronous Successive Halving Algorithm (ASHA) to aggressively terminate low-performing trials: scheduler = ASHAScheduler( max_t=max_num_epochs, grace_period=1, reduction_factor=2, ) Ray Tune also provides advanced search algorithms to smartly pick the next set of hyperparameters based on previous results, instead of relying only on random or grid search. Examples include Optuna and BayesOpt. Resource allocation# Tell Ray Tune what resources to allocate for each trial by passing a resources dictionary to tune.with_resources: tune.with_resources( partial(train_cifar, data_dir=data_dir), resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial} ) Ray Tune automatically manages the placement of these trials and ensures that the trials run in isolation, so you don\u2019t need to manually assign GPUs to processes. For example, if you are running this experiment on a cluster of 20 machines, each with 8 GPUs, you can set gpus_per_trial = 0.5 to schedule two concurrent trials per GPU. This configuration runs 320 trials in parallel across the cluster. Note To run this tutorial without GPUs, set gpus_per_trial=0 and expect significantly longer runtimes. To avoid long runtimes during development, start with a small number of trials and epochs. Creating the Tuner# The Ray Tune API is modular and composable. Pass your configuration to the tune.Tuner class to create a tuner object, then run tuner.fit() to start training: tuner = tune.Tuner( tune.with_resources( partial(train_cifar, data_dir=data_dir), resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial} ), tune_config=tune.TuneConfig( metric=\"loss\", mode=\"min\", scheduler=scheduler, num_samples=num_trials, ), param_space=config, ) results = tuner.fit() After training completes, retrieve the best performing trial, load its checkpoint, and evaluate on the test set. Putting it all together# def main(num_trials=10, max_num_epochs=10, gpus_per_trial=0, cpus_per_trial=2): print(\"Starting hyperparameter tuning.\") ray.init(include_dashboard=False) data_dir = os.path.abspath(\"./data\") load_data(data_dir) # Pre-download the dataset device = \"cuda\" if torch.cuda.is_available() else \"cpu\" config = { \"l1\": tune.choice([2**i for i in range(9)]), \"l2\": tune.choice([2**i for i in range(9)]), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]), \"device\": device, } scheduler = ASHAScheduler( max_t=max_num_epochs, grace_period=1, reduction_factor=2, ) tuner = tune.Tuner( tune.with_resources( partial(train_cifar, data_dir=data_dir), resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial} ), tune_config=tune.TuneConfig( metric=\"loss\", mode=\"min\", scheduler=scheduler, num_samples=num_trials, ), param_space=config, ) results = tuner.fit() best_result = results.get_best_result(\"loss\", \"min\") print(f\"Best trial config: {best_result.config}\") print(f\"Best trial final validation loss: {best_result.metrics[\u0027loss\u0027]}\") print(f\"Best trial final validation accuracy: {best_result.metrics[\u0027accuracy\u0027]}\") best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"]) best_trained_model = best_trained_model.to(device) if gpus_per_trial \u003e 1: best_trained_model = nn.DataParallel(best_trained_model) best_checkpoint = best_result.checkpoint with best_checkpoint.as_directory() as checkpoint_dir: checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\" best_checkpoint_data = torch.load(checkpoint_path) best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"]) test_acc = test_accuracy(best_trained_model, device, data_dir) print(f\"Best trial test set accuracy: {test_acc}\") if __name__ == \"__main__\": # Set the number of trials, epochs, and GPUs per trial here: main(num_trials=10, max_num_epochs=10, gpus_per_trial=1) Starting hyperparameter tuning. 2026-02-19 00:21:27,068 WARNING services.py:2137 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147471360 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing \u0027--shm-size=10.24gb\u0027 to \u0027docker run\u0027 (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM. 2026-02-19 00:21:27,235 INFO worker.py:2023 -- Started a local Ray instance. /usr/local/lib/python3.10/dist-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0 0%| | 0.00/170M [00:00\u003c?, ?B/s] 0%| | 590k/170M [00:00\u003c00:28, 5.88MB/s] 5%|\u258d | 8.32M/170M [00:00\u003c00:03, 47.8MB/s] 11%|\u2588\u258f | 19.5M/170M [00:00\u003c00:01, 76.8MB/s] 18%|\u2588\u258a | 30.6M/170M [00:00\u003c00:01, 90.5MB/s] 25%|\u2588\u2588\u258d | 41.8M/170M [00:00\u003c00:01, 98.1MB/s] 31%|\u2588\u2588\u2588\u258f | 53.5M/170M [00:00\u003c00:01, 104MB/s] 38%|\u2588\u2588\u2588\u258a | 65.2M/170M [00:00\u003c00:00, 109MB/s] 45%|\u2588\u2588\u2588\u2588\u258c | 76.9M/170M [00:00\u003c00:00, 111MB/s] 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 88.6M/170M [00:00\u003c00:00, 113MB/s] 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 100M/170M [00:01\u003c00:00, 114MB/s] 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112M/170M [00:01\u003c00:00, 115MB/s] 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 124M/170M [00:01\u003c00:00, 116MB/s] 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 135M/170M [00:01\u003c00:00, 116MB/s] 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 147M/170M [00:01\u003c00:00, 116MB/s] 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 159M/170M [00:01\u003c00:00, 116MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170M/170M [00:01\u003c00:00, 116MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:01\u003c00:00, 106MB/s] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Configuration for experiment train_cifar_2026-02-19_00-21-32 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Search algorithm BasicVariantGenerator \u2502 \u2502 Scheduler AsyncHyperBandScheduler \u2502 \u2502 Number of trials 10 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f View detailed results here: /var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32 To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2026-02-19_00-21-25_906106_3915/artifacts/2026-02-19_00-21-32/train_cifar_2026-02-19_00-21-32/driver_artifacts` Trial status: 10 PENDING Current time: 2026-02-19 00:21:33. Total running time: 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 PENDING 256 16 0.0127436 4 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00000 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00000 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 device cuda \u2502 \u2502 l1 256 \u2502 \u2502 l2 16 \u2502 \u2502 lr 0.01274 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [1, 2000] loss: 2.229 (func pid=5038) [1, 4000] loss: 1.111 (func pid=5038) [1, 6000] loss: 0.742 (pid=gcs_server) [2026-02-19 00:21:56,163 E 3920 3920] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (raylet) [2026-02-19 00:21:57,169 E 4060 4060] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (bundle_reservation_check_func pid=4127) [2026-02-19 00:21:57,776 E 4127 4343] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=5038) [1, 8000] loss: 0.569 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:22:03. Total running time: 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [2026-02-19 00:22:03,749 E 5038 5073] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 [repeated 14x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.) (func pid=5038) [1, 10000] loss: 0.460 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000000) (func pid=5038) [2, 2000] loss: 2.286 (func pid=5038) [2, 4000] loss: 1.155 (func pid=5038) [2, 6000] loss: 0.770 (func pid=5038) [2, 8000] loss: 0.577 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:22:33. Total running time: 1min 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.2855792963981627 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 1 33.694 2.28558 0.1153 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [2, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000001) (func pid=5038) [3, 2000] loss: 2.309 (func pid=5038) [3, 4000] loss: 1.155 (func pid=5038) [3, 6000] loss: 0.770 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:23:03. Total running time: 1min 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.311649545574188 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 2 65.489 2.31165 0.1004 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [3, 8000] loss: 0.577 (func pid=5038) [3, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000002) (func pid=5038) [4, 2000] loss: 2.311 (func pid=5038) [4, 4000] loss: 1.155 (func pid=5038) [4, 6000] loss: 0.770 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:23:33. Total running time: 2min 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.3093930989265443 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 3 96.8854 2.30939 0.1011 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [4, 8000] loss: 0.577 (func pid=5038) [4, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000003) (func pid=5038) [5, 2000] loss: 2.310 (func pid=5038) [5, 4000] loss: 1.155 (func pid=5038) [5, 6000] loss: 0.770 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:24:03. Total running time: 2min 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.3065259016036985 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 4 128.365 2.30653 0.0989 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [5, 8000] loss: 0.578 (func pid=5038) [5, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000004) (func pid=5038) [6, 2000] loss: 2.310 (func pid=5038) [6, 4000] loss: 1.155 (func pid=5038) [6, 6000] loss: 0.770 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:24:33. Total running time: 3min 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.3228442967414855 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 5 159.733 2.32284 0.1011 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [6, 8000] loss: 0.577 (func pid=5038) [6, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000005) (func pid=5038) [7, 2000] loss: 2.309 (func pid=5038) [7, 4000] loss: 1.156 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:25:03. Total running time: 3min 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.3060613892555235 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 6 191.092 2.30606 0.0973 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [7, 6000] loss: 0.770 (func pid=5038) [7, 8000] loss: 0.578 (func pid=5038) [7, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000006) (func pid=5038) [8, 2000] loss: 2.310 (func pid=5038) [8, 4000] loss: 1.155 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:25:33. Total running time: 4min 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.3189553537368774 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 7 222.355 2.31896 0.0973 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [8, 6000] loss: 0.770 (func pid=5038) [8, 8000] loss: 0.577 (func pid=5038) [8, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000007) (func pid=5038) [9, 2000] loss: 2.310 (func pid=5038) [9, 4000] loss: 1.155 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:26:03. Total running time: 4min 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.310531533908844 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 8 253.395 2.31053 0.0973 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [9, 6000] loss: 0.770 (func pid=5038) [9, 8000] loss: 0.577 (func pid=5038) [9, 10000] loss: 0.462 (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000008) (func pid=5038) [10, 2000] loss: 2.310 (func pid=5038) [10, 4000] loss: 1.155 Trial status: 1 RUNNING | 9 PENDING Current time: 2026-02-19 00:26:33. Total running time: 5min 0s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.306463604450226 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 RUNNING 256 16 0.0127436 4 9 284.858 2.30646 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 PENDING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) [10, 6000] loss: 0.770 (func pid=5038) [10, 8000] loss: 0.577 (func pid=5038) [10, 10000] loss: 0.462 Trial train_cifar_f19bc_00000 completed after 10 iterations at 2026-02-19 00:26:53. Total running time: 5min 20s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00000 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000009 \u2502 \u2502 time_this_iter_s 31.45353 \u2502 \u2502 time_total_s 316.31201 \u2502 \u2502 training_iteration 10 \u2502 \u2502 accuracy 0.0977 \u2502 \u2502 loss 2.30515 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5038) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00000_0_batch_size=4,l1=256,l2=16,lr=0.0127_2026-02-19_00-21-32/checkpoint_000009) Trial train_cifar_f19bc_00001 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00001 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 device cuda \u2502 \u2502 l1 1 \u2502 \u2502 l2 16 \u2502 \u2502 lr 0.01513 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING Current time: 2026-02-19 00:27:03. Total running time: 5min 30s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.305153544998169 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00001 RUNNING 1 16 0.0151318 4 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5779) [1, 2000] loss: 2.314 (func pid=5779) [1, 4000] loss: 1.156 (func pid=5779) [1, 6000] loss: 0.771 (func pid=5779) [1, 8000] loss: 0.578 (func pid=5779) [2026-02-19 00:27:24,637 E 5779 5814] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=5779) [1, 10000] loss: 0.462 Trial train_cifar_f19bc_00001 completed after 1 iterations at 2026-02-19 00:27:31. Total running time: 5min 58s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 33.60916 \u2502 \u2502 time_total_s 33.60916 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.0996 \u2502 \u2502 loss 2.30601 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5779) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00001_1_batch_size=4,l1=1,l2=16,lr=0.0151_2026-02-19_00-21-32/checkpoint_000000) Trial status: 2 TERMINATED | 8 PENDING Current time: 2026-02-19 00:27:33. Total running time: 6min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.305153544998169 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 PENDING 16 64 0.0411267 16 \u2502 \u2502 train_cifar_f19bc_00003 PENDING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00002 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00002 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 device cuda \u2502 \u2502 l1 16 \u2502 \u2502 l2 64 \u2502 \u2502 lr 0.04113 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5912) [1, 2000] loss: 2.249 Trial train_cifar_f19bc_00002 completed after 1 iterations at 2026-02-19 00:27:46. Total running time: 6min 13s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00002 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 11.04507 \u2502 \u2502 time_total_s 11.04507 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.0997 \u2502 \u2502 loss 2.30548 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=5912) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00002_2_batch_size=16,l1=16,l2=64,lr=0.0411_2026-02-19_00-21-32/checkpoint_000000) Trial train_cifar_f19bc_00003 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00003 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 2 \u2502 \u2502 device cuda \u2502 \u2502 l1 64 \u2502 \u2502 l2 16 \u2502 \u2502 lr 0.00209 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [1, 2000] loss: 2.173 (func pid=6043) [1, 4000] loss: 0.983 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:28:03. Total running time: 6min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.305153544998169 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [1, 6000] loss: 0.617 (func pid=6043) [1, 8000] loss: 0.466 (func pid=6043) [2026-02-19 00:28:17,631 E 6043 6078] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=6043) [1, 10000] loss: 0.363 (func pid=6043) [1, 12000] loss: 0.302 (func pid=6043) [1, 14000] loss: 0.261 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:28:34. Total running time: 7min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00000 with loss=2.305153544998169 and params={\u0027l1\u0027: 256, \u0027l2\u0027: 16, \u0027lr\u0027: 0.012743624317955983, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [1, 16000] loss: 0.226 (func pid=6043) [1, 18000] loss: 0.198 (func pid=6043) [1, 20000] loss: 0.181 (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000000) (func pid=6043) [2, 2000] loss: 1.750 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:29:04. Total running time: 7min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7880410368874669 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 1 63.7784 1.78804 0.3669 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [2, 4000] loss: 0.875 (func pid=6043) [2, 6000] loss: 0.578 (func pid=6043) [2, 8000] loss: 0.446 (func pid=6043) [2, 10000] loss: 0.351 (func pid=6043) [2, 12000] loss: 0.298 (func pid=6043) [2, 14000] loss: 0.256 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:29:34. Total running time: 8min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7880410368874669 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 1 63.7784 1.78804 0.3669 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [2, 16000] loss: 0.220 (func pid=6043) [2, 18000] loss: 0.197 (func pid=6043) [2, 20000] loss: 0.180 (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000001) (func pid=6043) [3, 2000] loss: 1.743 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:30:04. Total running time: 8min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7180798332419247 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 2 125.663 1.71808 0.3757 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [3, 4000] loss: 0.868 (func pid=6043) [3, 6000] loss: 0.593 (func pid=6043) [3, 8000] loss: 0.439 (func pid=6043) [3, 10000] loss: 0.360 (func pid=6043) [3, 12000] loss: 0.291 (func pid=6043) [3, 14000] loss: 0.252 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:30:34. Total running time: 9min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7180798332419247 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 2 125.663 1.71808 0.3757 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [3, 16000] loss: 0.221 (func pid=6043) [3, 18000] loss: 0.200 (func pid=6043) [3, 20000] loss: 0.182 (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000002) (func pid=6043) [4, 2000] loss: 1.751 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:31:04. Total running time: 9min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7497133266493679 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 3 187.743 1.74971 0.3626 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [4, 4000] loss: 0.885 (func pid=6043) [4, 6000] loss: 0.589 (func pid=6043) [4, 8000] loss: 0.452 (func pid=6043) [4, 10000] loss: 0.363 (func pid=6043) [4, 12000] loss: 0.302 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:31:34. Total running time: 10min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.7497133266493679 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 3 187.743 1.74971 0.3626 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [4, 14000] loss: 0.258 (func pid=6043) [4, 16000] loss: 0.223 (func pid=6043) [4, 18000] loss: 0.199 (func pid=6043) [4, 20000] loss: 0.180 (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000003) Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:32:04. Total running time: 10min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.8110783561166375 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 4 249.872 1.81108 0.3437 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [5, 2000] loss: 1.830 (func pid=6043) [5, 4000] loss: 0.905 (func pid=6043) [5, 6000] loss: 0.609 (func pid=6043) [5, 8000] loss: 0.443 (func pid=6043) [5, 10000] loss: 0.355 (func pid=6043) [5, 12000] loss: 0.292 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:32:34. Total running time: 11min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.8110783561166375 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 4 249.872 1.81108 0.3437 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [5, 14000] loss: 0.258 (func pid=6043) [5, 16000] loss: 0.224 (func pid=6043) [5, 18000] loss: 0.198 (func pid=6043) [5, 20000] loss: 0.181 (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000004) Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:33:04. Total running time: 11min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.759163848245889 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 5 311.983 1.75916 0.3694 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [6, 2000] loss: 1.818 (func pid=6043) [6, 4000] loss: 0.908 (func pid=6043) [6, 6000] loss: 0.607 (func pid=6043) [6, 8000] loss: 0.457 (func pid=6043) [6, 10000] loss: 0.363 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:33:34. Total running time: 12min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.759163848245889 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 5 311.983 1.75916 0.3694 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [6, 12000] loss: 0.307 (func pid=6043) [6, 14000] loss: 0.262 (func pid=6043) [6, 16000] loss: 0.237 (func pid=6043) [6, 18000] loss: 0.203 (func pid=6043) [6, 20000] loss: 0.187 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:34:04. Total running time: 12min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.759163848245889 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 5 311.983 1.75916 0.3694 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000005) (func pid=6043) [7, 2000] loss: 1.836 (func pid=6043) [7, 4000] loss: 0.912 (func pid=6043) [7, 6000] loss: 0.611 (func pid=6043) [7, 8000] loss: 0.451 (func pid=6043) [7, 10000] loss: 0.361 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:34:34. Total running time: 13min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.8926031447559595 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 6 374.083 1.8926 0.3244 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [7, 12000] loss: 0.309 (func pid=6043) [7, 14000] loss: 0.261 (func pid=6043) [7, 16000] loss: 0.227 (func pid=6043) [7, 18000] loss: 0.208 (func pid=6043) [7, 20000] loss: 0.179 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:35:04. Total running time: 13min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.8926031447559595 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 6 374.083 1.8926 0.3244 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000006) (func pid=6043) [8, 2000] loss: 1.846 (func pid=6043) [8, 4000] loss: 0.905 (func pid=6043) [8, 6000] loss: 0.624 (func pid=6043) [8, 8000] loss: 0.465 (func pid=6043) [8, 10000] loss: 0.377 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:35:34. Total running time: 14min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=2.000167779254168 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 7 436.423 2.00017 0.2984 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [8, 12000] loss: 0.309 (func pid=6043) [8, 14000] loss: 0.279 (func pid=6043) [8, 16000] loss: 0.240 (func pid=6043) [8, 18000] loss: 0.208 (func pid=6043) [8, 20000] loss: 0.191 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:36:04. Total running time: 14min 31s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=2.000167779254168 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 7 436.423 2.00017 0.2984 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000007) (func pid=6043) [9, 2000] loss: 1.863 (func pid=6043) [9, 4000] loss: 0.929 (func pid=6043) [9, 6000] loss: 0.622 (func pid=6043) [9, 8000] loss: 0.461 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:36:34. Total running time: 15min 1s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9275328431497094 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 8 497.929 1.92753 0.3171 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [9, 10000] loss: 0.370 (func pid=6043) [9, 12000] loss: 0.325 (func pid=6043) [9, 14000] loss: 0.277 (func pid=6043) [9, 16000] loss: 0.243 (func pid=6043) [9, 18000] loss: 0.213 (func pid=6043) [9, 20000] loss: 0.185 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:37:04. Total running time: 15min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9275328431497094 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 8 497.929 1.92753 0.3171 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000008) (func pid=6043) [10, 2000] loss: 1.848 (func pid=6043) [10, 4000] loss: 0.959 (func pid=6043) [10, 6000] loss: 0.632 (func pid=6043) [10, 8000] loss: 0.470 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:37:34. Total running time: 16min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9394293992295861 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 9 559.907 1.93943 0.295 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) [10, 10000] loss: 0.369 (func pid=6043) [10, 12000] loss: 0.315 (func pid=6043) [10, 14000] loss: 0.264 (func pid=6043) [10, 16000] loss: 0.242 (func pid=6043) [10, 18000] loss: 0.212 (func pid=6043) [10, 20000] loss: 0.192 Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING Current time: 2026-02-19 00:38:05. Total running time: 16min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9394293992295861 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00003 RUNNING 64 16 0.00209069 2 9 559.907 1.93943 0.295 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00004 PENDING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00003 completed after 10 iterations at 2026-02-19 00:38:12. Total running time: 16min 39s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00003 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000009 \u2502 \u2502 time_this_iter_s 61.70261 \u2502 \u2502 time_total_s 621.60953 \u2502 \u2502 training_iteration 10 \u2502 \u2502 accuracy 0.3082 \u2502 \u2502 loss 1.90101 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6043) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00003_3_batch_size=2,l1=64,l2=16,lr=0.0021_2026-02-19_00-21-32/checkpoint_000009) Trial train_cifar_f19bc_00004 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00004 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 device cuda \u2502 \u2502 l1 16 \u2502 \u2502 l2 128 \u2502 \u2502 lr 0.01055 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6810) [1, 2000] loss: 2.169 (func pid=6810) [1, 4000] loss: 1.065 (func pid=6810) [1, 6000] loss: 0.737 Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING Current time: 2026-02-19 00:38:35. Total running time: 17min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00004 RUNNING 16 128 0.0105504 4 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6810) [1, 8000] loss: 0.559 (func pid=6810) [2026-02-19 00:38:43,722 E 6810 6845] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=6810) [1, 10000] loss: 0.446 (func pid=6810) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00004_4_batch_size=4,l1=16,l2=128,lr=0.0106_2026-02-19_00-21-32/checkpoint_000000) (func pid=6810) [2, 2000] loss: 2.260 (func pid=6810) [2, 4000] loss: 1.149 Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING Current time: 2026-02-19 00:39:05. Total running time: 17min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00004 RUNNING 16 128 0.0105504 4 1 33.4177 2.2142 0.1507 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00005 PENDING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6810) [2, 6000] loss: 0.755 (func pid=6810) [2, 8000] loss: 0.568 (func pid=6810) [2, 10000] loss: 0.462 Trial train_cifar_f19bc_00004 completed after 2 iterations at 2026-02-19 00:39:21. Total running time: 17min 48s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00004 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 31.36562 \u2502 \u2502 time_total_s 64.78331 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.1006 \u2502 \u2502 loss 2.30682 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=6810) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00004_4_batch_size=4,l1=16,l2=128,lr=0.0106_2026-02-19_00-21-32/checkpoint_000001) Trial train_cifar_f19bc_00005 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00005 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 2 \u2502 \u2502 device cuda \u2502 \u2502 l1 32 \u2502 \u2502 l2 4 \u2502 \u2502 lr 0.00244 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [1, 2000] loss: 2.247 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:39:35. Total running time: 18min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [1, 4000] loss: 1.048 (func pid=7010) [1, 6000] loss: 0.680 (func pid=7010) [1, 8000] loss: 0.504 (func pid=7010) [2026-02-19 00:39:52,730 E 7010 7045] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=7010) [1, 10000] loss: 0.398 (func pid=7010) [1, 12000] loss: 0.330 (func pid=7010) [1, 14000] loss: 0.284 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:40:05. Total running time: 18min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [1, 16000] loss: 0.252 (func pid=7010) [1, 18000] loss: 0.217 (func pid=7010) [1, 20000] loss: 0.193 (func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000000) (func pid=7010) [2, 2000] loss: 1.936 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:40:35. Total running time: 19min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 1 63.2569 1.9359 0.2656 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [2, 4000] loss: 0.957 (func pid=7010) [2, 6000] loss: 0.629 (func pid=7010) [2, 8000] loss: 0.487 (func pid=7010) [2, 10000] loss: 0.390 (func pid=7010) [2, 12000] loss: 0.324 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:41:05. Total running time: 19min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 1 63.2569 1.9359 0.2656 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [2, 14000] loss: 0.276 (func pid=7010) [2, 16000] loss: 0.246 (func pid=7010) [2, 18000] loss: 0.216 (func pid=7010) [2, 20000] loss: 0.195 (func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000001) Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:41:35. Total running time: 20min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 2 124.7 1.98254 0.2347 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [3, 2000] loss: 1.940 (func pid=7010) [3, 4000] loss: 0.984 (func pid=7010) [3, 6000] loss: 0.646 (func pid=7010) [3, 8000] loss: 0.482 (func pid=7010) [3, 10000] loss: 0.386 (func pid=7010) [3, 12000] loss: 0.323 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:42:05. Total running time: 20min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 2 124.7 1.98254 0.2347 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [3, 14000] loss: 0.269 (func pid=7010) [3, 16000] loss: 0.239 (func pid=7010) [3, 18000] loss: 0.216 (func pid=7010) [3, 20000] loss: 0.194 (func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000002) Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:42:35. Total running time: 21min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00005 with loss=1.8970869152605534 and params={\u0027l1\u0027: 32, \u0027l2\u0027: 4, \u0027lr\u0027: 0.002438900339047484, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 3 185.479 1.89709 0.27 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [4, 2000] loss: 1.898 (func pid=7010) [4, 4000] loss: 0.968 (func pid=7010) [4, 6000] loss: 0.639 (func pid=7010) [4, 8000] loss: 0.485 (func pid=7010) [4, 10000] loss: 0.380 (func pid=7010) [4, 12000] loss: 0.316 Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING Current time: 2026-02-19 00:43:05. Total running time: 21min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00005 with loss=1.8970869152605534 and params={\u0027l1\u0027: 32, \u0027l2\u0027: 4, \u0027lr\u0027: 0.002438900339047484, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00005 RUNNING 32 4 0.0024389 2 3 185.479 1.89709 0.27 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) [4, 14000] loss: 0.271 (func pid=7010) [4, 16000] loss: 0.244 (func pid=7010) [4, 18000] loss: 0.217 (func pid=7010) [4, 20000] loss: 0.199 Trial train_cifar_f19bc_00005 completed after 4 iterations at 2026-02-19 00:43:32. Total running time: 21min 59s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00005 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000003 \u2502 \u2502 time_this_iter_s 61.3401 \u2502 \u2502 time_total_s 246.81928 \u2502 \u2502 training_iteration 4 \u2502 \u2502 accuracy 0.1474 \u2502 \u2502 loss 2.2912 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7010) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00005_5_batch_size=2,l1=32,l2=4,lr=0.0024_2026-02-19_00-21-32/checkpoint_000003) Trial status: 6 TERMINATED | 4 PENDING Current time: 2026-02-19 00:43:35. Total running time: 22min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 PENDING 64 128 0.090221 16 \u2502 \u2502 train_cifar_f19bc_00007 PENDING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00006 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00006 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 device cuda \u2502 \u2502 l1 64 \u2502 \u2502 l2 128 \u2502 \u2502 lr 0.09022 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7356) [1, 2000] loss: 2.198 Trial train_cifar_f19bc_00006 completed after 1 iterations at 2026-02-19 00:43:47. Total running time: 22min 14s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00006 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 11.06867 \u2502 \u2502 time_total_s 11.06867 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.1415 \u2502 \u2502 loss 2.25626 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7356) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00006_6_batch_size=16,l1=64,l2=128,lr=0.0902_2026-02-19_00-21-32/checkpoint_000000) Trial train_cifar_f19bc_00007 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00007 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 8 \u2502 \u2502 device cuda \u2502 \u2502 l1 2 \u2502 \u2502 l2 2 \u2502 \u2502 lr 0.02633 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7486) [1, 2000] loss: 2.311 (func pid=7486) [1, 4000] loss: 1.155 Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING Current time: 2026-02-19 00:44:05. Total running time: 22min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00007 RUNNING 2 2 0.0263252 8 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00008 PENDING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00007 completed after 1 iterations at 2026-02-19 00:44:10. Total running time: 22min 37s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00007 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 18.46077 \u2502 \u2502 time_total_s 18.46077 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.0983 \u2502 \u2502 loss 2.31514 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7486) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00007_7_batch_size=8,l1=2,l2=2,lr=0.0263_2026-02-19_00-21-32/checkpoint_000000) Trial train_cifar_f19bc_00008 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00008 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 device cuda \u2502 \u2502 l1 4 \u2502 \u2502 l2 64 \u2502 \u2502 lr 0.01741 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7618) [1, 2000] loss: 2.314 (func pid=7618) [1, 4000] loss: 1.156 (func pid=7618) [1, 6000] loss: 0.770 Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING Current time: 2026-02-19 00:44:35. Total running time: 23min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00008 RUNNING 4 64 0.0174143 4 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00009 PENDING 2 32 0.000418759 4 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7618) [1, 8000] loss: 0.578 (func pid=7618) [2026-02-19 00:44:41,753 E 7618 7653] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=7618) [1, 10000] loss: 0.463 Trial train_cifar_f19bc_00008 completed after 1 iterations at 2026-02-19 00:44:48. Total running time: 23min 15s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00008 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 33.60499 \u2502 \u2502 time_total_s 33.60499 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.1 \u2502 \u2502 loss 2.31214 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7618) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00008_8_batch_size=4,l1=4,l2=64,lr=0.0174_2026-02-19_00-21-33/checkpoint_000000) Trial train_cifar_f19bc_00009 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00009 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 device cuda \u2502 \u2502 l1 2 \u2502 \u2502 l2 32 \u2502 \u2502 lr 0.00042 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [1, 2000] loss: 2.273 (func pid=7750) [1, 4000] loss: 1.034 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:45:05. Total running time: 23min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00003 with loss=1.9010113405533136 and params={\u0027l1\u0027: 64, \u0027l2\u0027: 16, \u0027lr\u0027: 0.0020906918730271263, \u0027batch_size\u0027: 2, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [1, 6000] loss: 0.656 (func pid=7750) [1, 8000] loss: 0.478 (func pid=7750) [2026-02-19 00:45:19,758 E 7750 7785] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 (func pid=7750) [1, 10000] loss: 0.364 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000000) (func pid=7750) [2, 2000] loss: 1.774 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:45:35. Total running time: 24min 2s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.8082569658756256 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 1 33.7361 1.80826 0.2745 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [2, 4000] loss: 0.874 (func pid=7750) [2, 6000] loss: 0.578 (func pid=7750) [2, 8000] loss: 0.427 (func pid=7750) [2, 10000] loss: 0.341 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000001) (func pid=7750) [3, 2000] loss: 1.657 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:46:05. Total running time: 24min 32s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.7081201287031174 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 2 64.9707 1.70812 0.308 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [3, 4000] loss: 0.836 (func pid=7750) [3, 6000] loss: 0.548 (func pid=7750) [3, 8000] loss: 0.402 (func pid=7750) [3, 10000] loss: 0.323 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000002) (func pid=7750) [4, 2000] loss: 1.611 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:46:35. Total running time: 25min 3s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.7130692383050918 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 3 96.6171 1.71307 0.3287 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [4, 4000] loss: 0.797 (func pid=7750) [4, 6000] loss: 0.530 (func pid=7750) [4, 8000] loss: 0.394 (func pid=7750) [4, 10000] loss: 0.314 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000003) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:47:05. Total running time: 25min 33s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.6541795006990432 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 4 128.138 1.65418 0.3575 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [5, 2000] loss: 1.560 (func pid=7750) [5, 4000] loss: 0.791 (func pid=7750) [5, 6000] loss: 0.519 (func pid=7750) [5, 8000] loss: 0.389 (func pid=7750) [5, 10000] loss: 0.308 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000004) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:47:36. Total running time: 26min 3s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.574478276658058 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 5 159.744 1.57448 0.3775 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [6, 2000] loss: 1.530 (func pid=7750) [6, 4000] loss: 0.768 (func pid=7750) [6, 6000] loss: 0.511 (func pid=7750) [6, 8000] loss: 0.385 (func pid=7750) [6, 10000] loss: 0.306 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000005) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:48:06. Total running time: 26min 33s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.5453639246702193 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 6 191.206 1.54536 0.3986 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [7, 2000] loss: 1.509 (func pid=7750) [7, 4000] loss: 0.758 (func pid=7750) [7, 6000] loss: 0.507 (func pid=7750) [7, 8000] loss: 0.378 (func pid=7750) [7, 10000] loss: 0.303 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000006) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:48:36. Total running time: 27min 3s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.539768257367611 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 7 222.796 1.53977 0.4019 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) [8, 2000] loss: 1.491 (func pid=7750) [8, 4000] loss: 0.754 (func pid=7750) [8, 6000] loss: 0.497 (func pid=7750) [8, 8000] loss: 0.375 (func pid=7750) [8, 10000] loss: 0.301 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:49:06. Total running time: 27min 33s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.539768257367611 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 7 222.796 1.53977 0.4019 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000007) (func pid=7750) [9, 2000] loss: 1.489 (func pid=7750) [9, 4000] loss: 0.743 (func pid=7750) [9, 6000] loss: 0.491 (func pid=7750) [9, 8000] loss: 0.371 (func pid=7750) [9, 10000] loss: 0.299 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:49:36. Total running time: 28min 3s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.5236012852072716 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 8 254.392 1.5236 0.4036 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000008) (func pid=7750) [10, 2000] loss: 1.488 (func pid=7750) [10, 4000] loss: 0.734 (func pid=7750) [10, 6000] loss: 0.495 (func pid=7750) [10, 8000] loss: 0.369 (func pid=7750) [10, 10000] loss: 0.295 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2026-02-19 00:50:06. Total running time: 28min 33s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.5332289441108704 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00009 RUNNING 2 32 0.000418759 4 9 286.01 1.53323 0.405 \u2502 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f19bc_00009 completed after 10 iterations at 2026-02-19 00:50:09. Total running time: 28min 37s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f19bc_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000009 \u2502 \u2502 time_this_iter_s 31.09251 \u2502 \u2502 time_total_s 317.10224 \u2502 \u2502 training_iteration 10 \u2502 \u2502 accuracy 0.4094 \u2502 \u2502 loss 1.52454 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f 2026-02-19 00:50:09,949 INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to \u0027/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32\u0027 in 0.0103s. Trial status: 10 TERMINATED Current time: 2026-02-19 00:50:09. Total running time: 28min 37s Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A10G) Current best trial: f19bc_00009 with loss=1.5245350178837775 and params={\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f19bc_00000 TERMINATED 256 16 0.0127436 4 10 316.312 2.30515 0.0977 \u2502 \u2502 train_cifar_f19bc_00001 TERMINATED 1 16 0.0151318 4 1 33.6092 2.30601 0.0996 \u2502 \u2502 train_cifar_f19bc_00002 TERMINATED 16 64 0.0411267 16 1 11.0451 2.30548 0.0997 \u2502 \u2502 train_cifar_f19bc_00003 TERMINATED 64 16 0.00209069 2 10 621.61 1.90101 0.3082 \u2502 \u2502 train_cifar_f19bc_00004 TERMINATED 16 128 0.0105504 4 2 64.7833 2.30682 0.1006 \u2502 \u2502 train_cifar_f19bc_00005 TERMINATED 32 4 0.0024389 2 4 246.819 2.2912 0.1474 \u2502 \u2502 train_cifar_f19bc_00006 TERMINATED 64 128 0.090221 16 1 11.0687 2.25626 0.1415 \u2502 \u2502 train_cifar_f19bc_00007 TERMINATED 2 2 0.0263252 8 1 18.4608 2.31514 0.0983 \u2502 \u2502 train_cifar_f19bc_00008 TERMINATED 4 64 0.0174143 4 1 33.605 2.31214 0.1 \u2502 \u2502 train_cifar_f19bc_00009 TERMINATED 2 32 0.000418759 4 10 317.102 1.52454 0.4094 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Best trial config: {\u0027l1\u0027: 2, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0004187588842664496, \u0027batch_size\u0027: 4, \u0027device\u0027: \u0027cuda\u0027} Best trial final validation loss: 1.5245350178837775 Best trial final validation accuracy: 0.4094 (func pid=7750) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2026-02-19_00-21-32/train_cifar_f19bc_00009_9_batch_size=4,l1=2,l2=32,lr=0.0004_2026-02-19_00-21-33/checkpoint_000009) Best trial test set accuracy: 0.4001 Results# Your Ray Tune trial summary output looks something like this. The text table summarizes the validation performance of the trials and highlights the best hyperparameter configuration: Number of trials: 10/10 (10 TERMINATED) +-----+--------------+------+------+-------------+--------+---------+------------+ | ... | batch_size | l1 | l2 | lr | iter | loss | accuracy | |-----+--------------+------+------+-------------+--------+---------+------------| | ... | 2 | 1 | 256 | 0.000668163 | 1 | 2.31479 | 0.0977 | | ... | 4 | 64 | 8 | 0.0331514 | 1 | 2.31605 | 0.0983 | | ... | 4 | 2 | 1 | 0.000150295 | 1 | 2.30755 | 0.1023 | | ... | 16 | 32 | 32 | 0.0128248 | 10 | 1.66912 | 0.4391 | | ... | 4 | 8 | 128 | 0.00464561 | 2 | 1.7316 | 0.3463 | | ... | 8 | 256 | 8 | 0.00031556 | 1 | 2.19409 | 0.1736 | | ... | 4 | 16 | 256 | 0.00574329 | 2 | 1.85679 | 0.3368 | | ... | 8 | 2 | 2 | 0.00325652 | 1 | 2.30272 | 0.0984 | | ... | 2 | 2 | 2 | 0.000342987 | 2 | 1.76044 | 0.292 | | ... | 4 | 64 | 32 | 0.003734 | 8 | 1.53101 | 0.4761 | +-----+--------------+------+------+-------------+--------+---------+------------+ Best trial config: {\u0027l1\u0027: 64, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0037339984519545164, \u0027batch_size\u0027: 4} Best trial final validation loss: 1.5310075663924216 Best trial final validation accuracy: 0.4761 Best trial test set accuracy: 0.4737 Most trials stopped early to conserve resources. The best performing trial achieved a validation accuracy of approximately 47%, which the test set confirms. Observability# Monitoring is critical when running large-scale experiments. Ray provides a dashboard that lets you view the status of your trials, check cluster resource use, and inspect logs in real time. For debugging, Ray also offers distributed debugging tools that let you attach a debugger to running trials across the cluster. Conclusion# In this tutorial, you learned how to tune the hyperparameters of a PyTorch model using Ray Tune. You saw how to integrate Ray Tune into your PyTorch training loop, define a search space for your hyperparameters, use an efficient scheduler like ASHAScheduler to terminate low-performing trials early, save checkpoints and report metrics to Ray Tune, and run the hyperparameter search and analyze the results. Ray Tune makes it straightforward to scale your experiments from a single machine to a large cluster, helping you find the best model configuration efficiently. Further reading# Ray Tune documentation Ray Tune examples Total running time of the script: (28 minutes 50.467 seconds) Download Jupyter notebook: hyperparameter_tuning_tutorial.ipynb Download Python source code: hyperparameter_tuning_tutorial.py Download zipped: hyperparameter_tuning_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/hyperparameter_tuning_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>