
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Chatbot Tutorial — PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'beginner/chatbot_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/beginner/chatbot_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/beginner/chatbot_tutorial.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Chatbot Tutorial</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Chatbot Tutorial" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
</div>
<script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">beginner/chatbot_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-chatbot-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="chatbot-tutorial">
<span id="sphx-glr-beginner-chatbot-tutorial-py"></span><h1>Chatbot Tutorial<a class="headerlink" href="#chatbot-tutorial" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Aug 14, 2018 | Last Updated: Jan 24, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></p>
<p>In this tutorial, we explore a fun and interesting use-case of recurrent
sequence-to-sequence models. We will train a simple chatbot using movie
scripts from the <a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs
Corpus</a>.</p>
<p>Conversational models are a hot topic in artificial intelligence
research. Chatbots can be found in a variety of settings, including
customer service applications and online helpdesks. These bots are often
powered by retrieval-based models, which output predefined responses to
questions of certain forms. In a highly restricted domain like a
company’s IT helpdesk, these models may be sufficient, however, they are
not robust enough for more general use-cases. Teaching a machine to
carry out a meaningful conversation with a human in multiple domains is
a research question that is far from solved. Recently, the deep learning
boom has allowed for powerful generative models like Google’s <a class="reference external" href="https://arxiv.org/abs/1506.05869">Neural
Conversational Model</a>, which marks
a large step towards multi-domain generative conversational models. In
this tutorial, we will implement this kind of model in PyTorch.</p>
<figure class="align-center">
<img alt="bot" src="../_images/bot.png"/>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">hello</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">hello</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">where</span> <span class="n">am</span> <span class="n">I</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">you</span> <span class="n">re</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">hospital</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">who</span> <span class="n">are</span> <span class="n">you</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">i</span> <span class="n">m</span> <span class="n">a</span> <span class="n">lawyer</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">how</span> <span class="n">are</span> <span class="n">you</span> <span class="n">doing</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">i</span> <span class="n">m</span> <span class="n">fine</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">are</span> <span class="n">you</span> <span class="n">my</span> <span class="n">friend</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">no</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">you</span><span class="s1">'re under arrest</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">i</span> <span class="n">m</span> <span class="n">trying</span> <span class="n">to</span> <span class="n">help</span> <span class="n">you</span> <span class="err">!</span>
<span class="o">&gt;</span> <span class="n">i</span><span class="s1">'m just kidding</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">i</span> <span class="n">m</span> <span class="n">sorry</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">where</span> <span class="n">are</span> <span class="n">you</span> <span class="n">from</span><span class="err">?</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">san</span> <span class="n">francisco</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">it</span><span class="s1">'s time for me to leave</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">i</span> <span class="n">know</span> <span class="o">.</span>
<span class="o">&gt;</span> <span class="n">goodbye</span>
<span class="n">Bot</span><span class="p">:</span> <span class="n">goodbye</span> <span class="o">.</span>
</pre></div>
</div>
<p><strong>Tutorial Highlights</strong></p>
<ul class="simple">
<li><p>Handle loading and preprocessing of <a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs
Corpus</a>
dataset</p></li>
<li><p>Implement a sequence-to-sequence model with <a class="reference external" href="https://arxiv.org/abs/1508.04025">Luong attention
mechanism(s)</a></p></li>
<li><p>Jointly train encoder and decoder models using mini-batches</p></li>
<li><p>Implement greedy-search decoding module</p></li>
<li><p>Interact with trained chatbot</p></li>
</ul>
<p><strong>Acknowledgments</strong></p>
<p>This tutorial borrows code from the following sources:</p>
<ol class="arabic simple">
<li><p>Yuan-Kuei Wu’s pytorch-chatbot implementation:
<a class="github reference external" href="https://github.com/ywk991112/pytorch-chatbot">ywk991112/pytorch-chatbot</a></p></li>
<li><p>Sean Robertson’s practical-pytorch seq2seq-translation example:
<a class="github reference external" href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation">spro/practical-pytorch</a></p></li>
<li><p>FloydHub Cornell Movie Corpus preprocessing code:
<a class="github reference external" href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus">floydhub/textutil-preprocess-cornell-movie-corpus</a></p></li>
</ol>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Link to this heading">#</a></h2>
<p>To get started, <a class="reference external" href="https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip">download</a> the Movie-Dialogs Corpus zip file.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># and put in a ``data/`` directory under the current directory.</span>
<span class="c1">#</span>
<span class="c1"># After that, let’s import some necessities.</span>
<span class="c1">#</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.jit</span><span class="w"> </span><span class="kn">import</span> <span class="n">script</span><span class="p">,</span> <span class="n">trace</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">csv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">unicodedata</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">codecs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">io</span><span class="w"> </span><span class="kn">import</span> <span class="nb">open</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>


<span class="c1"># If the current `accelerator &lt;https://pytorch.org/docs/stable/torch.html#accelerators&gt;`__ is available,</span>
<span class="c1"># we will use it. Otherwise, we use the CPU.</span>
<span class="n">device</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator" title="torch.accelerator.current_accelerator"><span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">current_accelerator</span></a><span class="p">()</span><span class="o">.</span><span class="n">type</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.accelerator.is_available.html#torch.accelerator.is_available" title="torch.accelerator.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2"> device"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Using cuda device
</pre></div>
</div>
</section>
<section id="load-preprocess-data">
<h2>Load &amp; Preprocess Data<a class="headerlink" href="#load-preprocess-data" title="Link to this heading">#</a></h2>
<p>The next step is to reformat our data file and load the data into
structures that we can work with.</p>
<p>The <a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs
Corpus</a>
is a rich dataset of movie character dialog:</p>
<ul class="simple">
<li><p>220,579 conversational exchanges between 10,292 pairs of movie
characters</p></li>
<li><p>9,035 characters from 617 movies</p></li>
<li><p>304,713 total utterances</p></li>
</ul>
<p>This dataset is large and diverse, and there is a great variation of
language formality, time periods, sentiment, etc. Our hope is that this
diversity makes our model robust to many forms of inputs and queries.</p>
<p>First, we’ll take a look at some lines of our datafile to see the
original format.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_name</span> <span class="o">=</span> <span class="s2">"movie-corpus"</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"data"</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">printLines</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">datafile</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">datafile</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[:</span><span class="n">n</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

<span class="n">printLines</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">"utterances.jsonl"</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>b'{"id": "L1045", "conversation_id": "L1044", "text": "They do not!", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "They", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "not", "tag": "RB", "dep": "neg", "up": 1, "dn": []}, {"tok": "!", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": "L1044", "timestamp": null, "vectors": []}\n'
b'{"id": "L1044", "conversation_id": "L1044", "text": "They do to!", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "They", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "to", "tag": "TO", "dep": "dobj", "up": 1, "dn": []}, {"tok": "!", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\n'
b'{"id": "L985", "conversation_id": "L984", "text": "I hope so.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "I", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "hope", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "so", "tag": "RB", "dep": "advmod", "up": 1, "dn": []}, {"tok": ".", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": "L984", "timestamp": null, "vectors": []}\n'
b'{"id": "L984", "conversation_id": "L984", "text": "She okay?", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "She", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "okay", "tag": "RB", "dep": "ROOT", "dn": [0, 2]}, {"tok": "?", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\n'
b'{"id": "L925", "conversation_id": "L924", "text": "Let\'s go.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Let", "tag": "VB", "dep": "ROOT", "dn": [2, 3]}, {"tok": "\'s", "tag": "PRP", "dep": "nsubj", "up": 2, "dn": []}, {"tok": "go", "tag": "VB", "dep": "ccomp", "up": 0, "dn": [1]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 0, "dn": []}]}]}, "reply-to": "L924", "timestamp": null, "vectors": []}\n'
b'{"id": "L924", "conversation_id": "L924", "text": "Wow", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Wow", "tag": "UH", "dep": "ROOT", "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\n'
b'{"id": "L872", "conversation_id": "L870", "text": "Okay -- you\'re gonna need to learn how to lie.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 4, "toks": [{"tok": "Okay", "tag": "UH", "dep": "intj", "up": 4, "dn": []}, {"tok": "--", "tag": ":", "dep": "punct", "up": 4, "dn": []}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 4, "dn": []}, {"tok": "\'re", "tag": "VBP", "dep": "aux", "up": 4, "dn": []}, {"tok": "gon", "tag": "VBG", "dep": "ROOT", "dn": [0, 1, 2, 3, 6, 12]}, {"tok": "na", "tag": "TO", "dep": "aux", "up": 6, "dn": []}, {"tok": "need", "tag": "VB", "dep": "xcomp", "up": 4, "dn": [5, 8]}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 8, "dn": []}, {"tok": "learn", "tag": "VB", "dep": "xcomp", "up": 6, "dn": [7, 11]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 11, "dn": []}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 11, "dn": []}, {"tok": "lie", "tag": "VB", "dep": "xcomp", "up": 8, "dn": [9, 10]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 4, "dn": []}]}]}, "reply-to": "L871", "timestamp": null, "vectors": []}\n'
b'{"id": "L871", "conversation_id": "L870", "text": "No", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "No", "tag": "UH", "dep": "ROOT", "dn": []}]}]}, "reply-to": "L870", "timestamp": null, "vectors": []}\n'
b'{"id": "L870", "conversation_id": "L870", "text": "I\'m kidding.  You know how sometimes you just become this \\"persona\\"?  And you don\'t know how to quit?", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 2, "toks": [{"tok": "I", "tag": "PRP", "dep": "nsubj", "up": 2, "dn": []}, {"tok": "\'m", "tag": "VBP", "dep": "aux", "up": 2, "dn": []}, {"tok": "kidding", "tag": "VBG", "dep": "ROOT", "dn": [0, 1, 3]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 2, "dn": [4]}, {"tok": " ", "tag": "_SP", "dep": "", "up": 3, "dn": []}]}, {"rt": 1, "toks": [{"tok": "You", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "know", "tag": "VBP", "dep": "ROOT", "dn": [0, 6, 11]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 3, "dn": []}, {"tok": "sometimes", "tag": "RB", "dep": "advmod", "up": 6, "dn": [2]}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 6, "dn": []}, {"tok": "just", "tag": "RB", "dep": "advmod", "up": 6, "dn": []}, {"tok": "become", "tag": "VBP", "dep": "ccomp", "up": 1, "dn": [3, 4, 5, 9]}, {"tok": "this", "tag": "DT", "dep": "det", "up": 9, "dn": []}, {"tok": "\\"", "tag": "``", "dep": "punct", "up": 9, "dn": []}, {"tok": "persona", "tag": "NN", "dep": "attr", "up": 6, "dn": [7, 8, 10]}, {"tok": "\\"", "tag": "\'\'", "dep": "punct", "up": 9, "dn": []}, {"tok": "?", "tag": ".", "dep": "punct", "up": 1, "dn": [12]}, {"tok": " ", "tag": "_SP", "dep": "", "up": 11, "dn": []}]}, {"rt": 4, "toks": [{"tok": "And", "tag": "CC", "dep": "cc", "up": 4, "dn": []}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 4, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "aux", "up": 4, "dn": []}, {"tok": "n\'t", "tag": "RB", "dep": "neg", "up": 4, "dn": []}, {"tok": "know", "tag": "VB", "dep": "ROOT", "dn": [0, 1, 2, 3, 7, 8]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 7, "dn": []}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 7, "dn": []}, {"tok": "quit", "tag": "VB", "dep": "xcomp", "up": 4, "dn": [5, 6]}, {"tok": "?", "tag": ".", "dep": "punct", "up": 4, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\n'
b'{"id": "L869", "conversation_id": "L866", "text": "Like my fear of wearing pastels?", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Like", "tag": "IN", "dep": "ROOT", "dn": [2, 6]}, {"tok": "my", "tag": "PRP$", "dep": "poss", "up": 2, "dn": []}, {"tok": "fear", "tag": "NN", "dep": "pobj", "up": 0, "dn": [1, 3]}, {"tok": "of", "tag": "IN", "dep": "prep", "up": 2, "dn": [4]}, {"tok": "wearing", "tag": "VBG", "dep": "pcomp", "up": 3, "dn": [5]}, {"tok": "pastels", "tag": "NNS", "dep": "dobj", "up": 4, "dn": []}, {"tok": "?", "tag": ".", "dep": "punct", "up": 0, "dn": []}]}]}, "reply-to": "L868", "timestamp": null, "vectors": []}\n'
</pre></div>
</div>
<section id="create-formatted-data-file">
<h3>Create formatted data file<a class="headerlink" href="#create-formatted-data-file" title="Link to this heading">#</a></h3>
<p>For convenience, we’ll create a nicely formatted data file in which each line
contains a tab-separated <em>query sentence</em> and a <em>response sentence</em> pair.</p>
<p>The following functions facilitate the parsing of the raw
<code class="docutils literal notranslate"><span class="pre">utterances.jsonl</span></code> data file.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loadLinesAndConversations</span></code> splits each line of the file into a dictionary of
lines with fields: <code class="docutils literal notranslate"><span class="pre">lineID</span></code>, <code class="docutils literal notranslate"><span class="pre">characterID</span></code>, and text and then groups them
into conversations with fields: <code class="docutils literal notranslate"><span class="pre">conversationID</span></code>, <code class="docutils literal notranslate"><span class="pre">movieID</span></code>, and lines.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">extractSentencePairs</span></code> extracts pairs of sentences from
conversations</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Splits each line of the file to create lines and conversations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loadLinesAndConversations</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">conversations</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'iso-8859-1'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">lineJson</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="c1"># Extract fields for line object</span>
            <span class="n">lineObj</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">lineObj</span><span class="p">[</span><span class="s2">"lineID"</span><span class="p">]</span> <span class="o">=</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span>
            <span class="n">lineObj</span><span class="p">[</span><span class="s2">"characterID"</span><span class="p">]</span> <span class="o">=</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"speaker"</span><span class="p">]</span>
            <span class="n">lineObj</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span> <span class="o">=</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span>
            <span class="n">lines</span><span class="p">[</span><span class="n">lineObj</span><span class="p">[</span><span class="s1">'lineID'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">lineObj</span>

            <span class="c1"># Extract fields for conversation object</span>
            <span class="k">if</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"conversation_id"</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">conversations</span><span class="p">:</span>
                <span class="n">convObj</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">"conversationID"</span><span class="p">]</span> <span class="o">=</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"conversation_id"</span><span class="p">]</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">"movieID"</span><span class="p">]</span> <span class="o">=</span> <span class="n">lineJson</span><span class="p">[</span><span class="s2">"meta"</span><span class="p">][</span><span class="s2">"movie_id"</span><span class="p">]</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">"lines"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">lineObj</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">convObj</span> <span class="o">=</span> <span class="n">conversations</span><span class="p">[</span><span class="n">lineJson</span><span class="p">[</span><span class="s2">"conversation_id"</span><span class="p">]]</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">"lines"</span><span class="p">]</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">lineObj</span><span class="p">)</span>
            <span class="n">conversations</span><span class="p">[</span><span class="n">convObj</span><span class="p">[</span><span class="s2">"conversationID"</span><span class="p">]]</span> <span class="o">=</span> <span class="n">convObj</span>

    <span class="k">return</span> <span class="n">lines</span><span class="p">,</span> <span class="n">conversations</span>


<span class="c1"># Extracts pairs of sentences from conversations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
    <span class="n">qa_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">conversation</span> <span class="ow">in</span> <span class="n">conversations</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="c1"># Iterate over all the lines of the conversation</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">conversation</span><span class="p">[</span><span class="s2">"lines"</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># We ignore the last line (no answer for it)</span>
            <span class="n">inputLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">"lines"</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">targetLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">"lines"</span><span class="p">][</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="c1"># Filter wrong samples (if one of the lists is empty)</span>
            <span class="k">if</span> <span class="n">inputLine</span> <span class="ow">and</span> <span class="n">targetLine</span><span class="p">:</span>
                <span class="n">qa_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">inputLine</span><span class="p">,</span> <span class="n">targetLine</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">qa_pairs</span>
</pre></div>
</div>
<p>Now we’ll call these functions and create the file. We’ll call it
<code class="docutils literal notranslate"><span class="pre">formatted_movie_lines.txt</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define path to new file</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">"formatted_movie_lines.txt"</span><span class="p">)</span>

<span class="n">delimiter</span> <span class="o">=</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span>
<span class="c1"># Unescape the delimiter</span>
<span class="n">delimiter</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">codecs</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">delimiter</span><span class="p">,</span> <span class="s2">"unicode_escape"</span><span class="p">))</span>

<span class="c1"># Initialize lines dict and conversations dict</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">conversations</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># Load lines and conversations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Processing corpus into lines and conversations..."</span><span class="p">)</span>
<span class="n">lines</span><span class="p">,</span> <span class="n">conversations</span> <span class="o">=</span> <span class="n">loadLinesAndConversations</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">"utterances.jsonl"</span><span class="p">))</span>

<span class="c1"># Write new csv file</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Writing newly formatted file..."</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">outputfile</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">outputfile</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="n">delimiter</span><span class="p">,</span> <span class="n">lineterminator</span><span class="o">=</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

<span class="c1"># Print a sample of lines</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Sample lines from file:"</span><span class="p">)</span>
<span class="n">printLines</span><span class="p">(</span><span class="n">datafile</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Processing corpus into lines and conversations...

Writing newly formatted file...

Sample lines from file:
b'They do to!\tThey do not!\n'
b'She okay?\tI hope so.\n'
b"Wow\tLet's go.\n"
b'"I\'m kidding.  You know how sometimes you just become this ""persona""?  And you don\'t know how to quit?"\tNo\n'
b"No\tOkay -- you're gonna need to learn how to lie.\n"
b"I figured you'd get to the good stuff eventually.\tWhat good stuff?\n"
b'What good stuff?\t"The ""real you""."\n'
b'"The ""real you""."\tLike my fear of wearing pastels?\n'
b'do you listen to this crap?\tWhat crap?\n'
b"What crap?\tMe.  This endless ...blonde babble. I'm like, boring myself.\n"
</pre></div>
</div>
</section>
<section id="load-and-trim-data">
<h3>Load and trim data<a class="headerlink" href="#load-and-trim-data" title="Link to this heading">#</a></h3>
<p>Our next order of business is to create a vocabulary and load
query/response sentence pairs into memory.</p>
<p>Note that we are dealing with sequences of <strong>words</strong>, which do not have
an implicit mapping to a discrete numerical space. Thus, we must create
one by mapping each unique word that we encounter in our dataset to an
index value.</p>
<p>For this we define a <code class="docutils literal notranslate"><span class="pre">Voc</span></code> class, which keeps a mapping from words to
indexes, a reverse mapping of indexes to words, a count of each word and
a total word count. The class provides methods for adding a word to the
vocabulary (<code class="docutils literal notranslate"><span class="pre">addWord</span></code>), adding all words in a sentence
(<code class="docutils literal notranslate"><span class="pre">addSentence</span></code>) and trimming infrequently seen words (<code class="docutils literal notranslate"><span class="pre">trim</span></code>). More
on trimming later.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default word tokens</span>
<span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Used for padding short sentences</span>
<span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Start-of-sentence token</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># End-of-sentence token</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Voc</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Count SOS, EOS, PAD</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Remove words below a certain count threshold</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">trim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">keep_words</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="n">keep_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'keep_words </span><span class="si">{}</span><span class="s1"> / </span><span class="si">{}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>
        <span class="p">))</span>

        <span class="c1"># Reinitialize dictionaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Count default tokens</span>

        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keep_words</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can assemble our vocabulary and query/response sentence pairs.
Before we are ready to use this data, we must perform some
preprocessing.</p>
<p>First, we must convert the Unicode strings to ASCII using
<code class="docutils literal notranslate"><span class="pre">unicodeToAscii</span></code>. Next, we should convert all letters to lowercase and
trim all non-letter characters except for basic punctuation
(<code class="docutils literal notranslate"><span class="pre">normalizeString</span></code>). Finally, to aid in training convergence, we will
filter out sentences with length greater than the <code class="docutils literal notranslate"><span class="pre">MAX_LENGTH</span></code>
threshold (<code class="docutils literal notranslate"><span class="pre">filterPairs</span></code>).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum sentence length to consider</span>

<span class="c1"># Turn a Unicode string to plain ASCII, thanks to</span>
<span class="c1"># https://stackoverflow.com/a/518232/2809427</span>
<span class="k">def</span><span class="w"> </span><span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">'NFD'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">'Mn'</span>
    <span class="p">)</span>

<span class="c1"># Lowercase, trim, and remove non-letter characters</span>
<span class="k">def</span><span class="w"> </span><span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"([.!?])"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" \1"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^a-zA-Z.!?]+"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"\s+"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="c1"># Read query/response pairs and return a voc object</span>
<span class="k">def</span><span class="w"> </span><span class="nf">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Reading lines..."</span><span class="p">)</span>
    <span class="c1"># Read the file and split into lines</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Split every line into pairs and normalize</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="n">voc</span> <span class="o">=</span> <span class="n">Voc</span><span class="p">(</span><span class="n">corpus_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>

<span class="c1"># Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold</span>
<span class="k">def</span><span class="w"> </span><span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c1"># Input sequences need to preserve the last word for EOS token</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span>

<span class="c1"># Filter pairs using the ``filterPair`` condition</span>
<span class="k">def</span><span class="w"> </span><span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>

<span class="c1"># Using the functions defined above, return a populated voc object and pairs list</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Start preparing training data ..."</span><span class="p">)</span>
    <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Read </span><span class="si">{!s}</span><span class="s2"> sentence pairs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Trimmed to </span><span class="si">{!s}</span><span class="s2"> sentence pairs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counting words..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counted words:"</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>


<span class="c1"># Load/Assemble voc and pairs</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"data"</span><span class="p">,</span> <span class="s2">"save"</span><span class="p">)</span>
<span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
<span class="c1"># Print some pairs to validate</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">pairs:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Start preparing training data ...
Reading lines...
Read 221282 sentence pairs
Trimmed to 64313 sentence pairs
Counting words...
Counted words: 18082

pairs:
['they do to !', 'they do not !']
['she okay ?', 'i hope so .']
['wow', 'let s go .']
['what good stuff ?', 'the real you .']
['the real you .', 'like my fear of wearing pastels ?']
['do you listen to this crap ?', 'what crap ?']
['well no . . .', 'then that s all you had to say .']
['then that s all you had to say .', 'but']
['but', 'you always been this selfish ?']
['have fun tonight ?', 'tons']
</pre></div>
</div>
<p>Another tactic that is beneficial to achieving faster convergence during
training is trimming rarely used words out of our vocabulary. Decreasing
the feature space will also soften the difficulty of the function that
the model must learn to approximate. We will do this as a two-step
process:</p>
<ol class="arabic simple">
<li><p>Trim words used under <code class="docutils literal notranslate"><span class="pre">MIN_COUNT</span></code> threshold using the <code class="docutils literal notranslate"><span class="pre">voc.trim</span></code>
function.</p></li>
<li><p>Filter out pairs with trimmed words.</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">MIN_COUNT</span> <span class="o">=</span> <span class="mi">3</span>    <span class="c1"># Minimum word count threshold for trimming</span>

<span class="k">def</span><span class="w"> </span><span class="nf">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">):</span>
    <span class="c1"># Trim words used under the MIN_COUNT from the voc</span>
    <span class="n">voc</span><span class="o">.</span><span class="n">trim</span><span class="p">(</span><span class="n">MIN_COUNT</span><span class="p">)</span>
    <span class="c1"># Filter out pairs with trimmed words</span>
    <span class="n">keep_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">keep_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">keep_output</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Check input sentence</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_input</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>
        <span class="c1"># Check output sentence</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_output</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="c1"># Only keep pairs that do not contain trimmed word(s) in their input or output sentence</span>
        <span class="k">if</span> <span class="n">keep_input</span> <span class="ow">and</span> <span class="n">keep_output</span><span class="p">:</span>
            <span class="n">keep_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Trimmed from </span><span class="si">{}</span><span class="s2"> pairs to </span><span class="si">{}</span><span class="s2">, </span><span class="si">{:.4f}</span><span class="s2"> of total"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">keep_pairs</span>


<span class="c1"># Trim voc and pairs</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>keep_words 7833 / 18079 = 0.4333
Trimmed from 64313 pairs to 53131, 0.8261 of total
</pre></div>
</div>
</section>
</section>
<section id="prepare-data-for-models">
<h2>Prepare Data for Models<a class="headerlink" href="#prepare-data-for-models" title="Link to this heading">#</a></h2>
<p>Although we have put a great deal of effort into preparing and massaging our
data into a nice vocabulary object and list of sentence pairs, our models
will ultimately expect numerical torch tensors as inputs. One way to
prepare the processed data for the models can be found in the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">seq2seq
translation
tutorial</a>.
In that tutorial, we use a batch size of 1, meaning that all we have to
do is convert the words in our sentence pairs to their corresponding
indexes from the vocabulary and feed this to the models.</p>
<p>However, if you’re interested in speeding up training and/or would like
to leverage GPU parallelization capabilities, you will need to train
with mini-batches.</p>
<p>Using mini-batches also means that we must be mindful of the variation
of sentence length in our batches. To accommodate sentences of different
sizes in the same batch, we will make our batched input tensor of shape
<em>(max_length, batch_size)</em>, where sentences shorter than the
<em>max_length</em> are zero padded after an <em>EOS_token</em>.</p>
<p>If we simply convert our English sentences to tensors by converting
words to their indexes(<code class="docutils literal notranslate"><span class="pre">indexesFromSentence</span></code>) and zero-pad, our
tensor would have shape <em>(batch_size, max_length)</em> and indexing the
first dimension would return a full sequence across all time-steps.
However, we need to be able to index our batch along time, and across
all sequences in the batch. Therefore, we transpose our input batch
shape to <em>(max_length, batch_size)</em>, so that indexing across the first
dimension returns a time step across all sentences in the batch. We
handle this transpose implicitly in the <code class="docutils literal notranslate"><span class="pre">zeroPadding</span></code> function.</p>
<figure class="align-center">
<img alt="batches" src="../_images/seq2seq_batches.png"/>
</figure>
<p>The <code class="docutils literal notranslate"><span class="pre">inputVar</span></code> function handles the process of converting sentences to
tensor, ultimately creating a correctly shaped zero-padded tensor. It
also returns a tensor of <code class="docutils literal notranslate"><span class="pre">lengths</span></code> for each of the sequences in the
batch which will be passed to our decoder later.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">outputVar</span></code> function performs a similar function to <code class="docutils literal notranslate"><span class="pre">inputVar</span></code>,
but instead of returning a <code class="docutils literal notranslate"><span class="pre">lengths</span></code> tensor, it returns a binary mask
tensor and a maximum target sentence length. The binary mask tensor has
the same shape as the output target tensor, but every element that is a
<em>PAD_token</em> is 0 and all others are 1.</p>
<p><code class="docutils literal notranslate"><span class="pre">batch2TrainData</span></code> simply takes a bunch of pairs and returns the input
and target tensors using the aforementioned functions.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_token</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">zeroPadding</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">fillvalue</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">binaryMatrix</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
        <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">PAD_token</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1"># Returns padded input sequence tensor and lengths</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a>

<span class="c1"># Returns padded target sequence tensor, padding mask, and max target length</span>
<span class="k">def</span><span class="w"> </span><span class="nf">outputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">max_target_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a> <span class="o">=</span> <span class="n">binaryMatrix</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span>

<span class="c1"># Returns all items for a given batch of pairs</span>
<span class="k">def</span><span class="w"> </span><span class="nf">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pair_batch</span><span class="p">):</span>
    <span class="n">pair_batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">" "</span><span class="p">)),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pair_batch</span><span class="p">:</span>
        <span class="n">input_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">inp</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <span class="n">inputVar</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">outputVar</span><span class="p">(</span><span class="n">output_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inp</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span>


<span class="c1"># Example for validation</span>
<span class="n">small_batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">small_batch_size</span><span class="p">)])</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">batches</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"input_variable:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"lengths:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"target_variable:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"mask:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"max_target_len:"</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>input_variable: tensor([[ 162,   35,   19, 1810,  280],
        [  14,   19,   10,  495,    6],
        [  40,   17,   34,   14,    2],
        [ 104,   14,    6,    2,    0],
        [ 717,   14,    2,    0,    0],
        [ 135,   14,    0,    0,    0],
        [ 187,    2,    0,    0,    0],
        [ 112,    0,    0,    0,    0],
        [  14,    0,    0,    0,    0],
        [   2,    0,    0,    0,    0]])
lengths: tensor([10,  7,  5,  4,  3])
target_variable: tensor([[  48,   85,   39,   34, 1618],
        [  67,   17,   24,  495, 7664],
        [ 104,   62,   64,   14,   10],
        [  90, 1324,  397,    2,    2],
        [  10, 1179, 1011,    0,    0],
        [   2,   14,   22,    0,    0],
        [   0,    2, 1059,    0,    0],
        [   0,    0,   14,    0,    0],
        [   0,    0,    2,    0,    0]])
mask: tensor([[ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True, False, False],
        [ True,  True,  True, False, False],
        [False,  True,  True, False, False],
        [False, False,  True, False, False],
        [False, False,  True, False, False]])
max_target_len: 9
</pre></div>
</div>
</section>
<section id="define-models">
<h2>Define Models<a class="headerlink" href="#define-models" title="Link to this heading">#</a></h2>
<section id="seq2seq-model">
<h3>Seq2Seq Model<a class="headerlink" href="#seq2seq-model" title="Link to this heading">#</a></h3>
<p>The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The
goal of a seq2seq model is to take a variable-length sequence as an
input, and return a variable-length sequence as an output using a
fixed-sized model.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1409.3215">Sutskever et al.</a> discovered that
by using two separate recurrent neural nets together, we can accomplish
this task. One RNN acts as an <strong>encoder</strong>, which encodes a variable
length input sequence to a fixed-length context vector. In theory, this
context vector (the final hidden layer of the RNN) will contain semantic
information about the query sentence that is input to the bot. The
second RNN is a <strong>decoder</strong>, which takes an input word and the context
vector, and returns a guess for the next word in the sequence and a
hidden state to use in the next iteration.</p>
<figure class="align-center">
<img alt="model" src="../_images/seq2seq_ts.png"/>
</figure>
<p>Image source:
<a class="reference external" href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p>
</section>
<section id="encoder">
<h3>Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h3>
<p>The encoder RNN iterates through the input sentence one token
(e.g. word) at a time, at each time step outputting an “output” vector
and a “hidden state” vector. The hidden state vector is then passed to
the next time step, while the output vector is recorded. The encoder
transforms the context it saw at each point in the sequence into a set
of points in a high-dimensional space, which the decoder will use to
generate a meaningful output for the given task.</p>
<p>At the heart of our encoder is a multi-layered Gated Recurrent Unit,
invented by <a class="reference external" href="https://arxiv.org/pdf/1406.1078v3.pdf">Cho et al.</a> in
2014. We will use a bidirectional variant of the GRU, meaning that there
are essentially two independent RNNs: one that is fed the input sequence
in normal sequential order, and one that is fed the input sequence in
reverse order. The outputs of each network are summed at each time step.
Using a bidirectional GRU will give us the advantage of encoding both
past and future contexts.</p>
<p>Bidirectional RNN:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/RNN-bidirectional.png"><img alt="rnn_bidir" src="../_images/RNN-bidirectional.png" style="width: 70%;"/></a>
</figure>
<p>Image source: <a class="reference external" href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a></p>
<p>Note that an <code class="docutils literal notranslate"><span class="pre">embedding</span></code> layer is used to encode our word indices in
an arbitrarily sized feature space. For our models, this layer will map
each word to a feature space of size <em>hidden_size</em>. When trained, these
values should encode semantic similarity between similar meaning words.</p>
<p>Finally, if passing a padded batch of sequences to an RNN module, we
must pack and unpack padding around the RNN pass using
<code class="docutils literal notranslate"><span class="pre">nn.utils.rnn.pack_padded_sequence</span></code> and
<code class="docutils literal notranslate"><span class="pre">nn.utils.rnn.pad_packed_sequence</span></code> respectively.</p>
<p><strong>Computation Graph:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Convert word indexes to embeddings.</p></li>
<li><p>Pack padded batch of sequences for RNN module.</p></li>
<li><p>Forward pass through GRU.</p></li>
<li><p>Unpack padding.</p></li>
<li><p>Sum bidirectional GRU outputs.</p></li>
<li><p>Return output and final hidden state.</p></li>
</ol>
</div></blockquote>
<p><strong>Inputs:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_seq</span></code>: batch of input sentences; shape=<em>(max_length,
batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_lengths</span></code>: list of sentence lengths corresponding to each
sentence in the batch; shape=<em>(batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: hidden state; shape=<em>(n_layers x num_directions,
batch_size, hidden_size)</em></p></li>
</ul>
<p><strong>Outputs:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">outputs</span></code>: output features from the last hidden layer of the GRU
(sum of bidirectional outputs); shape=<em>(max_length, batch_size,
hidden_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: updated hidden state from GRU; shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">EncoderRNN</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">EncoderRNN</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a>

        <span class="c1"># Initialize GRU; the input_size and hidden_size parameters are both set to 'hidden_size'</span>
        <span class="c1">#   because our input size is a word embedding with number of features == hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU" title="torch.nn.GRU"><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                          <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Convert word indexes to embeddings</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="c1"># Pack padded batch of sequences for RNN module</span>
        <span class="n">packed</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-utils-rnn sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span></a><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">)</span>
        <span class="c1"># Forward pass through GRU</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># Unpack padding</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-utils-rnn sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence" title="torch.nn.utils.rnn.pad_packed_sequence"><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span></a><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># Sum bidirectional GRU outputs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:]</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</section>
<section id="decoder">
<h3>Decoder<a class="headerlink" href="#decoder" title="Link to this heading">#</a></h3>
<p>The decoder RNN generates the response sentence in a token-by-token
fashion. It uses the encoder’s context vectors, and internal hidden
states to generate the next word in the sequence. It continues
generating words until it outputs an <em>EOS_token</em>, representing the end
of the sentence. A common problem with a vanilla seq2seq decoder is that
if we rely solely on the context vector to encode the entire input
sequence’s meaning, it is likely that we will have information loss.
This is especially the case when dealing with long input sequences,
greatly limiting the capability of our decoder.</p>
<p>To combat this, <a class="reference external" href="https://arxiv.org/abs/1409.0473">Bahdanau et al.</a>
created an “attention mechanism” that allows the decoder to pay
attention to certain parts of the input sequence, rather than using the
entire fixed context at every step.</p>
<p>At a high level, attention is calculated using the decoder’s current
hidden state and the encoder’s outputs. The output attention weights
have the same shape as the input sequence, allowing us to multiply them
by the encoder outputs, giving us a weighted sum which indicates the
parts of encoder output to pay attention to. <a class="reference external" href="https://github.com/spro">Sean
Robertson’s</a> figure describes this very
well:</p>
<figure class="align-center">
<img alt="attn2" src="../_images/attn2.png"/>
</figure>
<p><a class="reference external" href="https://arxiv.org/abs/1508.04025">Luong et al.</a> improved upon
Bahdanau et al.’s groundwork by creating “Global attention”. The key
difference is that with “Global attention”, we consider all of the
encoder’s hidden states, as opposed to Bahdanau et al.’s “Local
attention”, which only considers the encoder’s hidden state from the
current time step. Another difference is that with “Global attention”,
we calculate attention weights, or energies, using the hidden state of
the decoder from the current time step only. Bahdanau et al.’s attention
calculation requires knowledge of the decoder’s state from the previous
time step. Also, Luong et al. provides various methods to calculate the
attention energies between the encoder output and decoder output which
are called “score functions”:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/scores.png"><img alt="scores" src="../_images/scores.png" style="width: 60%;"/></a>
</figure>
<p>where <span class="math">\(h_t\)</span> = current target decoder state and <span class="math">\(\bar{h}_s\)</span> =
all encoder states.</p>
<p>Overall, the Global attention mechanism can be summarized by the
following figure. Note that we will implement the “Attention Layer” as a
separate <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> called <code class="docutils literal notranslate"><span class="pre">Attn</span></code>. The output of this module is a
softmax normalized weights tensor of shape <em>(batch_size, 1,
max_length)</em>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/global_attn.png"><img alt="global_attn" src="../_images/global_attn.png" style="width: 60%;"/></a>
</figure>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Luong attention layer</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Attn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Attn</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'dot'</span><span class="p">,</span> <span class="s1">'general'</span><span class="p">,</span> <span class="s1">'concat'</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="s2">"is not an appropriate attention method."</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span></a><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dot_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" title="torch.sum"><span class="n">torch</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">general_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" title="torch.sum"><span class="n">torch</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">concat_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">((</span><span class="n">hidden</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" title="torch.sum"><span class="n">torch</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Calculate the attention weights (energies) based on the given method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">general_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'dot'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="c1"># Transpose max_length and batch_size dimensions</span>
        <span class="n">attn_energies</span> <span class="o">=</span> <span class="n">attn_energies</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="c1"># Return the softmax normalized probability scores (with added dimension)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><span class="n">F</span><span class="o">.</span><span class="n">softmax</span></a><span class="p">(</span><span class="n">attn_energies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Now that we have defined our attention submodule, we can implement the
actual decoder model. For the decoder, we will manually feed our batch
one time step at a time. This means that our embedded word tensor and
GRU output will both have shape <em>(1, batch_size, hidden_size)</em>.</p>
<p><strong>Computation Graph:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Get embedding of current input word.</p></li>
<li><p>Forward through unidirectional GRU.</p></li>
<li><p>Calculate attention weights from the current GRU output from (2).</p></li>
<li><p>Multiply attention weights to encoder outputs to get new “weighted sum” context vector.</p></li>
<li><p>Concatenate weighted context vector and GRU output using Luong eq. 5.</p></li>
<li><p>Predict next word using Luong eq. 6 (without softmax).</p></li>
<li><p>Return output and final hidden state.</p></li>
</ol>
</div></blockquote>
<p><strong>Inputs:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_step</span></code>: one time step (one word) of input sequence batch;
shape=<em>(1, batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">last_hidden</span></code>: final hidden layer of GRU; shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">encoder_outputs</span></code>: encoder model’s output; shape=<em>(max_length,
batch_size, hidden_size)</em></p></li>
</ul>
<p><strong>Outputs:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">output</span></code>: softmax normalized tensor giving probabilities of each
word being the correct next word in the decoded sequence;
shape=<em>(batch_size, voc.num_words)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: final hidden state of GRU; shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LuongAttnDecoderRNN</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_model</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">LuongAttnDecoderRNN</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Keep for reference</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Define layers</span>
        <span class="bp">self</span><span class="o">.</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout" title="torch.nn.Dropout"><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span></a><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU" title="torch.nn.GRU"><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Attn</span></a><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_step</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Note: we run this one step (word) at a time</span>
        <span class="c1"># Get embedding of current input word</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">(</span><span class="n">input_step</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="c1"># Forward through unidirectional GRU</span>
        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">)</span>
        <span class="c1"># Calculate attention weights from the current GRU output</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="c1"># Multiply attention weights to encoder outputs to get new "weighted sum" context vector</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Concatenate weighted context vector and GRU output using Luong eq. 5</span>
        <span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_input</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">((</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_output</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh" title="torch.tanh"><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_input</span><span class="p">))</span>
        <span class="c1"># Predict next word using Luong eq. 6</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><span class="n">F</span><span class="o">.</span><span class="n">softmax</span></a><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</section>
</section>
<section id="define-training-procedure">
<h2>Define Training Procedure<a class="headerlink" href="#define-training-procedure" title="Link to this heading">#</a></h2>
<section id="masked-loss">
<h3>Masked loss<a class="headerlink" href="#masked-loss" title="Link to this heading">#</a></h3>
<p>Since we are dealing with batches of padded sequences, we cannot simply
consider all elements of the tensor when calculating loss. We define
<code class="docutils literal notranslate"><span class="pre">maskNLLLoss</span></code> to calculate our loss based on our decoder’s output
tensor, the target tensor, and a binary mask tensor describing the
padding of the target tensor. This loss function calculates the average
negative log likelihood of the elements that correspond to a <em>1</em> in the
mask tensor.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">maskNLLLoss</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">):</span>
    <span class="n">nTotal</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">crossEntropy</span> <span class="o">=</span> <span class="o">-</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.log.html#torch.log" title="torch.log"><span class="n">torch</span><span class="o">.</span><span class="n">log</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.gather.html#torch.gather" title="torch.gather"><span class="n">torch</span><span class="o">.</span><span class="n">gather</span></a><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">crossEntropy</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">nTotal</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="single-training-iteration">
<h3>Single training iteration<a class="headerlink" href="#single-training-iteration" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">train</span></code> function contains the algorithm for a single training
iteration (a single batch of inputs).</p>
<p>We will use a couple of clever tricks to aid in convergence:</p>
<ul class="simple">
<li><p>The first trick is using <strong>teacher forcing</strong>. This means that at some
probability, set by <code class="docutils literal notranslate"><span class="pre">teacher_forcing_ratio</span></code>, we use the current
target word as the decoder’s next input rather than using the
decoder’s current guess. This technique acts as training wheels for
the decoder, aiding in more efficient training. However, teacher
forcing can lead to model instability during inference, as the
decoder may not have a sufficient chance to truly craft its own
output sequences during training. Thus, we must be mindful of how we
are setting the <code class="docutils literal notranslate"><span class="pre">teacher_forcing_ratio</span></code>, and not be fooled by fast
convergence.</p></li>
<li><p>The second trick that we implement is <strong>gradient clipping</strong>. This is
a commonly used technique for countering the “exploding gradient”
problem. In essence, by clipping or thresholding gradients to a
maximum value, we prevent the gradients from growing exponentially
and either overflow (NaN), or overshoot steep cliffs in the cost
function.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/grad_clip.png"><img alt="grad_clip" src="../_images/grad_clip.png" style="width: 60%;"/></a>
</figure>
<p>Image source: Goodfellow et al. <em>Deep Learning</em>. 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p><strong>Sequence of Operations:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Forward pass entire input batch through encoder.</p></li>
<li><p>Initialize decoder inputs as SOS_token, and hidden state as the encoder’s final hidden state.</p></li>
<li><p>Forward input batch sequence through decoder one time step at a time.</p></li>
<li><p>If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.</p></li>
<li><p>Calculate and accumulate loss.</p></li>
<li><p>Perform backpropagation.</p></li>
<li><p>Clip gradients.</p></li>
<li><p>Update encoder and decoder model parameters.</p></li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PyTorch’s RNN modules (<code class="docutils literal notranslate"><span class="pre">RNN</span></code>, <code class="docutils literal notranslate"><span class="pre">LSTM</span></code>, <code class="docutils literal notranslate"><span class="pre">GRU</span></code>) can be used like any
other non-recurrent layers by simply passing them the entire input
sequence (or batch of sequences). We use the <code class="docutils literal notranslate"><span class="pre">GRU</span></code> layer like this in
the <code class="docutils literal notranslate"><span class="pre">encoder</span></code>. The reality is that under the hood, there is an
iterative process looping over each time step calculating hidden states.
Alternatively, you can run these modules one time-step at a time. In
this case, we manually loop over the sequences during the training
process like we must do for the <code class="docutils literal notranslate"><span class="pre">decoder</span></code> model. As long as you
maintain the correct conceptual model of these modules, implementing
sequential models can be very straightforward.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span>
          <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>

    <span class="c1"># Zero gradients</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad"><span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad"><span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

    <span class="c1"># Set device options</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Lengths for RNN packing should always be on the CPU</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>

    <span class="c1"># Initialize variables</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">print_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_totals</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Forward pass through encoder</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">)</span>

    <span class="c1"># Create initial decoder input (start with SOS tokens for each sentence)</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">SOS_token</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set initial decoder hidden state to the encoder's final hidden state</span>
    <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>

    <span class="c1"># Determine if we are using teacher forcing this iteration</span>
    <span class="n">use_teacher_forcing</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">teacher_forcing_ratio</span> <span class="k">else</span> <span class="kc">False</span>

    <span class="c1"># Forward batch of sequences through decoder one time step at a time</span>
    <span class="k">if</span> <span class="n">use_teacher_forcing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># Teacher forcing: next input is current target</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">[</span><span class="n">t</span><span class="p">],</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># No teacher forcing: next input is decoder's own current output</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">topi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">[</span><span class="n">t</span><span class="p">],</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>

    <span class="c1"># Perform backpropagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Clip gradients: gradients are modified in place</span>
    <span class="n">_</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-utils sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-utils sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>

    <span class="c1"># Adjust model weights</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step"><span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.step" title="torch.optim.Adam.step"><span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>

    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">print_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_totals</span>
</pre></div>
</div>
</section>
<section id="training-iterations">
<h3>Training iterations<a class="headerlink" href="#training-iterations" title="Link to this heading">#</a></h3>
<p>It is finally time to tie the full training procedure together with the
data. The <code class="docutils literal notranslate"><span class="pre">trainIters</span></code> function is responsible for running
<code class="docutils literal notranslate"><span class="pre">n_iterations</span></code> of training given the passed models, optimizers, data,
etc. This function is quite self explanatory, as we have done the heavy
lifting with the <code class="docutils literal notranslate"><span class="pre">train</span></code> function.</p>
<p>One thing to note is that when we save our model, we save a tarball
containing the encoder and decoder <code class="docutils literal notranslate"><span class="pre">state_dicts</span></code> (parameters), the
optimizers’ <code class="docutils literal notranslate"><span class="pre">state_dicts</span></code>, the loss, the iteration, etc. Saving the model
in this way will give us the ultimate flexibility with the checkpoint.
After loading a checkpoint, we will be able to use the model parameters
to run inference, or we can continue training right where we left off.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">):</span>

    <span class="c1"># Load batches for each iteration</span>
    <span class="n">training_batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span>
                      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">)]</span>

    <span class="c1"># Initializations</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Initializing ...'</span><span class="p">)</span>
    <span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
        <span class="n">start_iteration</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'iteration'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Training loop</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iteration</span><span class="p">,</span> <span class="n">n_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">training_batch</span> <span class="o">=</span> <span class="n">training_batches</span><span class="p">[</span><span class="n">iteration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Extract fields from batch</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">training_batch</span>

        <span class="c1"># Run a training iteration with batch</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">input_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">target_variable</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mask</span></a><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span>
                     <span class="n">decoder</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">)</span>
        <span class="n">print_loss</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Iteration: </span><span class="si">{}</span><span class="s2">; Percent complete: </span><span class="si">{:.1f}</span><span class="s2">%; Average loss: </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">/</span> <span class="n">n_iteration</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>
            <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Save checkpoint</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="s1">'</span><span class="si">{}</span><span class="s1">-</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.save.html#torch.save" title="torch.save"><span class="n">torch</span><span class="o">.</span><span class="n">save</span></a><span class="p">({</span>
                <span class="s1">'iteration'</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
                <span class="s1">'en'</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span></a><span class="p">(),</span>
                <span class="s1">'de'</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><span class="n">decoder</span><span class="o">.</span><span class="n">state_dict</span></a><span class="p">(),</span>
                <span class="s1">'en_opt'</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.state_dict" title="torch.optim.Adam.state_dict"><span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span></a><span class="p">(),</span>
                <span class="s1">'de_opt'</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.state_dict" title="torch.optim.Adam.state_dict"><span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span></a><span class="p">(),</span>
                <span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                <span class="s1">'voc_dict'</span><span class="p">:</span> <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
                <span class="s1">'embedding'</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><span class="n">embedding</span><span class="o">.</span><span class="n">state_dict</span></a><span class="p">()</span>
            <span class="p">},</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="s1">'</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">.tar'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="s1">'checkpoint'</span><span class="p">)))</span>
</pre></div>
</div>
</section>
</section>
<section id="define-evaluation">
<h2>Define Evaluation<a class="headerlink" href="#define-evaluation" title="Link to this heading">#</a></h2>
<p>After training a model, we want to be able to talk to the bot ourselves.
First, we must define how we want the model to decode the encoded input.</p>
<section id="greedy-decoding">
<h3>Greedy decoding<a class="headerlink" href="#greedy-decoding" title="Link to this heading">#</a></h3>
<p>Greedy decoding is the decoding method that we use during training when
we are <strong>NOT</strong> using teacher forcing. In other words, for each time
step, we simply choose the word from <code class="docutils literal notranslate"><span class="pre">decoder_output</span></code> with the highest
softmax value. This decoding method is optimal on a single time-step
level.</p>
<p>To facilitate the greedy decoding operation, we define a
<code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> class. When run, an object of this class takes
an input sequence (<code class="docutils literal notranslate"><span class="pre">input_seq</span></code>) of shape <em>(input_seq length, 1)</em>, a
scalar input length (<code class="docutils literal notranslate"><span class="pre">input_length</span></code>) tensor, and a <code class="docutils literal notranslate"><span class="pre">max_length</span></code> to
bound the response sentence length. The input sentence is evaluated
using the following computational graph:</p>
<p><strong>Computation Graph:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Forward input through encoder model.</p></li>
<li><p>Prepare encoder’s final hidden layer to be first hidden input to the decoder.</p></li>
<li><p>Initialize decoder’s first input as SOS_token.</p></li>
<li><p>Initialize tensors to append decoded words to.</p></li>
<li><dl class="simple">
<dt>Iteratively decode one word token at a time:</dt><dd><ol class="loweralpha simple">
<li><p>Forward pass through decoder.</p></li>
<li><p>Obtain most likely word token and its softmax score.</p></li>
<li><p>Record token and score.</p></li>
<li><p>Prepare current token to be next decoder input.</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Return collections of word tokens and scores.</p></li>
</ol>
</div></blockquote>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GreedySearchDecoder</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GreedySearchDecoder</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="c1"># Forward input through encoder model</span>
        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
        <span class="c1"># Prepare encoder's final hidden layer to be first hidden input to the decoder</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>
        <span class="c1"># Initialize decoder input with SOS_token</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">long</span></a><span class="p">)</span> <span class="o">*</span> <span class="n">SOS_token</span>
        <span class="c1"># Initialize tensors to append decoded words to</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">long</span></a><span class="p">)</span>
        <span class="n">all_scores</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># Iteratively decode one word token at a time</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="c1"># Forward pass through decoder</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
            <span class="c1"># Obtain most likely word token and its softmax score</span>
            <span class="n">decoder_scores</span><span class="p">,</span> <span class="n">decoder_input</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.max.html#torch.max" title="torch.max"><span class="n">torch</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Record token and score</span>
            <span class="n">all_tokens</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">((</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">all_scores</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">((</span><span class="n">all_scores</span><span class="p">,</span> <span class="n">decoder_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Prepare current token to be next decoder input (add a dimension)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span></a><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Return collections of word tokens and scores</span>
        <span class="k">return</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_scores</span>
</pre></div>
</div>
</section>
<section id="evaluate-my-text">
<h3>Evaluate my text<a class="headerlink" href="#evaluate-my-text" title="Link to this heading">#</a></h3>
<p>Now that we have our decoding method defined, we can write functions for
evaluating a string input sentence. The <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> function manages
the low-level process of handling the input sentence. We first format
the sentence as an input batch of word indexes with <em>batch_size==1</em>. We
do this by converting the words of the sentence to their corresponding
indexes, and transposing the dimensions to prepare the tensor for our
models. We also create a <code class="docutils literal notranslate"><span class="pre">lengths</span></code> tensor which contains the length of
our input sentence. In this case, <code class="docutils literal notranslate"><span class="pre">lengths</span></code> is scalar because we are
only evaluating one sentence at a time (batch_size==1). Next, we obtain
the decoded response sentence tensor using our <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code>
object (<code class="docutils literal notranslate"><span class="pre">searcher</span></code>). Finally, we convert the response’s indexes to
words and return the list of decoded words.</p>
<p><code class="docutils literal notranslate"><span class="pre">evaluateInput</span></code> acts as the user interface for our chatbot. When
called, an input text field will spawn in which we can enter our query
sentence. After typing our input sentence and pressing <em>Enter</em>, our text
is normalized in the same way as our training data, and is ultimately
fed to the <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> function to obtain a decoded output sentence. We
loop this process, so we can keep chatting with our bot until we enter
either “q” or “quit”.</p>
<p>Finally, if a sentence is entered that contains a word that is not in
the vocabulary, we handle this gracefully by printing an error message
and prompting the user to enter another sentence.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="c1">### Format input sentence as a batch</span>
    <span class="c1"># words -&gt; indexes</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)]</span>
    <span class="c1"># Create lengths tensor</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="c1"># Transpose dimensions of batch to match models' expectations</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Use appropriate device</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
    <span class="c1"># Decode sentence with searcher</span>
    <span class="n">tokens</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">lengths</span></a><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># indexes -&gt; words</span>
    <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decoded_words</span>


<span class="k">def</span><span class="w"> </span><span class="nf">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Get input sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">'&gt; '</span><span class="p">)</span>
            <span class="c1"># Check if it is quit case</span>
            <span class="k">if</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">'q'</span> <span class="ow">or</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">'quit'</span><span class="p">:</span> <span class="k">break</span>
            <span class="c1"># Normalize sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Evaluate sentence</span>
            <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Format and print response sentence</span>
            <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s1">'EOS'</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">'PAD'</span><span class="p">)]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Bot:'</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Error: Encountered unknown word."</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="run-model">
<h2>Run Model<a class="headerlink" href="#run-model" title="Link to this heading">#</a></h2>
<p>Finally, it is time to run our model!</p>
<p>Regardless of whether we want to train or test the chatbot model, we
must initialize the individual encoder and decoder models. In the
following block, we set our desired configurations, choose to start from
scratch or set a checkpoint to load from, and build and initialize the
models. Feel free to play with different model configurations to
optimize performance.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure models</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">'cb_model'</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s1">'dot'</span>
<span class="c1">#``attn_model = 'general'``</span>
<span class="c1">#``attn_model = 'concat'``</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">encoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">decoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Set checkpoint to load from; set to None if starting from scratch</span>
<span class="n">loadFilename</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">checkpoint_iter</span> <span class="o">=</span> <span class="mi">4000</span>
</pre></div>
</div>
<p>Sample code to load from a checkpoint:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loadFilename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span>
                    <span class="s1">'</span><span class="si">{}</span><span class="s1">-</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
                    <span class="s1">'</span><span class="si">{}</span><span class="s1">_checkpoint.tar'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">checkpoint_iter</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load model if a ``loadFilename`` is provided</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="c1"># If loading on same machine the model was trained on</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="torch.load"><span class="n">torch</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">loadFilename</span><span class="p">)</span>
    <span class="c1"># If loading a model trained on GPU to CPU</span>
    <span class="c1">#checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))</span>
    <span class="n">encoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'en'</span><span class="p">]</span>
    <span class="n">decoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'de'</span><span class="p">]</span>
    <span class="n">encoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'en_opt'</span><span class="p">]</span>
    <span class="n">decoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'de_opt'</span><span class="p">]</span>
    <span class="n">embedding_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'embedding'</span><span class="p">]</span>
    <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'voc_dict'</span><span class="p">]</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">'Building encoder and decoder ...'</span><span class="p">)</span>
<span class="c1"># Initialize word embeddings</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span></a><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><span class="n">embedding</span><span class="o">.</span><span class="n">load_state_dict</span></a><span class="p">(</span><span class="n">embedding_sd</span><span class="p">)</span>
<span class="c1"># Initialize encoder &amp; decoder models</span>
<span class="n">encoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">EncoderRNN</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">LuongAttnDecoderRNN</span></a><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span></a><span class="p">(</span><span class="n">encoder_sd</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span></a><span class="p">(</span><span class="n">decoder_sd</span><span class="p">)</span>
<span class="c1"># Use appropriate device</span>
<span class="n">encoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><span class="n">encoder</span><span class="o">.</span><span class="n">to</span></a><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><span class="n">decoder</span><span class="o">.</span><span class="n">to</span></a><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Models built and ready to go!'</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Building encoder and decoder ...
Models built and ready to go!
</pre></div>
</div>
<section id="run-training">
<h3>Run Training<a class="headerlink" href="#run-training" title="Link to this heading">#</a></h3>
<p>Run the following block if you want to train the model.</p>
<p>First we set training parameters, then we initialize our optimizers, and
finally we call the <code class="docutils literal notranslate"><span class="pre">trainIters</span></code> function to run our training
iterations.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure training/optimization</span>
<span class="n">clip</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">teacher_forcing_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">decoder_learning_ratio</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Ensure dropout layers are in train mode</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="torch.nn.Module.train"><span class="n">encoder</span><span class="o">.</span><span class="n">train</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="torch.nn.Module.train"><span class="n">decoder</span><span class="o">.</span><span class="n">train</span></a><span class="p">()</span>

<span class="c1"># Initialize optimizers</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Building optimizers ...'</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">decoder_learning_ratio</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict" title="torch.optim.Adam.load_state_dict"><span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span></a><span class="p">(</span><span class="n">encoder_optimizer_sd</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict" title="torch.optim.Adam.load_state_dict"><span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span></a><span class="p">(</span><span class="n">decoder_optimizer_sd</span><span class="p">)</span>

<span class="c1"># If you have an accelerator, configure it to call</span>
<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Run training iterations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Starting Training!"</span><span class="p">)</span>
<span class="n">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">encoder_optimizer</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">decoder_optimizer</span></a><span class="p">,</span>
           <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><span class="n">embedding</span></a><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
           <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Building optimizers ...
Starting Training!
Initializing ...
Training...
Iteration: 1; Percent complete: 0.0%; Average loss: 8.9756
Iteration: 2; Percent complete: 0.1%; Average loss: 8.8667
Iteration: 3; Percent complete: 0.1%; Average loss: 8.6607
Iteration: 4; Percent complete: 0.1%; Average loss: 8.4327
Iteration: 5; Percent complete: 0.1%; Average loss: 8.0761
Iteration: 6; Percent complete: 0.1%; Average loss: 7.5822
Iteration: 7; Percent complete: 0.2%; Average loss: 7.1297
Iteration: 8; Percent complete: 0.2%; Average loss: 6.6842
Iteration: 9; Percent complete: 0.2%; Average loss: 6.7659
Iteration: 10; Percent complete: 0.2%; Average loss: 6.7540
Iteration: 11; Percent complete: 0.3%; Average loss: 6.2143
Iteration: 12; Percent complete: 0.3%; Average loss: 5.9623
Iteration: 13; Percent complete: 0.3%; Average loss: 5.6665
Iteration: 14; Percent complete: 0.4%; Average loss: 5.4779
Iteration: 15; Percent complete: 0.4%; Average loss: 5.5233
Iteration: 16; Percent complete: 0.4%; Average loss: 5.5385
Iteration: 17; Percent complete: 0.4%; Average loss: 5.0754
Iteration: 18; Percent complete: 0.4%; Average loss: 5.1344
Iteration: 19; Percent complete: 0.5%; Average loss: 5.1203
Iteration: 20; Percent complete: 0.5%; Average loss: 4.9330
Iteration: 21; Percent complete: 0.5%; Average loss: 4.9703
Iteration: 22; Percent complete: 0.5%; Average loss: 4.8878
Iteration: 23; Percent complete: 0.6%; Average loss: 5.1144
Iteration: 24; Percent complete: 0.6%; Average loss: 4.9549
Iteration: 25; Percent complete: 0.6%; Average loss: 4.9659
Iteration: 26; Percent complete: 0.7%; Average loss: 4.5693
Iteration: 27; Percent complete: 0.7%; Average loss: 4.8597
Iteration: 28; Percent complete: 0.7%; Average loss: 4.8144
Iteration: 29; Percent complete: 0.7%; Average loss: 4.9829
Iteration: 30; Percent complete: 0.8%; Average loss: 5.1429
Iteration: 31; Percent complete: 0.8%; Average loss: 4.5494
Iteration: 32; Percent complete: 0.8%; Average loss: 4.7390
Iteration: 33; Percent complete: 0.8%; Average loss: 4.7331
Iteration: 34; Percent complete: 0.9%; Average loss: 4.7900
Iteration: 35; Percent complete: 0.9%; Average loss: 4.8527
Iteration: 36; Percent complete: 0.9%; Average loss: 4.9294
Iteration: 37; Percent complete: 0.9%; Average loss: 4.6744
Iteration: 38; Percent complete: 0.9%; Average loss: 4.6147
Iteration: 39; Percent complete: 1.0%; Average loss: 4.8135
Iteration: 40; Percent complete: 1.0%; Average loss: 4.5181
Iteration: 41; Percent complete: 1.0%; Average loss: 4.6252
Iteration: 42; Percent complete: 1.1%; Average loss: 4.7419
Iteration: 43; Percent complete: 1.1%; Average loss: 4.5685
Iteration: 44; Percent complete: 1.1%; Average loss: 4.6670
Iteration: 45; Percent complete: 1.1%; Average loss: 4.5784
Iteration: 46; Percent complete: 1.1%; Average loss: 4.8193
Iteration: 47; Percent complete: 1.2%; Average loss: 4.9056
Iteration: 48; Percent complete: 1.2%; Average loss: 4.4678
Iteration: 49; Percent complete: 1.2%; Average loss: 4.7022
Iteration: 50; Percent complete: 1.2%; Average loss: 4.6481
Iteration: 51; Percent complete: 1.3%; Average loss: 4.6209
Iteration: 52; Percent complete: 1.3%; Average loss: 4.6002
Iteration: 53; Percent complete: 1.3%; Average loss: 4.6281
Iteration: 54; Percent complete: 1.4%; Average loss: 4.7856
Iteration: 55; Percent complete: 1.4%; Average loss: 4.6961
Iteration: 56; Percent complete: 1.4%; Average loss: 4.6516
Iteration: 57; Percent complete: 1.4%; Average loss: 4.6600
Iteration: 58; Percent complete: 1.5%; Average loss: 4.6082
Iteration: 59; Percent complete: 1.5%; Average loss: 4.4427
Iteration: 60; Percent complete: 1.5%; Average loss: 4.7418
Iteration: 61; Percent complete: 1.5%; Average loss: 4.7729
Iteration: 62; Percent complete: 1.6%; Average loss: 4.6912
Iteration: 63; Percent complete: 1.6%; Average loss: 4.4042
Iteration: 64; Percent complete: 1.6%; Average loss: 4.6691
Iteration: 65; Percent complete: 1.6%; Average loss: 4.6227
Iteration: 66; Percent complete: 1.7%; Average loss: 4.4480
Iteration: 67; Percent complete: 1.7%; Average loss: 4.7046
Iteration: 68; Percent complete: 1.7%; Average loss: 4.5236
Iteration: 69; Percent complete: 1.7%; Average loss: 4.6502
Iteration: 70; Percent complete: 1.8%; Average loss: 4.7270
Iteration: 71; Percent complete: 1.8%; Average loss: 4.4789
Iteration: 72; Percent complete: 1.8%; Average loss: 4.5091
Iteration: 73; Percent complete: 1.8%; Average loss: 4.4509
Iteration: 74; Percent complete: 1.8%; Average loss: 4.5240
Iteration: 75; Percent complete: 1.9%; Average loss: 4.1346
Iteration: 76; Percent complete: 1.9%; Average loss: 4.7585
Iteration: 77; Percent complete: 1.9%; Average loss: 4.3322
Iteration: 78; Percent complete: 1.9%; Average loss: 4.4739
Iteration: 79; Percent complete: 2.0%; Average loss: 4.1729
Iteration: 80; Percent complete: 2.0%; Average loss: 4.5390
Iteration: 81; Percent complete: 2.0%; Average loss: 4.5473
Iteration: 82; Percent complete: 2.1%; Average loss: 4.2703
Iteration: 83; Percent complete: 2.1%; Average loss: 4.4995
Iteration: 84; Percent complete: 2.1%; Average loss: 4.4703
Iteration: 85; Percent complete: 2.1%; Average loss: 4.3485
Iteration: 86; Percent complete: 2.1%; Average loss: 4.1946
Iteration: 87; Percent complete: 2.2%; Average loss: 4.3863
Iteration: 88; Percent complete: 2.2%; Average loss: 4.5126
Iteration: 89; Percent complete: 2.2%; Average loss: 4.6809
Iteration: 90; Percent complete: 2.2%; Average loss: 4.5945
Iteration: 91; Percent complete: 2.3%; Average loss: 4.5111
Iteration: 92; Percent complete: 2.3%; Average loss: 4.5442
Iteration: 93; Percent complete: 2.3%; Average loss: 4.4153
Iteration: 94; Percent complete: 2.4%; Average loss: 4.3059
Iteration: 95; Percent complete: 2.4%; Average loss: 4.4178
Iteration: 96; Percent complete: 2.4%; Average loss: 4.3698
Iteration: 97; Percent complete: 2.4%; Average loss: 4.5720
Iteration: 98; Percent complete: 2.5%; Average loss: 4.3105
Iteration: 99; Percent complete: 2.5%; Average loss: 4.3704
Iteration: 100; Percent complete: 2.5%; Average loss: 4.5521
Iteration: 101; Percent complete: 2.5%; Average loss: 4.4161
Iteration: 102; Percent complete: 2.5%; Average loss: 4.3238
Iteration: 103; Percent complete: 2.6%; Average loss: 4.3940
Iteration: 104; Percent complete: 2.6%; Average loss: 4.4251
Iteration: 105; Percent complete: 2.6%; Average loss: 4.3681
Iteration: 106; Percent complete: 2.6%; Average loss: 4.3026
Iteration: 107; Percent complete: 2.7%; Average loss: 4.7332
Iteration: 108; Percent complete: 2.7%; Average loss: 4.6414
Iteration: 109; Percent complete: 2.7%; Average loss: 4.5147
Iteration: 110; Percent complete: 2.8%; Average loss: 4.2995
Iteration: 111; Percent complete: 2.8%; Average loss: 4.4154
Iteration: 112; Percent complete: 2.8%; Average loss: 4.2703
Iteration: 113; Percent complete: 2.8%; Average loss: 4.3424
Iteration: 114; Percent complete: 2.9%; Average loss: 4.3429
Iteration: 115; Percent complete: 2.9%; Average loss: 4.2511
Iteration: 116; Percent complete: 2.9%; Average loss: 4.2147
Iteration: 117; Percent complete: 2.9%; Average loss: 4.2876
Iteration: 118; Percent complete: 2.9%; Average loss: 4.5823
Iteration: 119; Percent complete: 3.0%; Average loss: 4.3462
Iteration: 120; Percent complete: 3.0%; Average loss: 4.4426
Iteration: 121; Percent complete: 3.0%; Average loss: 4.3820
Iteration: 122; Percent complete: 3.0%; Average loss: 4.0776
Iteration: 123; Percent complete: 3.1%; Average loss: 4.3654
Iteration: 124; Percent complete: 3.1%; Average loss: 4.3790
Iteration: 125; Percent complete: 3.1%; Average loss: 4.3151
Iteration: 126; Percent complete: 3.1%; Average loss: 4.4324
Iteration: 127; Percent complete: 3.2%; Average loss: 4.3096
Iteration: 128; Percent complete: 3.2%; Average loss: 4.3883
Iteration: 129; Percent complete: 3.2%; Average loss: 4.0664
Iteration: 130; Percent complete: 3.2%; Average loss: 4.1046
Iteration: 131; Percent complete: 3.3%; Average loss: 4.2476
Iteration: 132; Percent complete: 3.3%; Average loss: 4.3515
Iteration: 133; Percent complete: 3.3%; Average loss: 4.3821
Iteration: 134; Percent complete: 3.4%; Average loss: 4.5355
Iteration: 135; Percent complete: 3.4%; Average loss: 4.3854
Iteration: 136; Percent complete: 3.4%; Average loss: 4.4099
Iteration: 137; Percent complete: 3.4%; Average loss: 4.4142
Iteration: 138; Percent complete: 3.5%; Average loss: 4.0639
Iteration: 139; Percent complete: 3.5%; Average loss: 4.3268
Iteration: 140; Percent complete: 3.5%; Average loss: 4.6308
Iteration: 141; Percent complete: 3.5%; Average loss: 4.2231
Iteration: 142; Percent complete: 3.5%; Average loss: 4.2517
Iteration: 143; Percent complete: 3.6%; Average loss: 4.2467
Iteration: 144; Percent complete: 3.6%; Average loss: 4.2183
Iteration: 145; Percent complete: 3.6%; Average loss: 4.0980
Iteration: 146; Percent complete: 3.6%; Average loss: 4.4000
Iteration: 147; Percent complete: 3.7%; Average loss: 4.3491
Iteration: 148; Percent complete: 3.7%; Average loss: 4.4067
Iteration: 149; Percent complete: 3.7%; Average loss: 4.0079
Iteration: 150; Percent complete: 3.8%; Average loss: 4.3088
Iteration: 151; Percent complete: 3.8%; Average loss: 4.4050
Iteration: 152; Percent complete: 3.8%; Average loss: 4.2857
Iteration: 153; Percent complete: 3.8%; Average loss: 4.1196
Iteration: 154; Percent complete: 3.9%; Average loss: 4.3951
Iteration: 155; Percent complete: 3.9%; Average loss: 4.1116
Iteration: 156; Percent complete: 3.9%; Average loss: 4.2435
Iteration: 157; Percent complete: 3.9%; Average loss: 4.3154
Iteration: 158; Percent complete: 4.0%; Average loss: 4.0113
Iteration: 159; Percent complete: 4.0%; Average loss: 4.3031
Iteration: 160; Percent complete: 4.0%; Average loss: 4.3604
Iteration: 161; Percent complete: 4.0%; Average loss: 3.9337
Iteration: 162; Percent complete: 4.0%; Average loss: 4.0941
Iteration: 163; Percent complete: 4.1%; Average loss: 4.1448
Iteration: 164; Percent complete: 4.1%; Average loss: 4.4160
Iteration: 165; Percent complete: 4.1%; Average loss: 4.0635
Iteration: 166; Percent complete: 4.2%; Average loss: 4.0130
Iteration: 167; Percent complete: 4.2%; Average loss: 4.0126
Iteration: 168; Percent complete: 4.2%; Average loss: 4.1332
Iteration: 169; Percent complete: 4.2%; Average loss: 4.1200
Iteration: 170; Percent complete: 4.2%; Average loss: 4.3998
Iteration: 171; Percent complete: 4.3%; Average loss: 4.0235
Iteration: 172; Percent complete: 4.3%; Average loss: 4.2458
Iteration: 173; Percent complete: 4.3%; Average loss: 4.3408
Iteration: 174; Percent complete: 4.3%; Average loss: 4.0092
Iteration: 175; Percent complete: 4.4%; Average loss: 4.1289
Iteration: 176; Percent complete: 4.4%; Average loss: 4.0282
Iteration: 177; Percent complete: 4.4%; Average loss: 3.9678
Iteration: 178; Percent complete: 4.5%; Average loss: 4.1102
Iteration: 179; Percent complete: 4.5%; Average loss: 4.3429
Iteration: 180; Percent complete: 4.5%; Average loss: 4.2584
Iteration: 181; Percent complete: 4.5%; Average loss: 3.9500
Iteration: 182; Percent complete: 4.5%; Average loss: 4.2582
Iteration: 183; Percent complete: 4.6%; Average loss: 4.2197
Iteration: 184; Percent complete: 4.6%; Average loss: 4.1177
Iteration: 185; Percent complete: 4.6%; Average loss: 3.9369
Iteration: 186; Percent complete: 4.7%; Average loss: 3.9190
Iteration: 187; Percent complete: 4.7%; Average loss: 4.1578
Iteration: 188; Percent complete: 4.7%; Average loss: 3.9495
Iteration: 189; Percent complete: 4.7%; Average loss: 3.9539
Iteration: 190; Percent complete: 4.8%; Average loss: 4.1388
Iteration: 191; Percent complete: 4.8%; Average loss: 4.2341
Iteration: 192; Percent complete: 4.8%; Average loss: 4.2562
Iteration: 193; Percent complete: 4.8%; Average loss: 4.1978
Iteration: 194; Percent complete: 4.9%; Average loss: 4.1372
Iteration: 195; Percent complete: 4.9%; Average loss: 4.2581
Iteration: 196; Percent complete: 4.9%; Average loss: 4.2444
Iteration: 197; Percent complete: 4.9%; Average loss: 4.0698
Iteration: 198; Percent complete: 5.0%; Average loss: 4.2402
Iteration: 199; Percent complete: 5.0%; Average loss: 4.0469
Iteration: 200; Percent complete: 5.0%; Average loss: 4.0102
Iteration: 201; Percent complete: 5.0%; Average loss: 4.2119
Iteration: 202; Percent complete: 5.1%; Average loss: 3.8905
Iteration: 203; Percent complete: 5.1%; Average loss: 4.0130
Iteration: 204; Percent complete: 5.1%; Average loss: 4.1105
Iteration: 205; Percent complete: 5.1%; Average loss: 4.0650
Iteration: 206; Percent complete: 5.1%; Average loss: 3.8371
Iteration: 207; Percent complete: 5.2%; Average loss: 4.1023
Iteration: 208; Percent complete: 5.2%; Average loss: 3.9663
Iteration: 209; Percent complete: 5.2%; Average loss: 4.1744
Iteration: 210; Percent complete: 5.2%; Average loss: 3.9062
Iteration: 211; Percent complete: 5.3%; Average loss: 3.9315
Iteration: 212; Percent complete: 5.3%; Average loss: 3.9648
Iteration: 213; Percent complete: 5.3%; Average loss: 4.1374
Iteration: 214; Percent complete: 5.3%; Average loss: 4.2198
Iteration: 215; Percent complete: 5.4%; Average loss: 3.8338
Iteration: 216; Percent complete: 5.4%; Average loss: 4.1694
Iteration: 217; Percent complete: 5.4%; Average loss: 4.2545
Iteration: 218; Percent complete: 5.5%; Average loss: 4.0823
Iteration: 219; Percent complete: 5.5%; Average loss: 3.8987
Iteration: 220; Percent complete: 5.5%; Average loss: 4.3054
Iteration: 221; Percent complete: 5.5%; Average loss: 4.2519
Iteration: 222; Percent complete: 5.5%; Average loss: 4.1531
Iteration: 223; Percent complete: 5.6%; Average loss: 4.2179
Iteration: 224; Percent complete: 5.6%; Average loss: 3.9710
Iteration: 225; Percent complete: 5.6%; Average loss: 4.2665
Iteration: 226; Percent complete: 5.7%; Average loss: 4.0739
Iteration: 227; Percent complete: 5.7%; Average loss: 3.8656
Iteration: 228; Percent complete: 5.7%; Average loss: 4.3188
Iteration: 229; Percent complete: 5.7%; Average loss: 3.9715
Iteration: 230; Percent complete: 5.8%; Average loss: 3.8609
Iteration: 231; Percent complete: 5.8%; Average loss: 4.3733
Iteration: 232; Percent complete: 5.8%; Average loss: 4.0167
Iteration: 233; Percent complete: 5.8%; Average loss: 3.9044
Iteration: 234; Percent complete: 5.9%; Average loss: 3.9252
Iteration: 235; Percent complete: 5.9%; Average loss: 3.9319
Iteration: 236; Percent complete: 5.9%; Average loss: 4.0813
Iteration: 237; Percent complete: 5.9%; Average loss: 3.8553
Iteration: 238; Percent complete: 5.9%; Average loss: 4.2141
Iteration: 239; Percent complete: 6.0%; Average loss: 4.2337
Iteration: 240; Percent complete: 6.0%; Average loss: 4.2982
Iteration: 241; Percent complete: 6.0%; Average loss: 3.7722
Iteration: 242; Percent complete: 6.0%; Average loss: 4.2424
Iteration: 243; Percent complete: 6.1%; Average loss: 3.9604
Iteration: 244; Percent complete: 6.1%; Average loss: 3.8432
Iteration: 245; Percent complete: 6.1%; Average loss: 3.9931
Iteration: 246; Percent complete: 6.2%; Average loss: 4.1019
Iteration: 247; Percent complete: 6.2%; Average loss: 4.0875
Iteration: 248; Percent complete: 6.2%; Average loss: 3.9039
Iteration: 249; Percent complete: 6.2%; Average loss: 4.0813
Iteration: 250; Percent complete: 6.2%; Average loss: 3.9606
Iteration: 251; Percent complete: 6.3%; Average loss: 4.0392
Iteration: 252; Percent complete: 6.3%; Average loss: 4.1931
Iteration: 253; Percent complete: 6.3%; Average loss: 4.0117
Iteration: 254; Percent complete: 6.3%; Average loss: 4.1002
Iteration: 255; Percent complete: 6.4%; Average loss: 3.8141
Iteration: 256; Percent complete: 6.4%; Average loss: 3.9038
Iteration: 257; Percent complete: 6.4%; Average loss: 4.0396
Iteration: 258; Percent complete: 6.5%; Average loss: 3.9385
Iteration: 259; Percent complete: 6.5%; Average loss: 3.8174
Iteration: 260; Percent complete: 6.5%; Average loss: 4.0905
Iteration: 261; Percent complete: 6.5%; Average loss: 3.8126
Iteration: 262; Percent complete: 6.6%; Average loss: 4.0326
Iteration: 263; Percent complete: 6.6%; Average loss: 3.8222
Iteration: 264; Percent complete: 6.6%; Average loss: 3.9978
Iteration: 265; Percent complete: 6.6%; Average loss: 3.8937
Iteration: 266; Percent complete: 6.7%; Average loss: 4.0828
Iteration: 267; Percent complete: 6.7%; Average loss: 4.0608
Iteration: 268; Percent complete: 6.7%; Average loss: 4.0653
Iteration: 269; Percent complete: 6.7%; Average loss: 4.0254
Iteration: 270; Percent complete: 6.8%; Average loss: 3.7896
Iteration: 271; Percent complete: 6.8%; Average loss: 4.0030
Iteration: 272; Percent complete: 6.8%; Average loss: 4.0295
Iteration: 273; Percent complete: 6.8%; Average loss: 4.1786
Iteration: 274; Percent complete: 6.9%; Average loss: 4.2398
Iteration: 275; Percent complete: 6.9%; Average loss: 4.0740
Iteration: 276; Percent complete: 6.9%; Average loss: 4.1029
Iteration: 277; Percent complete: 6.9%; Average loss: 3.9468
Iteration: 278; Percent complete: 7.0%; Average loss: 3.8306
Iteration: 279; Percent complete: 7.0%; Average loss: 3.8863
Iteration: 280; Percent complete: 7.0%; Average loss: 4.0676
Iteration: 281; Percent complete: 7.0%; Average loss: 4.1471
Iteration: 282; Percent complete: 7.0%; Average loss: 3.7536
Iteration: 283; Percent complete: 7.1%; Average loss: 3.7565
Iteration: 284; Percent complete: 7.1%; Average loss: 4.1690
Iteration: 285; Percent complete: 7.1%; Average loss: 3.9237
Iteration: 286; Percent complete: 7.1%; Average loss: 3.8931
Iteration: 287; Percent complete: 7.2%; Average loss: 4.0724
Iteration: 288; Percent complete: 7.2%; Average loss: 3.6871
Iteration: 289; Percent complete: 7.2%; Average loss: 3.7048
Iteration: 290; Percent complete: 7.2%; Average loss: 4.0547
Iteration: 291; Percent complete: 7.3%; Average loss: 4.0709
Iteration: 292; Percent complete: 7.3%; Average loss: 3.8564
Iteration: 293; Percent complete: 7.3%; Average loss: 3.8243
Iteration: 294; Percent complete: 7.3%; Average loss: 3.8689
Iteration: 295; Percent complete: 7.4%; Average loss: 4.1418
Iteration: 296; Percent complete: 7.4%; Average loss: 4.1690
Iteration: 297; Percent complete: 7.4%; Average loss: 4.2614
Iteration: 298; Percent complete: 7.4%; Average loss: 3.9150
Iteration: 299; Percent complete: 7.5%; Average loss: 3.7848
Iteration: 300; Percent complete: 7.5%; Average loss: 3.8905
Iteration: 301; Percent complete: 7.5%; Average loss: 3.7434
Iteration: 302; Percent complete: 7.5%; Average loss: 3.7664
Iteration: 303; Percent complete: 7.6%; Average loss: 3.8246
Iteration: 304; Percent complete: 7.6%; Average loss: 3.9367
Iteration: 305; Percent complete: 7.6%; Average loss: 4.1597
Iteration: 306; Percent complete: 7.6%; Average loss: 4.0126
Iteration: 307; Percent complete: 7.7%; Average loss: 3.9040
Iteration: 308; Percent complete: 7.7%; Average loss: 3.6686
Iteration: 309; Percent complete: 7.7%; Average loss: 3.9476
Iteration: 310; Percent complete: 7.8%; Average loss: 3.5270
Iteration: 311; Percent complete: 7.8%; Average loss: 3.9742
Iteration: 312; Percent complete: 7.8%; Average loss: 3.9565
Iteration: 313; Percent complete: 7.8%; Average loss: 3.7915
Iteration: 314; Percent complete: 7.8%; Average loss: 3.8458
Iteration: 315; Percent complete: 7.9%; Average loss: 3.8274
Iteration: 316; Percent complete: 7.9%; Average loss: 4.0187
Iteration: 317; Percent complete: 7.9%; Average loss: 3.9187
Iteration: 318; Percent complete: 8.0%; Average loss: 3.9373
Iteration: 319; Percent complete: 8.0%; Average loss: 3.7506
Iteration: 320; Percent complete: 8.0%; Average loss: 4.1697
Iteration: 321; Percent complete: 8.0%; Average loss: 3.9130
Iteration: 322; Percent complete: 8.1%; Average loss: 3.7959
Iteration: 323; Percent complete: 8.1%; Average loss: 3.6426
Iteration: 324; Percent complete: 8.1%; Average loss: 3.6684
Iteration: 325; Percent complete: 8.1%; Average loss: 3.7139
Iteration: 326; Percent complete: 8.2%; Average loss: 3.9993
Iteration: 327; Percent complete: 8.2%; Average loss: 3.8237
Iteration: 328; Percent complete: 8.2%; Average loss: 3.9764
Iteration: 329; Percent complete: 8.2%; Average loss: 3.6514
Iteration: 330; Percent complete: 8.2%; Average loss: 4.0228
Iteration: 331; Percent complete: 8.3%; Average loss: 3.8762
Iteration: 332; Percent complete: 8.3%; Average loss: 4.0881
Iteration: 333; Percent complete: 8.3%; Average loss: 3.7423
Iteration: 334; Percent complete: 8.3%; Average loss: 3.7220
Iteration: 335; Percent complete: 8.4%; Average loss: 4.0425
Iteration: 336; Percent complete: 8.4%; Average loss: 3.7790
Iteration: 337; Percent complete: 8.4%; Average loss: 3.7951
Iteration: 338; Percent complete: 8.5%; Average loss: 3.9750
Iteration: 339; Percent complete: 8.5%; Average loss: 4.1157
Iteration: 340; Percent complete: 8.5%; Average loss: 3.9190
Iteration: 341; Percent complete: 8.5%; Average loss: 3.7664
Iteration: 342; Percent complete: 8.6%; Average loss: 3.8566
Iteration: 343; Percent complete: 8.6%; Average loss: 3.8795
Iteration: 344; Percent complete: 8.6%; Average loss: 3.8601
Iteration: 345; Percent complete: 8.6%; Average loss: 4.0740
Iteration: 346; Percent complete: 8.6%; Average loss: 3.8743
Iteration: 347; Percent complete: 8.7%; Average loss: 3.9843
Iteration: 348; Percent complete: 8.7%; Average loss: 3.7432
Iteration: 349; Percent complete: 8.7%; Average loss: 3.9001
Iteration: 350; Percent complete: 8.8%; Average loss: 3.9852
Iteration: 351; Percent complete: 8.8%; Average loss: 3.8096
Iteration: 352; Percent complete: 8.8%; Average loss: 4.0090
Iteration: 353; Percent complete: 8.8%; Average loss: 3.7908
Iteration: 354; Percent complete: 8.8%; Average loss: 4.0879
Iteration: 355; Percent complete: 8.9%; Average loss: 3.9825
Iteration: 356; Percent complete: 8.9%; Average loss: 3.8816
Iteration: 357; Percent complete: 8.9%; Average loss: 3.6711
Iteration: 358; Percent complete: 8.9%; Average loss: 3.9173
Iteration: 359; Percent complete: 9.0%; Average loss: 3.7453
Iteration: 360; Percent complete: 9.0%; Average loss: 3.9662
Iteration: 361; Percent complete: 9.0%; Average loss: 3.6735
Iteration: 362; Percent complete: 9.0%; Average loss: 3.9023
Iteration: 363; Percent complete: 9.1%; Average loss: 3.8795
Iteration: 364; Percent complete: 9.1%; Average loss: 3.7925
Iteration: 365; Percent complete: 9.1%; Average loss: 3.6494
Iteration: 366; Percent complete: 9.2%; Average loss: 3.6937
Iteration: 367; Percent complete: 9.2%; Average loss: 3.6153
Iteration: 368; Percent complete: 9.2%; Average loss: 4.0357
Iteration: 369; Percent complete: 9.2%; Average loss: 3.8784
Iteration: 370; Percent complete: 9.2%; Average loss: 4.0499
Iteration: 371; Percent complete: 9.3%; Average loss: 3.9273
Iteration: 372; Percent complete: 9.3%; Average loss: 3.7436
Iteration: 373; Percent complete: 9.3%; Average loss: 3.9384
Iteration: 374; Percent complete: 9.3%; Average loss: 3.9976
Iteration: 375; Percent complete: 9.4%; Average loss: 3.9210
Iteration: 376; Percent complete: 9.4%; Average loss: 3.6776
Iteration: 377; Percent complete: 9.4%; Average loss: 3.7840
Iteration: 378; Percent complete: 9.4%; Average loss: 4.1935
Iteration: 379; Percent complete: 9.5%; Average loss: 3.8242
Iteration: 380; Percent complete: 9.5%; Average loss: 3.8671
Iteration: 381; Percent complete: 9.5%; Average loss: 3.6727
Iteration: 382; Percent complete: 9.6%; Average loss: 3.8424
Iteration: 383; Percent complete: 9.6%; Average loss: 4.2503
Iteration: 384; Percent complete: 9.6%; Average loss: 4.0939
Iteration: 385; Percent complete: 9.6%; Average loss: 3.6899
Iteration: 386; Percent complete: 9.7%; Average loss: 3.7289
Iteration: 387; Percent complete: 9.7%; Average loss: 3.9006
Iteration: 388; Percent complete: 9.7%; Average loss: 3.8941
Iteration: 389; Percent complete: 9.7%; Average loss: 3.9536
Iteration: 390; Percent complete: 9.8%; Average loss: 3.6524
Iteration: 391; Percent complete: 9.8%; Average loss: 3.6636
Iteration: 392; Percent complete: 9.8%; Average loss: 4.0040
Iteration: 393; Percent complete: 9.8%; Average loss: 4.1662
Iteration: 394; Percent complete: 9.8%; Average loss: 3.6631
Iteration: 395; Percent complete: 9.9%; Average loss: 3.8412
Iteration: 396; Percent complete: 9.9%; Average loss: 3.6785
Iteration: 397; Percent complete: 9.9%; Average loss: 3.7956
Iteration: 398; Percent complete: 10.0%; Average loss: 3.9529
Iteration: 399; Percent complete: 10.0%; Average loss: 3.8220
Iteration: 400; Percent complete: 10.0%; Average loss: 3.7838
Iteration: 401; Percent complete: 10.0%; Average loss: 3.7761
Iteration: 402; Percent complete: 10.1%; Average loss: 3.6371
Iteration: 403; Percent complete: 10.1%; Average loss: 3.5735
Iteration: 404; Percent complete: 10.1%; Average loss: 3.6639
Iteration: 405; Percent complete: 10.1%; Average loss: 3.8436
Iteration: 406; Percent complete: 10.2%; Average loss: 3.9763
Iteration: 407; Percent complete: 10.2%; Average loss: 3.8025
Iteration: 408; Percent complete: 10.2%; Average loss: 3.7171
Iteration: 409; Percent complete: 10.2%; Average loss: 3.6045
Iteration: 410; Percent complete: 10.2%; Average loss: 3.9035
Iteration: 411; Percent complete: 10.3%; Average loss: 3.7347
Iteration: 412; Percent complete: 10.3%; Average loss: 3.7916
Iteration: 413; Percent complete: 10.3%; Average loss: 3.8472
Iteration: 414; Percent complete: 10.3%; Average loss: 4.0208
Iteration: 415; Percent complete: 10.4%; Average loss: 3.7478
Iteration: 416; Percent complete: 10.4%; Average loss: 3.8706
Iteration: 417; Percent complete: 10.4%; Average loss: 3.8347
Iteration: 418; Percent complete: 10.4%; Average loss: 3.9374
Iteration: 419; Percent complete: 10.5%; Average loss: 3.5887
Iteration: 420; Percent complete: 10.5%; Average loss: 3.9890
Iteration: 421; Percent complete: 10.5%; Average loss: 3.8349
Iteration: 422; Percent complete: 10.5%; Average loss: 3.9375
Iteration: 423; Percent complete: 10.6%; Average loss: 3.8246
Iteration: 424; Percent complete: 10.6%; Average loss: 4.0132
Iteration: 425; Percent complete: 10.6%; Average loss: 3.6273
Iteration: 426; Percent complete: 10.7%; Average loss: 3.6084
Iteration: 427; Percent complete: 10.7%; Average loss: 3.6830
Iteration: 428; Percent complete: 10.7%; Average loss: 4.0477
Iteration: 429; Percent complete: 10.7%; Average loss: 3.5428
Iteration: 430; Percent complete: 10.8%; Average loss: 3.8053
Iteration: 431; Percent complete: 10.8%; Average loss: 4.1039
Iteration: 432; Percent complete: 10.8%; Average loss: 3.9015
Iteration: 433; Percent complete: 10.8%; Average loss: 3.7782
Iteration: 434; Percent complete: 10.8%; Average loss: 3.8073
Iteration: 435; Percent complete: 10.9%; Average loss: 3.8359
Iteration: 436; Percent complete: 10.9%; Average loss: 3.7974
Iteration: 437; Percent complete: 10.9%; Average loss: 3.5793
Iteration: 438; Percent complete: 10.9%; Average loss: 3.8747
Iteration: 439; Percent complete: 11.0%; Average loss: 3.7048
Iteration: 440; Percent complete: 11.0%; Average loss: 3.8742
Iteration: 441; Percent complete: 11.0%; Average loss: 3.8835
Iteration: 442; Percent complete: 11.1%; Average loss: 3.7671
Iteration: 443; Percent complete: 11.1%; Average loss: 3.8183
Iteration: 444; Percent complete: 11.1%; Average loss: 3.5914
Iteration: 445; Percent complete: 11.1%; Average loss: 3.6467
Iteration: 446; Percent complete: 11.2%; Average loss: 4.1770
Iteration: 447; Percent complete: 11.2%; Average loss: 3.5894
Iteration: 448; Percent complete: 11.2%; Average loss: 3.9268
Iteration: 449; Percent complete: 11.2%; Average loss: 3.6881
Iteration: 450; Percent complete: 11.2%; Average loss: 3.6542
Iteration: 451; Percent complete: 11.3%; Average loss: 3.6058
Iteration: 452; Percent complete: 11.3%; Average loss: 3.7535
Iteration: 453; Percent complete: 11.3%; Average loss: 4.0341
Iteration: 454; Percent complete: 11.3%; Average loss: 3.5259
Iteration: 455; Percent complete: 11.4%; Average loss: 3.5237
Iteration: 456; Percent complete: 11.4%; Average loss: 3.6494
Iteration: 457; Percent complete: 11.4%; Average loss: 3.8239
Iteration: 458; Percent complete: 11.5%; Average loss: 3.6876
Iteration: 459; Percent complete: 11.5%; Average loss: 3.6188
Iteration: 460; Percent complete: 11.5%; Average loss: 3.6964
Iteration: 461; Percent complete: 11.5%; Average loss: 3.7003
Iteration: 462; Percent complete: 11.6%; Average loss: 3.5728
Iteration: 463; Percent complete: 11.6%; Average loss: 3.7111
Iteration: 464; Percent complete: 11.6%; Average loss: 3.9350
Iteration: 465; Percent complete: 11.6%; Average loss: 3.9611
Iteration: 466; Percent complete: 11.7%; Average loss: 3.9878
Iteration: 467; Percent complete: 11.7%; Average loss: 3.7157
Iteration: 468; Percent complete: 11.7%; Average loss: 3.6982
Iteration: 469; Percent complete: 11.7%; Average loss: 3.9292
Iteration: 470; Percent complete: 11.8%; Average loss: 3.6340
Iteration: 471; Percent complete: 11.8%; Average loss: 3.5767
Iteration: 472; Percent complete: 11.8%; Average loss: 3.7061
Iteration: 473; Percent complete: 11.8%; Average loss: 3.4571
Iteration: 474; Percent complete: 11.8%; Average loss: 4.0826
Iteration: 475; Percent complete: 11.9%; Average loss: 3.9763
Iteration: 476; Percent complete: 11.9%; Average loss: 3.7742
Iteration: 477; Percent complete: 11.9%; Average loss: 3.6747
Iteration: 478; Percent complete: 11.9%; Average loss: 3.7080
Iteration: 479; Percent complete: 12.0%; Average loss: 3.7840
Iteration: 480; Percent complete: 12.0%; Average loss: 4.0200
Iteration: 481; Percent complete: 12.0%; Average loss: 3.9979
Iteration: 482; Percent complete: 12.0%; Average loss: 3.8290
Iteration: 483; Percent complete: 12.1%; Average loss: 3.9083
Iteration: 484; Percent complete: 12.1%; Average loss: 3.7496
Iteration: 485; Percent complete: 12.1%; Average loss: 3.6685
Iteration: 486; Percent complete: 12.2%; Average loss: 3.6077
Iteration: 487; Percent complete: 12.2%; Average loss: 3.6229
Iteration: 488; Percent complete: 12.2%; Average loss: 3.7313
Iteration: 489; Percent complete: 12.2%; Average loss: 3.4573
Iteration: 490; Percent complete: 12.2%; Average loss: 3.9110
Iteration: 491; Percent complete: 12.3%; Average loss: 3.5708
Iteration: 492; Percent complete: 12.3%; Average loss: 3.4428
Iteration: 493; Percent complete: 12.3%; Average loss: 3.6250
Iteration: 494; Percent complete: 12.3%; Average loss: 3.8401
Iteration: 495; Percent complete: 12.4%; Average loss: 3.5267
Iteration: 496; Percent complete: 12.4%; Average loss: 3.4872
Iteration: 497; Percent complete: 12.4%; Average loss: 3.8448
Iteration: 498; Percent complete: 12.4%; Average loss: 3.8414
Iteration: 499; Percent complete: 12.5%; Average loss: 3.6201
Iteration: 500; Percent complete: 12.5%; Average loss: 3.7278
Iteration: 501; Percent complete: 12.5%; Average loss: 3.6465
Iteration: 502; Percent complete: 12.6%; Average loss: 3.7115
Iteration: 503; Percent complete: 12.6%; Average loss: 3.5751
Iteration: 504; Percent complete: 12.6%; Average loss: 3.7741
Iteration: 505; Percent complete: 12.6%; Average loss: 3.8538
Iteration: 506; Percent complete: 12.7%; Average loss: 3.9116
Iteration: 507; Percent complete: 12.7%; Average loss: 3.6859
Iteration: 508; Percent complete: 12.7%; Average loss: 3.8692
Iteration: 509; Percent complete: 12.7%; Average loss: 3.7456
Iteration: 510; Percent complete: 12.8%; Average loss: 3.8843
Iteration: 511; Percent complete: 12.8%; Average loss: 3.7346
Iteration: 512; Percent complete: 12.8%; Average loss: 3.6633
Iteration: 513; Percent complete: 12.8%; Average loss: 3.6086
Iteration: 514; Percent complete: 12.8%; Average loss: 4.0130
Iteration: 515; Percent complete: 12.9%; Average loss: 3.8067
Iteration: 516; Percent complete: 12.9%; Average loss: 3.4940
Iteration: 517; Percent complete: 12.9%; Average loss: 3.7906
Iteration: 518; Percent complete: 13.0%; Average loss: 3.7453
Iteration: 519; Percent complete: 13.0%; Average loss: 3.7317
Iteration: 520; Percent complete: 13.0%; Average loss: 3.4098
Iteration: 521; Percent complete: 13.0%; Average loss: 3.8869
Iteration: 522; Percent complete: 13.1%; Average loss: 3.9227
Iteration: 523; Percent complete: 13.1%; Average loss: 3.9113
Iteration: 524; Percent complete: 13.1%; Average loss: 3.6720
Iteration: 525; Percent complete: 13.1%; Average loss: 3.8332
Iteration: 526; Percent complete: 13.2%; Average loss: 3.9244
Iteration: 527; Percent complete: 13.2%; Average loss: 3.7948
Iteration: 528; Percent complete: 13.2%; Average loss: 3.6549
Iteration: 529; Percent complete: 13.2%; Average loss: 3.7200
Iteration: 530; Percent complete: 13.2%; Average loss: 3.7915
Iteration: 531; Percent complete: 13.3%; Average loss: 3.6985
Iteration: 532; Percent complete: 13.3%; Average loss: 3.6895
Iteration: 533; Percent complete: 13.3%; Average loss: 3.5698
Iteration: 534; Percent complete: 13.4%; Average loss: 3.5777
Iteration: 535; Percent complete: 13.4%; Average loss: 3.5592
Iteration: 536; Percent complete: 13.4%; Average loss: 3.7459
Iteration: 537; Percent complete: 13.4%; Average loss: 3.5808
Iteration: 538; Percent complete: 13.5%; Average loss: 3.6476
Iteration: 539; Percent complete: 13.5%; Average loss: 4.0026
Iteration: 540; Percent complete: 13.5%; Average loss: 3.7889
Iteration: 541; Percent complete: 13.5%; Average loss: 3.9443
Iteration: 542; Percent complete: 13.6%; Average loss: 3.7708
Iteration: 543; Percent complete: 13.6%; Average loss: 3.7578
Iteration: 544; Percent complete: 13.6%; Average loss: 3.8539
Iteration: 545; Percent complete: 13.6%; Average loss: 3.5869
Iteration: 546; Percent complete: 13.7%; Average loss: 3.7823
Iteration: 547; Percent complete: 13.7%; Average loss: 4.0667
Iteration: 548; Percent complete: 13.7%; Average loss: 3.5022
Iteration: 549; Percent complete: 13.7%; Average loss: 3.8831
Iteration: 550; Percent complete: 13.8%; Average loss: 3.6687
Iteration: 551; Percent complete: 13.8%; Average loss: 3.7143
Iteration: 552; Percent complete: 13.8%; Average loss: 4.0907
Iteration: 553; Percent complete: 13.8%; Average loss: 3.6730
Iteration: 554; Percent complete: 13.9%; Average loss: 3.8940
Iteration: 555; Percent complete: 13.9%; Average loss: 3.7960
Iteration: 556; Percent complete: 13.9%; Average loss: 3.6539
Iteration: 557; Percent complete: 13.9%; Average loss: 3.4448
Iteration: 558; Percent complete: 14.0%; Average loss: 3.8843
Iteration: 559; Percent complete: 14.0%; Average loss: 3.5870
Iteration: 560; Percent complete: 14.0%; Average loss: 3.8990
Iteration: 561; Percent complete: 14.0%; Average loss: 3.7573
Iteration: 562; Percent complete: 14.1%; Average loss: 3.6309
Iteration: 563; Percent complete: 14.1%; Average loss: 3.4695
Iteration: 564; Percent complete: 14.1%; Average loss: 3.6592
Iteration: 565; Percent complete: 14.1%; Average loss: 3.5284
Iteration: 566; Percent complete: 14.1%; Average loss: 3.6090
Iteration: 567; Percent complete: 14.2%; Average loss: 3.6157
Iteration: 568; Percent complete: 14.2%; Average loss: 3.6231
Iteration: 569; Percent complete: 14.2%; Average loss: 3.5885
Iteration: 570; Percent complete: 14.2%; Average loss: 3.7504
Iteration: 571; Percent complete: 14.3%; Average loss: 3.5600
Iteration: 572; Percent complete: 14.3%; Average loss: 3.5519
Iteration: 573; Percent complete: 14.3%; Average loss: 3.6517
Iteration: 574; Percent complete: 14.3%; Average loss: 3.6215
Iteration: 575; Percent complete: 14.4%; Average loss: 3.6275
Iteration: 576; Percent complete: 14.4%; Average loss: 3.4602
Iteration: 577; Percent complete: 14.4%; Average loss: 3.5683
Iteration: 578; Percent complete: 14.4%; Average loss: 3.7060
Iteration: 579; Percent complete: 14.5%; Average loss: 3.5885
Iteration: 580; Percent complete: 14.5%; Average loss: 3.8502
Iteration: 581; Percent complete: 14.5%; Average loss: 3.4425
Iteration: 582; Percent complete: 14.5%; Average loss: 3.8642
Iteration: 583; Percent complete: 14.6%; Average loss: 3.7559
Iteration: 584; Percent complete: 14.6%; Average loss: 3.4155
Iteration: 585; Percent complete: 14.6%; Average loss: 3.6386
Iteration: 586; Percent complete: 14.6%; Average loss: 3.7383
Iteration: 587; Percent complete: 14.7%; Average loss: 3.6886
Iteration: 588; Percent complete: 14.7%; Average loss: 3.9131
Iteration: 589; Percent complete: 14.7%; Average loss: 3.6342
Iteration: 590; Percent complete: 14.8%; Average loss: 3.8054
Iteration: 591; Percent complete: 14.8%; Average loss: 3.8479
Iteration: 592; Percent complete: 14.8%; Average loss: 3.5324
Iteration: 593; Percent complete: 14.8%; Average loss: 3.8679
Iteration: 594; Percent complete: 14.8%; Average loss: 3.5635
Iteration: 595; Percent complete: 14.9%; Average loss: 3.6341
Iteration: 596; Percent complete: 14.9%; Average loss: 3.5701
Iteration: 597; Percent complete: 14.9%; Average loss: 3.5757
Iteration: 598; Percent complete: 14.9%; Average loss: 3.8664
Iteration: 599; Percent complete: 15.0%; Average loss: 3.8431
Iteration: 600; Percent complete: 15.0%; Average loss: 3.6757
Iteration: 601; Percent complete: 15.0%; Average loss: 3.5568
Iteration: 602; Percent complete: 15.0%; Average loss: 3.5255
Iteration: 603; Percent complete: 15.1%; Average loss: 3.6832
Iteration: 604; Percent complete: 15.1%; Average loss: 3.5041
Iteration: 605; Percent complete: 15.1%; Average loss: 3.4693
Iteration: 606; Percent complete: 15.2%; Average loss: 3.7391
Iteration: 607; Percent complete: 15.2%; Average loss: 3.5533
Iteration: 608; Percent complete: 15.2%; Average loss: 3.6217
Iteration: 609; Percent complete: 15.2%; Average loss: 3.6252
Iteration: 610; Percent complete: 15.2%; Average loss: 3.6748
Iteration: 611; Percent complete: 15.3%; Average loss: 3.4140
Iteration: 612; Percent complete: 15.3%; Average loss: 4.0225
Iteration: 613; Percent complete: 15.3%; Average loss: 3.7063
Iteration: 614; Percent complete: 15.3%; Average loss: 3.4902
Iteration: 615; Percent complete: 15.4%; Average loss: 3.7020
Iteration: 616; Percent complete: 15.4%; Average loss: 3.6972
Iteration: 617; Percent complete: 15.4%; Average loss: 3.4077
Iteration: 618; Percent complete: 15.4%; Average loss: 3.5445
Iteration: 619; Percent complete: 15.5%; Average loss: 3.5710
Iteration: 620; Percent complete: 15.5%; Average loss: 3.6647
Iteration: 621; Percent complete: 15.5%; Average loss: 3.8920
Iteration: 622; Percent complete: 15.6%; Average loss: 3.8137
Iteration: 623; Percent complete: 15.6%; Average loss: 3.5768
Iteration: 624; Percent complete: 15.6%; Average loss: 3.8745
Iteration: 625; Percent complete: 15.6%; Average loss: 3.4053
Iteration: 626; Percent complete: 15.7%; Average loss: 3.5182
Iteration: 627; Percent complete: 15.7%; Average loss: 4.0964
Iteration: 628; Percent complete: 15.7%; Average loss: 3.5067
Iteration: 629; Percent complete: 15.7%; Average loss: 3.7664
Iteration: 630; Percent complete: 15.8%; Average loss: 3.5864
Iteration: 631; Percent complete: 15.8%; Average loss: 3.7993
Iteration: 632; Percent complete: 15.8%; Average loss: 3.5678
Iteration: 633; Percent complete: 15.8%; Average loss: 3.7831
Iteration: 634; Percent complete: 15.8%; Average loss: 3.5835
Iteration: 635; Percent complete: 15.9%; Average loss: 3.5797
Iteration: 636; Percent complete: 15.9%; Average loss: 3.5247
Iteration: 637; Percent complete: 15.9%; Average loss: 3.8920
Iteration: 638; Percent complete: 16.0%; Average loss: 3.5004
Iteration: 639; Percent complete: 16.0%; Average loss: 3.6167
Iteration: 640; Percent complete: 16.0%; Average loss: 3.6565
Iteration: 641; Percent complete: 16.0%; Average loss: 3.7118
Iteration: 642; Percent complete: 16.1%; Average loss: 3.3290
Iteration: 643; Percent complete: 16.1%; Average loss: 3.8205
Iteration: 644; Percent complete: 16.1%; Average loss: 3.8775
Iteration: 645; Percent complete: 16.1%; Average loss: 3.6254
Iteration: 646; Percent complete: 16.2%; Average loss: 3.6253
Iteration: 647; Percent complete: 16.2%; Average loss: 3.6529
Iteration: 648; Percent complete: 16.2%; Average loss: 3.4786
Iteration: 649; Percent complete: 16.2%; Average loss: 3.7553
Iteration: 650; Percent complete: 16.2%; Average loss: 3.7958
Iteration: 651; Percent complete: 16.3%; Average loss: 3.7888
Iteration: 652; Percent complete: 16.3%; Average loss: 3.4761
Iteration: 653; Percent complete: 16.3%; Average loss: 3.4374
Iteration: 654; Percent complete: 16.4%; Average loss: 3.5033
Iteration: 655; Percent complete: 16.4%; Average loss: 3.9741
Iteration: 656; Percent complete: 16.4%; Average loss: 3.8172
Iteration: 657; Percent complete: 16.4%; Average loss: 3.6626
Iteration: 658; Percent complete: 16.4%; Average loss: 3.4722
Iteration: 659; Percent complete: 16.5%; Average loss: 3.8056
Iteration: 660; Percent complete: 16.5%; Average loss: 3.7402
Iteration: 661; Percent complete: 16.5%; Average loss: 3.6150
Iteration: 662; Percent complete: 16.6%; Average loss: 3.6006
Iteration: 663; Percent complete: 16.6%; Average loss: 3.4548
Iteration: 664; Percent complete: 16.6%; Average loss: 3.6442
Iteration: 665; Percent complete: 16.6%; Average loss: 3.6635
Iteration: 666; Percent complete: 16.7%; Average loss: 3.4318
Iteration: 667; Percent complete: 16.7%; Average loss: 3.4845
Iteration: 668; Percent complete: 16.7%; Average loss: 3.4367
Iteration: 669; Percent complete: 16.7%; Average loss: 3.8478
Iteration: 670; Percent complete: 16.8%; Average loss: 3.7855
Iteration: 671; Percent complete: 16.8%; Average loss: 3.4413
Iteration: 672; Percent complete: 16.8%; Average loss: 3.6745
Iteration: 673; Percent complete: 16.8%; Average loss: 3.4916
Iteration: 674; Percent complete: 16.9%; Average loss: 3.5772
Iteration: 675; Percent complete: 16.9%; Average loss: 3.5467
Iteration: 676; Percent complete: 16.9%; Average loss: 3.5360
Iteration: 677; Percent complete: 16.9%; Average loss: 3.7590
Iteration: 678; Percent complete: 17.0%; Average loss: 3.6524
Iteration: 679; Percent complete: 17.0%; Average loss: 3.3031
Iteration: 680; Percent complete: 17.0%; Average loss: 3.7814
Iteration: 681; Percent complete: 17.0%; Average loss: 3.8696
Iteration: 682; Percent complete: 17.1%; Average loss: 3.7246
Iteration: 683; Percent complete: 17.1%; Average loss: 3.4913
Iteration: 684; Percent complete: 17.1%; Average loss: 3.8657
Iteration: 685; Percent complete: 17.1%; Average loss: 3.6651
Iteration: 686; Percent complete: 17.2%; Average loss: 3.4994
Iteration: 687; Percent complete: 17.2%; Average loss: 3.7060
Iteration: 688; Percent complete: 17.2%; Average loss: 3.5047
Iteration: 689; Percent complete: 17.2%; Average loss: 3.7251
Iteration: 690; Percent complete: 17.2%; Average loss: 3.7566
Iteration: 691; Percent complete: 17.3%; Average loss: 3.7596
Iteration: 692; Percent complete: 17.3%; Average loss: 3.4761
Iteration: 693; Percent complete: 17.3%; Average loss: 3.8354
Iteration: 694; Percent complete: 17.3%; Average loss: 3.7101
Iteration: 695; Percent complete: 17.4%; Average loss: 3.5857
Iteration: 696; Percent complete: 17.4%; Average loss: 3.6699
Iteration: 697; Percent complete: 17.4%; Average loss: 3.4157
Iteration: 698; Percent complete: 17.4%; Average loss: 3.7081
Iteration: 699; Percent complete: 17.5%; Average loss: 3.7905
Iteration: 700; Percent complete: 17.5%; Average loss: 3.6708
Iteration: 701; Percent complete: 17.5%; Average loss: 3.5294
Iteration: 702; Percent complete: 17.5%; Average loss: 3.5501
Iteration: 703; Percent complete: 17.6%; Average loss: 3.3321
Iteration: 704; Percent complete: 17.6%; Average loss: 3.6214
Iteration: 705; Percent complete: 17.6%; Average loss: 3.8308
Iteration: 706; Percent complete: 17.6%; Average loss: 3.5605
Iteration: 707; Percent complete: 17.7%; Average loss: 3.7015
Iteration: 708; Percent complete: 17.7%; Average loss: 3.7471
Iteration: 709; Percent complete: 17.7%; Average loss: 3.4249
Iteration: 710; Percent complete: 17.8%; Average loss: 3.7454
Iteration: 711; Percent complete: 17.8%; Average loss: 3.7104
Iteration: 712; Percent complete: 17.8%; Average loss: 3.4503
Iteration: 713; Percent complete: 17.8%; Average loss: 3.4711
Iteration: 714; Percent complete: 17.8%; Average loss: 3.9769
Iteration: 715; Percent complete: 17.9%; Average loss: 3.6794
Iteration: 716; Percent complete: 17.9%; Average loss: 3.4821
Iteration: 717; Percent complete: 17.9%; Average loss: 3.5484
Iteration: 718; Percent complete: 17.9%; Average loss: 3.4589
Iteration: 719; Percent complete: 18.0%; Average loss: 3.4465
Iteration: 720; Percent complete: 18.0%; Average loss: 3.5725
Iteration: 721; Percent complete: 18.0%; Average loss: 3.7296
Iteration: 722; Percent complete: 18.1%; Average loss: 3.8700
Iteration: 723; Percent complete: 18.1%; Average loss: 3.3565
Iteration: 724; Percent complete: 18.1%; Average loss: 3.6770
Iteration: 725; Percent complete: 18.1%; Average loss: 3.4511
Iteration: 726; Percent complete: 18.1%; Average loss: 3.5928
Iteration: 727; Percent complete: 18.2%; Average loss: 3.3567
Iteration: 728; Percent complete: 18.2%; Average loss: 3.5285
Iteration: 729; Percent complete: 18.2%; Average loss: 3.7483
Iteration: 730; Percent complete: 18.2%; Average loss: 3.6571
Iteration: 731; Percent complete: 18.3%; Average loss: 3.5836
Iteration: 732; Percent complete: 18.3%; Average loss: 3.5767
Iteration: 733; Percent complete: 18.3%; Average loss: 3.8608
Iteration: 734; Percent complete: 18.4%; Average loss: 3.7554
Iteration: 735; Percent complete: 18.4%; Average loss: 3.4166
Iteration: 736; Percent complete: 18.4%; Average loss: 3.6720
Iteration: 737; Percent complete: 18.4%; Average loss: 3.5104
Iteration: 738; Percent complete: 18.4%; Average loss: 3.7443
Iteration: 739; Percent complete: 18.5%; Average loss: 3.5940
Iteration: 740; Percent complete: 18.5%; Average loss: 3.6642
Iteration: 741; Percent complete: 18.5%; Average loss: 3.6126
Iteration: 742; Percent complete: 18.6%; Average loss: 3.8553
Iteration: 743; Percent complete: 18.6%; Average loss: 3.4120
Iteration: 744; Percent complete: 18.6%; Average loss: 3.7858
Iteration: 745; Percent complete: 18.6%; Average loss: 3.5265
Iteration: 746; Percent complete: 18.6%; Average loss: 3.5867
Iteration: 747; Percent complete: 18.7%; Average loss: 3.6414
Iteration: 748; Percent complete: 18.7%; Average loss: 3.7228
Iteration: 749; Percent complete: 18.7%; Average loss: 3.6811
Iteration: 750; Percent complete: 18.8%; Average loss: 3.6291
Iteration: 751; Percent complete: 18.8%; Average loss: 3.9091
Iteration: 752; Percent complete: 18.8%; Average loss: 3.7639
Iteration: 753; Percent complete: 18.8%; Average loss: 3.6157
Iteration: 754; Percent complete: 18.9%; Average loss: 3.4204
Iteration: 755; Percent complete: 18.9%; Average loss: 3.5056
Iteration: 756; Percent complete: 18.9%; Average loss: 3.4939
Iteration: 757; Percent complete: 18.9%; Average loss: 3.5918
Iteration: 758; Percent complete: 18.9%; Average loss: 3.5301
Iteration: 759; Percent complete: 19.0%; Average loss: 3.6288
Iteration: 760; Percent complete: 19.0%; Average loss: 3.6519
Iteration: 761; Percent complete: 19.0%; Average loss: 3.6578
Iteration: 762; Percent complete: 19.1%; Average loss: 3.5480
Iteration: 763; Percent complete: 19.1%; Average loss: 4.1786
Iteration: 764; Percent complete: 19.1%; Average loss: 3.4167
Iteration: 765; Percent complete: 19.1%; Average loss: 3.6874
Iteration: 766; Percent complete: 19.1%; Average loss: 3.6532
Iteration: 767; Percent complete: 19.2%; Average loss: 3.5401
Iteration: 768; Percent complete: 19.2%; Average loss: 3.3460
Iteration: 769; Percent complete: 19.2%; Average loss: 3.6328
Iteration: 770; Percent complete: 19.2%; Average loss: 3.7002
Iteration: 771; Percent complete: 19.3%; Average loss: 3.5127
Iteration: 772; Percent complete: 19.3%; Average loss: 3.6769
Iteration: 773; Percent complete: 19.3%; Average loss: 3.5349
Iteration: 774; Percent complete: 19.4%; Average loss: 3.4672
Iteration: 775; Percent complete: 19.4%; Average loss: 3.5078
Iteration: 776; Percent complete: 19.4%; Average loss: 3.1873
Iteration: 777; Percent complete: 19.4%; Average loss: 3.6004
Iteration: 778; Percent complete: 19.4%; Average loss: 3.5847
Iteration: 779; Percent complete: 19.5%; Average loss: 3.6367
Iteration: 780; Percent complete: 19.5%; Average loss: 3.3701
Iteration: 781; Percent complete: 19.5%; Average loss: 3.3984
Iteration: 782; Percent complete: 19.6%; Average loss: 3.4242
Iteration: 783; Percent complete: 19.6%; Average loss: 3.5546
Iteration: 784; Percent complete: 19.6%; Average loss: 3.4181
Iteration: 785; Percent complete: 19.6%; Average loss: 3.6756
Iteration: 786; Percent complete: 19.7%; Average loss: 3.5761
Iteration: 787; Percent complete: 19.7%; Average loss: 3.5875
Iteration: 788; Percent complete: 19.7%; Average loss: 3.5991
Iteration: 789; Percent complete: 19.7%; Average loss: 3.4455
Iteration: 790; Percent complete: 19.8%; Average loss: 3.8295
Iteration: 791; Percent complete: 19.8%; Average loss: 3.6048
Iteration: 792; Percent complete: 19.8%; Average loss: 3.8313
Iteration: 793; Percent complete: 19.8%; Average loss: 3.5996
Iteration: 794; Percent complete: 19.9%; Average loss: 3.7168
Iteration: 795; Percent complete: 19.9%; Average loss: 3.4881
Iteration: 796; Percent complete: 19.9%; Average loss: 3.8928
Iteration: 797; Percent complete: 19.9%; Average loss: 3.8045
Iteration: 798; Percent complete: 20.0%; Average loss: 3.6000
Iteration: 799; Percent complete: 20.0%; Average loss: 3.6082
Iteration: 800; Percent complete: 20.0%; Average loss: 3.4202
Iteration: 801; Percent complete: 20.0%; Average loss: 3.3625
Iteration: 802; Percent complete: 20.1%; Average loss: 3.6204
Iteration: 803; Percent complete: 20.1%; Average loss: 3.7309
Iteration: 804; Percent complete: 20.1%; Average loss: 3.4829
Iteration: 805; Percent complete: 20.1%; Average loss: 3.7324
Iteration: 806; Percent complete: 20.2%; Average loss: 3.3346
Iteration: 807; Percent complete: 20.2%; Average loss: 3.4380
Iteration: 808; Percent complete: 20.2%; Average loss: 3.5204
Iteration: 809; Percent complete: 20.2%; Average loss: 3.5121
Iteration: 810; Percent complete: 20.2%; Average loss: 3.9458
Iteration: 811; Percent complete: 20.3%; Average loss: 3.6748
Iteration: 812; Percent complete: 20.3%; Average loss: 3.6910
Iteration: 813; Percent complete: 20.3%; Average loss: 3.5063
Iteration: 814; Percent complete: 20.3%; Average loss: 3.7836
Iteration: 815; Percent complete: 20.4%; Average loss: 3.6605
Iteration: 816; Percent complete: 20.4%; Average loss: 3.7650
Iteration: 817; Percent complete: 20.4%; Average loss: 3.5724
Iteration: 818; Percent complete: 20.4%; Average loss: 3.7947
Iteration: 819; Percent complete: 20.5%; Average loss: 3.8254
Iteration: 820; Percent complete: 20.5%; Average loss: 3.6078
Iteration: 821; Percent complete: 20.5%; Average loss: 3.4210
Iteration: 822; Percent complete: 20.5%; Average loss: 3.5031
Iteration: 823; Percent complete: 20.6%; Average loss: 3.5266
Iteration: 824; Percent complete: 20.6%; Average loss: 3.5579
Iteration: 825; Percent complete: 20.6%; Average loss: 3.7550
Iteration: 826; Percent complete: 20.6%; Average loss: 3.5722
Iteration: 827; Percent complete: 20.7%; Average loss: 3.4871
Iteration: 828; Percent complete: 20.7%; Average loss: 3.4543
Iteration: 829; Percent complete: 20.7%; Average loss: 3.4841
Iteration: 830; Percent complete: 20.8%; Average loss: 3.6156
Iteration: 831; Percent complete: 20.8%; Average loss: 3.4467
Iteration: 832; Percent complete: 20.8%; Average loss: 3.8080
Iteration: 833; Percent complete: 20.8%; Average loss: 3.5139
Iteration: 834; Percent complete: 20.8%; Average loss: 3.4338
Iteration: 835; Percent complete: 20.9%; Average loss: 3.4961
Iteration: 836; Percent complete: 20.9%; Average loss: 3.6224
Iteration: 837; Percent complete: 20.9%; Average loss: 3.6306
Iteration: 838; Percent complete: 20.9%; Average loss: 3.6733
Iteration: 839; Percent complete: 21.0%; Average loss: 3.5079
Iteration: 840; Percent complete: 21.0%; Average loss: 3.7091
Iteration: 841; Percent complete: 21.0%; Average loss: 3.5388
Iteration: 842; Percent complete: 21.1%; Average loss: 3.7554
Iteration: 843; Percent complete: 21.1%; Average loss: 3.5814
Iteration: 844; Percent complete: 21.1%; Average loss: 3.4944
Iteration: 845; Percent complete: 21.1%; Average loss: 3.6068
Iteration: 846; Percent complete: 21.1%; Average loss: 3.4408
Iteration: 847; Percent complete: 21.2%; Average loss: 3.4530
Iteration: 848; Percent complete: 21.2%; Average loss: 3.7785
Iteration: 849; Percent complete: 21.2%; Average loss: 3.5396
Iteration: 850; Percent complete: 21.2%; Average loss: 3.4701
Iteration: 851; Percent complete: 21.3%; Average loss: 3.5384
Iteration: 852; Percent complete: 21.3%; Average loss: 3.3757
Iteration: 853; Percent complete: 21.3%; Average loss: 3.6732
Iteration: 854; Percent complete: 21.3%; Average loss: 3.4488
Iteration: 855; Percent complete: 21.4%; Average loss: 3.3713
Iteration: 856; Percent complete: 21.4%; Average loss: 3.6021
Iteration: 857; Percent complete: 21.4%; Average loss: 3.8309
Iteration: 858; Percent complete: 21.4%; Average loss: 3.8395
Iteration: 859; Percent complete: 21.5%; Average loss: 3.7310
Iteration: 860; Percent complete: 21.5%; Average loss: 3.4144
Iteration: 861; Percent complete: 21.5%; Average loss: 3.3466
Iteration: 862; Percent complete: 21.6%; Average loss: 3.6603
Iteration: 863; Percent complete: 21.6%; Average loss: 3.5511
Iteration: 864; Percent complete: 21.6%; Average loss: 3.5495
Iteration: 865; Percent complete: 21.6%; Average loss: 3.6713
Iteration: 866; Percent complete: 21.6%; Average loss: 3.5399
Iteration: 867; Percent complete: 21.7%; Average loss: 3.7692
Iteration: 868; Percent complete: 21.7%; Average loss: 3.6146
Iteration: 869; Percent complete: 21.7%; Average loss: 3.6035
Iteration: 870; Percent complete: 21.8%; Average loss: 3.4795
Iteration: 871; Percent complete: 21.8%; Average loss: 3.6295
Iteration: 872; Percent complete: 21.8%; Average loss: 3.4730
Iteration: 873; Percent complete: 21.8%; Average loss: 3.3979
Iteration: 874; Percent complete: 21.9%; Average loss: 3.6712
Iteration: 875; Percent complete: 21.9%; Average loss: 3.6100
Iteration: 876; Percent complete: 21.9%; Average loss: 3.4999
Iteration: 877; Percent complete: 21.9%; Average loss: 3.7838
Iteration: 878; Percent complete: 21.9%; Average loss: 3.4534
Iteration: 879; Percent complete: 22.0%; Average loss: 3.6838
Iteration: 880; Percent complete: 22.0%; Average loss: 3.7096
Iteration: 881; Percent complete: 22.0%; Average loss: 3.7338
Iteration: 882; Percent complete: 22.1%; Average loss: 3.4327
Iteration: 883; Percent complete: 22.1%; Average loss: 3.4220
Iteration: 884; Percent complete: 22.1%; Average loss: 3.5996
Iteration: 885; Percent complete: 22.1%; Average loss: 3.6561
Iteration: 886; Percent complete: 22.1%; Average loss: 3.4134
Iteration: 887; Percent complete: 22.2%; Average loss: 3.3858
Iteration: 888; Percent complete: 22.2%; Average loss: 3.5435
Iteration: 889; Percent complete: 22.2%; Average loss: 3.6374
Iteration: 890; Percent complete: 22.2%; Average loss: 3.6111
Iteration: 891; Percent complete: 22.3%; Average loss: 3.6029
Iteration: 892; Percent complete: 22.3%; Average loss: 3.5496
Iteration: 893; Percent complete: 22.3%; Average loss: 3.6360
Iteration: 894; Percent complete: 22.4%; Average loss: 3.6071
Iteration: 895; Percent complete: 22.4%; Average loss: 3.6390
Iteration: 896; Percent complete: 22.4%; Average loss: 3.3238
Iteration: 897; Percent complete: 22.4%; Average loss: 3.7024
Iteration: 898; Percent complete: 22.4%; Average loss: 3.7487
Iteration: 899; Percent complete: 22.5%; Average loss: 3.4458
Iteration: 900; Percent complete: 22.5%; Average loss: 3.5874
Iteration: 901; Percent complete: 22.5%; Average loss: 3.6236
Iteration: 902; Percent complete: 22.6%; Average loss: 3.5138
Iteration: 903; Percent complete: 22.6%; Average loss: 3.3747
Iteration: 904; Percent complete: 22.6%; Average loss: 3.6796
Iteration: 905; Percent complete: 22.6%; Average loss: 3.5164
Iteration: 906; Percent complete: 22.7%; Average loss: 3.5000
Iteration: 907; Percent complete: 22.7%; Average loss: 3.5040
Iteration: 908; Percent complete: 22.7%; Average loss: 3.6096
Iteration: 909; Percent complete: 22.7%; Average loss: 3.3654
Iteration: 910; Percent complete: 22.8%; Average loss: 3.5613
Iteration: 911; Percent complete: 22.8%; Average loss: 3.3179
Iteration: 912; Percent complete: 22.8%; Average loss: 3.3383
Iteration: 913; Percent complete: 22.8%; Average loss: 3.4643
Iteration: 914; Percent complete: 22.9%; Average loss: 3.6939
Iteration: 915; Percent complete: 22.9%; Average loss: 3.5031
Iteration: 916; Percent complete: 22.9%; Average loss: 3.5192
Iteration: 917; Percent complete: 22.9%; Average loss: 3.6442
Iteration: 918; Percent complete: 22.9%; Average loss: 3.6219
Iteration: 919; Percent complete: 23.0%; Average loss: 3.2370
Iteration: 920; Percent complete: 23.0%; Average loss: 3.6036
Iteration: 921; Percent complete: 23.0%; Average loss: 3.5306
Iteration: 922; Percent complete: 23.1%; Average loss: 3.9722
Iteration: 923; Percent complete: 23.1%; Average loss: 3.6352
Iteration: 924; Percent complete: 23.1%; Average loss: 3.5550
Iteration: 925; Percent complete: 23.1%; Average loss: 3.3815
Iteration: 926; Percent complete: 23.2%; Average loss: 3.4231
Iteration: 927; Percent complete: 23.2%; Average loss: 3.5543
Iteration: 928; Percent complete: 23.2%; Average loss: 3.5012
Iteration: 929; Percent complete: 23.2%; Average loss: 3.3187
Iteration: 930; Percent complete: 23.2%; Average loss: 3.6258
Iteration: 931; Percent complete: 23.3%; Average loss: 3.5970
Iteration: 932; Percent complete: 23.3%; Average loss: 3.5319
Iteration: 933; Percent complete: 23.3%; Average loss: 3.5063
Iteration: 934; Percent complete: 23.4%; Average loss: 3.2207
Iteration: 935; Percent complete: 23.4%; Average loss: 3.6131
Iteration: 936; Percent complete: 23.4%; Average loss: 3.7260
Iteration: 937; Percent complete: 23.4%; Average loss: 3.4936
Iteration: 938; Percent complete: 23.4%; Average loss: 3.7430
Iteration: 939; Percent complete: 23.5%; Average loss: 3.3320
Iteration: 940; Percent complete: 23.5%; Average loss: 3.4399
Iteration: 941; Percent complete: 23.5%; Average loss: 3.2794
Iteration: 942; Percent complete: 23.5%; Average loss: 3.5269
Iteration: 943; Percent complete: 23.6%; Average loss: 3.5199
Iteration: 944; Percent complete: 23.6%; Average loss: 3.3009
Iteration: 945; Percent complete: 23.6%; Average loss: 3.2852
Iteration: 946; Percent complete: 23.6%; Average loss: 3.3087
Iteration: 947; Percent complete: 23.7%; Average loss: 3.3618
Iteration: 948; Percent complete: 23.7%; Average loss: 3.4958
Iteration: 949; Percent complete: 23.7%; Average loss: 3.5799
Iteration: 950; Percent complete: 23.8%; Average loss: 3.1719
Iteration: 951; Percent complete: 23.8%; Average loss: 3.6137
Iteration: 952; Percent complete: 23.8%; Average loss: 3.6082
Iteration: 953; Percent complete: 23.8%; Average loss: 3.5706
Iteration: 954; Percent complete: 23.8%; Average loss: 3.4365
Iteration: 955; Percent complete: 23.9%; Average loss: 3.5402
Iteration: 956; Percent complete: 23.9%; Average loss: 3.5633
Iteration: 957; Percent complete: 23.9%; Average loss: 3.5311
Iteration: 958; Percent complete: 23.9%; Average loss: 3.2248
Iteration: 959; Percent complete: 24.0%; Average loss: 3.5902
Iteration: 960; Percent complete: 24.0%; Average loss: 3.4654
Iteration: 961; Percent complete: 24.0%; Average loss: 3.3947
Iteration: 962; Percent complete: 24.1%; Average loss: 3.5248
Iteration: 963; Percent complete: 24.1%; Average loss: 3.3059
Iteration: 964; Percent complete: 24.1%; Average loss: 3.4158
Iteration: 965; Percent complete: 24.1%; Average loss: 3.3753
Iteration: 966; Percent complete: 24.1%; Average loss: 3.6563
Iteration: 967; Percent complete: 24.2%; Average loss: 3.5264
Iteration: 968; Percent complete: 24.2%; Average loss: 3.6272
Iteration: 969; Percent complete: 24.2%; Average loss: 3.5403
Iteration: 970; Percent complete: 24.2%; Average loss: 3.5453
Iteration: 971; Percent complete: 24.3%; Average loss: 3.5868
Iteration: 972; Percent complete: 24.3%; Average loss: 3.4309
Iteration: 973; Percent complete: 24.3%; Average loss: 3.4027
Iteration: 974; Percent complete: 24.3%; Average loss: 3.3117
Iteration: 975; Percent complete: 24.4%; Average loss: 3.3891
Iteration: 976; Percent complete: 24.4%; Average loss: 3.4421
Iteration: 977; Percent complete: 24.4%; Average loss: 3.6162
Iteration: 978; Percent complete: 24.4%; Average loss: 3.3439
Iteration: 979; Percent complete: 24.5%; Average loss: 3.4579
Iteration: 980; Percent complete: 24.5%; Average loss: 3.5925
Iteration: 981; Percent complete: 24.5%; Average loss: 3.6405
Iteration: 982; Percent complete: 24.6%; Average loss: 3.4974
Iteration: 983; Percent complete: 24.6%; Average loss: 3.5548
Iteration: 984; Percent complete: 24.6%; Average loss: 3.6539
Iteration: 985; Percent complete: 24.6%; Average loss: 3.5643
Iteration: 986; Percent complete: 24.6%; Average loss: 3.5989
Iteration: 987; Percent complete: 24.7%; Average loss: 3.5971
Iteration: 988; Percent complete: 24.7%; Average loss: 3.5695
Iteration: 989; Percent complete: 24.7%; Average loss: 3.7366
Iteration: 990; Percent complete: 24.8%; Average loss: 3.4818
Iteration: 991; Percent complete: 24.8%; Average loss: 3.6065
Iteration: 992; Percent complete: 24.8%; Average loss: 3.4019
Iteration: 993; Percent complete: 24.8%; Average loss: 3.6955
Iteration: 994; Percent complete: 24.9%; Average loss: 3.4423
Iteration: 995; Percent complete: 24.9%; Average loss: 3.6290
Iteration: 996; Percent complete: 24.9%; Average loss: 3.3760
Iteration: 997; Percent complete: 24.9%; Average loss: 3.2708
Iteration: 998; Percent complete: 24.9%; Average loss: 3.5385
Iteration: 999; Percent complete: 25.0%; Average loss: 3.5764
Iteration: 1000; Percent complete: 25.0%; Average loss: 3.5835
Iteration: 1001; Percent complete: 25.0%; Average loss: 3.6666
Iteration: 1002; Percent complete: 25.1%; Average loss: 3.5882
Iteration: 1003; Percent complete: 25.1%; Average loss: 3.4410
Iteration: 1004; Percent complete: 25.1%; Average loss: 3.2638
Iteration: 1005; Percent complete: 25.1%; Average loss: 3.3953
Iteration: 1006; Percent complete: 25.1%; Average loss: 3.7288
Iteration: 1007; Percent complete: 25.2%; Average loss: 3.3369
Iteration: 1008; Percent complete: 25.2%; Average loss: 3.4223
Iteration: 1009; Percent complete: 25.2%; Average loss: 3.6371
Iteration: 1010; Percent complete: 25.2%; Average loss: 3.5037
Iteration: 1011; Percent complete: 25.3%; Average loss: 3.5227
Iteration: 1012; Percent complete: 25.3%; Average loss: 3.5478
Iteration: 1013; Percent complete: 25.3%; Average loss: 3.6536
Iteration: 1014; Percent complete: 25.4%; Average loss: 3.4728
Iteration: 1015; Percent complete: 25.4%; Average loss: 3.3962
Iteration: 1016; Percent complete: 25.4%; Average loss: 3.3546
Iteration: 1017; Percent complete: 25.4%; Average loss: 3.4085
Iteration: 1018; Percent complete: 25.4%; Average loss: 3.6940
Iteration: 1019; Percent complete: 25.5%; Average loss: 3.2618
Iteration: 1020; Percent complete: 25.5%; Average loss: 3.6615
Iteration: 1021; Percent complete: 25.5%; Average loss: 3.6274
Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4881
Iteration: 1023; Percent complete: 25.6%; Average loss: 3.3866
Iteration: 1024; Percent complete: 25.6%; Average loss: 3.2793
Iteration: 1025; Percent complete: 25.6%; Average loss: 3.7651
Iteration: 1026; Percent complete: 25.7%; Average loss: 3.4508
Iteration: 1027; Percent complete: 25.7%; Average loss: 3.3056
Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5260
Iteration: 1029; Percent complete: 25.7%; Average loss: 3.4646
Iteration: 1030; Percent complete: 25.8%; Average loss: 3.3311
Iteration: 1031; Percent complete: 25.8%; Average loss: 3.4757
Iteration: 1032; Percent complete: 25.8%; Average loss: 3.6143
Iteration: 1033; Percent complete: 25.8%; Average loss: 3.4232
Iteration: 1034; Percent complete: 25.9%; Average loss: 3.3160
Iteration: 1035; Percent complete: 25.9%; Average loss: 3.5201
Iteration: 1036; Percent complete: 25.9%; Average loss: 3.6992
Iteration: 1037; Percent complete: 25.9%; Average loss: 3.5522
Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3460
Iteration: 1039; Percent complete: 26.0%; Average loss: 3.6363
Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3753
Iteration: 1041; Percent complete: 26.0%; Average loss: 3.4394
Iteration: 1042; Percent complete: 26.1%; Average loss: 3.6315
Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4188
Iteration: 1044; Percent complete: 26.1%; Average loss: 3.4129
Iteration: 1045; Percent complete: 26.1%; Average loss: 3.2494
Iteration: 1046; Percent complete: 26.2%; Average loss: 3.2902
Iteration: 1047; Percent complete: 26.2%; Average loss: 2.8987
Iteration: 1048; Percent complete: 26.2%; Average loss: 3.6109
Iteration: 1049; Percent complete: 26.2%; Average loss: 3.3000
Iteration: 1050; Percent complete: 26.2%; Average loss: 3.5121
Iteration: 1051; Percent complete: 26.3%; Average loss: 3.5420
Iteration: 1052; Percent complete: 26.3%; Average loss: 3.4491
Iteration: 1053; Percent complete: 26.3%; Average loss: 3.4365
Iteration: 1054; Percent complete: 26.4%; Average loss: 4.0352
Iteration: 1055; Percent complete: 26.4%; Average loss: 3.1702
Iteration: 1056; Percent complete: 26.4%; Average loss: 3.1852
Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4867
Iteration: 1058; Percent complete: 26.5%; Average loss: 3.6960
Iteration: 1059; Percent complete: 26.5%; Average loss: 3.4307
Iteration: 1060; Percent complete: 26.5%; Average loss: 3.2801
Iteration: 1061; Percent complete: 26.5%; Average loss: 3.6296
Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4889
Iteration: 1063; Percent complete: 26.6%; Average loss: 3.6195
Iteration: 1064; Percent complete: 26.6%; Average loss: 3.4649
Iteration: 1065; Percent complete: 26.6%; Average loss: 3.1247
Iteration: 1066; Percent complete: 26.7%; Average loss: 3.2391
Iteration: 1067; Percent complete: 26.7%; Average loss: 3.4222
Iteration: 1068; Percent complete: 26.7%; Average loss: 3.3815
Iteration: 1069; Percent complete: 26.7%; Average loss: 3.2545
Iteration: 1070; Percent complete: 26.8%; Average loss: 3.4986
Iteration: 1071; Percent complete: 26.8%; Average loss: 3.2708
Iteration: 1072; Percent complete: 26.8%; Average loss: 3.2492
Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5521
Iteration: 1074; Percent complete: 26.9%; Average loss: 3.2817
Iteration: 1075; Percent complete: 26.9%; Average loss: 3.4295
Iteration: 1076; Percent complete: 26.9%; Average loss: 3.2755
Iteration: 1077; Percent complete: 26.9%; Average loss: 3.1587
Iteration: 1078; Percent complete: 27.0%; Average loss: 3.2524
Iteration: 1079; Percent complete: 27.0%; Average loss: 3.1584
Iteration: 1080; Percent complete: 27.0%; Average loss: 3.4187
Iteration: 1081; Percent complete: 27.0%; Average loss: 3.2773
Iteration: 1082; Percent complete: 27.1%; Average loss: 3.3619
Iteration: 1083; Percent complete: 27.1%; Average loss: 3.6978
Iteration: 1084; Percent complete: 27.1%; Average loss: 3.4924
Iteration: 1085; Percent complete: 27.1%; Average loss: 3.4806
Iteration: 1086; Percent complete: 27.2%; Average loss: 3.5957
Iteration: 1087; Percent complete: 27.2%; Average loss: 3.6100
Iteration: 1088; Percent complete: 27.2%; Average loss: 3.4899
Iteration: 1089; Percent complete: 27.2%; Average loss: 3.6349
Iteration: 1090; Percent complete: 27.3%; Average loss: 3.4053
Iteration: 1091; Percent complete: 27.3%; Average loss: 3.3123
Iteration: 1092; Percent complete: 27.3%; Average loss: 3.0027
Iteration: 1093; Percent complete: 27.3%; Average loss: 3.5749
Iteration: 1094; Percent complete: 27.4%; Average loss: 3.2798
Iteration: 1095; Percent complete: 27.4%; Average loss: 3.4246
Iteration: 1096; Percent complete: 27.4%; Average loss: 3.5275
Iteration: 1097; Percent complete: 27.4%; Average loss: 3.3511
Iteration: 1098; Percent complete: 27.5%; Average loss: 3.4239
Iteration: 1099; Percent complete: 27.5%; Average loss: 3.4000
Iteration: 1100; Percent complete: 27.5%; Average loss: 3.3448
Iteration: 1101; Percent complete: 27.5%; Average loss: 3.2467
Iteration: 1102; Percent complete: 27.6%; Average loss: 3.4202
Iteration: 1103; Percent complete: 27.6%; Average loss: 3.4471
Iteration: 1104; Percent complete: 27.6%; Average loss: 3.5045
Iteration: 1105; Percent complete: 27.6%; Average loss: 3.3867
Iteration: 1106; Percent complete: 27.7%; Average loss: 3.2456
Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5195
Iteration: 1108; Percent complete: 27.7%; Average loss: 3.5303
Iteration: 1109; Percent complete: 27.7%; Average loss: 3.5356
Iteration: 1110; Percent complete: 27.8%; Average loss: 3.6261
Iteration: 1111; Percent complete: 27.8%; Average loss: 3.1564
Iteration: 1112; Percent complete: 27.8%; Average loss: 3.6672
Iteration: 1113; Percent complete: 27.8%; Average loss: 3.7637
Iteration: 1114; Percent complete: 27.9%; Average loss: 3.4584
Iteration: 1115; Percent complete: 27.9%; Average loss: 3.5285
Iteration: 1116; Percent complete: 27.9%; Average loss: 3.3918
Iteration: 1117; Percent complete: 27.9%; Average loss: 3.2234
Iteration: 1118; Percent complete: 28.0%; Average loss: 3.4087
Iteration: 1119; Percent complete: 28.0%; Average loss: 3.4003
Iteration: 1120; Percent complete: 28.0%; Average loss: 3.2559
Iteration: 1121; Percent complete: 28.0%; Average loss: 3.1856
Iteration: 1122; Percent complete: 28.1%; Average loss: 3.4303
Iteration: 1123; Percent complete: 28.1%; Average loss: 3.4626
Iteration: 1124; Percent complete: 28.1%; Average loss: 3.3345
Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5955
Iteration: 1126; Percent complete: 28.1%; Average loss: 3.3973
Iteration: 1127; Percent complete: 28.2%; Average loss: 3.4468
Iteration: 1128; Percent complete: 28.2%; Average loss: 3.3445
Iteration: 1129; Percent complete: 28.2%; Average loss: 3.3969
Iteration: 1130; Percent complete: 28.2%; Average loss: 3.0674
Iteration: 1131; Percent complete: 28.3%; Average loss: 3.5602
Iteration: 1132; Percent complete: 28.3%; Average loss: 3.4168
Iteration: 1133; Percent complete: 28.3%; Average loss: 3.3123
Iteration: 1134; Percent complete: 28.3%; Average loss: 3.6757
Iteration: 1135; Percent complete: 28.4%; Average loss: 3.4056
Iteration: 1136; Percent complete: 28.4%; Average loss: 3.4888
Iteration: 1137; Percent complete: 28.4%; Average loss: 3.4755
Iteration: 1138; Percent complete: 28.4%; Average loss: 3.7109
Iteration: 1139; Percent complete: 28.5%; Average loss: 3.4640
Iteration: 1140; Percent complete: 28.5%; Average loss: 3.6623
Iteration: 1141; Percent complete: 28.5%; Average loss: 3.4532
Iteration: 1142; Percent complete: 28.5%; Average loss: 3.3533
Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2811
Iteration: 1144; Percent complete: 28.6%; Average loss: 3.4230
Iteration: 1145; Percent complete: 28.6%; Average loss: 3.6672
Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3249
Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5826
Iteration: 1148; Percent complete: 28.7%; Average loss: 3.2909
Iteration: 1149; Percent complete: 28.7%; Average loss: 3.2574
Iteration: 1150; Percent complete: 28.7%; Average loss: 3.4375
Iteration: 1151; Percent complete: 28.8%; Average loss: 2.9891
Iteration: 1152; Percent complete: 28.8%; Average loss: 3.6208
Iteration: 1153; Percent complete: 28.8%; Average loss: 3.3724
Iteration: 1154; Percent complete: 28.8%; Average loss: 3.2111
Iteration: 1155; Percent complete: 28.9%; Average loss: 3.4181
Iteration: 1156; Percent complete: 28.9%; Average loss: 3.4257
Iteration: 1157; Percent complete: 28.9%; Average loss: 3.4218
Iteration: 1158; Percent complete: 28.9%; Average loss: 3.2635
Iteration: 1159; Percent complete: 29.0%; Average loss: 3.4570
Iteration: 1160; Percent complete: 29.0%; Average loss: 3.2414
Iteration: 1161; Percent complete: 29.0%; Average loss: 3.4035
Iteration: 1162; Percent complete: 29.0%; Average loss: 3.4728
Iteration: 1163; Percent complete: 29.1%; Average loss: 3.3862
Iteration: 1164; Percent complete: 29.1%; Average loss: 3.5708
Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4590
Iteration: 1166; Percent complete: 29.1%; Average loss: 3.3614
Iteration: 1167; Percent complete: 29.2%; Average loss: 3.3953
Iteration: 1168; Percent complete: 29.2%; Average loss: 3.4563
Iteration: 1169; Percent complete: 29.2%; Average loss: 3.1485
Iteration: 1170; Percent complete: 29.2%; Average loss: 3.1245
Iteration: 1171; Percent complete: 29.3%; Average loss: 3.3549
Iteration: 1172; Percent complete: 29.3%; Average loss: 3.3154
Iteration: 1173; Percent complete: 29.3%; Average loss: 3.5246
Iteration: 1174; Percent complete: 29.3%; Average loss: 3.3889
Iteration: 1175; Percent complete: 29.4%; Average loss: 3.5563
Iteration: 1176; Percent complete: 29.4%; Average loss: 3.2295
Iteration: 1177; Percent complete: 29.4%; Average loss: 3.4436
Iteration: 1178; Percent complete: 29.4%; Average loss: 3.4054
Iteration: 1179; Percent complete: 29.5%; Average loss: 3.3573
Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5811
Iteration: 1181; Percent complete: 29.5%; Average loss: 3.5508
Iteration: 1182; Percent complete: 29.5%; Average loss: 3.3261
Iteration: 1183; Percent complete: 29.6%; Average loss: 3.1573
Iteration: 1184; Percent complete: 29.6%; Average loss: 3.2140
Iteration: 1185; Percent complete: 29.6%; Average loss: 3.2972
Iteration: 1186; Percent complete: 29.6%; Average loss: 3.5942
Iteration: 1187; Percent complete: 29.7%; Average loss: 3.4566
Iteration: 1188; Percent complete: 29.7%; Average loss: 3.2896
Iteration: 1189; Percent complete: 29.7%; Average loss: 3.5416
Iteration: 1190; Percent complete: 29.8%; Average loss: 3.3749
Iteration: 1191; Percent complete: 29.8%; Average loss: 3.3892
Iteration: 1192; Percent complete: 29.8%; Average loss: 3.4891
Iteration: 1193; Percent complete: 29.8%; Average loss: 3.1602
Iteration: 1194; Percent complete: 29.8%; Average loss: 3.6090
Iteration: 1195; Percent complete: 29.9%; Average loss: 3.5056
Iteration: 1196; Percent complete: 29.9%; Average loss: 3.2549
Iteration: 1197; Percent complete: 29.9%; Average loss: 3.2926
Iteration: 1198; Percent complete: 29.9%; Average loss: 3.4504
Iteration: 1199; Percent complete: 30.0%; Average loss: 3.1391
Iteration: 1200; Percent complete: 30.0%; Average loss: 3.5963
Iteration: 1201; Percent complete: 30.0%; Average loss: 3.3830
Iteration: 1202; Percent complete: 30.0%; Average loss: 3.2398
Iteration: 1203; Percent complete: 30.1%; Average loss: 3.4291
Iteration: 1204; Percent complete: 30.1%; Average loss: 3.2550
Iteration: 1205; Percent complete: 30.1%; Average loss: 3.4283
Iteration: 1206; Percent complete: 30.1%; Average loss: 3.4440
Iteration: 1207; Percent complete: 30.2%; Average loss: 3.3955
Iteration: 1208; Percent complete: 30.2%; Average loss: 3.3856
Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3696
Iteration: 1210; Percent complete: 30.2%; Average loss: 3.5425
Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2798
Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4320
Iteration: 1213; Percent complete: 30.3%; Average loss: 3.4009
Iteration: 1214; Percent complete: 30.3%; Average loss: 3.5388
Iteration: 1215; Percent complete: 30.4%; Average loss: 3.4524
Iteration: 1216; Percent complete: 30.4%; Average loss: 3.2551
Iteration: 1217; Percent complete: 30.4%; Average loss: 3.4475
Iteration: 1218; Percent complete: 30.4%; Average loss: 3.5178
Iteration: 1219; Percent complete: 30.5%; Average loss: 3.4757
Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5385
Iteration: 1221; Percent complete: 30.5%; Average loss: 3.6644
Iteration: 1222; Percent complete: 30.6%; Average loss: 3.4217
Iteration: 1223; Percent complete: 30.6%; Average loss: 3.2523
Iteration: 1224; Percent complete: 30.6%; Average loss: 3.0176
Iteration: 1225; Percent complete: 30.6%; Average loss: 3.3162
Iteration: 1226; Percent complete: 30.6%; Average loss: 3.3668
Iteration: 1227; Percent complete: 30.7%; Average loss: 3.2712
Iteration: 1228; Percent complete: 30.7%; Average loss: 3.3755
Iteration: 1229; Percent complete: 30.7%; Average loss: 3.3903
Iteration: 1230; Percent complete: 30.8%; Average loss: 3.4267
Iteration: 1231; Percent complete: 30.8%; Average loss: 3.4983
Iteration: 1232; Percent complete: 30.8%; Average loss: 3.4982
Iteration: 1233; Percent complete: 30.8%; Average loss: 3.3616
Iteration: 1234; Percent complete: 30.9%; Average loss: 3.4407
Iteration: 1235; Percent complete: 30.9%; Average loss: 3.3135
Iteration: 1236; Percent complete: 30.9%; Average loss: 3.5517
Iteration: 1237; Percent complete: 30.9%; Average loss: 3.3171
Iteration: 1238; Percent complete: 30.9%; Average loss: 3.2207
Iteration: 1239; Percent complete: 31.0%; Average loss: 3.2958
Iteration: 1240; Percent complete: 31.0%; Average loss: 3.4932
Iteration: 1241; Percent complete: 31.0%; Average loss: 3.6039
Iteration: 1242; Percent complete: 31.1%; Average loss: 3.3654
Iteration: 1243; Percent complete: 31.1%; Average loss: 3.4679
Iteration: 1244; Percent complete: 31.1%; Average loss: 3.4936
Iteration: 1245; Percent complete: 31.1%; Average loss: 3.3719
Iteration: 1246; Percent complete: 31.1%; Average loss: 3.2561
Iteration: 1247; Percent complete: 31.2%; Average loss: 3.4829
Iteration: 1248; Percent complete: 31.2%; Average loss: 3.3445
Iteration: 1249; Percent complete: 31.2%; Average loss: 3.5326
Iteration: 1250; Percent complete: 31.2%; Average loss: 3.2863
Iteration: 1251; Percent complete: 31.3%; Average loss: 3.6407
Iteration: 1252; Percent complete: 31.3%; Average loss: 3.4096
Iteration: 1253; Percent complete: 31.3%; Average loss: 3.2648
Iteration: 1254; Percent complete: 31.4%; Average loss: 3.4306
Iteration: 1255; Percent complete: 31.4%; Average loss: 3.5320
Iteration: 1256; Percent complete: 31.4%; Average loss: 3.2160
Iteration: 1257; Percent complete: 31.4%; Average loss: 3.5834
Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2080
Iteration: 1259; Percent complete: 31.5%; Average loss: 3.3095
Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2808
Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3778
Iteration: 1262; Percent complete: 31.6%; Average loss: 3.3737
Iteration: 1263; Percent complete: 31.6%; Average loss: 3.2947
Iteration: 1264; Percent complete: 31.6%; Average loss: 3.1727
Iteration: 1265; Percent complete: 31.6%; Average loss: 3.2412
Iteration: 1266; Percent complete: 31.6%; Average loss: 3.2721
Iteration: 1267; Percent complete: 31.7%; Average loss: 3.3056
Iteration: 1268; Percent complete: 31.7%; Average loss: 3.2298
Iteration: 1269; Percent complete: 31.7%; Average loss: 3.7730
Iteration: 1270; Percent complete: 31.8%; Average loss: 3.5188
Iteration: 1271; Percent complete: 31.8%; Average loss: 3.4896
Iteration: 1272; Percent complete: 31.8%; Average loss: 3.4604
Iteration: 1273; Percent complete: 31.8%; Average loss: 3.1022
Iteration: 1274; Percent complete: 31.9%; Average loss: 3.3908
Iteration: 1275; Percent complete: 31.9%; Average loss: 3.3941
Iteration: 1276; Percent complete: 31.9%; Average loss: 3.5349
Iteration: 1277; Percent complete: 31.9%; Average loss: 3.2741
Iteration: 1278; Percent complete: 31.9%; Average loss: 3.2191
Iteration: 1279; Percent complete: 32.0%; Average loss: 3.1909
Iteration: 1280; Percent complete: 32.0%; Average loss: 3.3359
Iteration: 1281; Percent complete: 32.0%; Average loss: 3.7551
Iteration: 1282; Percent complete: 32.0%; Average loss: 3.1264
Iteration: 1283; Percent complete: 32.1%; Average loss: 3.5457
Iteration: 1284; Percent complete: 32.1%; Average loss: 3.4926
Iteration: 1285; Percent complete: 32.1%; Average loss: 3.6239
Iteration: 1286; Percent complete: 32.1%; Average loss: 3.3105
Iteration: 1287; Percent complete: 32.2%; Average loss: 3.3636
Iteration: 1288; Percent complete: 32.2%; Average loss: 3.4786
Iteration: 1289; Percent complete: 32.2%; Average loss: 3.3796
Iteration: 1290; Percent complete: 32.2%; Average loss: 3.4699
Iteration: 1291; Percent complete: 32.3%; Average loss: 3.4117
Iteration: 1292; Percent complete: 32.3%; Average loss: 3.4235
Iteration: 1293; Percent complete: 32.3%; Average loss: 3.4031
Iteration: 1294; Percent complete: 32.4%; Average loss: 3.0346
Iteration: 1295; Percent complete: 32.4%; Average loss: 3.5452
Iteration: 1296; Percent complete: 32.4%; Average loss: 3.3560
Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2351
Iteration: 1298; Percent complete: 32.5%; Average loss: 3.2569
Iteration: 1299; Percent complete: 32.5%; Average loss: 3.5985
Iteration: 1300; Percent complete: 32.5%; Average loss: 3.5154
Iteration: 1301; Percent complete: 32.5%; Average loss: 3.2262
Iteration: 1302; Percent complete: 32.6%; Average loss: 3.3006
Iteration: 1303; Percent complete: 32.6%; Average loss: 3.2717
Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4018
Iteration: 1305; Percent complete: 32.6%; Average loss: 3.5098
Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4878
Iteration: 1307; Percent complete: 32.7%; Average loss: 3.3512
Iteration: 1308; Percent complete: 32.7%; Average loss: 3.2762
Iteration: 1309; Percent complete: 32.7%; Average loss: 3.5118
Iteration: 1310; Percent complete: 32.8%; Average loss: 3.2859
Iteration: 1311; Percent complete: 32.8%; Average loss: 3.5237
Iteration: 1312; Percent complete: 32.8%; Average loss: 3.4307
Iteration: 1313; Percent complete: 32.8%; Average loss: 3.4123
Iteration: 1314; Percent complete: 32.9%; Average loss: 3.4541
Iteration: 1315; Percent complete: 32.9%; Average loss: 3.3647
Iteration: 1316; Percent complete: 32.9%; Average loss: 3.2860
Iteration: 1317; Percent complete: 32.9%; Average loss: 3.3292
Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3472
Iteration: 1319; Percent complete: 33.0%; Average loss: 3.1754
Iteration: 1320; Percent complete: 33.0%; Average loss: 3.3058
Iteration: 1321; Percent complete: 33.0%; Average loss: 3.4378
Iteration: 1322; Percent complete: 33.1%; Average loss: 3.4060
Iteration: 1323; Percent complete: 33.1%; Average loss: 3.4366
Iteration: 1324; Percent complete: 33.1%; Average loss: 3.6099
Iteration: 1325; Percent complete: 33.1%; Average loss: 3.2110
Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3512
Iteration: 1327; Percent complete: 33.2%; Average loss: 3.4975
Iteration: 1328; Percent complete: 33.2%; Average loss: 3.2937
Iteration: 1329; Percent complete: 33.2%; Average loss: 3.3910
Iteration: 1330; Percent complete: 33.2%; Average loss: 3.1525
Iteration: 1331; Percent complete: 33.3%; Average loss: 3.2483
Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3157
Iteration: 1333; Percent complete: 33.3%; Average loss: 3.5312
Iteration: 1334; Percent complete: 33.4%; Average loss: 3.2872
Iteration: 1335; Percent complete: 33.4%; Average loss: 3.3198
Iteration: 1336; Percent complete: 33.4%; Average loss: 3.3977
Iteration: 1337; Percent complete: 33.4%; Average loss: 3.4129
Iteration: 1338; Percent complete: 33.5%; Average loss: 3.4102
Iteration: 1339; Percent complete: 33.5%; Average loss: 3.1361
Iteration: 1340; Percent complete: 33.5%; Average loss: 3.2423
Iteration: 1341; Percent complete: 33.5%; Average loss: 3.2877
Iteration: 1342; Percent complete: 33.6%; Average loss: 3.1018
Iteration: 1343; Percent complete: 33.6%; Average loss: 3.2981
Iteration: 1344; Percent complete: 33.6%; Average loss: 3.4417
Iteration: 1345; Percent complete: 33.6%; Average loss: 3.5177
Iteration: 1346; Percent complete: 33.7%; Average loss: 3.4550
Iteration: 1347; Percent complete: 33.7%; Average loss: 3.4000
Iteration: 1348; Percent complete: 33.7%; Average loss: 3.2927
Iteration: 1349; Percent complete: 33.7%; Average loss: 3.3140
Iteration: 1350; Percent complete: 33.8%; Average loss: 3.3301
Iteration: 1351; Percent complete: 33.8%; Average loss: 3.1570
Iteration: 1352; Percent complete: 33.8%; Average loss: 3.1186
Iteration: 1353; Percent complete: 33.8%; Average loss: 3.5372
Iteration: 1354; Percent complete: 33.9%; Average loss: 3.0916
Iteration: 1355; Percent complete: 33.9%; Average loss: 3.4463
Iteration: 1356; Percent complete: 33.9%; Average loss: 3.2154
Iteration: 1357; Percent complete: 33.9%; Average loss: 3.3875
Iteration: 1358; Percent complete: 34.0%; Average loss: 3.1615
Iteration: 1359; Percent complete: 34.0%; Average loss: 3.1588
Iteration: 1360; Percent complete: 34.0%; Average loss: 3.4078
Iteration: 1361; Percent complete: 34.0%; Average loss: 3.6468
Iteration: 1362; Percent complete: 34.1%; Average loss: 3.3011
Iteration: 1363; Percent complete: 34.1%; Average loss: 3.0562
Iteration: 1364; Percent complete: 34.1%; Average loss: 3.6795
Iteration: 1365; Percent complete: 34.1%; Average loss: 3.5559
Iteration: 1366; Percent complete: 34.2%; Average loss: 3.4192
Iteration: 1367; Percent complete: 34.2%; Average loss: 3.1223
Iteration: 1368; Percent complete: 34.2%; Average loss: 3.1133
Iteration: 1369; Percent complete: 34.2%; Average loss: 3.6308
Iteration: 1370; Percent complete: 34.2%; Average loss: 3.2064
Iteration: 1371; Percent complete: 34.3%; Average loss: 3.2045
Iteration: 1372; Percent complete: 34.3%; Average loss: 3.4136
Iteration: 1373; Percent complete: 34.3%; Average loss: 3.3787
Iteration: 1374; Percent complete: 34.4%; Average loss: 3.2713
Iteration: 1375; Percent complete: 34.4%; Average loss: 3.4048
Iteration: 1376; Percent complete: 34.4%; Average loss: 3.5985
Iteration: 1377; Percent complete: 34.4%; Average loss: 3.6378
Iteration: 1378; Percent complete: 34.4%; Average loss: 3.0926
Iteration: 1379; Percent complete: 34.5%; Average loss: 3.3930
Iteration: 1380; Percent complete: 34.5%; Average loss: 3.4076
Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4102
Iteration: 1382; Percent complete: 34.5%; Average loss: 3.2385
Iteration: 1383; Percent complete: 34.6%; Average loss: 3.2605
Iteration: 1384; Percent complete: 34.6%; Average loss: 3.2787
Iteration: 1385; Percent complete: 34.6%; Average loss: 3.2901
Iteration: 1386; Percent complete: 34.6%; Average loss: 3.3305
Iteration: 1387; Percent complete: 34.7%; Average loss: 3.2499
Iteration: 1388; Percent complete: 34.7%; Average loss: 3.5784
Iteration: 1389; Percent complete: 34.7%; Average loss: 3.3248
Iteration: 1390; Percent complete: 34.8%; Average loss: 3.2589
Iteration: 1391; Percent complete: 34.8%; Average loss: 3.3062
Iteration: 1392; Percent complete: 34.8%; Average loss: 3.2074
Iteration: 1393; Percent complete: 34.8%; Average loss: 3.2260
Iteration: 1394; Percent complete: 34.8%; Average loss: 3.2535
Iteration: 1395; Percent complete: 34.9%; Average loss: 3.3730
Iteration: 1396; Percent complete: 34.9%; Average loss: 3.0788
Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3010
Iteration: 1398; Percent complete: 34.9%; Average loss: 3.3369
Iteration: 1399; Percent complete: 35.0%; Average loss: 3.0594
Iteration: 1400; Percent complete: 35.0%; Average loss: 3.4156
Iteration: 1401; Percent complete: 35.0%; Average loss: 3.3990
Iteration: 1402; Percent complete: 35.0%; Average loss: 3.2819
Iteration: 1403; Percent complete: 35.1%; Average loss: 3.6316
Iteration: 1404; Percent complete: 35.1%; Average loss: 3.3607
Iteration: 1405; Percent complete: 35.1%; Average loss: 3.3051
Iteration: 1406; Percent complete: 35.1%; Average loss: 3.3546
Iteration: 1407; Percent complete: 35.2%; Average loss: 3.5810
Iteration: 1408; Percent complete: 35.2%; Average loss: 3.4460
Iteration: 1409; Percent complete: 35.2%; Average loss: 3.1311
Iteration: 1410; Percent complete: 35.2%; Average loss: 3.1871
Iteration: 1411; Percent complete: 35.3%; Average loss: 3.0655
Iteration: 1412; Percent complete: 35.3%; Average loss: 3.5743
Iteration: 1413; Percent complete: 35.3%; Average loss: 3.3125
Iteration: 1414; Percent complete: 35.4%; Average loss: 3.1450
Iteration: 1415; Percent complete: 35.4%; Average loss: 3.4077
Iteration: 1416; Percent complete: 35.4%; Average loss: 3.5340
Iteration: 1417; Percent complete: 35.4%; Average loss: 3.3795
Iteration: 1418; Percent complete: 35.4%; Average loss: 3.2985
Iteration: 1419; Percent complete: 35.5%; Average loss: 3.6224
Iteration: 1420; Percent complete: 35.5%; Average loss: 3.3417
Iteration: 1421; Percent complete: 35.5%; Average loss: 3.4217
Iteration: 1422; Percent complete: 35.5%; Average loss: 3.5704
Iteration: 1423; Percent complete: 35.6%; Average loss: 3.4412
Iteration: 1424; Percent complete: 35.6%; Average loss: 3.3127
Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2959
Iteration: 1426; Percent complete: 35.6%; Average loss: 3.1869
Iteration: 1427; Percent complete: 35.7%; Average loss: 3.0883
Iteration: 1428; Percent complete: 35.7%; Average loss: 3.4662
Iteration: 1429; Percent complete: 35.7%; Average loss: 3.1438
Iteration: 1430; Percent complete: 35.8%; Average loss: 3.2759
Iteration: 1431; Percent complete: 35.8%; Average loss: 3.0650
Iteration: 1432; Percent complete: 35.8%; Average loss: 3.1093
Iteration: 1433; Percent complete: 35.8%; Average loss: 3.4859
Iteration: 1434; Percent complete: 35.9%; Average loss: 3.2363
Iteration: 1435; Percent complete: 35.9%; Average loss: 3.2696
Iteration: 1436; Percent complete: 35.9%; Average loss: 3.3600
Iteration: 1437; Percent complete: 35.9%; Average loss: 3.4008
Iteration: 1438; Percent complete: 35.9%; Average loss: 3.1511
Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2601
Iteration: 1440; Percent complete: 36.0%; Average loss: 3.4012
Iteration: 1441; Percent complete: 36.0%; Average loss: 3.1501
Iteration: 1442; Percent complete: 36.0%; Average loss: 3.5515
Iteration: 1443; Percent complete: 36.1%; Average loss: 3.0292
Iteration: 1444; Percent complete: 36.1%; Average loss: 3.2407
Iteration: 1445; Percent complete: 36.1%; Average loss: 3.3892
Iteration: 1446; Percent complete: 36.1%; Average loss: 3.0944
Iteration: 1447; Percent complete: 36.2%; Average loss: 3.4417
Iteration: 1448; Percent complete: 36.2%; Average loss: 3.4977
Iteration: 1449; Percent complete: 36.2%; Average loss: 3.2666
Iteration: 1450; Percent complete: 36.2%; Average loss: 3.0903
Iteration: 1451; Percent complete: 36.3%; Average loss: 3.3159
Iteration: 1452; Percent complete: 36.3%; Average loss: 3.3400
Iteration: 1453; Percent complete: 36.3%; Average loss: 3.4265
Iteration: 1454; Percent complete: 36.4%; Average loss: 3.2540
Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4771
Iteration: 1456; Percent complete: 36.4%; Average loss: 3.2859
Iteration: 1457; Percent complete: 36.4%; Average loss: 3.4423
Iteration: 1458; Percent complete: 36.4%; Average loss: 3.5000
Iteration: 1459; Percent complete: 36.5%; Average loss: 3.3131
Iteration: 1460; Percent complete: 36.5%; Average loss: 3.1602
Iteration: 1461; Percent complete: 36.5%; Average loss: 3.0817
Iteration: 1462; Percent complete: 36.5%; Average loss: 3.3780
Iteration: 1463; Percent complete: 36.6%; Average loss: 3.1346
Iteration: 1464; Percent complete: 36.6%; Average loss: 3.4133
Iteration: 1465; Percent complete: 36.6%; Average loss: 3.3356
Iteration: 1466; Percent complete: 36.6%; Average loss: 3.0293
Iteration: 1467; Percent complete: 36.7%; Average loss: 3.2114
Iteration: 1468; Percent complete: 36.7%; Average loss: 3.2568
Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2487
Iteration: 1470; Percent complete: 36.8%; Average loss: 3.3245
Iteration: 1471; Percent complete: 36.8%; Average loss: 3.3014
Iteration: 1472; Percent complete: 36.8%; Average loss: 3.3982
Iteration: 1473; Percent complete: 36.8%; Average loss: 3.2846
Iteration: 1474; Percent complete: 36.9%; Average loss: 3.2199
Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3856
Iteration: 1476; Percent complete: 36.9%; Average loss: 3.4653
Iteration: 1477; Percent complete: 36.9%; Average loss: 3.3666
Iteration: 1478; Percent complete: 37.0%; Average loss: 3.0966
Iteration: 1479; Percent complete: 37.0%; Average loss: 3.5325
Iteration: 1480; Percent complete: 37.0%; Average loss: 3.4514
Iteration: 1481; Percent complete: 37.0%; Average loss: 3.4329
Iteration: 1482; Percent complete: 37.0%; Average loss: 3.2273
Iteration: 1483; Percent complete: 37.1%; Average loss: 3.6875
Iteration: 1484; Percent complete: 37.1%; Average loss: 3.1712
Iteration: 1485; Percent complete: 37.1%; Average loss: 3.1798
Iteration: 1486; Percent complete: 37.1%; Average loss: 3.4597
Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1170
Iteration: 1488; Percent complete: 37.2%; Average loss: 3.3318
Iteration: 1489; Percent complete: 37.2%; Average loss: 3.4839
Iteration: 1490; Percent complete: 37.2%; Average loss: 3.4545
Iteration: 1491; Percent complete: 37.3%; Average loss: 3.2632
Iteration: 1492; Percent complete: 37.3%; Average loss: 3.4525
Iteration: 1493; Percent complete: 37.3%; Average loss: 3.4058
Iteration: 1494; Percent complete: 37.4%; Average loss: 3.3444
Iteration: 1495; Percent complete: 37.4%; Average loss: 3.3328
Iteration: 1496; Percent complete: 37.4%; Average loss: 3.1337
Iteration: 1497; Percent complete: 37.4%; Average loss: 3.2866
Iteration: 1498; Percent complete: 37.5%; Average loss: 3.0240
Iteration: 1499; Percent complete: 37.5%; Average loss: 3.2799
Iteration: 1500; Percent complete: 37.5%; Average loss: 3.2164
Iteration: 1501; Percent complete: 37.5%; Average loss: 3.3125
Iteration: 1502; Percent complete: 37.5%; Average loss: 3.2946
Iteration: 1503; Percent complete: 37.6%; Average loss: 3.2588
Iteration: 1504; Percent complete: 37.6%; Average loss: 3.1946
Iteration: 1505; Percent complete: 37.6%; Average loss: 3.5868
Iteration: 1506; Percent complete: 37.6%; Average loss: 3.1659
Iteration: 1507; Percent complete: 37.7%; Average loss: 3.2829
Iteration: 1508; Percent complete: 37.7%; Average loss: 3.4228
Iteration: 1509; Percent complete: 37.7%; Average loss: 3.1732
Iteration: 1510; Percent complete: 37.8%; Average loss: 2.9865
Iteration: 1511; Percent complete: 37.8%; Average loss: 3.7110
Iteration: 1512; Percent complete: 37.8%; Average loss: 3.3741
Iteration: 1513; Percent complete: 37.8%; Average loss: 3.2747
Iteration: 1514; Percent complete: 37.9%; Average loss: 3.1599
Iteration: 1515; Percent complete: 37.9%; Average loss: 3.4713
Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2873
Iteration: 1517; Percent complete: 37.9%; Average loss: 3.2530
Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3592
Iteration: 1519; Percent complete: 38.0%; Average loss: 3.4801
Iteration: 1520; Percent complete: 38.0%; Average loss: 3.3616
Iteration: 1521; Percent complete: 38.0%; Average loss: 3.3834
Iteration: 1522; Percent complete: 38.0%; Average loss: 3.1273
Iteration: 1523; Percent complete: 38.1%; Average loss: 3.5313
Iteration: 1524; Percent complete: 38.1%; Average loss: 3.1972
Iteration: 1525; Percent complete: 38.1%; Average loss: 3.1213
Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1717
Iteration: 1527; Percent complete: 38.2%; Average loss: 3.2082
Iteration: 1528; Percent complete: 38.2%; Average loss: 3.3992
Iteration: 1529; Percent complete: 38.2%; Average loss: 3.3326
Iteration: 1530; Percent complete: 38.2%; Average loss: 3.0717
Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3488
Iteration: 1532; Percent complete: 38.3%; Average loss: 3.1780
Iteration: 1533; Percent complete: 38.3%; Average loss: 3.3625
Iteration: 1534; Percent complete: 38.4%; Average loss: 3.3810
Iteration: 1535; Percent complete: 38.4%; Average loss: 3.4223
Iteration: 1536; Percent complete: 38.4%; Average loss: 3.4768
Iteration: 1537; Percent complete: 38.4%; Average loss: 3.1695
Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2979
Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2310
Iteration: 1540; Percent complete: 38.5%; Average loss: 3.3374
Iteration: 1541; Percent complete: 38.5%; Average loss: 3.2722
Iteration: 1542; Percent complete: 38.6%; Average loss: 3.4297
Iteration: 1543; Percent complete: 38.6%; Average loss: 3.1907
Iteration: 1544; Percent complete: 38.6%; Average loss: 3.4096
Iteration: 1545; Percent complete: 38.6%; Average loss: 3.1113
Iteration: 1546; Percent complete: 38.6%; Average loss: 3.4330
Iteration: 1547; Percent complete: 38.7%; Average loss: 3.2288
Iteration: 1548; Percent complete: 38.7%; Average loss: 3.3236
Iteration: 1549; Percent complete: 38.7%; Average loss: 3.4752
Iteration: 1550; Percent complete: 38.8%; Average loss: 3.3948
Iteration: 1551; Percent complete: 38.8%; Average loss: 3.5097
Iteration: 1552; Percent complete: 38.8%; Average loss: 3.6375
Iteration: 1553; Percent complete: 38.8%; Average loss: 3.3665
Iteration: 1554; Percent complete: 38.9%; Average loss: 3.3911
Iteration: 1555; Percent complete: 38.9%; Average loss: 3.3434
Iteration: 1556; Percent complete: 38.9%; Average loss: 3.4880
Iteration: 1557; Percent complete: 38.9%; Average loss: 3.2318
Iteration: 1558; Percent complete: 39.0%; Average loss: 3.3511
Iteration: 1559; Percent complete: 39.0%; Average loss: 3.3005
Iteration: 1560; Percent complete: 39.0%; Average loss: 3.1619
Iteration: 1561; Percent complete: 39.0%; Average loss: 3.2699
Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4281
Iteration: 1563; Percent complete: 39.1%; Average loss: 3.2585
Iteration: 1564; Percent complete: 39.1%; Average loss: 3.2603
Iteration: 1565; Percent complete: 39.1%; Average loss: 3.4680
Iteration: 1566; Percent complete: 39.1%; Average loss: 3.1595
Iteration: 1567; Percent complete: 39.2%; Average loss: 3.4048
Iteration: 1568; Percent complete: 39.2%; Average loss: 3.4642
Iteration: 1569; Percent complete: 39.2%; Average loss: 3.1179
Iteration: 1570; Percent complete: 39.2%; Average loss: 3.4441
Iteration: 1571; Percent complete: 39.3%; Average loss: 3.3405
Iteration: 1572; Percent complete: 39.3%; Average loss: 3.3930
Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4923
Iteration: 1574; Percent complete: 39.4%; Average loss: 3.2645
Iteration: 1575; Percent complete: 39.4%; Average loss: 3.3127
Iteration: 1576; Percent complete: 39.4%; Average loss: 3.0435
Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3554
Iteration: 1578; Percent complete: 39.5%; Average loss: 3.1275
Iteration: 1579; Percent complete: 39.5%; Average loss: 3.0764
Iteration: 1580; Percent complete: 39.5%; Average loss: 3.1599
Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4129
Iteration: 1582; Percent complete: 39.6%; Average loss: 3.3901
Iteration: 1583; Percent complete: 39.6%; Average loss: 3.1290
Iteration: 1584; Percent complete: 39.6%; Average loss: 3.0889
Iteration: 1585; Percent complete: 39.6%; Average loss: 3.7038
Iteration: 1586; Percent complete: 39.6%; Average loss: 2.9876
Iteration: 1587; Percent complete: 39.7%; Average loss: 3.1982
Iteration: 1588; Percent complete: 39.7%; Average loss: 3.4515
Iteration: 1589; Percent complete: 39.7%; Average loss: 3.3631
Iteration: 1590; Percent complete: 39.8%; Average loss: 2.9818
Iteration: 1591; Percent complete: 39.8%; Average loss: 3.2659
Iteration: 1592; Percent complete: 39.8%; Average loss: 3.2922
Iteration: 1593; Percent complete: 39.8%; Average loss: 3.3134
Iteration: 1594; Percent complete: 39.9%; Average loss: 3.1412
Iteration: 1595; Percent complete: 39.9%; Average loss: 3.2426
Iteration: 1596; Percent complete: 39.9%; Average loss: 3.2817
Iteration: 1597; Percent complete: 39.9%; Average loss: 3.2492
Iteration: 1598; Percent complete: 40.0%; Average loss: 3.2902
Iteration: 1599; Percent complete: 40.0%; Average loss: 3.2677
Iteration: 1600; Percent complete: 40.0%; Average loss: 3.2467
Iteration: 1601; Percent complete: 40.0%; Average loss: 3.3063
Iteration: 1602; Percent complete: 40.1%; Average loss: 3.1434
Iteration: 1603; Percent complete: 40.1%; Average loss: 3.0706
Iteration: 1604; Percent complete: 40.1%; Average loss: 3.1218
Iteration: 1605; Percent complete: 40.1%; Average loss: 3.1164
Iteration: 1606; Percent complete: 40.2%; Average loss: 3.1038
Iteration: 1607; Percent complete: 40.2%; Average loss: 3.0928
Iteration: 1608; Percent complete: 40.2%; Average loss: 3.1569
Iteration: 1609; Percent complete: 40.2%; Average loss: 3.1271
Iteration: 1610; Percent complete: 40.2%; Average loss: 3.4363
Iteration: 1611; Percent complete: 40.3%; Average loss: 3.2735
Iteration: 1612; Percent complete: 40.3%; Average loss: 3.2958
Iteration: 1613; Percent complete: 40.3%; Average loss: 3.2095
Iteration: 1614; Percent complete: 40.4%; Average loss: 3.1391
Iteration: 1615; Percent complete: 40.4%; Average loss: 3.1524
Iteration: 1616; Percent complete: 40.4%; Average loss: 3.5148
Iteration: 1617; Percent complete: 40.4%; Average loss: 3.1639
Iteration: 1618; Percent complete: 40.5%; Average loss: 3.2185
Iteration: 1619; Percent complete: 40.5%; Average loss: 3.4011
Iteration: 1620; Percent complete: 40.5%; Average loss: 3.5213
Iteration: 1621; Percent complete: 40.5%; Average loss: 3.4618
Iteration: 1622; Percent complete: 40.6%; Average loss: 3.4498
Iteration: 1623; Percent complete: 40.6%; Average loss: 2.9532
Iteration: 1624; Percent complete: 40.6%; Average loss: 3.3195
Iteration: 1625; Percent complete: 40.6%; Average loss: 3.4536
Iteration: 1626; Percent complete: 40.6%; Average loss: 3.2901
Iteration: 1627; Percent complete: 40.7%; Average loss: 3.5448
Iteration: 1628; Percent complete: 40.7%; Average loss: 3.2011
Iteration: 1629; Percent complete: 40.7%; Average loss: 3.4767
Iteration: 1630; Percent complete: 40.8%; Average loss: 3.4475
Iteration: 1631; Percent complete: 40.8%; Average loss: 3.0126
Iteration: 1632; Percent complete: 40.8%; Average loss: 3.3468
Iteration: 1633; Percent complete: 40.8%; Average loss: 3.4301
Iteration: 1634; Percent complete: 40.8%; Average loss: 3.2859
Iteration: 1635; Percent complete: 40.9%; Average loss: 3.1498
Iteration: 1636; Percent complete: 40.9%; Average loss: 3.3057
Iteration: 1637; Percent complete: 40.9%; Average loss: 3.5481
Iteration: 1638; Percent complete: 40.9%; Average loss: 3.2757
Iteration: 1639; Percent complete: 41.0%; Average loss: 3.2519
Iteration: 1640; Percent complete: 41.0%; Average loss: 3.4574
Iteration: 1641; Percent complete: 41.0%; Average loss: 3.0593
Iteration: 1642; Percent complete: 41.0%; Average loss: 3.1927
Iteration: 1643; Percent complete: 41.1%; Average loss: 3.1699
Iteration: 1644; Percent complete: 41.1%; Average loss: 3.2359
Iteration: 1645; Percent complete: 41.1%; Average loss: 3.2573
Iteration: 1646; Percent complete: 41.1%; Average loss: 3.3166
Iteration: 1647; Percent complete: 41.2%; Average loss: 3.4670
Iteration: 1648; Percent complete: 41.2%; Average loss: 3.2071
Iteration: 1649; Percent complete: 41.2%; Average loss: 3.3730
Iteration: 1650; Percent complete: 41.2%; Average loss: 3.1438
Iteration: 1651; Percent complete: 41.3%; Average loss: 3.1626
Iteration: 1652; Percent complete: 41.3%; Average loss: 3.4830
Iteration: 1653; Percent complete: 41.3%; Average loss: 3.2268
Iteration: 1654; Percent complete: 41.3%; Average loss: 3.3280
Iteration: 1655; Percent complete: 41.4%; Average loss: 3.3214
Iteration: 1656; Percent complete: 41.4%; Average loss: 3.3766
Iteration: 1657; Percent complete: 41.4%; Average loss: 3.2628
Iteration: 1658; Percent complete: 41.4%; Average loss: 3.3462
Iteration: 1659; Percent complete: 41.5%; Average loss: 3.2713
Iteration: 1660; Percent complete: 41.5%; Average loss: 3.1532
Iteration: 1661; Percent complete: 41.5%; Average loss: 3.2225
Iteration: 1662; Percent complete: 41.5%; Average loss: 3.4193
Iteration: 1663; Percent complete: 41.6%; Average loss: 3.2750
Iteration: 1664; Percent complete: 41.6%; Average loss: 3.1409
Iteration: 1665; Percent complete: 41.6%; Average loss: 3.1378
Iteration: 1666; Percent complete: 41.6%; Average loss: 3.2707
Iteration: 1667; Percent complete: 41.7%; Average loss: 3.5263
Iteration: 1668; Percent complete: 41.7%; Average loss: 3.3075
Iteration: 1669; Percent complete: 41.7%; Average loss: 3.1993
Iteration: 1670; Percent complete: 41.8%; Average loss: 3.2695
Iteration: 1671; Percent complete: 41.8%; Average loss: 3.0414
Iteration: 1672; Percent complete: 41.8%; Average loss: 3.2188
Iteration: 1673; Percent complete: 41.8%; Average loss: 3.1233
Iteration: 1674; Percent complete: 41.9%; Average loss: 3.4019
Iteration: 1675; Percent complete: 41.9%; Average loss: 3.1182
Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1970
Iteration: 1677; Percent complete: 41.9%; Average loss: 3.1060
Iteration: 1678; Percent complete: 41.9%; Average loss: 3.2728
Iteration: 1679; Percent complete: 42.0%; Average loss: 3.4736
Iteration: 1680; Percent complete: 42.0%; Average loss: 3.2797
Iteration: 1681; Percent complete: 42.0%; Average loss: 3.1724
Iteration: 1682; Percent complete: 42.0%; Average loss: 3.1412
Iteration: 1683; Percent complete: 42.1%; Average loss: 3.2605
Iteration: 1684; Percent complete: 42.1%; Average loss: 3.2501
Iteration: 1685; Percent complete: 42.1%; Average loss: 3.4020
Iteration: 1686; Percent complete: 42.1%; Average loss: 3.3215
Iteration: 1687; Percent complete: 42.2%; Average loss: 3.4016
Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2471
Iteration: 1689; Percent complete: 42.2%; Average loss: 3.3043
Iteration: 1690; Percent complete: 42.2%; Average loss: 3.4693
Iteration: 1691; Percent complete: 42.3%; Average loss: 3.1482
Iteration: 1692; Percent complete: 42.3%; Average loss: 3.4002
Iteration: 1693; Percent complete: 42.3%; Average loss: 3.5696
Iteration: 1694; Percent complete: 42.4%; Average loss: 3.0822
Iteration: 1695; Percent complete: 42.4%; Average loss: 3.2774
Iteration: 1696; Percent complete: 42.4%; Average loss: 3.1763
Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1843
Iteration: 1698; Percent complete: 42.4%; Average loss: 3.2078
Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2551
Iteration: 1700; Percent complete: 42.5%; Average loss: 3.4726
Iteration: 1701; Percent complete: 42.5%; Average loss: 3.0031
Iteration: 1702; Percent complete: 42.5%; Average loss: 3.2619
Iteration: 1703; Percent complete: 42.6%; Average loss: 3.1689
Iteration: 1704; Percent complete: 42.6%; Average loss: 3.3563
Iteration: 1705; Percent complete: 42.6%; Average loss: 3.2487
Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2766
Iteration: 1707; Percent complete: 42.7%; Average loss: 3.3415
Iteration: 1708; Percent complete: 42.7%; Average loss: 3.3225
Iteration: 1709; Percent complete: 42.7%; Average loss: 3.3163
Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2116
Iteration: 1711; Percent complete: 42.8%; Average loss: 3.4488
Iteration: 1712; Percent complete: 42.8%; Average loss: 3.2190
Iteration: 1713; Percent complete: 42.8%; Average loss: 3.1963
Iteration: 1714; Percent complete: 42.9%; Average loss: 3.0645
Iteration: 1715; Percent complete: 42.9%; Average loss: 3.2170
Iteration: 1716; Percent complete: 42.9%; Average loss: 3.0569
Iteration: 1717; Percent complete: 42.9%; Average loss: 3.3507
Iteration: 1718; Percent complete: 43.0%; Average loss: 3.1546
Iteration: 1719; Percent complete: 43.0%; Average loss: 3.0325
Iteration: 1720; Percent complete: 43.0%; Average loss: 3.0731
Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3000
Iteration: 1722; Percent complete: 43.0%; Average loss: 3.6698
Iteration: 1723; Percent complete: 43.1%; Average loss: 2.9787
Iteration: 1724; Percent complete: 43.1%; Average loss: 3.2896
Iteration: 1725; Percent complete: 43.1%; Average loss: 3.3481
Iteration: 1726; Percent complete: 43.1%; Average loss: 3.4355
Iteration: 1727; Percent complete: 43.2%; Average loss: 3.1458
Iteration: 1728; Percent complete: 43.2%; Average loss: 3.3915
Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0075
Iteration: 1730; Percent complete: 43.2%; Average loss: 3.3863
Iteration: 1731; Percent complete: 43.3%; Average loss: 3.1421
Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2612
Iteration: 1733; Percent complete: 43.3%; Average loss: 3.4238
Iteration: 1734; Percent complete: 43.4%; Average loss: 3.5788
Iteration: 1735; Percent complete: 43.4%; Average loss: 3.3611
Iteration: 1736; Percent complete: 43.4%; Average loss: 3.1344
Iteration: 1737; Percent complete: 43.4%; Average loss: 3.4480
Iteration: 1738; Percent complete: 43.5%; Average loss: 3.2246
Iteration: 1739; Percent complete: 43.5%; Average loss: 2.9605
Iteration: 1740; Percent complete: 43.5%; Average loss: 3.3506
Iteration: 1741; Percent complete: 43.5%; Average loss: 3.2539
Iteration: 1742; Percent complete: 43.5%; Average loss: 3.1053
Iteration: 1743; Percent complete: 43.6%; Average loss: 3.0102
Iteration: 1744; Percent complete: 43.6%; Average loss: 3.6755
Iteration: 1745; Percent complete: 43.6%; Average loss: 3.2601
Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3843
Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2221
Iteration: 1748; Percent complete: 43.7%; Average loss: 3.4334
Iteration: 1749; Percent complete: 43.7%; Average loss: 3.0772
Iteration: 1750; Percent complete: 43.8%; Average loss: 3.0923
Iteration: 1751; Percent complete: 43.8%; Average loss: 3.1830
Iteration: 1752; Percent complete: 43.8%; Average loss: 3.2208
Iteration: 1753; Percent complete: 43.8%; Average loss: 3.5371
Iteration: 1754; Percent complete: 43.9%; Average loss: 3.1931
Iteration: 1755; Percent complete: 43.9%; Average loss: 3.1259
Iteration: 1756; Percent complete: 43.9%; Average loss: 3.2537
Iteration: 1757; Percent complete: 43.9%; Average loss: 3.1955
Iteration: 1758; Percent complete: 44.0%; Average loss: 3.1781
Iteration: 1759; Percent complete: 44.0%; Average loss: 2.9669
Iteration: 1760; Percent complete: 44.0%; Average loss: 3.2454
Iteration: 1761; Percent complete: 44.0%; Average loss: 2.9332
Iteration: 1762; Percent complete: 44.0%; Average loss: 3.1077
Iteration: 1763; Percent complete: 44.1%; Average loss: 3.1092
Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2088
Iteration: 1765; Percent complete: 44.1%; Average loss: 3.2224
Iteration: 1766; Percent complete: 44.1%; Average loss: 3.2454
Iteration: 1767; Percent complete: 44.2%; Average loss: 3.4496
Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2256
Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1094
Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2458
Iteration: 1771; Percent complete: 44.3%; Average loss: 3.1819
Iteration: 1772; Percent complete: 44.3%; Average loss: 3.2736
Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2426
Iteration: 1774; Percent complete: 44.4%; Average loss: 3.0377
Iteration: 1775; Percent complete: 44.4%; Average loss: 3.2220
Iteration: 1776; Percent complete: 44.4%; Average loss: 3.1462
Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9775
Iteration: 1778; Percent complete: 44.5%; Average loss: 3.3032
Iteration: 1779; Percent complete: 44.5%; Average loss: 3.2855
Iteration: 1780; Percent complete: 44.5%; Average loss: 3.3948
Iteration: 1781; Percent complete: 44.5%; Average loss: 3.1779
Iteration: 1782; Percent complete: 44.5%; Average loss: 3.2676
Iteration: 1783; Percent complete: 44.6%; Average loss: 3.0810
Iteration: 1784; Percent complete: 44.6%; Average loss: 3.2845
Iteration: 1785; Percent complete: 44.6%; Average loss: 3.2761
Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2958
Iteration: 1787; Percent complete: 44.7%; Average loss: 3.2853
Iteration: 1788; Percent complete: 44.7%; Average loss: 3.1711
Iteration: 1789; Percent complete: 44.7%; Average loss: 3.3819
Iteration: 1790; Percent complete: 44.8%; Average loss: 3.2027
Iteration: 1791; Percent complete: 44.8%; Average loss: 3.0985
Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2615
Iteration: 1793; Percent complete: 44.8%; Average loss: 3.1524
Iteration: 1794; Percent complete: 44.9%; Average loss: 3.1559
Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1663
Iteration: 1796; Percent complete: 44.9%; Average loss: 3.0649
Iteration: 1797; Percent complete: 44.9%; Average loss: 3.2108
Iteration: 1798; Percent complete: 45.0%; Average loss: 3.0422
Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1826
Iteration: 1800; Percent complete: 45.0%; Average loss: 3.0963
Iteration: 1801; Percent complete: 45.0%; Average loss: 3.0560
Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1991
Iteration: 1803; Percent complete: 45.1%; Average loss: 2.9104
Iteration: 1804; Percent complete: 45.1%; Average loss: 3.2447
Iteration: 1805; Percent complete: 45.1%; Average loss: 3.1976
Iteration: 1806; Percent complete: 45.1%; Average loss: 3.2907
Iteration: 1807; Percent complete: 45.2%; Average loss: 3.1917
Iteration: 1808; Percent complete: 45.2%; Average loss: 3.1141
Iteration: 1809; Percent complete: 45.2%; Average loss: 3.1918
Iteration: 1810; Percent complete: 45.2%; Average loss: 3.2792
Iteration: 1811; Percent complete: 45.3%; Average loss: 3.2783
Iteration: 1812; Percent complete: 45.3%; Average loss: 3.1454
Iteration: 1813; Percent complete: 45.3%; Average loss: 3.1015
Iteration: 1814; Percent complete: 45.4%; Average loss: 3.1420
Iteration: 1815; Percent complete: 45.4%; Average loss: 2.7707
Iteration: 1816; Percent complete: 45.4%; Average loss: 3.1911
Iteration: 1817; Percent complete: 45.4%; Average loss: 3.1716
Iteration: 1818; Percent complete: 45.5%; Average loss: 2.9869
Iteration: 1819; Percent complete: 45.5%; Average loss: 3.2687
Iteration: 1820; Percent complete: 45.5%; Average loss: 3.2327
Iteration: 1821; Percent complete: 45.5%; Average loss: 3.3191
Iteration: 1822; Percent complete: 45.6%; Average loss: 3.2466
Iteration: 1823; Percent complete: 45.6%; Average loss: 3.4279
Iteration: 1824; Percent complete: 45.6%; Average loss: 3.4130
Iteration: 1825; Percent complete: 45.6%; Average loss: 3.2090
Iteration: 1826; Percent complete: 45.6%; Average loss: 3.2403
Iteration: 1827; Percent complete: 45.7%; Average loss: 3.1620
Iteration: 1828; Percent complete: 45.7%; Average loss: 3.6215
Iteration: 1829; Percent complete: 45.7%; Average loss: 3.1014
Iteration: 1830; Percent complete: 45.8%; Average loss: 3.0118
Iteration: 1831; Percent complete: 45.8%; Average loss: 2.9912
Iteration: 1832; Percent complete: 45.8%; Average loss: 3.1008
Iteration: 1833; Percent complete: 45.8%; Average loss: 3.1427
Iteration: 1834; Percent complete: 45.9%; Average loss: 3.0986
Iteration: 1835; Percent complete: 45.9%; Average loss: 2.9797
Iteration: 1836; Percent complete: 45.9%; Average loss: 3.1775
Iteration: 1837; Percent complete: 45.9%; Average loss: 3.2430
Iteration: 1838; Percent complete: 46.0%; Average loss: 3.1108
Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2073
Iteration: 1840; Percent complete: 46.0%; Average loss: 3.2350
Iteration: 1841; Percent complete: 46.0%; Average loss: 3.3845
Iteration: 1842; Percent complete: 46.1%; Average loss: 3.2045
Iteration: 1843; Percent complete: 46.1%; Average loss: 3.2833
Iteration: 1844; Percent complete: 46.1%; Average loss: 3.2261
Iteration: 1845; Percent complete: 46.1%; Average loss: 3.2602
Iteration: 1846; Percent complete: 46.2%; Average loss: 3.1464
Iteration: 1847; Percent complete: 46.2%; Average loss: 3.1704
Iteration: 1848; Percent complete: 46.2%; Average loss: 3.1765
Iteration: 1849; Percent complete: 46.2%; Average loss: 3.3754
Iteration: 1850; Percent complete: 46.2%; Average loss: 3.1926
Iteration: 1851; Percent complete: 46.3%; Average loss: 3.2741
Iteration: 1852; Percent complete: 46.3%; Average loss: 3.2449
Iteration: 1853; Percent complete: 46.3%; Average loss: 3.1001
Iteration: 1854; Percent complete: 46.4%; Average loss: 3.2708
Iteration: 1855; Percent complete: 46.4%; Average loss: 3.5114
Iteration: 1856; Percent complete: 46.4%; Average loss: 3.0112
Iteration: 1857; Percent complete: 46.4%; Average loss: 3.2055
Iteration: 1858; Percent complete: 46.5%; Average loss: 3.6327
Iteration: 1859; Percent complete: 46.5%; Average loss: 3.1245
Iteration: 1860; Percent complete: 46.5%; Average loss: 3.0938
Iteration: 1861; Percent complete: 46.5%; Average loss: 3.1358
Iteration: 1862; Percent complete: 46.6%; Average loss: 3.2882
Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2291
Iteration: 1864; Percent complete: 46.6%; Average loss: 3.1844
Iteration: 1865; Percent complete: 46.6%; Average loss: 3.3168
Iteration: 1866; Percent complete: 46.7%; Average loss: 3.3370
Iteration: 1867; Percent complete: 46.7%; Average loss: 3.0526
Iteration: 1868; Percent complete: 46.7%; Average loss: 3.2473
Iteration: 1869; Percent complete: 46.7%; Average loss: 3.0285
Iteration: 1870; Percent complete: 46.8%; Average loss: 3.0706
Iteration: 1871; Percent complete: 46.8%; Average loss: 3.0571
Iteration: 1872; Percent complete: 46.8%; Average loss: 3.2842
Iteration: 1873; Percent complete: 46.8%; Average loss: 2.9786
Iteration: 1874; Percent complete: 46.9%; Average loss: 3.3603
Iteration: 1875; Percent complete: 46.9%; Average loss: 3.0579
Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0983
Iteration: 1877; Percent complete: 46.9%; Average loss: 3.4031
Iteration: 1878; Percent complete: 46.9%; Average loss: 3.0677
Iteration: 1879; Percent complete: 47.0%; Average loss: 2.9285
Iteration: 1880; Percent complete: 47.0%; Average loss: 3.2671
Iteration: 1881; Percent complete: 47.0%; Average loss: 3.2148
Iteration: 1882; Percent complete: 47.0%; Average loss: 2.9378
Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2267
Iteration: 1884; Percent complete: 47.1%; Average loss: 3.3522
Iteration: 1885; Percent complete: 47.1%; Average loss: 3.2891
Iteration: 1886; Percent complete: 47.1%; Average loss: 2.9964
Iteration: 1887; Percent complete: 47.2%; Average loss: 3.0176
Iteration: 1888; Percent complete: 47.2%; Average loss: 3.1454
Iteration: 1889; Percent complete: 47.2%; Average loss: 3.1915
Iteration: 1890; Percent complete: 47.2%; Average loss: 3.2359
Iteration: 1891; Percent complete: 47.3%; Average loss: 3.0962
Iteration: 1892; Percent complete: 47.3%; Average loss: 3.3692
Iteration: 1893; Percent complete: 47.3%; Average loss: 3.3912
Iteration: 1894; Percent complete: 47.3%; Average loss: 3.0405
Iteration: 1895; Percent complete: 47.4%; Average loss: 3.1106
Iteration: 1896; Percent complete: 47.4%; Average loss: 3.0846
Iteration: 1897; Percent complete: 47.4%; Average loss: 3.4277
Iteration: 1898; Percent complete: 47.4%; Average loss: 2.9799
Iteration: 1899; Percent complete: 47.5%; Average loss: 3.1453
Iteration: 1900; Percent complete: 47.5%; Average loss: 3.1121
Iteration: 1901; Percent complete: 47.5%; Average loss: 3.1978
Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1718
Iteration: 1903; Percent complete: 47.6%; Average loss: 3.1191
Iteration: 1904; Percent complete: 47.6%; Average loss: 2.8197
Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1070
Iteration: 1906; Percent complete: 47.6%; Average loss: 3.1851
Iteration: 1907; Percent complete: 47.7%; Average loss: 3.3788
Iteration: 1908; Percent complete: 47.7%; Average loss: 2.8880
Iteration: 1909; Percent complete: 47.7%; Average loss: 2.9629
Iteration: 1910; Percent complete: 47.8%; Average loss: 3.3981
Iteration: 1911; Percent complete: 47.8%; Average loss: 3.0858
Iteration: 1912; Percent complete: 47.8%; Average loss: 3.4882
Iteration: 1913; Percent complete: 47.8%; Average loss: 3.0333
Iteration: 1914; Percent complete: 47.9%; Average loss: 3.2414
Iteration: 1915; Percent complete: 47.9%; Average loss: 2.8341
Iteration: 1916; Percent complete: 47.9%; Average loss: 3.3463
Iteration: 1917; Percent complete: 47.9%; Average loss: 3.0890
Iteration: 1918; Percent complete: 47.9%; Average loss: 2.9568
Iteration: 1919; Percent complete: 48.0%; Average loss: 3.1573
Iteration: 1920; Percent complete: 48.0%; Average loss: 3.2544
Iteration: 1921; Percent complete: 48.0%; Average loss: 3.2395
Iteration: 1922; Percent complete: 48.0%; Average loss: 3.1368
Iteration: 1923; Percent complete: 48.1%; Average loss: 3.2152
Iteration: 1924; Percent complete: 48.1%; Average loss: 3.1945
Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2614
Iteration: 1926; Percent complete: 48.1%; Average loss: 2.9677
Iteration: 1927; Percent complete: 48.2%; Average loss: 2.9873
Iteration: 1928; Percent complete: 48.2%; Average loss: 3.2643
Iteration: 1929; Percent complete: 48.2%; Average loss: 3.2558
Iteration: 1930; Percent complete: 48.2%; Average loss: 3.3733
Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1466
Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2971
Iteration: 1933; Percent complete: 48.3%; Average loss: 3.2172
Iteration: 1934; Percent complete: 48.4%; Average loss: 3.3892
Iteration: 1935; Percent complete: 48.4%; Average loss: 3.1875
Iteration: 1936; Percent complete: 48.4%; Average loss: 3.1404
Iteration: 1937; Percent complete: 48.4%; Average loss: 3.1504
Iteration: 1938; Percent complete: 48.4%; Average loss: 3.3288
Iteration: 1939; Percent complete: 48.5%; Average loss: 3.0991
Iteration: 1940; Percent complete: 48.5%; Average loss: 3.2125
Iteration: 1941; Percent complete: 48.5%; Average loss: 3.0157
Iteration: 1942; Percent complete: 48.5%; Average loss: 3.3990
Iteration: 1943; Percent complete: 48.6%; Average loss: 3.4287
Iteration: 1944; Percent complete: 48.6%; Average loss: 3.3431
Iteration: 1945; Percent complete: 48.6%; Average loss: 3.1859
Iteration: 1946; Percent complete: 48.6%; Average loss: 3.2922
Iteration: 1947; Percent complete: 48.7%; Average loss: 2.9843
Iteration: 1948; Percent complete: 48.7%; Average loss: 3.0806
Iteration: 1949; Percent complete: 48.7%; Average loss: 3.1620
Iteration: 1950; Percent complete: 48.8%; Average loss: 3.0785
Iteration: 1951; Percent complete: 48.8%; Average loss: 3.3131
Iteration: 1952; Percent complete: 48.8%; Average loss: 3.1016
Iteration: 1953; Percent complete: 48.8%; Average loss: 3.0700
Iteration: 1954; Percent complete: 48.9%; Average loss: 3.2372
Iteration: 1955; Percent complete: 48.9%; Average loss: 3.1492
Iteration: 1956; Percent complete: 48.9%; Average loss: 3.3137
Iteration: 1957; Percent complete: 48.9%; Average loss: 3.2153
Iteration: 1958; Percent complete: 48.9%; Average loss: 3.2165
Iteration: 1959; Percent complete: 49.0%; Average loss: 2.8605
Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2427
Iteration: 1961; Percent complete: 49.0%; Average loss: 3.0256
Iteration: 1962; Percent complete: 49.0%; Average loss: 3.2262
Iteration: 1963; Percent complete: 49.1%; Average loss: 3.0678
Iteration: 1964; Percent complete: 49.1%; Average loss: 3.1483
Iteration: 1965; Percent complete: 49.1%; Average loss: 3.4390
Iteration: 1966; Percent complete: 49.1%; Average loss: 3.1905
Iteration: 1967; Percent complete: 49.2%; Average loss: 3.3930
Iteration: 1968; Percent complete: 49.2%; Average loss: 2.9543
Iteration: 1969; Percent complete: 49.2%; Average loss: 2.9691
Iteration: 1970; Percent complete: 49.2%; Average loss: 3.1899
Iteration: 1971; Percent complete: 49.3%; Average loss: 3.3687
Iteration: 1972; Percent complete: 49.3%; Average loss: 3.0602
Iteration: 1973; Percent complete: 49.3%; Average loss: 3.2373
Iteration: 1974; Percent complete: 49.4%; Average loss: 3.0415
Iteration: 1975; Percent complete: 49.4%; Average loss: 3.0197
Iteration: 1976; Percent complete: 49.4%; Average loss: 3.0326
Iteration: 1977; Percent complete: 49.4%; Average loss: 3.1913
Iteration: 1978; Percent complete: 49.5%; Average loss: 2.9980
Iteration: 1979; Percent complete: 49.5%; Average loss: 2.8606
Iteration: 1980; Percent complete: 49.5%; Average loss: 3.2751
Iteration: 1981; Percent complete: 49.5%; Average loss: 3.1523
Iteration: 1982; Percent complete: 49.5%; Average loss: 3.0918
Iteration: 1983; Percent complete: 49.6%; Average loss: 2.9924
Iteration: 1984; Percent complete: 49.6%; Average loss: 3.3509
Iteration: 1985; Percent complete: 49.6%; Average loss: 3.2010
Iteration: 1986; Percent complete: 49.6%; Average loss: 3.0353
Iteration: 1987; Percent complete: 49.7%; Average loss: 3.2421
Iteration: 1988; Percent complete: 49.7%; Average loss: 3.1356
Iteration: 1989; Percent complete: 49.7%; Average loss: 3.4368
Iteration: 1990; Percent complete: 49.8%; Average loss: 3.2295
Iteration: 1991; Percent complete: 49.8%; Average loss: 3.3265
Iteration: 1992; Percent complete: 49.8%; Average loss: 3.3066
Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2507
Iteration: 1994; Percent complete: 49.9%; Average loss: 3.1183
Iteration: 1995; Percent complete: 49.9%; Average loss: 3.2044
Iteration: 1996; Percent complete: 49.9%; Average loss: 3.2723
Iteration: 1997; Percent complete: 49.9%; Average loss: 3.2805
Iteration: 1998; Percent complete: 50.0%; Average loss: 3.3458
Iteration: 1999; Percent complete: 50.0%; Average loss: 3.1202
Iteration: 2000; Percent complete: 50.0%; Average loss: 3.3546
Iteration: 2001; Percent complete: 50.0%; Average loss: 3.1956
Iteration: 2002; Percent complete: 50.0%; Average loss: 3.3476
Iteration: 2003; Percent complete: 50.1%; Average loss: 3.1374
Iteration: 2004; Percent complete: 50.1%; Average loss: 3.0831
Iteration: 2005; Percent complete: 50.1%; Average loss: 2.9801
Iteration: 2006; Percent complete: 50.1%; Average loss: 3.2901
Iteration: 2007; Percent complete: 50.2%; Average loss: 3.1725
Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0565
Iteration: 2009; Percent complete: 50.2%; Average loss: 3.1183
Iteration: 2010; Percent complete: 50.2%; Average loss: 2.9414
Iteration: 2011; Percent complete: 50.3%; Average loss: 3.2771
Iteration: 2012; Percent complete: 50.3%; Average loss: 3.2008
Iteration: 2013; Percent complete: 50.3%; Average loss: 2.9653
Iteration: 2014; Percent complete: 50.3%; Average loss: 3.1646
Iteration: 2015; Percent complete: 50.4%; Average loss: 2.9365
Iteration: 2016; Percent complete: 50.4%; Average loss: 3.1802
Iteration: 2017; Percent complete: 50.4%; Average loss: 3.2576
Iteration: 2018; Percent complete: 50.4%; Average loss: 3.3129
Iteration: 2019; Percent complete: 50.5%; Average loss: 3.2613
Iteration: 2020; Percent complete: 50.5%; Average loss: 3.2725
Iteration: 2021; Percent complete: 50.5%; Average loss: 2.9849
Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0380
Iteration: 2023; Percent complete: 50.6%; Average loss: 3.0434
Iteration: 2024; Percent complete: 50.6%; Average loss: 3.2042
Iteration: 2025; Percent complete: 50.6%; Average loss: 3.2327
Iteration: 2026; Percent complete: 50.6%; Average loss: 3.0675
Iteration: 2027; Percent complete: 50.7%; Average loss: 3.3589
Iteration: 2028; Percent complete: 50.7%; Average loss: 3.0197
Iteration: 2029; Percent complete: 50.7%; Average loss: 3.0023
Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1059
Iteration: 2031; Percent complete: 50.8%; Average loss: 2.8385
Iteration: 2032; Percent complete: 50.8%; Average loss: 2.9412
Iteration: 2033; Percent complete: 50.8%; Average loss: 3.1979
Iteration: 2034; Percent complete: 50.8%; Average loss: 2.8318
Iteration: 2035; Percent complete: 50.9%; Average loss: 3.1291
Iteration: 2036; Percent complete: 50.9%; Average loss: 3.2190
Iteration: 2037; Percent complete: 50.9%; Average loss: 3.3984
Iteration: 2038; Percent complete: 50.9%; Average loss: 3.1209
Iteration: 2039; Percent complete: 51.0%; Average loss: 2.9138
Iteration: 2040; Percent complete: 51.0%; Average loss: 3.1049
Iteration: 2041; Percent complete: 51.0%; Average loss: 3.2126
Iteration: 2042; Percent complete: 51.0%; Average loss: 3.0747
Iteration: 2043; Percent complete: 51.1%; Average loss: 3.2095
Iteration: 2044; Percent complete: 51.1%; Average loss: 2.9979
Iteration: 2045; Percent complete: 51.1%; Average loss: 3.1341
Iteration: 2046; Percent complete: 51.1%; Average loss: 3.1161
Iteration: 2047; Percent complete: 51.2%; Average loss: 3.3410
Iteration: 2048; Percent complete: 51.2%; Average loss: 3.1131
Iteration: 2049; Percent complete: 51.2%; Average loss: 3.0937
Iteration: 2050; Percent complete: 51.2%; Average loss: 3.1124
Iteration: 2051; Percent complete: 51.3%; Average loss: 3.0688
Iteration: 2052; Percent complete: 51.3%; Average loss: 2.9768
Iteration: 2053; Percent complete: 51.3%; Average loss: 3.0976
Iteration: 2054; Percent complete: 51.3%; Average loss: 3.0278
Iteration: 2055; Percent complete: 51.4%; Average loss: 3.3135
Iteration: 2056; Percent complete: 51.4%; Average loss: 3.2416
Iteration: 2057; Percent complete: 51.4%; Average loss: 2.9672
Iteration: 2058; Percent complete: 51.4%; Average loss: 3.0429
Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0737
Iteration: 2060; Percent complete: 51.5%; Average loss: 3.4118
Iteration: 2061; Percent complete: 51.5%; Average loss: 3.1033
Iteration: 2062; Percent complete: 51.5%; Average loss: 3.1453
Iteration: 2063; Percent complete: 51.6%; Average loss: 3.2424
Iteration: 2064; Percent complete: 51.6%; Average loss: 3.2154
Iteration: 2065; Percent complete: 51.6%; Average loss: 3.0748
Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1009
Iteration: 2067; Percent complete: 51.7%; Average loss: 2.8797
Iteration: 2068; Percent complete: 51.7%; Average loss: 3.0679
Iteration: 2069; Percent complete: 51.7%; Average loss: 2.9289
Iteration: 2070; Percent complete: 51.7%; Average loss: 3.2909
Iteration: 2071; Percent complete: 51.8%; Average loss: 3.0431
Iteration: 2072; Percent complete: 51.8%; Average loss: 3.2676
Iteration: 2073; Percent complete: 51.8%; Average loss: 3.0464
Iteration: 2074; Percent complete: 51.8%; Average loss: 3.0591
Iteration: 2075; Percent complete: 51.9%; Average loss: 3.3184
Iteration: 2076; Percent complete: 51.9%; Average loss: 3.2609
Iteration: 2077; Percent complete: 51.9%; Average loss: 3.1005
Iteration: 2078; Percent complete: 51.9%; Average loss: 3.4461
Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2502
Iteration: 2080; Percent complete: 52.0%; Average loss: 3.0639
Iteration: 2081; Percent complete: 52.0%; Average loss: 3.1261
Iteration: 2082; Percent complete: 52.0%; Average loss: 3.1780
Iteration: 2083; Percent complete: 52.1%; Average loss: 3.2539
Iteration: 2084; Percent complete: 52.1%; Average loss: 3.0667
Iteration: 2085; Percent complete: 52.1%; Average loss: 3.4732
Iteration: 2086; Percent complete: 52.1%; Average loss: 3.2303
Iteration: 2087; Percent complete: 52.2%; Average loss: 3.1046
Iteration: 2088; Percent complete: 52.2%; Average loss: 2.9764
Iteration: 2089; Percent complete: 52.2%; Average loss: 3.4462
Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0562
Iteration: 2091; Percent complete: 52.3%; Average loss: 2.9813
Iteration: 2092; Percent complete: 52.3%; Average loss: 3.2426
Iteration: 2093; Percent complete: 52.3%; Average loss: 3.2314
Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0830
Iteration: 2095; Percent complete: 52.4%; Average loss: 3.0841
Iteration: 2096; Percent complete: 52.4%; Average loss: 3.1752
Iteration: 2097; Percent complete: 52.4%; Average loss: 3.2856
Iteration: 2098; Percent complete: 52.4%; Average loss: 3.1171
Iteration: 2099; Percent complete: 52.5%; Average loss: 3.3201
Iteration: 2100; Percent complete: 52.5%; Average loss: 3.2419
Iteration: 2101; Percent complete: 52.5%; Average loss: 3.1215
Iteration: 2102; Percent complete: 52.5%; Average loss: 3.0835
Iteration: 2103; Percent complete: 52.6%; Average loss: 3.1122
Iteration: 2104; Percent complete: 52.6%; Average loss: 3.2119
Iteration: 2105; Percent complete: 52.6%; Average loss: 3.2200
Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0409
Iteration: 2107; Percent complete: 52.7%; Average loss: 3.0825
Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0013
Iteration: 2109; Percent complete: 52.7%; Average loss: 3.0907
Iteration: 2110; Percent complete: 52.8%; Average loss: 3.0096
Iteration: 2111; Percent complete: 52.8%; Average loss: 2.7031
Iteration: 2112; Percent complete: 52.8%; Average loss: 3.1733
Iteration: 2113; Percent complete: 52.8%; Average loss: 3.0530
Iteration: 2114; Percent complete: 52.8%; Average loss: 3.0050
Iteration: 2115; Percent complete: 52.9%; Average loss: 3.2045
Iteration: 2116; Percent complete: 52.9%; Average loss: 3.0263
Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0305
Iteration: 2118; Percent complete: 52.9%; Average loss: 3.0058
Iteration: 2119; Percent complete: 53.0%; Average loss: 3.2442
Iteration: 2120; Percent complete: 53.0%; Average loss: 2.9049
Iteration: 2121; Percent complete: 53.0%; Average loss: 3.1599
Iteration: 2122; Percent complete: 53.0%; Average loss: 3.2436
Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2942
Iteration: 2124; Percent complete: 53.1%; Average loss: 3.2995
Iteration: 2125; Percent complete: 53.1%; Average loss: 3.3042
Iteration: 2126; Percent complete: 53.1%; Average loss: 3.2598
Iteration: 2127; Percent complete: 53.2%; Average loss: 3.1431
Iteration: 2128; Percent complete: 53.2%; Average loss: 2.9628
Iteration: 2129; Percent complete: 53.2%; Average loss: 2.9348
Iteration: 2130; Percent complete: 53.2%; Average loss: 3.0923
Iteration: 2131; Percent complete: 53.3%; Average loss: 3.2826
Iteration: 2132; Percent complete: 53.3%; Average loss: 3.2938
Iteration: 2133; Percent complete: 53.3%; Average loss: 3.3821
Iteration: 2134; Percent complete: 53.3%; Average loss: 2.9238
Iteration: 2135; Percent complete: 53.4%; Average loss: 3.0247
Iteration: 2136; Percent complete: 53.4%; Average loss: 3.2990
Iteration: 2137; Percent complete: 53.4%; Average loss: 2.9808
Iteration: 2138; Percent complete: 53.4%; Average loss: 3.2112
Iteration: 2139; Percent complete: 53.5%; Average loss: 3.2955
Iteration: 2140; Percent complete: 53.5%; Average loss: 3.1112
Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0548
Iteration: 2142; Percent complete: 53.5%; Average loss: 3.1217
Iteration: 2143; Percent complete: 53.6%; Average loss: 3.2373
Iteration: 2144; Percent complete: 53.6%; Average loss: 3.0418
Iteration: 2145; Percent complete: 53.6%; Average loss: 3.1832
Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1107
Iteration: 2147; Percent complete: 53.7%; Average loss: 2.8719
Iteration: 2148; Percent complete: 53.7%; Average loss: 3.1498
Iteration: 2149; Percent complete: 53.7%; Average loss: 3.2259
Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0889
Iteration: 2151; Percent complete: 53.8%; Average loss: 3.0834
Iteration: 2152; Percent complete: 53.8%; Average loss: 3.1860
Iteration: 2153; Percent complete: 53.8%; Average loss: 3.2135
Iteration: 2154; Percent complete: 53.8%; Average loss: 2.9301
Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1562
Iteration: 2156; Percent complete: 53.9%; Average loss: 3.1032
Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2843
Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1409
Iteration: 2159; Percent complete: 54.0%; Average loss: 3.1804
Iteration: 2160; Percent complete: 54.0%; Average loss: 3.0017
Iteration: 2161; Percent complete: 54.0%; Average loss: 3.2979
Iteration: 2162; Percent complete: 54.0%; Average loss: 2.9884
Iteration: 2163; Percent complete: 54.1%; Average loss: 2.9322
Iteration: 2164; Percent complete: 54.1%; Average loss: 3.3154
Iteration: 2165; Percent complete: 54.1%; Average loss: 3.3131
Iteration: 2166; Percent complete: 54.1%; Average loss: 3.0377
Iteration: 2167; Percent complete: 54.2%; Average loss: 3.1361
Iteration: 2168; Percent complete: 54.2%; Average loss: 3.0827
Iteration: 2169; Percent complete: 54.2%; Average loss: 3.0727
Iteration: 2170; Percent complete: 54.2%; Average loss: 3.0143
Iteration: 2171; Percent complete: 54.3%; Average loss: 3.1060
Iteration: 2172; Percent complete: 54.3%; Average loss: 3.2135
Iteration: 2173; Percent complete: 54.3%; Average loss: 2.9126
Iteration: 2174; Percent complete: 54.4%; Average loss: 3.1115
Iteration: 2175; Percent complete: 54.4%; Average loss: 3.1472
Iteration: 2176; Percent complete: 54.4%; Average loss: 3.2552
Iteration: 2177; Percent complete: 54.4%; Average loss: 3.2198
Iteration: 2178; Percent complete: 54.4%; Average loss: 3.2614
Iteration: 2179; Percent complete: 54.5%; Average loss: 3.3390
Iteration: 2180; Percent complete: 54.5%; Average loss: 3.1390
Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9987
Iteration: 2182; Percent complete: 54.5%; Average loss: 2.9912
Iteration: 2183; Percent complete: 54.6%; Average loss: 3.0339
Iteration: 2184; Percent complete: 54.6%; Average loss: 3.1301
Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1500
Iteration: 2186; Percent complete: 54.6%; Average loss: 3.0220
Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1923
Iteration: 2188; Percent complete: 54.7%; Average loss: 2.8926
Iteration: 2189; Percent complete: 54.7%; Average loss: 3.4892
Iteration: 2190; Percent complete: 54.8%; Average loss: 3.0132
Iteration: 2191; Percent complete: 54.8%; Average loss: 3.1026
Iteration: 2192; Percent complete: 54.8%; Average loss: 3.1462
Iteration: 2193; Percent complete: 54.8%; Average loss: 2.8901
Iteration: 2194; Percent complete: 54.9%; Average loss: 2.8679
Iteration: 2195; Percent complete: 54.9%; Average loss: 3.0384
Iteration: 2196; Percent complete: 54.9%; Average loss: 3.2498
Iteration: 2197; Percent complete: 54.9%; Average loss: 3.4152
Iteration: 2198; Percent complete: 54.9%; Average loss: 3.2113
Iteration: 2199; Percent complete: 55.0%; Average loss: 2.8610
Iteration: 2200; Percent complete: 55.0%; Average loss: 3.0128
Iteration: 2201; Percent complete: 55.0%; Average loss: 2.9122
Iteration: 2202; Percent complete: 55.0%; Average loss: 3.2180
Iteration: 2203; Percent complete: 55.1%; Average loss: 2.9439
Iteration: 2204; Percent complete: 55.1%; Average loss: 3.3303
Iteration: 2205; Percent complete: 55.1%; Average loss: 3.0232
Iteration: 2206; Percent complete: 55.1%; Average loss: 3.1805
Iteration: 2207; Percent complete: 55.2%; Average loss: 3.0913
Iteration: 2208; Percent complete: 55.2%; Average loss: 2.9517
Iteration: 2209; Percent complete: 55.2%; Average loss: 3.2063
Iteration: 2210; Percent complete: 55.2%; Average loss: 3.1697
Iteration: 2211; Percent complete: 55.3%; Average loss: 3.1781
Iteration: 2212; Percent complete: 55.3%; Average loss: 3.2732
Iteration: 2213; Percent complete: 55.3%; Average loss: 2.8888
Iteration: 2214; Percent complete: 55.4%; Average loss: 2.8368
Iteration: 2215; Percent complete: 55.4%; Average loss: 2.8400
Iteration: 2216; Percent complete: 55.4%; Average loss: 3.0987
Iteration: 2217; Percent complete: 55.4%; Average loss: 3.1582
Iteration: 2218; Percent complete: 55.5%; Average loss: 2.7920
Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0000
Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9638
Iteration: 2221; Percent complete: 55.5%; Average loss: 3.0302
Iteration: 2222; Percent complete: 55.5%; Average loss: 3.0181
Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0998
Iteration: 2224; Percent complete: 55.6%; Average loss: 3.0554
Iteration: 2225; Percent complete: 55.6%; Average loss: 2.8464
Iteration: 2226; Percent complete: 55.6%; Average loss: 3.0448
Iteration: 2227; Percent complete: 55.7%; Average loss: 3.0630
Iteration: 2228; Percent complete: 55.7%; Average loss: 3.1950
Iteration: 2229; Percent complete: 55.7%; Average loss: 3.1169
Iteration: 2230; Percent complete: 55.8%; Average loss: 3.1213
Iteration: 2231; Percent complete: 55.8%; Average loss: 3.1055
Iteration: 2232; Percent complete: 55.8%; Average loss: 2.8968
Iteration: 2233; Percent complete: 55.8%; Average loss: 3.2895
Iteration: 2234; Percent complete: 55.9%; Average loss: 2.8392
Iteration: 2235; Percent complete: 55.9%; Average loss: 3.0391
Iteration: 2236; Percent complete: 55.9%; Average loss: 2.7693
Iteration: 2237; Percent complete: 55.9%; Average loss: 3.0824
Iteration: 2238; Percent complete: 56.0%; Average loss: 3.0933
Iteration: 2239; Percent complete: 56.0%; Average loss: 3.3870
Iteration: 2240; Percent complete: 56.0%; Average loss: 3.0724
Iteration: 2241; Percent complete: 56.0%; Average loss: 3.1873
Iteration: 2242; Percent complete: 56.0%; Average loss: 3.1191
Iteration: 2243; Percent complete: 56.1%; Average loss: 3.2350
Iteration: 2244; Percent complete: 56.1%; Average loss: 3.1224
Iteration: 2245; Percent complete: 56.1%; Average loss: 2.9236
Iteration: 2246; Percent complete: 56.1%; Average loss: 3.2879
Iteration: 2247; Percent complete: 56.2%; Average loss: 3.1348
Iteration: 2248; Percent complete: 56.2%; Average loss: 3.3120
Iteration: 2249; Percent complete: 56.2%; Average loss: 2.8640
Iteration: 2250; Percent complete: 56.2%; Average loss: 2.8977
Iteration: 2251; Percent complete: 56.3%; Average loss: 3.1312
Iteration: 2252; Percent complete: 56.3%; Average loss: 3.1883
Iteration: 2253; Percent complete: 56.3%; Average loss: 2.9763
Iteration: 2254; Percent complete: 56.4%; Average loss: 3.0954
Iteration: 2255; Percent complete: 56.4%; Average loss: 3.1808
Iteration: 2256; Percent complete: 56.4%; Average loss: 2.9695
Iteration: 2257; Percent complete: 56.4%; Average loss: 3.1071
Iteration: 2258; Percent complete: 56.5%; Average loss: 3.2404
Iteration: 2259; Percent complete: 56.5%; Average loss: 2.8706
Iteration: 2260; Percent complete: 56.5%; Average loss: 3.0331
Iteration: 2261; Percent complete: 56.5%; Average loss: 3.2041
Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0222
Iteration: 2263; Percent complete: 56.6%; Average loss: 3.1625
Iteration: 2264; Percent complete: 56.6%; Average loss: 3.0137
Iteration: 2265; Percent complete: 56.6%; Average loss: 3.0775
Iteration: 2266; Percent complete: 56.6%; Average loss: 3.0243
Iteration: 2267; Percent complete: 56.7%; Average loss: 3.0034
Iteration: 2268; Percent complete: 56.7%; Average loss: 3.2269
Iteration: 2269; Percent complete: 56.7%; Average loss: 3.2067
Iteration: 2270; Percent complete: 56.8%; Average loss: 2.7770
Iteration: 2271; Percent complete: 56.8%; Average loss: 2.9114
Iteration: 2272; Percent complete: 56.8%; Average loss: 3.1411
Iteration: 2273; Percent complete: 56.8%; Average loss: 2.7882
Iteration: 2274; Percent complete: 56.9%; Average loss: 3.2345
Iteration: 2275; Percent complete: 56.9%; Average loss: 2.9891
Iteration: 2276; Percent complete: 56.9%; Average loss: 3.1682
Iteration: 2277; Percent complete: 56.9%; Average loss: 3.0280
Iteration: 2278; Percent complete: 57.0%; Average loss: 3.1017
Iteration: 2279; Percent complete: 57.0%; Average loss: 3.0785
Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0376
Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1108
Iteration: 2282; Percent complete: 57.0%; Average loss: 2.9810
Iteration: 2283; Percent complete: 57.1%; Average loss: 3.1257
Iteration: 2284; Percent complete: 57.1%; Average loss: 3.0359
Iteration: 2285; Percent complete: 57.1%; Average loss: 3.0268
Iteration: 2286; Percent complete: 57.1%; Average loss: 3.0498
Iteration: 2287; Percent complete: 57.2%; Average loss: 3.0149
Iteration: 2288; Percent complete: 57.2%; Average loss: 3.0173
Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0555
Iteration: 2290; Percent complete: 57.2%; Average loss: 3.0348
Iteration: 2291; Percent complete: 57.3%; Average loss: 2.9604
Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0491
Iteration: 2293; Percent complete: 57.3%; Average loss: 3.1823
Iteration: 2294; Percent complete: 57.4%; Average loss: 2.9983
Iteration: 2295; Percent complete: 57.4%; Average loss: 2.9055
Iteration: 2296; Percent complete: 57.4%; Average loss: 3.0790
Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0457
Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0458
Iteration: 2299; Percent complete: 57.5%; Average loss: 2.9167
Iteration: 2300; Percent complete: 57.5%; Average loss: 3.0950
Iteration: 2301; Percent complete: 57.5%; Average loss: 2.9468
Iteration: 2302; Percent complete: 57.6%; Average loss: 3.2159
Iteration: 2303; Percent complete: 57.6%; Average loss: 3.1529
Iteration: 2304; Percent complete: 57.6%; Average loss: 3.0623
Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1036
Iteration: 2306; Percent complete: 57.6%; Average loss: 3.2615
Iteration: 2307; Percent complete: 57.7%; Average loss: 3.1033
Iteration: 2308; Percent complete: 57.7%; Average loss: 2.8780
Iteration: 2309; Percent complete: 57.7%; Average loss: 3.1406
Iteration: 2310; Percent complete: 57.8%; Average loss: 3.3305
Iteration: 2311; Percent complete: 57.8%; Average loss: 2.8320
Iteration: 2312; Percent complete: 57.8%; Average loss: 3.0199
Iteration: 2313; Percent complete: 57.8%; Average loss: 3.1209
Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2403
Iteration: 2315; Percent complete: 57.9%; Average loss: 3.1784
Iteration: 2316; Percent complete: 57.9%; Average loss: 3.0789
Iteration: 2317; Percent complete: 57.9%; Average loss: 3.1977
Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0165
Iteration: 2319; Percent complete: 58.0%; Average loss: 2.8454
Iteration: 2320; Percent complete: 58.0%; Average loss: 3.0900
Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0062
Iteration: 2322; Percent complete: 58.1%; Average loss: 3.1715
Iteration: 2323; Percent complete: 58.1%; Average loss: 3.0566
Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9597
Iteration: 2325; Percent complete: 58.1%; Average loss: 2.8965
Iteration: 2326; Percent complete: 58.1%; Average loss: 2.9583
Iteration: 2327; Percent complete: 58.2%; Average loss: 3.1536
Iteration: 2328; Percent complete: 58.2%; Average loss: 2.8782
Iteration: 2329; Percent complete: 58.2%; Average loss: 3.0814
Iteration: 2330; Percent complete: 58.2%; Average loss: 2.8340
Iteration: 2331; Percent complete: 58.3%; Average loss: 3.0194
Iteration: 2332; Percent complete: 58.3%; Average loss: 3.0427
Iteration: 2333; Percent complete: 58.3%; Average loss: 3.1660
Iteration: 2334; Percent complete: 58.4%; Average loss: 2.9728
Iteration: 2335; Percent complete: 58.4%; Average loss: 3.2884
Iteration: 2336; Percent complete: 58.4%; Average loss: 2.8075
Iteration: 2337; Percent complete: 58.4%; Average loss: 3.0595
Iteration: 2338; Percent complete: 58.5%; Average loss: 3.3721
Iteration: 2339; Percent complete: 58.5%; Average loss: 2.9347
Iteration: 2340; Percent complete: 58.5%; Average loss: 3.0833
Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9608
Iteration: 2342; Percent complete: 58.6%; Average loss: 3.1427
Iteration: 2343; Percent complete: 58.6%; Average loss: 2.9137
Iteration: 2344; Percent complete: 58.6%; Average loss: 3.2405
Iteration: 2345; Percent complete: 58.6%; Average loss: 3.1545
Iteration: 2346; Percent complete: 58.7%; Average loss: 3.0198
Iteration: 2347; Percent complete: 58.7%; Average loss: 3.2179
Iteration: 2348; Percent complete: 58.7%; Average loss: 2.9848
Iteration: 2349; Percent complete: 58.7%; Average loss: 3.0862
Iteration: 2350; Percent complete: 58.8%; Average loss: 3.1892
Iteration: 2351; Percent complete: 58.8%; Average loss: 3.0031
Iteration: 2352; Percent complete: 58.8%; Average loss: 2.9914
Iteration: 2353; Percent complete: 58.8%; Average loss: 3.0637
Iteration: 2354; Percent complete: 58.9%; Average loss: 3.1568
Iteration: 2355; Percent complete: 58.9%; Average loss: 3.1258
Iteration: 2356; Percent complete: 58.9%; Average loss: 3.2422
Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0489
Iteration: 2358; Percent complete: 59.0%; Average loss: 3.1715
Iteration: 2359; Percent complete: 59.0%; Average loss: 3.0309
Iteration: 2360; Percent complete: 59.0%; Average loss: 3.1805
Iteration: 2361; Percent complete: 59.0%; Average loss: 3.1988
Iteration: 2362; Percent complete: 59.1%; Average loss: 3.4646
Iteration: 2363; Percent complete: 59.1%; Average loss: 2.9572
Iteration: 2364; Percent complete: 59.1%; Average loss: 3.2112
Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0356
Iteration: 2366; Percent complete: 59.2%; Average loss: 3.0455
Iteration: 2367; Percent complete: 59.2%; Average loss: 2.9878
Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0219
Iteration: 2369; Percent complete: 59.2%; Average loss: 3.1053
Iteration: 2370; Percent complete: 59.2%; Average loss: 3.1336
Iteration: 2371; Percent complete: 59.3%; Average loss: 2.9981
Iteration: 2372; Percent complete: 59.3%; Average loss: 2.8602
Iteration: 2373; Percent complete: 59.3%; Average loss: 2.8818
Iteration: 2374; Percent complete: 59.4%; Average loss: 2.8658
Iteration: 2375; Percent complete: 59.4%; Average loss: 3.1041
Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0129
Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9055
Iteration: 2378; Percent complete: 59.5%; Average loss: 3.0099
Iteration: 2379; Percent complete: 59.5%; Average loss: 2.8627
Iteration: 2380; Percent complete: 59.5%; Average loss: 3.2245
Iteration: 2381; Percent complete: 59.5%; Average loss: 2.9531
Iteration: 2382; Percent complete: 59.6%; Average loss: 3.1522
Iteration: 2383; Percent complete: 59.6%; Average loss: 2.9352
Iteration: 2384; Percent complete: 59.6%; Average loss: 2.9468
Iteration: 2385; Percent complete: 59.6%; Average loss: 3.0468
Iteration: 2386; Percent complete: 59.7%; Average loss: 3.3041
Iteration: 2387; Percent complete: 59.7%; Average loss: 3.1390
Iteration: 2388; Percent complete: 59.7%; Average loss: 3.0712
Iteration: 2389; Percent complete: 59.7%; Average loss: 3.2613
Iteration: 2390; Percent complete: 59.8%; Average loss: 3.1104
Iteration: 2391; Percent complete: 59.8%; Average loss: 3.1027
Iteration: 2392; Percent complete: 59.8%; Average loss: 2.9220
Iteration: 2393; Percent complete: 59.8%; Average loss: 2.7582
Iteration: 2394; Percent complete: 59.9%; Average loss: 2.9519
Iteration: 2395; Percent complete: 59.9%; Average loss: 3.0700
Iteration: 2396; Percent complete: 59.9%; Average loss: 2.9421
Iteration: 2397; Percent complete: 59.9%; Average loss: 3.1277
Iteration: 2398; Percent complete: 60.0%; Average loss: 3.1273
Iteration: 2399; Percent complete: 60.0%; Average loss: 2.8617
Iteration: 2400; Percent complete: 60.0%; Average loss: 3.2461
Iteration: 2401; Percent complete: 60.0%; Average loss: 2.9264
Iteration: 2402; Percent complete: 60.1%; Average loss: 3.0161
Iteration: 2403; Percent complete: 60.1%; Average loss: 3.1109
Iteration: 2404; Percent complete: 60.1%; Average loss: 3.2606
Iteration: 2405; Percent complete: 60.1%; Average loss: 2.8673
Iteration: 2406; Percent complete: 60.2%; Average loss: 2.9993
Iteration: 2407; Percent complete: 60.2%; Average loss: 3.2134
Iteration: 2408; Percent complete: 60.2%; Average loss: 2.8263
Iteration: 2409; Percent complete: 60.2%; Average loss: 2.7882
Iteration: 2410; Percent complete: 60.2%; Average loss: 3.0431
Iteration: 2411; Percent complete: 60.3%; Average loss: 3.0518
Iteration: 2412; Percent complete: 60.3%; Average loss: 3.0085
Iteration: 2413; Percent complete: 60.3%; Average loss: 3.1448
Iteration: 2414; Percent complete: 60.4%; Average loss: 3.1769
Iteration: 2415; Percent complete: 60.4%; Average loss: 2.9600
Iteration: 2416; Percent complete: 60.4%; Average loss: 3.2283
Iteration: 2417; Percent complete: 60.4%; Average loss: 3.0538
Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9939
Iteration: 2419; Percent complete: 60.5%; Average loss: 3.1233
Iteration: 2420; Percent complete: 60.5%; Average loss: 2.7492
Iteration: 2421; Percent complete: 60.5%; Average loss: 3.0136
Iteration: 2422; Percent complete: 60.6%; Average loss: 2.7565
Iteration: 2423; Percent complete: 60.6%; Average loss: 3.0735
Iteration: 2424; Percent complete: 60.6%; Average loss: 3.1032
Iteration: 2425; Percent complete: 60.6%; Average loss: 2.8280
Iteration: 2426; Percent complete: 60.7%; Average loss: 3.1329
Iteration: 2427; Percent complete: 60.7%; Average loss: 2.9362
Iteration: 2428; Percent complete: 60.7%; Average loss: 3.1600
Iteration: 2429; Percent complete: 60.7%; Average loss: 2.9634
Iteration: 2430; Percent complete: 60.8%; Average loss: 3.2153
Iteration: 2431; Percent complete: 60.8%; Average loss: 3.1281
Iteration: 2432; Percent complete: 60.8%; Average loss: 3.0623
Iteration: 2433; Percent complete: 60.8%; Average loss: 2.9320
Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0338
Iteration: 2435; Percent complete: 60.9%; Average loss: 2.9103
Iteration: 2436; Percent complete: 60.9%; Average loss: 3.0746
Iteration: 2437; Percent complete: 60.9%; Average loss: 3.3086
Iteration: 2438; Percent complete: 61.0%; Average loss: 3.0874
Iteration: 2439; Percent complete: 61.0%; Average loss: 3.1493
Iteration: 2440; Percent complete: 61.0%; Average loss: 3.0229
Iteration: 2441; Percent complete: 61.0%; Average loss: 3.0438
Iteration: 2442; Percent complete: 61.1%; Average loss: 2.9483
Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1269
Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0764
Iteration: 2445; Percent complete: 61.1%; Average loss: 3.2643
Iteration: 2446; Percent complete: 61.2%; Average loss: 2.9247
Iteration: 2447; Percent complete: 61.2%; Average loss: 3.0072
Iteration: 2448; Percent complete: 61.2%; Average loss: 2.8210
Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0877
Iteration: 2450; Percent complete: 61.3%; Average loss: 3.1041
Iteration: 2451; Percent complete: 61.3%; Average loss: 2.9320
Iteration: 2452; Percent complete: 61.3%; Average loss: 3.0115
Iteration: 2453; Percent complete: 61.3%; Average loss: 2.8060
Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0587
Iteration: 2455; Percent complete: 61.4%; Average loss: 3.0052
Iteration: 2456; Percent complete: 61.4%; Average loss: 3.2530
Iteration: 2457; Percent complete: 61.4%; Average loss: 3.0144
Iteration: 2458; Percent complete: 61.5%; Average loss: 3.3349
Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8693
Iteration: 2460; Percent complete: 61.5%; Average loss: 2.9783
Iteration: 2461; Percent complete: 61.5%; Average loss: 2.8159
Iteration: 2462; Percent complete: 61.6%; Average loss: 3.0540
Iteration: 2463; Percent complete: 61.6%; Average loss: 3.0719
Iteration: 2464; Percent complete: 61.6%; Average loss: 3.3003
Iteration: 2465; Percent complete: 61.6%; Average loss: 3.0105
Iteration: 2466; Percent complete: 61.7%; Average loss: 3.0307
Iteration: 2467; Percent complete: 61.7%; Average loss: 3.0135
Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0549
Iteration: 2469; Percent complete: 61.7%; Average loss: 2.9122
Iteration: 2470; Percent complete: 61.8%; Average loss: 2.8348
Iteration: 2471; Percent complete: 61.8%; Average loss: 3.0166
Iteration: 2472; Percent complete: 61.8%; Average loss: 2.9974
Iteration: 2473; Percent complete: 61.8%; Average loss: 2.9090
Iteration: 2474; Percent complete: 61.9%; Average loss: 2.8073
Iteration: 2475; Percent complete: 61.9%; Average loss: 3.0480
Iteration: 2476; Percent complete: 61.9%; Average loss: 3.1765
Iteration: 2477; Percent complete: 61.9%; Average loss: 3.1683
Iteration: 2478; Percent complete: 62.0%; Average loss: 2.9080
Iteration: 2479; Percent complete: 62.0%; Average loss: 2.9812
Iteration: 2480; Percent complete: 62.0%; Average loss: 2.9407
Iteration: 2481; Percent complete: 62.0%; Average loss: 3.0896
Iteration: 2482; Percent complete: 62.1%; Average loss: 2.7915
Iteration: 2483; Percent complete: 62.1%; Average loss: 2.9394
Iteration: 2484; Percent complete: 62.1%; Average loss: 3.2039
Iteration: 2485; Percent complete: 62.1%; Average loss: 2.7345
Iteration: 2486; Percent complete: 62.2%; Average loss: 2.7746
Iteration: 2487; Percent complete: 62.2%; Average loss: 2.9798
Iteration: 2488; Percent complete: 62.2%; Average loss: 3.0595
Iteration: 2489; Percent complete: 62.2%; Average loss: 3.1850
Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1692
Iteration: 2491; Percent complete: 62.3%; Average loss: 3.3004
Iteration: 2492; Percent complete: 62.3%; Average loss: 2.9401
Iteration: 2493; Percent complete: 62.3%; Average loss: 2.9413
Iteration: 2494; Percent complete: 62.4%; Average loss: 2.7718
Iteration: 2495; Percent complete: 62.4%; Average loss: 3.0345
Iteration: 2496; Percent complete: 62.4%; Average loss: 2.8252
Iteration: 2497; Percent complete: 62.4%; Average loss: 2.9005
Iteration: 2498; Percent complete: 62.5%; Average loss: 3.2856
Iteration: 2499; Percent complete: 62.5%; Average loss: 2.8441
Iteration: 2500; Percent complete: 62.5%; Average loss: 2.8413
Iteration: 2501; Percent complete: 62.5%; Average loss: 3.1272
Iteration: 2502; Percent complete: 62.5%; Average loss: 2.7544
Iteration: 2503; Percent complete: 62.6%; Average loss: 2.8886
Iteration: 2504; Percent complete: 62.6%; Average loss: 3.0993
Iteration: 2505; Percent complete: 62.6%; Average loss: 3.2615
Iteration: 2506; Percent complete: 62.6%; Average loss: 2.9981
Iteration: 2507; Percent complete: 62.7%; Average loss: 3.2945
Iteration: 2508; Percent complete: 62.7%; Average loss: 2.8503
Iteration: 2509; Percent complete: 62.7%; Average loss: 2.9599
Iteration: 2510; Percent complete: 62.7%; Average loss: 2.9801
Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0135
Iteration: 2512; Percent complete: 62.8%; Average loss: 3.0740
Iteration: 2513; Percent complete: 62.8%; Average loss: 3.1955
Iteration: 2514; Percent complete: 62.8%; Average loss: 2.9684
Iteration: 2515; Percent complete: 62.9%; Average loss: 2.9019
Iteration: 2516; Percent complete: 62.9%; Average loss: 3.0028
Iteration: 2517; Percent complete: 62.9%; Average loss: 2.8783
Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8856
Iteration: 2519; Percent complete: 63.0%; Average loss: 3.0284
Iteration: 2520; Percent complete: 63.0%; Average loss: 2.9636
Iteration: 2521; Percent complete: 63.0%; Average loss: 2.7766
Iteration: 2522; Percent complete: 63.0%; Average loss: 2.7459
Iteration: 2523; Percent complete: 63.1%; Average loss: 3.1688
Iteration: 2524; Percent complete: 63.1%; Average loss: 2.9331
Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9748
Iteration: 2526; Percent complete: 63.1%; Average loss: 3.2294
Iteration: 2527; Percent complete: 63.2%; Average loss: 3.1383
Iteration: 2528; Percent complete: 63.2%; Average loss: 2.9339
Iteration: 2529; Percent complete: 63.2%; Average loss: 3.0296
Iteration: 2530; Percent complete: 63.2%; Average loss: 3.1753
Iteration: 2531; Percent complete: 63.3%; Average loss: 2.6389
Iteration: 2532; Percent complete: 63.3%; Average loss: 2.7770
Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1313
Iteration: 2534; Percent complete: 63.3%; Average loss: 2.7986
Iteration: 2535; Percent complete: 63.4%; Average loss: 3.3283
Iteration: 2536; Percent complete: 63.4%; Average loss: 2.8831
Iteration: 2537; Percent complete: 63.4%; Average loss: 3.2484
Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1242
Iteration: 2539; Percent complete: 63.5%; Average loss: 3.0232
Iteration: 2540; Percent complete: 63.5%; Average loss: 3.0750
Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0290
Iteration: 2542; Percent complete: 63.5%; Average loss: 3.1119
Iteration: 2543; Percent complete: 63.6%; Average loss: 3.1334
Iteration: 2544; Percent complete: 63.6%; Average loss: 3.2672
Iteration: 2545; Percent complete: 63.6%; Average loss: 3.1018
Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0342
Iteration: 2547; Percent complete: 63.7%; Average loss: 3.0437
Iteration: 2548; Percent complete: 63.7%; Average loss: 2.8805
Iteration: 2549; Percent complete: 63.7%; Average loss: 2.9823
Iteration: 2550; Percent complete: 63.7%; Average loss: 2.8398
Iteration: 2551; Percent complete: 63.8%; Average loss: 3.1862
Iteration: 2552; Percent complete: 63.8%; Average loss: 2.8505
Iteration: 2553; Percent complete: 63.8%; Average loss: 2.8340
Iteration: 2554; Percent complete: 63.8%; Average loss: 3.2693
Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0484
Iteration: 2556; Percent complete: 63.9%; Average loss: 3.1435
Iteration: 2557; Percent complete: 63.9%; Average loss: 3.0810
Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0952
Iteration: 2559; Percent complete: 64.0%; Average loss: 2.8666
Iteration: 2560; Percent complete: 64.0%; Average loss: 3.0456
Iteration: 2561; Percent complete: 64.0%; Average loss: 2.8361
Iteration: 2562; Percent complete: 64.0%; Average loss: 2.8853
Iteration: 2563; Percent complete: 64.1%; Average loss: 3.0061
Iteration: 2564; Percent complete: 64.1%; Average loss: 2.7737
Iteration: 2565; Percent complete: 64.1%; Average loss: 3.1567
Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9995
Iteration: 2567; Percent complete: 64.2%; Average loss: 3.1011
Iteration: 2568; Percent complete: 64.2%; Average loss: 2.9193
Iteration: 2569; Percent complete: 64.2%; Average loss: 3.0728
Iteration: 2570; Percent complete: 64.2%; Average loss: 2.9686
Iteration: 2571; Percent complete: 64.3%; Average loss: 2.9707
Iteration: 2572; Percent complete: 64.3%; Average loss: 2.9283
Iteration: 2573; Percent complete: 64.3%; Average loss: 2.8691
Iteration: 2574; Percent complete: 64.3%; Average loss: 2.8737
Iteration: 2575; Percent complete: 64.4%; Average loss: 3.0436
Iteration: 2576; Percent complete: 64.4%; Average loss: 3.0803
Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9889
Iteration: 2578; Percent complete: 64.5%; Average loss: 2.7886
Iteration: 2579; Percent complete: 64.5%; Average loss: 3.1133
Iteration: 2580; Percent complete: 64.5%; Average loss: 3.1995
Iteration: 2581; Percent complete: 64.5%; Average loss: 2.8235
Iteration: 2582; Percent complete: 64.5%; Average loss: 2.8938
Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8342
Iteration: 2584; Percent complete: 64.6%; Average loss: 3.0567
Iteration: 2585; Percent complete: 64.6%; Average loss: 2.9049
Iteration: 2586; Percent complete: 64.6%; Average loss: 3.0516
Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0693
Iteration: 2588; Percent complete: 64.7%; Average loss: 3.0267
Iteration: 2589; Percent complete: 64.7%; Average loss: 2.9125
Iteration: 2590; Percent complete: 64.8%; Average loss: 3.0484
Iteration: 2591; Percent complete: 64.8%; Average loss: 2.8200
Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9754
Iteration: 2593; Percent complete: 64.8%; Average loss: 2.7906
Iteration: 2594; Percent complete: 64.8%; Average loss: 2.9763
Iteration: 2595; Percent complete: 64.9%; Average loss: 3.0039
Iteration: 2596; Percent complete: 64.9%; Average loss: 2.8341
Iteration: 2597; Percent complete: 64.9%; Average loss: 2.8217
Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0818
Iteration: 2599; Percent complete: 65.0%; Average loss: 2.8517
Iteration: 2600; Percent complete: 65.0%; Average loss: 3.0641
Iteration: 2601; Percent complete: 65.0%; Average loss: 2.8008
Iteration: 2602; Percent complete: 65.0%; Average loss: 2.9656
Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0249
Iteration: 2604; Percent complete: 65.1%; Average loss: 3.0578
Iteration: 2605; Percent complete: 65.1%; Average loss: 3.0076
Iteration: 2606; Percent complete: 65.1%; Average loss: 3.1911
Iteration: 2607; Percent complete: 65.2%; Average loss: 2.9554
Iteration: 2608; Percent complete: 65.2%; Average loss: 3.3508
Iteration: 2609; Percent complete: 65.2%; Average loss: 2.7313
Iteration: 2610; Percent complete: 65.2%; Average loss: 2.8613
Iteration: 2611; Percent complete: 65.3%; Average loss: 2.9278
Iteration: 2612; Percent complete: 65.3%; Average loss: 2.8899
Iteration: 2613; Percent complete: 65.3%; Average loss: 2.9105
Iteration: 2614; Percent complete: 65.3%; Average loss: 2.9889
Iteration: 2615; Percent complete: 65.4%; Average loss: 3.2389
Iteration: 2616; Percent complete: 65.4%; Average loss: 3.1052
Iteration: 2617; Percent complete: 65.4%; Average loss: 2.8186
Iteration: 2618; Percent complete: 65.5%; Average loss: 2.7755
Iteration: 2619; Percent complete: 65.5%; Average loss: 2.9713
Iteration: 2620; Percent complete: 65.5%; Average loss: 2.8084
Iteration: 2621; Percent complete: 65.5%; Average loss: 3.0791
Iteration: 2622; Percent complete: 65.5%; Average loss: 3.1711
Iteration: 2623; Percent complete: 65.6%; Average loss: 2.8928
Iteration: 2624; Percent complete: 65.6%; Average loss: 2.8690
Iteration: 2625; Percent complete: 65.6%; Average loss: 3.1626
Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0258
Iteration: 2627; Percent complete: 65.7%; Average loss: 3.0813
Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0679
Iteration: 2629; Percent complete: 65.7%; Average loss: 3.2515
Iteration: 2630; Percent complete: 65.8%; Average loss: 3.0112
Iteration: 2631; Percent complete: 65.8%; Average loss: 3.2863
Iteration: 2632; Percent complete: 65.8%; Average loss: 2.7774
Iteration: 2633; Percent complete: 65.8%; Average loss: 3.0006
Iteration: 2634; Percent complete: 65.8%; Average loss: 2.9435
Iteration: 2635; Percent complete: 65.9%; Average loss: 2.6353
Iteration: 2636; Percent complete: 65.9%; Average loss: 2.9537
Iteration: 2637; Percent complete: 65.9%; Average loss: 2.9645
Iteration: 2638; Percent complete: 66.0%; Average loss: 2.9122
Iteration: 2639; Percent complete: 66.0%; Average loss: 2.7983
Iteration: 2640; Percent complete: 66.0%; Average loss: 3.1623
Iteration: 2641; Percent complete: 66.0%; Average loss: 2.8879
Iteration: 2642; Percent complete: 66.0%; Average loss: 2.8149
Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8597
Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9114
Iteration: 2645; Percent complete: 66.1%; Average loss: 2.9978
Iteration: 2646; Percent complete: 66.1%; Average loss: 3.0980
Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9715
Iteration: 2648; Percent complete: 66.2%; Average loss: 3.1596
Iteration: 2649; Percent complete: 66.2%; Average loss: 2.7925
Iteration: 2650; Percent complete: 66.2%; Average loss: 2.8568
Iteration: 2651; Percent complete: 66.3%; Average loss: 3.0559
Iteration: 2652; Percent complete: 66.3%; Average loss: 2.7448
Iteration: 2653; Percent complete: 66.3%; Average loss: 3.1605
Iteration: 2654; Percent complete: 66.3%; Average loss: 2.9927
Iteration: 2655; Percent complete: 66.4%; Average loss: 2.7089
Iteration: 2656; Percent complete: 66.4%; Average loss: 2.9251
Iteration: 2657; Percent complete: 66.4%; Average loss: 2.8872
Iteration: 2658; Percent complete: 66.5%; Average loss: 2.9504
Iteration: 2659; Percent complete: 66.5%; Average loss: 3.1010
Iteration: 2660; Percent complete: 66.5%; Average loss: 2.8557
Iteration: 2661; Percent complete: 66.5%; Average loss: 2.8999
Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0074
Iteration: 2663; Percent complete: 66.6%; Average loss: 2.8513
Iteration: 2664; Percent complete: 66.6%; Average loss: 3.0145
Iteration: 2665; Percent complete: 66.6%; Average loss: 3.0367
Iteration: 2666; Percent complete: 66.6%; Average loss: 2.8531
Iteration: 2667; Percent complete: 66.7%; Average loss: 3.0589
Iteration: 2668; Percent complete: 66.7%; Average loss: 3.0710
Iteration: 2669; Percent complete: 66.7%; Average loss: 2.7680
Iteration: 2670; Percent complete: 66.8%; Average loss: 3.0574
Iteration: 2671; Percent complete: 66.8%; Average loss: 3.0735
Iteration: 2672; Percent complete: 66.8%; Average loss: 3.0834
Iteration: 2673; Percent complete: 66.8%; Average loss: 2.8882
Iteration: 2674; Percent complete: 66.8%; Average loss: 2.9548
Iteration: 2675; Percent complete: 66.9%; Average loss: 2.9422
Iteration: 2676; Percent complete: 66.9%; Average loss: 2.7966
Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9648
Iteration: 2678; Percent complete: 67.0%; Average loss: 3.0058
Iteration: 2679; Percent complete: 67.0%; Average loss: 2.8208
Iteration: 2680; Percent complete: 67.0%; Average loss: 3.1237
Iteration: 2681; Percent complete: 67.0%; Average loss: 3.0942
Iteration: 2682; Percent complete: 67.0%; Average loss: 2.8342
Iteration: 2683; Percent complete: 67.1%; Average loss: 2.7847
Iteration: 2684; Percent complete: 67.1%; Average loss: 2.8463
Iteration: 2685; Percent complete: 67.1%; Average loss: 3.0027
Iteration: 2686; Percent complete: 67.2%; Average loss: 3.0846
Iteration: 2687; Percent complete: 67.2%; Average loss: 3.1774
Iteration: 2688; Percent complete: 67.2%; Average loss: 2.9649
Iteration: 2689; Percent complete: 67.2%; Average loss: 3.0302
Iteration: 2690; Percent complete: 67.2%; Average loss: 2.7577
Iteration: 2691; Percent complete: 67.3%; Average loss: 3.1749
Iteration: 2692; Percent complete: 67.3%; Average loss: 3.0016
Iteration: 2693; Percent complete: 67.3%; Average loss: 2.8782
Iteration: 2694; Percent complete: 67.3%; Average loss: 2.9790
Iteration: 2695; Percent complete: 67.4%; Average loss: 2.9258
Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9982
Iteration: 2697; Percent complete: 67.4%; Average loss: 3.3170
Iteration: 2698; Percent complete: 67.5%; Average loss: 2.9693
Iteration: 2699; Percent complete: 67.5%; Average loss: 3.0606
Iteration: 2700; Percent complete: 67.5%; Average loss: 2.9834
Iteration: 2701; Percent complete: 67.5%; Average loss: 3.2088
Iteration: 2702; Percent complete: 67.5%; Average loss: 2.9910
Iteration: 2703; Percent complete: 67.6%; Average loss: 2.8734
Iteration: 2704; Percent complete: 67.6%; Average loss: 2.9153
Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0608
Iteration: 2706; Percent complete: 67.7%; Average loss: 2.9478
Iteration: 2707; Percent complete: 67.7%; Average loss: 3.3227
Iteration: 2708; Percent complete: 67.7%; Average loss: 2.8279
Iteration: 2709; Percent complete: 67.7%; Average loss: 2.8431
Iteration: 2710; Percent complete: 67.8%; Average loss: 2.9521
Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8186
Iteration: 2712; Percent complete: 67.8%; Average loss: 3.0365
Iteration: 2713; Percent complete: 67.8%; Average loss: 2.7710
Iteration: 2714; Percent complete: 67.8%; Average loss: 3.0324
Iteration: 2715; Percent complete: 67.9%; Average loss: 2.8389
Iteration: 2716; Percent complete: 67.9%; Average loss: 3.0831
Iteration: 2717; Percent complete: 67.9%; Average loss: 2.8422
Iteration: 2718; Percent complete: 68.0%; Average loss: 3.0787
Iteration: 2719; Percent complete: 68.0%; Average loss: 3.1670
Iteration: 2720; Percent complete: 68.0%; Average loss: 2.8523
Iteration: 2721; Percent complete: 68.0%; Average loss: 2.9262
Iteration: 2722; Percent complete: 68.0%; Average loss: 3.1981
Iteration: 2723; Percent complete: 68.1%; Average loss: 3.1106
Iteration: 2724; Percent complete: 68.1%; Average loss: 2.9482
Iteration: 2725; Percent complete: 68.1%; Average loss: 3.0815
Iteration: 2726; Percent complete: 68.2%; Average loss: 3.2107
Iteration: 2727; Percent complete: 68.2%; Average loss: 2.9726
Iteration: 2728; Percent complete: 68.2%; Average loss: 3.0498
Iteration: 2729; Percent complete: 68.2%; Average loss: 3.0838
Iteration: 2730; Percent complete: 68.2%; Average loss: 2.8946
Iteration: 2731; Percent complete: 68.3%; Average loss: 2.9872
Iteration: 2732; Percent complete: 68.3%; Average loss: 3.0012
Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9639
Iteration: 2734; Percent complete: 68.3%; Average loss: 2.8706
Iteration: 2735; Percent complete: 68.4%; Average loss: 2.8216
Iteration: 2736; Percent complete: 68.4%; Average loss: 3.0474
Iteration: 2737; Percent complete: 68.4%; Average loss: 3.0928
Iteration: 2738; Percent complete: 68.5%; Average loss: 2.8906
Iteration: 2739; Percent complete: 68.5%; Average loss: 3.0172
Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9404
Iteration: 2741; Percent complete: 68.5%; Average loss: 2.6217
Iteration: 2742; Percent complete: 68.5%; Average loss: 2.9364
Iteration: 2743; Percent complete: 68.6%; Average loss: 3.0814
Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0591
Iteration: 2745; Percent complete: 68.6%; Average loss: 3.1256
Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0071
Iteration: 2747; Percent complete: 68.7%; Average loss: 2.7152
Iteration: 2748; Percent complete: 68.7%; Average loss: 3.2410
Iteration: 2749; Percent complete: 68.7%; Average loss: 3.0299
Iteration: 2750; Percent complete: 68.8%; Average loss: 2.9470
Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0017
Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8554
Iteration: 2753; Percent complete: 68.8%; Average loss: 3.0169
Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8472
Iteration: 2755; Percent complete: 68.9%; Average loss: 2.6713
Iteration: 2756; Percent complete: 68.9%; Average loss: 2.9615
Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0460
Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0288
Iteration: 2759; Percent complete: 69.0%; Average loss: 2.8848
Iteration: 2760; Percent complete: 69.0%; Average loss: 2.9872
Iteration: 2761; Percent complete: 69.0%; Average loss: 3.0629
Iteration: 2762; Percent complete: 69.0%; Average loss: 2.8128
Iteration: 2763; Percent complete: 69.1%; Average loss: 2.8483
Iteration: 2764; Percent complete: 69.1%; Average loss: 2.8363
Iteration: 2765; Percent complete: 69.1%; Average loss: 2.9523
Iteration: 2766; Percent complete: 69.2%; Average loss: 2.9454
Iteration: 2767; Percent complete: 69.2%; Average loss: 3.3464
Iteration: 2768; Percent complete: 69.2%; Average loss: 3.1293
Iteration: 2769; Percent complete: 69.2%; Average loss: 2.9229
Iteration: 2770; Percent complete: 69.2%; Average loss: 3.1116
Iteration: 2771; Percent complete: 69.3%; Average loss: 2.9889
Iteration: 2772; Percent complete: 69.3%; Average loss: 2.8270
Iteration: 2773; Percent complete: 69.3%; Average loss: 2.9177
Iteration: 2774; Percent complete: 69.3%; Average loss: 2.8812
Iteration: 2775; Percent complete: 69.4%; Average loss: 2.5925
Iteration: 2776; Percent complete: 69.4%; Average loss: 2.7181
Iteration: 2777; Percent complete: 69.4%; Average loss: 2.8853
Iteration: 2778; Percent complete: 69.5%; Average loss: 3.0394
Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9626
Iteration: 2780; Percent complete: 69.5%; Average loss: 2.7945
Iteration: 2781; Percent complete: 69.5%; Average loss: 3.0437
Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9119
Iteration: 2783; Percent complete: 69.6%; Average loss: 2.6435
Iteration: 2784; Percent complete: 69.6%; Average loss: 2.8534
Iteration: 2785; Percent complete: 69.6%; Average loss: 3.1532
Iteration: 2786; Percent complete: 69.7%; Average loss: 2.7463
Iteration: 2787; Percent complete: 69.7%; Average loss: 2.8628
Iteration: 2788; Percent complete: 69.7%; Average loss: 2.9051
Iteration: 2789; Percent complete: 69.7%; Average loss: 3.0059
Iteration: 2790; Percent complete: 69.8%; Average loss: 2.9914
Iteration: 2791; Percent complete: 69.8%; Average loss: 2.9405
Iteration: 2792; Percent complete: 69.8%; Average loss: 2.8637
Iteration: 2793; Percent complete: 69.8%; Average loss: 2.8020
Iteration: 2794; Percent complete: 69.8%; Average loss: 2.6122
Iteration: 2795; Percent complete: 69.9%; Average loss: 2.8357
Iteration: 2796; Percent complete: 69.9%; Average loss: 2.9496
Iteration: 2797; Percent complete: 69.9%; Average loss: 2.9483
Iteration: 2798; Percent complete: 70.0%; Average loss: 2.9574
Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9151
Iteration: 2800; Percent complete: 70.0%; Average loss: 3.1191
Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8555
Iteration: 2802; Percent complete: 70.0%; Average loss: 2.6340
Iteration: 2803; Percent complete: 70.1%; Average loss: 2.8717
Iteration: 2804; Percent complete: 70.1%; Average loss: 2.8891
Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9483
Iteration: 2806; Percent complete: 70.2%; Average loss: 3.0727
Iteration: 2807; Percent complete: 70.2%; Average loss: 3.1897
Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0563
Iteration: 2809; Percent complete: 70.2%; Average loss: 2.9623
Iteration: 2810; Percent complete: 70.2%; Average loss: 3.1789
Iteration: 2811; Percent complete: 70.3%; Average loss: 3.0903
Iteration: 2812; Percent complete: 70.3%; Average loss: 2.9646
Iteration: 2813; Percent complete: 70.3%; Average loss: 2.9356
Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8996
Iteration: 2815; Percent complete: 70.4%; Average loss: 2.7105
Iteration: 2816; Percent complete: 70.4%; Average loss: 2.7669
Iteration: 2817; Percent complete: 70.4%; Average loss: 2.8119
Iteration: 2818; Percent complete: 70.5%; Average loss: 3.2486
Iteration: 2819; Percent complete: 70.5%; Average loss: 2.9912
Iteration: 2820; Percent complete: 70.5%; Average loss: 2.7983
Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8762
Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0260
Iteration: 2823; Percent complete: 70.6%; Average loss: 3.0546
Iteration: 2824; Percent complete: 70.6%; Average loss: 2.8984
Iteration: 2825; Percent complete: 70.6%; Average loss: 2.9298
Iteration: 2826; Percent complete: 70.7%; Average loss: 2.9173
Iteration: 2827; Percent complete: 70.7%; Average loss: 2.9132
Iteration: 2828; Percent complete: 70.7%; Average loss: 2.8236
Iteration: 2829; Percent complete: 70.7%; Average loss: 3.1194
Iteration: 2830; Percent complete: 70.8%; Average loss: 2.8900
Iteration: 2831; Percent complete: 70.8%; Average loss: 3.0743
Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9295
Iteration: 2833; Percent complete: 70.8%; Average loss: 2.8008
Iteration: 2834; Percent complete: 70.9%; Average loss: 2.8604
Iteration: 2835; Percent complete: 70.9%; Average loss: 2.9667
Iteration: 2836; Percent complete: 70.9%; Average loss: 2.9627
Iteration: 2837; Percent complete: 70.9%; Average loss: 2.7512
Iteration: 2838; Percent complete: 71.0%; Average loss: 2.7317
Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8027
Iteration: 2840; Percent complete: 71.0%; Average loss: 3.0991
Iteration: 2841; Percent complete: 71.0%; Average loss: 2.8449
Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9406
Iteration: 2843; Percent complete: 71.1%; Average loss: 2.8649
Iteration: 2844; Percent complete: 71.1%; Average loss: 2.8318
Iteration: 2845; Percent complete: 71.1%; Average loss: 3.0933
Iteration: 2846; Percent complete: 71.2%; Average loss: 2.9027
Iteration: 2847; Percent complete: 71.2%; Average loss: 2.8930
Iteration: 2848; Percent complete: 71.2%; Average loss: 2.8110
Iteration: 2849; Percent complete: 71.2%; Average loss: 2.8697
Iteration: 2850; Percent complete: 71.2%; Average loss: 2.7753
Iteration: 2851; Percent complete: 71.3%; Average loss: 2.8739
Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9882
Iteration: 2853; Percent complete: 71.3%; Average loss: 2.8291
Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9852
Iteration: 2855; Percent complete: 71.4%; Average loss: 2.8480
Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8427
Iteration: 2857; Percent complete: 71.4%; Average loss: 3.0807
Iteration: 2858; Percent complete: 71.5%; Average loss: 3.0335
Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8332
Iteration: 2860; Percent complete: 71.5%; Average loss: 2.9118
Iteration: 2861; Percent complete: 71.5%; Average loss: 2.7608
Iteration: 2862; Percent complete: 71.5%; Average loss: 2.7904
Iteration: 2863; Percent complete: 71.6%; Average loss: 3.0003
Iteration: 2864; Percent complete: 71.6%; Average loss: 2.8170
Iteration: 2865; Percent complete: 71.6%; Average loss: 3.0308
Iteration: 2866; Percent complete: 71.7%; Average loss: 2.8075
Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9253
Iteration: 2868; Percent complete: 71.7%; Average loss: 2.6983
Iteration: 2869; Percent complete: 71.7%; Average loss: 2.8471
Iteration: 2870; Percent complete: 71.8%; Average loss: 2.9655
Iteration: 2871; Percent complete: 71.8%; Average loss: 3.1801
Iteration: 2872; Percent complete: 71.8%; Average loss: 2.8851
Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0887
Iteration: 2874; Percent complete: 71.9%; Average loss: 2.9433
Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9169
Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9673
Iteration: 2877; Percent complete: 71.9%; Average loss: 2.8460
Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8527
Iteration: 2879; Percent complete: 72.0%; Average loss: 2.8740
Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8514
Iteration: 2881; Percent complete: 72.0%; Average loss: 2.8554
Iteration: 2882; Percent complete: 72.0%; Average loss: 2.9390
Iteration: 2883; Percent complete: 72.1%; Average loss: 2.9010
Iteration: 2884; Percent complete: 72.1%; Average loss: 2.6885
Iteration: 2885; Percent complete: 72.1%; Average loss: 3.0749
Iteration: 2886; Percent complete: 72.2%; Average loss: 2.8276
Iteration: 2887; Percent complete: 72.2%; Average loss: 2.7556
Iteration: 2888; Percent complete: 72.2%; Average loss: 3.0712
Iteration: 2889; Percent complete: 72.2%; Average loss: 2.7802
Iteration: 2890; Percent complete: 72.2%; Average loss: 2.8922
Iteration: 2891; Percent complete: 72.3%; Average loss: 3.0274
Iteration: 2892; Percent complete: 72.3%; Average loss: 2.8782
Iteration: 2893; Percent complete: 72.3%; Average loss: 2.7411
Iteration: 2894; Percent complete: 72.4%; Average loss: 2.6468
Iteration: 2895; Percent complete: 72.4%; Average loss: 3.0694
Iteration: 2896; Percent complete: 72.4%; Average loss: 2.7082
Iteration: 2897; Percent complete: 72.4%; Average loss: 2.7636
Iteration: 2898; Percent complete: 72.5%; Average loss: 2.6953
Iteration: 2899; Percent complete: 72.5%; Average loss: 2.5704
Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9935
Iteration: 2901; Percent complete: 72.5%; Average loss: 2.6925
Iteration: 2902; Percent complete: 72.5%; Average loss: 3.0798
Iteration: 2903; Percent complete: 72.6%; Average loss: 2.7649
Iteration: 2904; Percent complete: 72.6%; Average loss: 2.9993
Iteration: 2905; Percent complete: 72.6%; Average loss: 2.9260
Iteration: 2906; Percent complete: 72.7%; Average loss: 2.9192
Iteration: 2907; Percent complete: 72.7%; Average loss: 2.9466
Iteration: 2908; Percent complete: 72.7%; Average loss: 2.8860
Iteration: 2909; Percent complete: 72.7%; Average loss: 2.9092
Iteration: 2910; Percent complete: 72.8%; Average loss: 2.8376
Iteration: 2911; Percent complete: 72.8%; Average loss: 2.7872
Iteration: 2912; Percent complete: 72.8%; Average loss: 3.0203
Iteration: 2913; Percent complete: 72.8%; Average loss: 2.9417
Iteration: 2914; Percent complete: 72.9%; Average loss: 2.8927
Iteration: 2915; Percent complete: 72.9%; Average loss: 2.7659
Iteration: 2916; Percent complete: 72.9%; Average loss: 3.0578
Iteration: 2917; Percent complete: 72.9%; Average loss: 2.7945
Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9576
Iteration: 2919; Percent complete: 73.0%; Average loss: 3.1665
Iteration: 2920; Percent complete: 73.0%; Average loss: 2.6441
Iteration: 2921; Percent complete: 73.0%; Average loss: 3.1765
Iteration: 2922; Percent complete: 73.0%; Average loss: 3.0600
Iteration: 2923; Percent complete: 73.1%; Average loss: 3.1879
Iteration: 2924; Percent complete: 73.1%; Average loss: 2.9281
Iteration: 2925; Percent complete: 73.1%; Average loss: 2.8572
Iteration: 2926; Percent complete: 73.2%; Average loss: 2.7520
Iteration: 2927; Percent complete: 73.2%; Average loss: 2.8416
Iteration: 2928; Percent complete: 73.2%; Average loss: 2.5569
Iteration: 2929; Percent complete: 73.2%; Average loss: 2.9574
Iteration: 2930; Percent complete: 73.2%; Average loss: 2.9354
Iteration: 2931; Percent complete: 73.3%; Average loss: 2.6870
Iteration: 2932; Percent complete: 73.3%; Average loss: 2.9385
Iteration: 2933; Percent complete: 73.3%; Average loss: 2.9433
Iteration: 2934; Percent complete: 73.4%; Average loss: 2.8217
Iteration: 2935; Percent complete: 73.4%; Average loss: 2.8426
Iteration: 2936; Percent complete: 73.4%; Average loss: 2.8986
Iteration: 2937; Percent complete: 73.4%; Average loss: 2.8919
Iteration: 2938; Percent complete: 73.5%; Average loss: 2.8699
Iteration: 2939; Percent complete: 73.5%; Average loss: 3.0618
Iteration: 2940; Percent complete: 73.5%; Average loss: 2.9268
Iteration: 2941; Percent complete: 73.5%; Average loss: 2.8913
Iteration: 2942; Percent complete: 73.6%; Average loss: 2.9999
Iteration: 2943; Percent complete: 73.6%; Average loss: 2.8531
Iteration: 2944; Percent complete: 73.6%; Average loss: 3.0234
Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8584
Iteration: 2946; Percent complete: 73.7%; Average loss: 3.0159
Iteration: 2947; Percent complete: 73.7%; Average loss: 2.9612
Iteration: 2948; Percent complete: 73.7%; Average loss: 2.8266
Iteration: 2949; Percent complete: 73.7%; Average loss: 2.7707
Iteration: 2950; Percent complete: 73.8%; Average loss: 2.9449
Iteration: 2951; Percent complete: 73.8%; Average loss: 2.9393
Iteration: 2952; Percent complete: 73.8%; Average loss: 2.9744
Iteration: 2953; Percent complete: 73.8%; Average loss: 2.9331
Iteration: 2954; Percent complete: 73.9%; Average loss: 2.7976
Iteration: 2955; Percent complete: 73.9%; Average loss: 3.2895
Iteration: 2956; Percent complete: 73.9%; Average loss: 2.8712
Iteration: 2957; Percent complete: 73.9%; Average loss: 2.6149
Iteration: 2958; Percent complete: 74.0%; Average loss: 3.1430
Iteration: 2959; Percent complete: 74.0%; Average loss: 2.8189
Iteration: 2960; Percent complete: 74.0%; Average loss: 3.0931
Iteration: 2961; Percent complete: 74.0%; Average loss: 3.0536
Iteration: 2962; Percent complete: 74.1%; Average loss: 3.0741
Iteration: 2963; Percent complete: 74.1%; Average loss: 3.0255
Iteration: 2964; Percent complete: 74.1%; Average loss: 3.0595
Iteration: 2965; Percent complete: 74.1%; Average loss: 3.1002
Iteration: 2966; Percent complete: 74.2%; Average loss: 2.8155
Iteration: 2967; Percent complete: 74.2%; Average loss: 2.9351
Iteration: 2968; Percent complete: 74.2%; Average loss: 2.7739
Iteration: 2969; Percent complete: 74.2%; Average loss: 2.6940
Iteration: 2970; Percent complete: 74.2%; Average loss: 3.0183
Iteration: 2971; Percent complete: 74.3%; Average loss: 2.9247
Iteration: 2972; Percent complete: 74.3%; Average loss: 2.8965
Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0855
Iteration: 2974; Percent complete: 74.4%; Average loss: 3.0527
Iteration: 2975; Percent complete: 74.4%; Average loss: 2.9082
Iteration: 2976; Percent complete: 74.4%; Average loss: 2.9744
Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9103
Iteration: 2978; Percent complete: 74.5%; Average loss: 2.9555
Iteration: 2979; Percent complete: 74.5%; Average loss: 2.8775
Iteration: 2980; Percent complete: 74.5%; Average loss: 2.5978
Iteration: 2981; Percent complete: 74.5%; Average loss: 2.7261
Iteration: 2982; Percent complete: 74.6%; Average loss: 2.8320
Iteration: 2983; Percent complete: 74.6%; Average loss: 2.9820
Iteration: 2984; Percent complete: 74.6%; Average loss: 2.8926
Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9800
Iteration: 2986; Percent complete: 74.7%; Average loss: 2.8141
Iteration: 2987; Percent complete: 74.7%; Average loss: 3.1539
Iteration: 2988; Percent complete: 74.7%; Average loss: 2.9164
Iteration: 2989; Percent complete: 74.7%; Average loss: 2.7686
Iteration: 2990; Percent complete: 74.8%; Average loss: 3.0157
Iteration: 2991; Percent complete: 74.8%; Average loss: 2.5433
Iteration: 2992; Percent complete: 74.8%; Average loss: 3.0755
Iteration: 2993; Percent complete: 74.8%; Average loss: 2.9134
Iteration: 2994; Percent complete: 74.9%; Average loss: 3.1198
Iteration: 2995; Percent complete: 74.9%; Average loss: 2.8238
Iteration: 2996; Percent complete: 74.9%; Average loss: 2.9042
Iteration: 2997; Percent complete: 74.9%; Average loss: 2.8500
Iteration: 2998; Percent complete: 75.0%; Average loss: 2.7370
Iteration: 2999; Percent complete: 75.0%; Average loss: 2.7632
Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9042
Iteration: 3001; Percent complete: 75.0%; Average loss: 2.9104
Iteration: 3002; Percent complete: 75.0%; Average loss: 2.9315
Iteration: 3003; Percent complete: 75.1%; Average loss: 2.5538
Iteration: 3004; Percent complete: 75.1%; Average loss: 2.7653
Iteration: 3005; Percent complete: 75.1%; Average loss: 2.8091
Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8280
Iteration: 3007; Percent complete: 75.2%; Average loss: 2.8973
Iteration: 3008; Percent complete: 75.2%; Average loss: 2.9303
Iteration: 3009; Percent complete: 75.2%; Average loss: 2.8992
Iteration: 3010; Percent complete: 75.2%; Average loss: 2.9465
Iteration: 3011; Percent complete: 75.3%; Average loss: 2.7476
Iteration: 3012; Percent complete: 75.3%; Average loss: 2.8692
Iteration: 3013; Percent complete: 75.3%; Average loss: 3.0576
Iteration: 3014; Percent complete: 75.3%; Average loss: 2.9363
Iteration: 3015; Percent complete: 75.4%; Average loss: 2.8506
Iteration: 3016; Percent complete: 75.4%; Average loss: 3.0690
Iteration: 3017; Percent complete: 75.4%; Average loss: 3.0610
Iteration: 3018; Percent complete: 75.4%; Average loss: 2.7825
Iteration: 3019; Percent complete: 75.5%; Average loss: 2.5459
Iteration: 3020; Percent complete: 75.5%; Average loss: 3.0053
Iteration: 3021; Percent complete: 75.5%; Average loss: 2.9335
Iteration: 3022; Percent complete: 75.5%; Average loss: 3.0949
Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8999
Iteration: 3024; Percent complete: 75.6%; Average loss: 3.0097
Iteration: 3025; Percent complete: 75.6%; Average loss: 2.8900
Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8316
Iteration: 3027; Percent complete: 75.7%; Average loss: 2.8314
Iteration: 3028; Percent complete: 75.7%; Average loss: 2.7724
Iteration: 3029; Percent complete: 75.7%; Average loss: 2.7451
Iteration: 3030; Percent complete: 75.8%; Average loss: 2.8051
Iteration: 3031; Percent complete: 75.8%; Average loss: 2.8484
Iteration: 3032; Percent complete: 75.8%; Average loss: 2.7615
Iteration: 3033; Percent complete: 75.8%; Average loss: 2.9300
Iteration: 3034; Percent complete: 75.8%; Average loss: 3.0415
Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7075
Iteration: 3036; Percent complete: 75.9%; Average loss: 2.8187
Iteration: 3037; Percent complete: 75.9%; Average loss: 2.7239
Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8551
Iteration: 3039; Percent complete: 76.0%; Average loss: 2.8072
Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9392
Iteration: 3041; Percent complete: 76.0%; Average loss: 2.8911
Iteration: 3042; Percent complete: 76.0%; Average loss: 2.9741
Iteration: 3043; Percent complete: 76.1%; Average loss: 2.8365
Iteration: 3044; Percent complete: 76.1%; Average loss: 2.9648
Iteration: 3045; Percent complete: 76.1%; Average loss: 2.6674
Iteration: 3046; Percent complete: 76.1%; Average loss: 2.8556
Iteration: 3047; Percent complete: 76.2%; Average loss: 3.2320
Iteration: 3048; Percent complete: 76.2%; Average loss: 2.6103
Iteration: 3049; Percent complete: 76.2%; Average loss: 2.9026
Iteration: 3050; Percent complete: 76.2%; Average loss: 2.7588
Iteration: 3051; Percent complete: 76.3%; Average loss: 2.7987
Iteration: 3052; Percent complete: 76.3%; Average loss: 3.0229
Iteration: 3053; Percent complete: 76.3%; Average loss: 2.9316
Iteration: 3054; Percent complete: 76.3%; Average loss: 2.8221
Iteration: 3055; Percent complete: 76.4%; Average loss: 2.7022
Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8500
Iteration: 3057; Percent complete: 76.4%; Average loss: 2.8739
Iteration: 3058; Percent complete: 76.4%; Average loss: 2.8017
Iteration: 3059; Percent complete: 76.5%; Average loss: 2.8212
Iteration: 3060; Percent complete: 76.5%; Average loss: 2.9076
Iteration: 3061; Percent complete: 76.5%; Average loss: 3.1309
Iteration: 3062; Percent complete: 76.5%; Average loss: 2.9417
Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8713
Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8195
Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9570
Iteration: 3066; Percent complete: 76.6%; Average loss: 2.8774
Iteration: 3067; Percent complete: 76.7%; Average loss: 2.8918
Iteration: 3068; Percent complete: 76.7%; Average loss: 2.8781
Iteration: 3069; Percent complete: 76.7%; Average loss: 2.7337
Iteration: 3070; Percent complete: 76.8%; Average loss: 3.1019
Iteration: 3071; Percent complete: 76.8%; Average loss: 2.9705
Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8116
Iteration: 3073; Percent complete: 76.8%; Average loss: 2.8756
Iteration: 3074; Percent complete: 76.8%; Average loss: 2.7602
Iteration: 3075; Percent complete: 76.9%; Average loss: 2.7899
Iteration: 3076; Percent complete: 76.9%; Average loss: 2.8827
Iteration: 3077; Percent complete: 76.9%; Average loss: 2.9504
Iteration: 3078; Percent complete: 77.0%; Average loss: 2.8038
Iteration: 3079; Percent complete: 77.0%; Average loss: 3.0024
Iteration: 3080; Percent complete: 77.0%; Average loss: 2.7422
Iteration: 3081; Percent complete: 77.0%; Average loss: 2.7029
Iteration: 3082; Percent complete: 77.0%; Average loss: 2.8107
Iteration: 3083; Percent complete: 77.1%; Average loss: 2.6870
Iteration: 3084; Percent complete: 77.1%; Average loss: 2.5395
Iteration: 3085; Percent complete: 77.1%; Average loss: 2.8710
Iteration: 3086; Percent complete: 77.1%; Average loss: 2.7820
Iteration: 3087; Percent complete: 77.2%; Average loss: 2.9683
Iteration: 3088; Percent complete: 77.2%; Average loss: 2.6662
Iteration: 3089; Percent complete: 77.2%; Average loss: 2.8119
Iteration: 3090; Percent complete: 77.2%; Average loss: 3.0061
Iteration: 3091; Percent complete: 77.3%; Average loss: 2.6946
Iteration: 3092; Percent complete: 77.3%; Average loss: 2.9894
Iteration: 3093; Percent complete: 77.3%; Average loss: 3.0249
Iteration: 3094; Percent complete: 77.3%; Average loss: 2.8570
Iteration: 3095; Percent complete: 77.4%; Average loss: 2.8867
Iteration: 3096; Percent complete: 77.4%; Average loss: 2.6720
Iteration: 3097; Percent complete: 77.4%; Average loss: 2.9286
Iteration: 3098; Percent complete: 77.5%; Average loss: 2.6378
Iteration: 3099; Percent complete: 77.5%; Average loss: 2.7723
Iteration: 3100; Percent complete: 77.5%; Average loss: 2.9296
Iteration: 3101; Percent complete: 77.5%; Average loss: 2.7979
Iteration: 3102; Percent complete: 77.5%; Average loss: 2.8227
Iteration: 3103; Percent complete: 77.6%; Average loss: 2.7246
Iteration: 3104; Percent complete: 77.6%; Average loss: 2.5275
Iteration: 3105; Percent complete: 77.6%; Average loss: 2.8894
Iteration: 3106; Percent complete: 77.6%; Average loss: 2.9235
Iteration: 3107; Percent complete: 77.7%; Average loss: 2.8868
Iteration: 3108; Percent complete: 77.7%; Average loss: 2.8655
Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8612
Iteration: 3110; Percent complete: 77.8%; Average loss: 3.0113
Iteration: 3111; Percent complete: 77.8%; Average loss: 3.0005
Iteration: 3112; Percent complete: 77.8%; Average loss: 3.0154
Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9004
Iteration: 3114; Percent complete: 77.8%; Average loss: 2.8492
Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7946
Iteration: 3116; Percent complete: 77.9%; Average loss: 2.7277
Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8577
Iteration: 3118; Percent complete: 78.0%; Average loss: 2.6949
Iteration: 3119; Percent complete: 78.0%; Average loss: 2.8490
Iteration: 3120; Percent complete: 78.0%; Average loss: 2.9207
Iteration: 3121; Percent complete: 78.0%; Average loss: 2.6902
Iteration: 3122; Percent complete: 78.0%; Average loss: 2.8039
Iteration: 3123; Percent complete: 78.1%; Average loss: 2.9765
Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7022
Iteration: 3125; Percent complete: 78.1%; Average loss: 2.6665
Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8983
Iteration: 3127; Percent complete: 78.2%; Average loss: 2.9523
Iteration: 3128; Percent complete: 78.2%; Average loss: 2.9094
Iteration: 3129; Percent complete: 78.2%; Average loss: 2.7803
Iteration: 3130; Percent complete: 78.2%; Average loss: 2.7735
Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7078
Iteration: 3132; Percent complete: 78.3%; Average loss: 2.6066
Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8564
Iteration: 3134; Percent complete: 78.3%; Average loss: 2.8057
Iteration: 3135; Percent complete: 78.4%; Average loss: 2.9009
Iteration: 3136; Percent complete: 78.4%; Average loss: 2.8712
Iteration: 3137; Percent complete: 78.4%; Average loss: 2.8816
Iteration: 3138; Percent complete: 78.5%; Average loss: 2.6431
Iteration: 3139; Percent complete: 78.5%; Average loss: 2.7677
Iteration: 3140; Percent complete: 78.5%; Average loss: 2.8159
Iteration: 3141; Percent complete: 78.5%; Average loss: 2.7317
Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8099
Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8721
Iteration: 3144; Percent complete: 78.6%; Average loss: 2.9329
Iteration: 3145; Percent complete: 78.6%; Average loss: 2.9142
Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7942
Iteration: 3147; Percent complete: 78.7%; Average loss: 3.3282
Iteration: 3148; Percent complete: 78.7%; Average loss: 2.8843
Iteration: 3149; Percent complete: 78.7%; Average loss: 2.8390
Iteration: 3150; Percent complete: 78.8%; Average loss: 2.8359
Iteration: 3151; Percent complete: 78.8%; Average loss: 2.9266
Iteration: 3152; Percent complete: 78.8%; Average loss: 2.6332
Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6443
Iteration: 3154; Percent complete: 78.8%; Average loss: 2.8584
Iteration: 3155; Percent complete: 78.9%; Average loss: 2.9708
Iteration: 3156; Percent complete: 78.9%; Average loss: 2.7314
Iteration: 3157; Percent complete: 78.9%; Average loss: 2.6603
Iteration: 3158; Percent complete: 79.0%; Average loss: 3.1238
Iteration: 3159; Percent complete: 79.0%; Average loss: 2.7714
Iteration: 3160; Percent complete: 79.0%; Average loss: 2.9591
Iteration: 3161; Percent complete: 79.0%; Average loss: 3.1092
Iteration: 3162; Percent complete: 79.0%; Average loss: 2.7033
Iteration: 3163; Percent complete: 79.1%; Average loss: 2.5335
Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8166
Iteration: 3165; Percent complete: 79.1%; Average loss: 2.9550
Iteration: 3166; Percent complete: 79.1%; Average loss: 2.9861
Iteration: 3167; Percent complete: 79.2%; Average loss: 2.7951
Iteration: 3168; Percent complete: 79.2%; Average loss: 2.9197
Iteration: 3169; Percent complete: 79.2%; Average loss: 2.8039
Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9605
Iteration: 3171; Percent complete: 79.3%; Average loss: 2.8932
Iteration: 3172; Percent complete: 79.3%; Average loss: 2.8407
Iteration: 3173; Percent complete: 79.3%; Average loss: 3.0987
Iteration: 3174; Percent complete: 79.3%; Average loss: 3.0832
Iteration: 3175; Percent complete: 79.4%; Average loss: 2.7120
Iteration: 3176; Percent complete: 79.4%; Average loss: 2.8610
Iteration: 3177; Percent complete: 79.4%; Average loss: 2.6021
Iteration: 3178; Percent complete: 79.5%; Average loss: 2.9299
Iteration: 3179; Percent complete: 79.5%; Average loss: 2.9136
Iteration: 3180; Percent complete: 79.5%; Average loss: 3.1010
Iteration: 3181; Percent complete: 79.5%; Average loss: 3.0585
Iteration: 3182; Percent complete: 79.5%; Average loss: 3.0169
Iteration: 3183; Percent complete: 79.6%; Average loss: 2.9386
Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8119
Iteration: 3185; Percent complete: 79.6%; Average loss: 2.6383
Iteration: 3186; Percent complete: 79.7%; Average loss: 3.0301
Iteration: 3187; Percent complete: 79.7%; Average loss: 2.7493
Iteration: 3188; Percent complete: 79.7%; Average loss: 2.8811
Iteration: 3189; Percent complete: 79.7%; Average loss: 2.8886
Iteration: 3190; Percent complete: 79.8%; Average loss: 2.7996
Iteration: 3191; Percent complete: 79.8%; Average loss: 2.8282
Iteration: 3192; Percent complete: 79.8%; Average loss: 2.6497
Iteration: 3193; Percent complete: 79.8%; Average loss: 2.8673
Iteration: 3194; Percent complete: 79.8%; Average loss: 2.7617
Iteration: 3195; Percent complete: 79.9%; Average loss: 3.0431
Iteration: 3196; Percent complete: 79.9%; Average loss: 2.9346
Iteration: 3197; Percent complete: 79.9%; Average loss: 2.6489
Iteration: 3198; Percent complete: 80.0%; Average loss: 2.7939
Iteration: 3199; Percent complete: 80.0%; Average loss: 2.8033
Iteration: 3200; Percent complete: 80.0%; Average loss: 2.7293
Iteration: 3201; Percent complete: 80.0%; Average loss: 2.9392
Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8895
Iteration: 3203; Percent complete: 80.1%; Average loss: 2.9887
Iteration: 3204; Percent complete: 80.1%; Average loss: 2.7517
Iteration: 3205; Percent complete: 80.1%; Average loss: 2.7560
Iteration: 3206; Percent complete: 80.2%; Average loss: 2.6589
Iteration: 3207; Percent complete: 80.2%; Average loss: 2.7717
Iteration: 3208; Percent complete: 80.2%; Average loss: 2.8106
Iteration: 3209; Percent complete: 80.2%; Average loss: 2.8480
Iteration: 3210; Percent complete: 80.2%; Average loss: 2.8923
Iteration: 3211; Percent complete: 80.3%; Average loss: 2.6894
Iteration: 3212; Percent complete: 80.3%; Average loss: 2.7134
Iteration: 3213; Percent complete: 80.3%; Average loss: 2.6193
Iteration: 3214; Percent complete: 80.3%; Average loss: 3.0627
Iteration: 3215; Percent complete: 80.4%; Average loss: 3.0382
Iteration: 3216; Percent complete: 80.4%; Average loss: 2.8695
Iteration: 3217; Percent complete: 80.4%; Average loss: 2.7243
Iteration: 3218; Percent complete: 80.5%; Average loss: 2.8867
Iteration: 3219; Percent complete: 80.5%; Average loss: 3.1501
Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8389
Iteration: 3221; Percent complete: 80.5%; Average loss: 2.7975
Iteration: 3222; Percent complete: 80.5%; Average loss: 2.6297
Iteration: 3223; Percent complete: 80.6%; Average loss: 2.7997
Iteration: 3224; Percent complete: 80.6%; Average loss: 2.7528
Iteration: 3225; Percent complete: 80.6%; Average loss: 2.7327
Iteration: 3226; Percent complete: 80.7%; Average loss: 2.8161
Iteration: 3227; Percent complete: 80.7%; Average loss: 2.7669
Iteration: 3228; Percent complete: 80.7%; Average loss: 2.9076
Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7975
Iteration: 3230; Percent complete: 80.8%; Average loss: 2.4685
Iteration: 3231; Percent complete: 80.8%; Average loss: 2.9595
Iteration: 3232; Percent complete: 80.8%; Average loss: 2.7549
Iteration: 3233; Percent complete: 80.8%; Average loss: 2.8397
Iteration: 3234; Percent complete: 80.8%; Average loss: 2.6217
Iteration: 3235; Percent complete: 80.9%; Average loss: 2.7724
Iteration: 3236; Percent complete: 80.9%; Average loss: 2.9681
Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7615
Iteration: 3238; Percent complete: 81.0%; Average loss: 2.9355
Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8458
Iteration: 3240; Percent complete: 81.0%; Average loss: 2.9161
Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7090
Iteration: 3242; Percent complete: 81.0%; Average loss: 2.9326
Iteration: 3243; Percent complete: 81.1%; Average loss: 2.8715
Iteration: 3244; Percent complete: 81.1%; Average loss: 2.6919
Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7020
Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9264
Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7968
Iteration: 3248; Percent complete: 81.2%; Average loss: 2.6390
Iteration: 3249; Percent complete: 81.2%; Average loss: 2.6668
Iteration: 3250; Percent complete: 81.2%; Average loss: 2.7627
Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8597
Iteration: 3252; Percent complete: 81.3%; Average loss: 2.9135
Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9317
Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8430
Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9661
Iteration: 3256; Percent complete: 81.4%; Average loss: 3.3176
Iteration: 3257; Percent complete: 81.4%; Average loss: 2.6542
Iteration: 3258; Percent complete: 81.5%; Average loss: 2.7711
Iteration: 3259; Percent complete: 81.5%; Average loss: 2.9601
Iteration: 3260; Percent complete: 81.5%; Average loss: 2.8149
Iteration: 3261; Percent complete: 81.5%; Average loss: 2.8393
Iteration: 3262; Percent complete: 81.5%; Average loss: 2.6799
Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9596
Iteration: 3264; Percent complete: 81.6%; Average loss: 2.8757
Iteration: 3265; Percent complete: 81.6%; Average loss: 2.6411
Iteration: 3266; Percent complete: 81.7%; Average loss: 2.7922
Iteration: 3267; Percent complete: 81.7%; Average loss: 2.7385
Iteration: 3268; Percent complete: 81.7%; Average loss: 2.9999
Iteration: 3269; Percent complete: 81.7%; Average loss: 2.8503
Iteration: 3270; Percent complete: 81.8%; Average loss: 2.9187
Iteration: 3271; Percent complete: 81.8%; Average loss: 2.6218
Iteration: 3272; Percent complete: 81.8%; Average loss: 3.1177
Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7214
Iteration: 3274; Percent complete: 81.8%; Average loss: 2.7754
Iteration: 3275; Percent complete: 81.9%; Average loss: 2.9491
Iteration: 3276; Percent complete: 81.9%; Average loss: 2.8814
Iteration: 3277; Percent complete: 81.9%; Average loss: 2.8941
Iteration: 3278; Percent complete: 82.0%; Average loss: 2.8560
Iteration: 3279; Percent complete: 82.0%; Average loss: 2.7694
Iteration: 3280; Percent complete: 82.0%; Average loss: 2.9491
Iteration: 3281; Percent complete: 82.0%; Average loss: 2.7715
Iteration: 3282; Percent complete: 82.0%; Average loss: 2.8141
Iteration: 3283; Percent complete: 82.1%; Average loss: 2.6711
Iteration: 3284; Percent complete: 82.1%; Average loss: 2.8983
Iteration: 3285; Percent complete: 82.1%; Average loss: 2.9840
Iteration: 3286; Percent complete: 82.2%; Average loss: 2.4865
Iteration: 3287; Percent complete: 82.2%; Average loss: 2.8675
Iteration: 3288; Percent complete: 82.2%; Average loss: 2.7325
Iteration: 3289; Percent complete: 82.2%; Average loss: 2.6046
Iteration: 3290; Percent complete: 82.2%; Average loss: 2.8153
Iteration: 3291; Percent complete: 82.3%; Average loss: 2.6478
Iteration: 3292; Percent complete: 82.3%; Average loss: 2.6390
Iteration: 3293; Percent complete: 82.3%; Average loss: 2.6301
Iteration: 3294; Percent complete: 82.3%; Average loss: 2.7821
Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8347
Iteration: 3296; Percent complete: 82.4%; Average loss: 2.7727
Iteration: 3297; Percent complete: 82.4%; Average loss: 3.1048
Iteration: 3298; Percent complete: 82.5%; Average loss: 2.7168
Iteration: 3299; Percent complete: 82.5%; Average loss: 2.6872
Iteration: 3300; Percent complete: 82.5%; Average loss: 2.6663
Iteration: 3301; Percent complete: 82.5%; Average loss: 2.8109
Iteration: 3302; Percent complete: 82.5%; Average loss: 2.6053
Iteration: 3303; Percent complete: 82.6%; Average loss: 2.7403
Iteration: 3304; Percent complete: 82.6%; Average loss: 2.6275
Iteration: 3305; Percent complete: 82.6%; Average loss: 2.8244
Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8806
Iteration: 3307; Percent complete: 82.7%; Average loss: 3.0202
Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7257
Iteration: 3309; Percent complete: 82.7%; Average loss: 2.8465
Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7214
Iteration: 3311; Percent complete: 82.8%; Average loss: 2.6771
Iteration: 3312; Percent complete: 82.8%; Average loss: 2.9289
Iteration: 3313; Percent complete: 82.8%; Average loss: 2.7344
Iteration: 3314; Percent complete: 82.8%; Average loss: 2.6457
Iteration: 3315; Percent complete: 82.9%; Average loss: 2.7354
Iteration: 3316; Percent complete: 82.9%; Average loss: 2.9240
Iteration: 3317; Percent complete: 82.9%; Average loss: 2.8632
Iteration: 3318; Percent complete: 83.0%; Average loss: 2.6166
Iteration: 3319; Percent complete: 83.0%; Average loss: 3.1565
Iteration: 3320; Percent complete: 83.0%; Average loss: 2.9158
Iteration: 3321; Percent complete: 83.0%; Average loss: 2.8173
Iteration: 3322; Percent complete: 83.0%; Average loss: 2.6573
Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7586
Iteration: 3324; Percent complete: 83.1%; Average loss: 2.6959
Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7122
Iteration: 3326; Percent complete: 83.2%; Average loss: 2.8687
Iteration: 3327; Percent complete: 83.2%; Average loss: 2.8104
Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6543
Iteration: 3329; Percent complete: 83.2%; Average loss: 2.5764
Iteration: 3330; Percent complete: 83.2%; Average loss: 2.8283
Iteration: 3331; Percent complete: 83.3%; Average loss: 2.7068
Iteration: 3332; Percent complete: 83.3%; Average loss: 2.6352
Iteration: 3333; Percent complete: 83.3%; Average loss: 2.7618
Iteration: 3334; Percent complete: 83.4%; Average loss: 2.7234
Iteration: 3335; Percent complete: 83.4%; Average loss: 2.7991
Iteration: 3336; Percent complete: 83.4%; Average loss: 3.2406
Iteration: 3337; Percent complete: 83.4%; Average loss: 2.7847
Iteration: 3338; Percent complete: 83.5%; Average loss: 2.9362
Iteration: 3339; Percent complete: 83.5%; Average loss: 2.7474
Iteration: 3340; Percent complete: 83.5%; Average loss: 2.7756
Iteration: 3341; Percent complete: 83.5%; Average loss: 2.9787
Iteration: 3342; Percent complete: 83.5%; Average loss: 2.7549
Iteration: 3343; Percent complete: 83.6%; Average loss: 2.5395
Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7748
Iteration: 3345; Percent complete: 83.6%; Average loss: 2.5706
Iteration: 3346; Percent complete: 83.7%; Average loss: 2.6230
Iteration: 3347; Percent complete: 83.7%; Average loss: 2.9153
Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6769
Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8751
Iteration: 3350; Percent complete: 83.8%; Average loss: 2.6116
Iteration: 3351; Percent complete: 83.8%; Average loss: 3.0434
Iteration: 3352; Percent complete: 83.8%; Average loss: 2.8971
Iteration: 3353; Percent complete: 83.8%; Average loss: 2.8728
Iteration: 3354; Percent complete: 83.9%; Average loss: 2.7236
Iteration: 3355; Percent complete: 83.9%; Average loss: 2.8872
Iteration: 3356; Percent complete: 83.9%; Average loss: 2.9297
Iteration: 3357; Percent complete: 83.9%; Average loss: 2.4624
Iteration: 3358; Percent complete: 84.0%; Average loss: 2.6156
Iteration: 3359; Percent complete: 84.0%; Average loss: 2.6399
Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6900
Iteration: 3361; Percent complete: 84.0%; Average loss: 2.8120
Iteration: 3362; Percent complete: 84.0%; Average loss: 2.7338
Iteration: 3363; Percent complete: 84.1%; Average loss: 2.8510
Iteration: 3364; Percent complete: 84.1%; Average loss: 2.8404
Iteration: 3365; Percent complete: 84.1%; Average loss: 2.6501
Iteration: 3366; Percent complete: 84.2%; Average loss: 2.8302
Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7096
Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9962
Iteration: 3369; Percent complete: 84.2%; Average loss: 2.6763
Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6098
Iteration: 3371; Percent complete: 84.3%; Average loss: 2.8035
Iteration: 3372; Percent complete: 84.3%; Average loss: 2.4390
Iteration: 3373; Percent complete: 84.3%; Average loss: 3.0294
Iteration: 3374; Percent complete: 84.4%; Average loss: 2.4713
Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8440
Iteration: 3376; Percent complete: 84.4%; Average loss: 2.8451
Iteration: 3377; Percent complete: 84.4%; Average loss: 2.7753
Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7155
Iteration: 3379; Percent complete: 84.5%; Average loss: 2.8847
Iteration: 3380; Percent complete: 84.5%; Average loss: 2.9252
Iteration: 3381; Percent complete: 84.5%; Average loss: 2.8666
Iteration: 3382; Percent complete: 84.5%; Average loss: 2.8278
Iteration: 3383; Percent complete: 84.6%; Average loss: 2.8084
Iteration: 3384; Percent complete: 84.6%; Average loss: 2.7055
Iteration: 3385; Percent complete: 84.6%; Average loss: 2.8228
Iteration: 3386; Percent complete: 84.7%; Average loss: 2.4837
Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7085
Iteration: 3388; Percent complete: 84.7%; Average loss: 2.6598
Iteration: 3389; Percent complete: 84.7%; Average loss: 2.5747
Iteration: 3390; Percent complete: 84.8%; Average loss: 2.8001
Iteration: 3391; Percent complete: 84.8%; Average loss: 2.8125
Iteration: 3392; Percent complete: 84.8%; Average loss: 2.8607
Iteration: 3393; Percent complete: 84.8%; Average loss: 2.7869
Iteration: 3394; Percent complete: 84.9%; Average loss: 2.8262
Iteration: 3395; Percent complete: 84.9%; Average loss: 2.7201
Iteration: 3396; Percent complete: 84.9%; Average loss: 2.6119
Iteration: 3397; Percent complete: 84.9%; Average loss: 2.8198
Iteration: 3398; Percent complete: 85.0%; Average loss: 2.7679
Iteration: 3399; Percent complete: 85.0%; Average loss: 2.6924
Iteration: 3400; Percent complete: 85.0%; Average loss: 3.0022
Iteration: 3401; Percent complete: 85.0%; Average loss: 2.9101
Iteration: 3402; Percent complete: 85.0%; Average loss: 2.9995
Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8642
Iteration: 3404; Percent complete: 85.1%; Average loss: 2.7667
Iteration: 3405; Percent complete: 85.1%; Average loss: 2.9222
Iteration: 3406; Percent complete: 85.2%; Average loss: 2.8285
Iteration: 3407; Percent complete: 85.2%; Average loss: 2.7064
Iteration: 3408; Percent complete: 85.2%; Average loss: 2.8224
Iteration: 3409; Percent complete: 85.2%; Average loss: 2.7011
Iteration: 3410; Percent complete: 85.2%; Average loss: 2.8177
Iteration: 3411; Percent complete: 85.3%; Average loss: 2.7281
Iteration: 3412; Percent complete: 85.3%; Average loss: 2.5660
Iteration: 3413; Percent complete: 85.3%; Average loss: 2.7405
Iteration: 3414; Percent complete: 85.4%; Average loss: 2.6388
Iteration: 3415; Percent complete: 85.4%; Average loss: 2.8238
Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8022
Iteration: 3417; Percent complete: 85.4%; Average loss: 2.9583
Iteration: 3418; Percent complete: 85.5%; Average loss: 2.7848
Iteration: 3419; Percent complete: 85.5%; Average loss: 3.0869
Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8273
Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7476
Iteration: 3422; Percent complete: 85.5%; Average loss: 3.0241
Iteration: 3423; Percent complete: 85.6%; Average loss: 2.6781
Iteration: 3424; Percent complete: 85.6%; Average loss: 2.7106
Iteration: 3425; Percent complete: 85.6%; Average loss: 3.0663
Iteration: 3426; Percent complete: 85.7%; Average loss: 2.9754
Iteration: 3427; Percent complete: 85.7%; Average loss: 2.8757
Iteration: 3428; Percent complete: 85.7%; Average loss: 2.9953
Iteration: 3429; Percent complete: 85.7%; Average loss: 2.7984
Iteration: 3430; Percent complete: 85.8%; Average loss: 2.9903
Iteration: 3431; Percent complete: 85.8%; Average loss: 2.4823
Iteration: 3432; Percent complete: 85.8%; Average loss: 2.7056
Iteration: 3433; Percent complete: 85.8%; Average loss: 2.7967
Iteration: 3434; Percent complete: 85.9%; Average loss: 2.7027
Iteration: 3435; Percent complete: 85.9%; Average loss: 3.0630
Iteration: 3436; Percent complete: 85.9%; Average loss: 2.8327
Iteration: 3437; Percent complete: 85.9%; Average loss: 2.7289
Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8060
Iteration: 3439; Percent complete: 86.0%; Average loss: 2.7837
Iteration: 3440; Percent complete: 86.0%; Average loss: 2.7605
Iteration: 3441; Percent complete: 86.0%; Average loss: 2.7223
Iteration: 3442; Percent complete: 86.1%; Average loss: 2.9967
Iteration: 3443; Percent complete: 86.1%; Average loss: 2.7990
Iteration: 3444; Percent complete: 86.1%; Average loss: 2.6517
Iteration: 3445; Percent complete: 86.1%; Average loss: 2.7093
Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5278
Iteration: 3447; Percent complete: 86.2%; Average loss: 3.1060
Iteration: 3448; Percent complete: 86.2%; Average loss: 2.6275
Iteration: 3449; Percent complete: 86.2%; Average loss: 2.7917
Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8431
Iteration: 3451; Percent complete: 86.3%; Average loss: 2.6315
Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8888
Iteration: 3453; Percent complete: 86.3%; Average loss: 2.8647
Iteration: 3454; Percent complete: 86.4%; Average loss: 2.4642
Iteration: 3455; Percent complete: 86.4%; Average loss: 2.7782
Iteration: 3456; Percent complete: 86.4%; Average loss: 2.8487
Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8076
Iteration: 3458; Percent complete: 86.5%; Average loss: 2.4438
Iteration: 3459; Percent complete: 86.5%; Average loss: 2.7463
Iteration: 3460; Percent complete: 86.5%; Average loss: 2.6891
Iteration: 3461; Percent complete: 86.5%; Average loss: 2.6447
Iteration: 3462; Percent complete: 86.6%; Average loss: 2.6058
Iteration: 3463; Percent complete: 86.6%; Average loss: 2.7323
Iteration: 3464; Percent complete: 86.6%; Average loss: 2.7739
Iteration: 3465; Percent complete: 86.6%; Average loss: 2.7082
Iteration: 3466; Percent complete: 86.7%; Average loss: 2.8581
Iteration: 3467; Percent complete: 86.7%; Average loss: 3.0025
Iteration: 3468; Percent complete: 86.7%; Average loss: 2.7160
Iteration: 3469; Percent complete: 86.7%; Average loss: 2.7803
Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7233
Iteration: 3471; Percent complete: 86.8%; Average loss: 2.5267
Iteration: 3472; Percent complete: 86.8%; Average loss: 2.7375
Iteration: 3473; Percent complete: 86.8%; Average loss: 2.7123
Iteration: 3474; Percent complete: 86.9%; Average loss: 2.8499
Iteration: 3475; Percent complete: 86.9%; Average loss: 2.5721
Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7998
Iteration: 3477; Percent complete: 86.9%; Average loss: 2.8672
Iteration: 3478; Percent complete: 87.0%; Average loss: 2.6062
Iteration: 3479; Percent complete: 87.0%; Average loss: 2.8151
Iteration: 3480; Percent complete: 87.0%; Average loss: 2.7301
Iteration: 3481; Percent complete: 87.0%; Average loss: 2.7585
Iteration: 3482; Percent complete: 87.1%; Average loss: 2.8795
Iteration: 3483; Percent complete: 87.1%; Average loss: 2.6545
Iteration: 3484; Percent complete: 87.1%; Average loss: 2.7672
Iteration: 3485; Percent complete: 87.1%; Average loss: 2.7513
Iteration: 3486; Percent complete: 87.2%; Average loss: 2.8752
Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8168
Iteration: 3488; Percent complete: 87.2%; Average loss: 2.9565
Iteration: 3489; Percent complete: 87.2%; Average loss: 2.7897
Iteration: 3490; Percent complete: 87.2%; Average loss: 2.8318
Iteration: 3491; Percent complete: 87.3%; Average loss: 2.4434
Iteration: 3492; Percent complete: 87.3%; Average loss: 2.7260
Iteration: 3493; Percent complete: 87.3%; Average loss: 2.6695
Iteration: 3494; Percent complete: 87.4%; Average loss: 2.9198
Iteration: 3495; Percent complete: 87.4%; Average loss: 2.6407
Iteration: 3496; Percent complete: 87.4%; Average loss: 3.1342
Iteration: 3497; Percent complete: 87.4%; Average loss: 2.6249
Iteration: 3498; Percent complete: 87.5%; Average loss: 3.0354
Iteration: 3499; Percent complete: 87.5%; Average loss: 2.6396
Iteration: 3500; Percent complete: 87.5%; Average loss: 2.7259
Iteration: 3501; Percent complete: 87.5%; Average loss: 2.7380
Iteration: 3502; Percent complete: 87.5%; Average loss: 2.9445
Iteration: 3503; Percent complete: 87.6%; Average loss: 2.5588
Iteration: 3504; Percent complete: 87.6%; Average loss: 2.8837
Iteration: 3505; Percent complete: 87.6%; Average loss: 2.7732
Iteration: 3506; Percent complete: 87.6%; Average loss: 2.7966
Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8140
Iteration: 3508; Percent complete: 87.7%; Average loss: 2.7502
Iteration: 3509; Percent complete: 87.7%; Average loss: 2.8737
Iteration: 3510; Percent complete: 87.8%; Average loss: 2.5366
Iteration: 3511; Percent complete: 87.8%; Average loss: 2.8272
Iteration: 3512; Percent complete: 87.8%; Average loss: 2.8242
Iteration: 3513; Percent complete: 87.8%; Average loss: 2.5206
Iteration: 3514; Percent complete: 87.8%; Average loss: 2.7685
Iteration: 3515; Percent complete: 87.9%; Average loss: 2.5191
Iteration: 3516; Percent complete: 87.9%; Average loss: 2.6273
Iteration: 3517; Percent complete: 87.9%; Average loss: 2.7113
Iteration: 3518; Percent complete: 87.9%; Average loss: 2.7691
Iteration: 3519; Percent complete: 88.0%; Average loss: 2.7507
Iteration: 3520; Percent complete: 88.0%; Average loss: 2.6292
Iteration: 3521; Percent complete: 88.0%; Average loss: 2.7815
Iteration: 3522; Percent complete: 88.0%; Average loss: 2.8682
Iteration: 3523; Percent complete: 88.1%; Average loss: 2.6226
Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7736
Iteration: 3525; Percent complete: 88.1%; Average loss: 2.5896
Iteration: 3526; Percent complete: 88.1%; Average loss: 2.7114
Iteration: 3527; Percent complete: 88.2%; Average loss: 2.7527
Iteration: 3528; Percent complete: 88.2%; Average loss: 2.9691
Iteration: 3529; Percent complete: 88.2%; Average loss: 2.7432
Iteration: 3530; Percent complete: 88.2%; Average loss: 2.7543
Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7214
Iteration: 3532; Percent complete: 88.3%; Average loss: 2.7186
Iteration: 3533; Percent complete: 88.3%; Average loss: 2.8295
Iteration: 3534; Percent complete: 88.3%; Average loss: 2.8928
Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8877
Iteration: 3536; Percent complete: 88.4%; Average loss: 2.8401
Iteration: 3537; Percent complete: 88.4%; Average loss: 2.6779
Iteration: 3538; Percent complete: 88.4%; Average loss: 2.7671
Iteration: 3539; Percent complete: 88.5%; Average loss: 2.9584
Iteration: 3540; Percent complete: 88.5%; Average loss: 2.5945
Iteration: 3541; Percent complete: 88.5%; Average loss: 2.5690
Iteration: 3542; Percent complete: 88.5%; Average loss: 2.5005
Iteration: 3543; Percent complete: 88.6%; Average loss: 2.8044
Iteration: 3544; Percent complete: 88.6%; Average loss: 2.5951
Iteration: 3545; Percent complete: 88.6%; Average loss: 2.5640
Iteration: 3546; Percent complete: 88.6%; Average loss: 2.8776
Iteration: 3547; Percent complete: 88.7%; Average loss: 2.7037
Iteration: 3548; Percent complete: 88.7%; Average loss: 2.7002
Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7734
Iteration: 3550; Percent complete: 88.8%; Average loss: 2.9073
Iteration: 3551; Percent complete: 88.8%; Average loss: 2.7140
Iteration: 3552; Percent complete: 88.8%; Average loss: 2.5015
Iteration: 3553; Percent complete: 88.8%; Average loss: 2.8978
Iteration: 3554; Percent complete: 88.8%; Average loss: 2.7488
Iteration: 3555; Percent complete: 88.9%; Average loss: 2.5834
Iteration: 3556; Percent complete: 88.9%; Average loss: 2.7427
Iteration: 3557; Percent complete: 88.9%; Average loss: 2.7349
Iteration: 3558; Percent complete: 88.9%; Average loss: 2.6462
Iteration: 3559; Percent complete: 89.0%; Average loss: 2.7126
Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6679
Iteration: 3561; Percent complete: 89.0%; Average loss: 3.0023
Iteration: 3562; Percent complete: 89.0%; Average loss: 2.8293
Iteration: 3563; Percent complete: 89.1%; Average loss: 2.9573
Iteration: 3564; Percent complete: 89.1%; Average loss: 2.8934
Iteration: 3565; Percent complete: 89.1%; Average loss: 3.0389
Iteration: 3566; Percent complete: 89.1%; Average loss: 2.5417
Iteration: 3567; Percent complete: 89.2%; Average loss: 2.4963
Iteration: 3568; Percent complete: 89.2%; Average loss: 2.5774
Iteration: 3569; Percent complete: 89.2%; Average loss: 2.7792
Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8621
Iteration: 3571; Percent complete: 89.3%; Average loss: 2.6069
Iteration: 3572; Percent complete: 89.3%; Average loss: 3.0751
Iteration: 3573; Percent complete: 89.3%; Average loss: 2.7948
Iteration: 3574; Percent complete: 89.3%; Average loss: 2.6695
Iteration: 3575; Percent complete: 89.4%; Average loss: 2.7968
Iteration: 3576; Percent complete: 89.4%; Average loss: 2.8746
Iteration: 3577; Percent complete: 89.4%; Average loss: 2.8912
Iteration: 3578; Percent complete: 89.5%; Average loss: 2.9147
Iteration: 3579; Percent complete: 89.5%; Average loss: 2.8887
Iteration: 3580; Percent complete: 89.5%; Average loss: 2.6602
Iteration: 3581; Percent complete: 89.5%; Average loss: 2.8228
Iteration: 3582; Percent complete: 89.5%; Average loss: 2.9505
Iteration: 3583; Percent complete: 89.6%; Average loss: 2.8533
Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7067
Iteration: 3585; Percent complete: 89.6%; Average loss: 2.7401
Iteration: 3586; Percent complete: 89.6%; Average loss: 2.6808
Iteration: 3587; Percent complete: 89.7%; Average loss: 2.6501
Iteration: 3588; Percent complete: 89.7%; Average loss: 2.4566
Iteration: 3589; Percent complete: 89.7%; Average loss: 2.8453
Iteration: 3590; Percent complete: 89.8%; Average loss: 2.9385
Iteration: 3591; Percent complete: 89.8%; Average loss: 2.5139
Iteration: 3592; Percent complete: 89.8%; Average loss: 2.6738
Iteration: 3593; Percent complete: 89.8%; Average loss: 2.7439
Iteration: 3594; Percent complete: 89.8%; Average loss: 2.6203
Iteration: 3595; Percent complete: 89.9%; Average loss: 2.7321
Iteration: 3596; Percent complete: 89.9%; Average loss: 2.8466
Iteration: 3597; Percent complete: 89.9%; Average loss: 2.8582
Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7680
Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6733
Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7236
Iteration: 3601; Percent complete: 90.0%; Average loss: 2.7208
Iteration: 3602; Percent complete: 90.0%; Average loss: 2.8232
Iteration: 3603; Percent complete: 90.1%; Average loss: 2.8498
Iteration: 3604; Percent complete: 90.1%; Average loss: 2.8375
Iteration: 3605; Percent complete: 90.1%; Average loss: 2.5860
Iteration: 3606; Percent complete: 90.1%; Average loss: 2.7870
Iteration: 3607; Percent complete: 90.2%; Average loss: 2.3556
Iteration: 3608; Percent complete: 90.2%; Average loss: 2.7868
Iteration: 3609; Percent complete: 90.2%; Average loss: 2.7471
Iteration: 3610; Percent complete: 90.2%; Average loss: 2.7859
Iteration: 3611; Percent complete: 90.3%; Average loss: 2.6757
Iteration: 3612; Percent complete: 90.3%; Average loss: 2.8288
Iteration: 3613; Percent complete: 90.3%; Average loss: 3.0294
Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6438
Iteration: 3615; Percent complete: 90.4%; Average loss: 2.9528
Iteration: 3616; Percent complete: 90.4%; Average loss: 2.6048
Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6991
Iteration: 3618; Percent complete: 90.5%; Average loss: 2.5222
Iteration: 3619; Percent complete: 90.5%; Average loss: 2.6258
Iteration: 3620; Percent complete: 90.5%; Average loss: 2.6860
Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7846
Iteration: 3622; Percent complete: 90.5%; Average loss: 2.7449
Iteration: 3623; Percent complete: 90.6%; Average loss: 2.7723
Iteration: 3624; Percent complete: 90.6%; Average loss: 2.5565
Iteration: 3625; Percent complete: 90.6%; Average loss: 2.4947
Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6439
Iteration: 3627; Percent complete: 90.7%; Average loss: 2.6985
Iteration: 3628; Percent complete: 90.7%; Average loss: 2.7331
Iteration: 3629; Percent complete: 90.7%; Average loss: 2.7844
Iteration: 3630; Percent complete: 90.8%; Average loss: 2.7134
Iteration: 3631; Percent complete: 90.8%; Average loss: 2.6845
Iteration: 3632; Percent complete: 90.8%; Average loss: 2.5318
Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7819
Iteration: 3634; Percent complete: 90.8%; Average loss: 2.8330
Iteration: 3635; Percent complete: 90.9%; Average loss: 2.6000
Iteration: 3636; Percent complete: 90.9%; Average loss: 2.4740
Iteration: 3637; Percent complete: 90.9%; Average loss: 2.8126
Iteration: 3638; Percent complete: 91.0%; Average loss: 2.7081
Iteration: 3639; Percent complete: 91.0%; Average loss: 2.4977
Iteration: 3640; Percent complete: 91.0%; Average loss: 2.5915
Iteration: 3641; Percent complete: 91.0%; Average loss: 2.3142
Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7087
Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7890
Iteration: 3644; Percent complete: 91.1%; Average loss: 2.5196
Iteration: 3645; Percent complete: 91.1%; Average loss: 2.6911
Iteration: 3646; Percent complete: 91.1%; Average loss: 2.7318
Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6789
Iteration: 3648; Percent complete: 91.2%; Average loss: 2.6548
Iteration: 3649; Percent complete: 91.2%; Average loss: 2.5602
Iteration: 3650; Percent complete: 91.2%; Average loss: 2.5718
Iteration: 3651; Percent complete: 91.3%; Average loss: 2.6904
Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7021
Iteration: 3653; Percent complete: 91.3%; Average loss: 2.8412
Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6709
Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6757
Iteration: 3656; Percent complete: 91.4%; Average loss: 2.7199
Iteration: 3657; Percent complete: 91.4%; Average loss: 2.5588
Iteration: 3658; Percent complete: 91.5%; Average loss: 2.7798
Iteration: 3659; Percent complete: 91.5%; Average loss: 2.7170
Iteration: 3660; Percent complete: 91.5%; Average loss: 2.1945
Iteration: 3661; Percent complete: 91.5%; Average loss: 2.6969
Iteration: 3662; Percent complete: 91.5%; Average loss: 2.7817
Iteration: 3663; Percent complete: 91.6%; Average loss: 2.4590
Iteration: 3664; Percent complete: 91.6%; Average loss: 2.5740
Iteration: 3665; Percent complete: 91.6%; Average loss: 2.5805
Iteration: 3666; Percent complete: 91.6%; Average loss: 2.6993
Iteration: 3667; Percent complete: 91.7%; Average loss: 2.6136
Iteration: 3668; Percent complete: 91.7%; Average loss: 2.6798
Iteration: 3669; Percent complete: 91.7%; Average loss: 2.6772
Iteration: 3670; Percent complete: 91.8%; Average loss: 2.6185
Iteration: 3671; Percent complete: 91.8%; Average loss: 2.9239
Iteration: 3672; Percent complete: 91.8%; Average loss: 2.4581
Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6778
Iteration: 3674; Percent complete: 91.8%; Average loss: 2.5263
Iteration: 3675; Percent complete: 91.9%; Average loss: 2.8935
Iteration: 3676; Percent complete: 91.9%; Average loss: 2.6322
Iteration: 3677; Percent complete: 91.9%; Average loss: 2.8365
Iteration: 3678; Percent complete: 92.0%; Average loss: 2.5844
Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7383
Iteration: 3680; Percent complete: 92.0%; Average loss: 2.5843
Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7317
Iteration: 3682; Percent complete: 92.0%; Average loss: 2.7277
Iteration: 3683; Percent complete: 92.1%; Average loss: 2.6092
Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6693
Iteration: 3685; Percent complete: 92.1%; Average loss: 2.4912
Iteration: 3686; Percent complete: 92.2%; Average loss: 2.6113
Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6892
Iteration: 3688; Percent complete: 92.2%; Average loss: 2.5918
Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5398
Iteration: 3690; Percent complete: 92.2%; Average loss: 2.7778
Iteration: 3691; Percent complete: 92.3%; Average loss: 2.6075
Iteration: 3692; Percent complete: 92.3%; Average loss: 2.6261
Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7985
Iteration: 3694; Percent complete: 92.3%; Average loss: 2.6538
Iteration: 3695; Percent complete: 92.4%; Average loss: 2.8084
Iteration: 3696; Percent complete: 92.4%; Average loss: 2.8812
Iteration: 3697; Percent complete: 92.4%; Average loss: 2.8325
Iteration: 3698; Percent complete: 92.5%; Average loss: 2.8862
Iteration: 3699; Percent complete: 92.5%; Average loss: 2.8058
Iteration: 3700; Percent complete: 92.5%; Average loss: 2.7812
Iteration: 3701; Percent complete: 92.5%; Average loss: 2.7479
Iteration: 3702; Percent complete: 92.5%; Average loss: 2.9007
Iteration: 3703; Percent complete: 92.6%; Average loss: 2.7062
Iteration: 3704; Percent complete: 92.6%; Average loss: 2.6978
Iteration: 3705; Percent complete: 92.6%; Average loss: 3.0413
Iteration: 3706; Percent complete: 92.7%; Average loss: 2.6656
Iteration: 3707; Percent complete: 92.7%; Average loss: 2.7610
Iteration: 3708; Percent complete: 92.7%; Average loss: 2.7962
Iteration: 3709; Percent complete: 92.7%; Average loss: 2.6843
Iteration: 3710; Percent complete: 92.8%; Average loss: 2.6062
Iteration: 3711; Percent complete: 92.8%; Average loss: 2.7765
Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6372
Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6883
Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5702
Iteration: 3715; Percent complete: 92.9%; Average loss: 2.7026
Iteration: 3716; Percent complete: 92.9%; Average loss: 2.8407
Iteration: 3717; Percent complete: 92.9%; Average loss: 2.6249
Iteration: 3718; Percent complete: 93.0%; Average loss: 2.7596
Iteration: 3719; Percent complete: 93.0%; Average loss: 2.8515
Iteration: 3720; Percent complete: 93.0%; Average loss: 2.5251
Iteration: 3721; Percent complete: 93.0%; Average loss: 2.8361
Iteration: 3722; Percent complete: 93.0%; Average loss: 2.3973
Iteration: 3723; Percent complete: 93.1%; Average loss: 2.7257
Iteration: 3724; Percent complete: 93.1%; Average loss: 2.7930
Iteration: 3725; Percent complete: 93.1%; Average loss: 2.7132
Iteration: 3726; Percent complete: 93.2%; Average loss: 2.8860
Iteration: 3727; Percent complete: 93.2%; Average loss: 2.5148
Iteration: 3728; Percent complete: 93.2%; Average loss: 2.9264
Iteration: 3729; Percent complete: 93.2%; Average loss: 2.7717
Iteration: 3730; Percent complete: 93.2%; Average loss: 2.4697
Iteration: 3731; Percent complete: 93.3%; Average loss: 2.8212
Iteration: 3732; Percent complete: 93.3%; Average loss: 2.6055
Iteration: 3733; Percent complete: 93.3%; Average loss: 2.7805
Iteration: 3734; Percent complete: 93.3%; Average loss: 2.6400
Iteration: 3735; Percent complete: 93.4%; Average loss: 2.8751
Iteration: 3736; Percent complete: 93.4%; Average loss: 2.9432
Iteration: 3737; Percent complete: 93.4%; Average loss: 2.8112
Iteration: 3738; Percent complete: 93.5%; Average loss: 2.6234
Iteration: 3739; Percent complete: 93.5%; Average loss: 2.6543
Iteration: 3740; Percent complete: 93.5%; Average loss: 2.7528
Iteration: 3741; Percent complete: 93.5%; Average loss: 2.5411
Iteration: 3742; Percent complete: 93.5%; Average loss: 2.4673
Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6613
Iteration: 3744; Percent complete: 93.6%; Average loss: 2.7095
Iteration: 3745; Percent complete: 93.6%; Average loss: 2.9082
Iteration: 3746; Percent complete: 93.7%; Average loss: 2.6593
Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6911
Iteration: 3748; Percent complete: 93.7%; Average loss: 2.5851
Iteration: 3749; Percent complete: 93.7%; Average loss: 2.6174
Iteration: 3750; Percent complete: 93.8%; Average loss: 2.7568
Iteration: 3751; Percent complete: 93.8%; Average loss: 2.7105
Iteration: 3752; Percent complete: 93.8%; Average loss: 2.7509
Iteration: 3753; Percent complete: 93.8%; Average loss: 2.6668
Iteration: 3754; Percent complete: 93.8%; Average loss: 2.6777
Iteration: 3755; Percent complete: 93.9%; Average loss: 2.7127
Iteration: 3756; Percent complete: 93.9%; Average loss: 2.7050
Iteration: 3757; Percent complete: 93.9%; Average loss: 2.9084
Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6841
Iteration: 3759; Percent complete: 94.0%; Average loss: 2.5651
Iteration: 3760; Percent complete: 94.0%; Average loss: 2.5326
Iteration: 3761; Percent complete: 94.0%; Average loss: 2.5820
Iteration: 3762; Percent complete: 94.0%; Average loss: 2.5241
Iteration: 3763; Percent complete: 94.1%; Average loss: 2.6565
Iteration: 3764; Percent complete: 94.1%; Average loss: 2.6826
Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6434
Iteration: 3766; Percent complete: 94.2%; Average loss: 2.6483
Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6925
Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5389
Iteration: 3769; Percent complete: 94.2%; Average loss: 2.6955
Iteration: 3770; Percent complete: 94.2%; Average loss: 2.7920
Iteration: 3771; Percent complete: 94.3%; Average loss: 2.7962
Iteration: 3772; Percent complete: 94.3%; Average loss: 2.7930
Iteration: 3773; Percent complete: 94.3%; Average loss: 2.6847
Iteration: 3774; Percent complete: 94.3%; Average loss: 2.7743
Iteration: 3775; Percent complete: 94.4%; Average loss: 2.6346
Iteration: 3776; Percent complete: 94.4%; Average loss: 2.6962
Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7107
Iteration: 3778; Percent complete: 94.5%; Average loss: 2.6187
Iteration: 3779; Percent complete: 94.5%; Average loss: 2.7352
Iteration: 3780; Percent complete: 94.5%; Average loss: 2.7330
Iteration: 3781; Percent complete: 94.5%; Average loss: 2.7142
Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6425
Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6270
Iteration: 3784; Percent complete: 94.6%; Average loss: 2.7350
Iteration: 3785; Percent complete: 94.6%; Average loss: 2.6032
Iteration: 3786; Percent complete: 94.7%; Average loss: 2.5346
Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7918
Iteration: 3788; Percent complete: 94.7%; Average loss: 2.6616
Iteration: 3789; Percent complete: 94.7%; Average loss: 2.6691
Iteration: 3790; Percent complete: 94.8%; Average loss: 2.5599
Iteration: 3791; Percent complete: 94.8%; Average loss: 2.6638
Iteration: 3792; Percent complete: 94.8%; Average loss: 2.6441
Iteration: 3793; Percent complete: 94.8%; Average loss: 2.6998
Iteration: 3794; Percent complete: 94.8%; Average loss: 2.7123
Iteration: 3795; Percent complete: 94.9%; Average loss: 2.7011
Iteration: 3796; Percent complete: 94.9%; Average loss: 2.5441
Iteration: 3797; Percent complete: 94.9%; Average loss: 2.4698
Iteration: 3798; Percent complete: 95.0%; Average loss: 2.6847
Iteration: 3799; Percent complete: 95.0%; Average loss: 2.7156
Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6954
Iteration: 3801; Percent complete: 95.0%; Average loss: 2.7549
Iteration: 3802; Percent complete: 95.0%; Average loss: 2.6027
Iteration: 3803; Percent complete: 95.1%; Average loss: 2.6849
Iteration: 3804; Percent complete: 95.1%; Average loss: 2.7157
Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6573
Iteration: 3806; Percent complete: 95.2%; Average loss: 2.6000
Iteration: 3807; Percent complete: 95.2%; Average loss: 2.8664
Iteration: 3808; Percent complete: 95.2%; Average loss: 2.5344
Iteration: 3809; Percent complete: 95.2%; Average loss: 2.7704
Iteration: 3810; Percent complete: 95.2%; Average loss: 2.6243
Iteration: 3811; Percent complete: 95.3%; Average loss: 2.2530
Iteration: 3812; Percent complete: 95.3%; Average loss: 2.6777
Iteration: 3813; Percent complete: 95.3%; Average loss: 2.5203
Iteration: 3814; Percent complete: 95.3%; Average loss: 2.7517
Iteration: 3815; Percent complete: 95.4%; Average loss: 2.4611
Iteration: 3816; Percent complete: 95.4%; Average loss: 2.5626
Iteration: 3817; Percent complete: 95.4%; Average loss: 2.2729
Iteration: 3818; Percent complete: 95.5%; Average loss: 2.7028
Iteration: 3819; Percent complete: 95.5%; Average loss: 2.6521
Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6974
Iteration: 3821; Percent complete: 95.5%; Average loss: 2.5632
Iteration: 3822; Percent complete: 95.5%; Average loss: 2.7989
Iteration: 3823; Percent complete: 95.6%; Average loss: 2.5610
Iteration: 3824; Percent complete: 95.6%; Average loss: 2.7097
Iteration: 3825; Percent complete: 95.6%; Average loss: 2.8630
Iteration: 3826; Percent complete: 95.7%; Average loss: 2.7525
Iteration: 3827; Percent complete: 95.7%; Average loss: 2.7541
Iteration: 3828; Percent complete: 95.7%; Average loss: 2.7641
Iteration: 3829; Percent complete: 95.7%; Average loss: 2.7419
Iteration: 3830; Percent complete: 95.8%; Average loss: 2.4675
Iteration: 3831; Percent complete: 95.8%; Average loss: 2.5662
Iteration: 3832; Percent complete: 95.8%; Average loss: 2.5485
Iteration: 3833; Percent complete: 95.8%; Average loss: 2.6425
Iteration: 3834; Percent complete: 95.9%; Average loss: 2.5751
Iteration: 3835; Percent complete: 95.9%; Average loss: 2.5921
Iteration: 3836; Percent complete: 95.9%; Average loss: 2.4374
Iteration: 3837; Percent complete: 95.9%; Average loss: 2.5669
Iteration: 3838; Percent complete: 96.0%; Average loss: 2.4904
Iteration: 3839; Percent complete: 96.0%; Average loss: 2.5498
Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6096
Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7638
Iteration: 3842; Percent complete: 96.0%; Average loss: 2.7508
Iteration: 3843; Percent complete: 96.1%; Average loss: 2.8292
Iteration: 3844; Percent complete: 96.1%; Average loss: 2.6815
Iteration: 3845; Percent complete: 96.1%; Average loss: 2.9526
Iteration: 3846; Percent complete: 96.2%; Average loss: 2.4789
Iteration: 3847; Percent complete: 96.2%; Average loss: 2.4900
Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6332
Iteration: 3849; Percent complete: 96.2%; Average loss: 2.5425
Iteration: 3850; Percent complete: 96.2%; Average loss: 2.5071
Iteration: 3851; Percent complete: 96.3%; Average loss: 2.6764
Iteration: 3852; Percent complete: 96.3%; Average loss: 2.6213
Iteration: 3853; Percent complete: 96.3%; Average loss: 2.6265
Iteration: 3854; Percent complete: 96.4%; Average loss: 2.4832
Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7230
Iteration: 3856; Percent complete: 96.4%; Average loss: 2.6200
Iteration: 3857; Percent complete: 96.4%; Average loss: 2.6516
Iteration: 3858; Percent complete: 96.5%; Average loss: 2.7206
Iteration: 3859; Percent complete: 96.5%; Average loss: 2.7094
Iteration: 3860; Percent complete: 96.5%; Average loss: 2.7111
Iteration: 3861; Percent complete: 96.5%; Average loss: 2.5660
Iteration: 3862; Percent complete: 96.5%; Average loss: 2.4195
Iteration: 3863; Percent complete: 96.6%; Average loss: 2.8211
Iteration: 3864; Percent complete: 96.6%; Average loss: 2.6808
Iteration: 3865; Percent complete: 96.6%; Average loss: 2.4598
Iteration: 3866; Percent complete: 96.7%; Average loss: 2.6108
Iteration: 3867; Percent complete: 96.7%; Average loss: 2.7470
Iteration: 3868; Percent complete: 96.7%; Average loss: 2.7210
Iteration: 3869; Percent complete: 96.7%; Average loss: 2.5721
Iteration: 3870; Percent complete: 96.8%; Average loss: 2.5732
Iteration: 3871; Percent complete: 96.8%; Average loss: 2.6927
Iteration: 3872; Percent complete: 96.8%; Average loss: 2.8439
Iteration: 3873; Percent complete: 96.8%; Average loss: 2.7675
Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6716
Iteration: 3875; Percent complete: 96.9%; Average loss: 2.8848
Iteration: 3876; Percent complete: 96.9%; Average loss: 2.5334
Iteration: 3877; Percent complete: 96.9%; Average loss: 2.6027
Iteration: 3878; Percent complete: 97.0%; Average loss: 2.5435
Iteration: 3879; Percent complete: 97.0%; Average loss: 2.6349
Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6351
Iteration: 3881; Percent complete: 97.0%; Average loss: 2.4971
Iteration: 3882; Percent complete: 97.0%; Average loss: 2.4575
Iteration: 3883; Percent complete: 97.1%; Average loss: 2.6995
Iteration: 3884; Percent complete: 97.1%; Average loss: 2.5582
Iteration: 3885; Percent complete: 97.1%; Average loss: 2.4982
Iteration: 3886; Percent complete: 97.2%; Average loss: 2.8728
Iteration: 3887; Percent complete: 97.2%; Average loss: 2.6808
Iteration: 3888; Percent complete: 97.2%; Average loss: 2.5871
Iteration: 3889; Percent complete: 97.2%; Average loss: 2.6470
Iteration: 3890; Percent complete: 97.2%; Average loss: 2.7127
Iteration: 3891; Percent complete: 97.3%; Average loss: 2.6149
Iteration: 3892; Percent complete: 97.3%; Average loss: 2.6328
Iteration: 3893; Percent complete: 97.3%; Average loss: 2.5805
Iteration: 3894; Percent complete: 97.4%; Average loss: 2.5563
Iteration: 3895; Percent complete: 97.4%; Average loss: 2.4224
Iteration: 3896; Percent complete: 97.4%; Average loss: 2.6262
Iteration: 3897; Percent complete: 97.4%; Average loss: 2.3236
Iteration: 3898; Percent complete: 97.5%; Average loss: 2.4078
Iteration: 3899; Percent complete: 97.5%; Average loss: 2.7367
Iteration: 3900; Percent complete: 97.5%; Average loss: 2.5749
Iteration: 3901; Percent complete: 97.5%; Average loss: 2.5296
Iteration: 3902; Percent complete: 97.5%; Average loss: 2.6909
Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6074
Iteration: 3904; Percent complete: 97.6%; Average loss: 2.6102
Iteration: 3905; Percent complete: 97.6%; Average loss: 2.5208
Iteration: 3906; Percent complete: 97.7%; Average loss: 2.7649
Iteration: 3907; Percent complete: 97.7%; Average loss: 2.3827
Iteration: 3908; Percent complete: 97.7%; Average loss: 2.6105
Iteration: 3909; Percent complete: 97.7%; Average loss: 2.7437
Iteration: 3910; Percent complete: 97.8%; Average loss: 2.6139
Iteration: 3911; Percent complete: 97.8%; Average loss: 2.5302
Iteration: 3912; Percent complete: 97.8%; Average loss: 2.7742
Iteration: 3913; Percent complete: 97.8%; Average loss: 2.6821
Iteration: 3914; Percent complete: 97.9%; Average loss: 2.9002
Iteration: 3915; Percent complete: 97.9%; Average loss: 2.6962
Iteration: 3916; Percent complete: 97.9%; Average loss: 2.5186
Iteration: 3917; Percent complete: 97.9%; Average loss: 2.4853
Iteration: 3918; Percent complete: 98.0%; Average loss: 2.5459
Iteration: 3919; Percent complete: 98.0%; Average loss: 2.5058
Iteration: 3920; Percent complete: 98.0%; Average loss: 2.5340
Iteration: 3921; Percent complete: 98.0%; Average loss: 2.6730
Iteration: 3922; Percent complete: 98.0%; Average loss: 2.4618
Iteration: 3923; Percent complete: 98.1%; Average loss: 2.6910
Iteration: 3924; Percent complete: 98.1%; Average loss: 2.5462
Iteration: 3925; Percent complete: 98.1%; Average loss: 2.7956
Iteration: 3926; Percent complete: 98.2%; Average loss: 2.6415
Iteration: 3927; Percent complete: 98.2%; Average loss: 2.8166
Iteration: 3928; Percent complete: 98.2%; Average loss: 2.7054
Iteration: 3929; Percent complete: 98.2%; Average loss: 2.5100
Iteration: 3930; Percent complete: 98.2%; Average loss: 2.6257
Iteration: 3931; Percent complete: 98.3%; Average loss: 2.5922
Iteration: 3932; Percent complete: 98.3%; Average loss: 2.5230
Iteration: 3933; Percent complete: 98.3%; Average loss: 2.6372
Iteration: 3934; Percent complete: 98.4%; Average loss: 2.5645
Iteration: 3935; Percent complete: 98.4%; Average loss: 2.6899
Iteration: 3936; Percent complete: 98.4%; Average loss: 2.7416
Iteration: 3937; Percent complete: 98.4%; Average loss: 2.7057
Iteration: 3938; Percent complete: 98.5%; Average loss: 2.8595
Iteration: 3939; Percent complete: 98.5%; Average loss: 2.6063
Iteration: 3940; Percent complete: 98.5%; Average loss: 2.7413
Iteration: 3941; Percent complete: 98.5%; Average loss: 2.4071
Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5229
Iteration: 3943; Percent complete: 98.6%; Average loss: 2.5431
Iteration: 3944; Percent complete: 98.6%; Average loss: 2.8482
Iteration: 3945; Percent complete: 98.6%; Average loss: 2.7996
Iteration: 3946; Percent complete: 98.7%; Average loss: 2.7710
Iteration: 3947; Percent complete: 98.7%; Average loss: 2.7102
Iteration: 3948; Percent complete: 98.7%; Average loss: 2.6340
Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6396
Iteration: 3950; Percent complete: 98.8%; Average loss: 2.7269
Iteration: 3951; Percent complete: 98.8%; Average loss: 2.5554
Iteration: 3952; Percent complete: 98.8%; Average loss: 2.5558
Iteration: 3953; Percent complete: 98.8%; Average loss: 2.2954
Iteration: 3954; Percent complete: 98.9%; Average loss: 2.7160
Iteration: 3955; Percent complete: 98.9%; Average loss: 2.7652
Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6656
Iteration: 3957; Percent complete: 98.9%; Average loss: 2.4357
Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4752
Iteration: 3959; Percent complete: 99.0%; Average loss: 2.4079
Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6974
Iteration: 3961; Percent complete: 99.0%; Average loss: 2.6180
Iteration: 3962; Percent complete: 99.1%; Average loss: 2.5828
Iteration: 3963; Percent complete: 99.1%; Average loss: 2.7850
Iteration: 3964; Percent complete: 99.1%; Average loss: 2.6524
Iteration: 3965; Percent complete: 99.1%; Average loss: 2.6048
Iteration: 3966; Percent complete: 99.2%; Average loss: 2.4392
Iteration: 3967; Percent complete: 99.2%; Average loss: 2.7728
Iteration: 3968; Percent complete: 99.2%; Average loss: 2.5519
Iteration: 3969; Percent complete: 99.2%; Average loss: 2.7046
Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5091
Iteration: 3971; Percent complete: 99.3%; Average loss: 2.5323
Iteration: 3972; Percent complete: 99.3%; Average loss: 2.5706
Iteration: 3973; Percent complete: 99.3%; Average loss: 2.5353
Iteration: 3974; Percent complete: 99.4%; Average loss: 2.8306
Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5334
Iteration: 3976; Percent complete: 99.4%; Average loss: 2.5948
Iteration: 3977; Percent complete: 99.4%; Average loss: 2.8497
Iteration: 3978; Percent complete: 99.5%; Average loss: 2.7015
Iteration: 3979; Percent complete: 99.5%; Average loss: 2.6840
Iteration: 3980; Percent complete: 99.5%; Average loss: 2.5775
Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5468
Iteration: 3982; Percent complete: 99.6%; Average loss: 2.4048
Iteration: 3983; Percent complete: 99.6%; Average loss: 2.4671
Iteration: 3984; Percent complete: 99.6%; Average loss: 2.6611
Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6338
Iteration: 3986; Percent complete: 99.7%; Average loss: 2.7020
Iteration: 3987; Percent complete: 99.7%; Average loss: 2.5422
Iteration: 3988; Percent complete: 99.7%; Average loss: 2.6078
Iteration: 3989; Percent complete: 99.7%; Average loss: 2.4650
Iteration: 3990; Percent complete: 99.8%; Average loss: 2.5320
Iteration: 3991; Percent complete: 99.8%; Average loss: 2.5551
Iteration: 3992; Percent complete: 99.8%; Average loss: 2.7428
Iteration: 3993; Percent complete: 99.8%; Average loss: 2.5203
Iteration: 3994; Percent complete: 99.9%; Average loss: 2.4324
Iteration: 3995; Percent complete: 99.9%; Average loss: 2.7006
Iteration: 3996; Percent complete: 99.9%; Average loss: 2.4844
Iteration: 3997; Percent complete: 99.9%; Average loss: 2.4931
Iteration: 3998; Percent complete: 100.0%; Average loss: 2.6130
Iteration: 3999; Percent complete: 100.0%; Average loss: 2.5353
Iteration: 4000; Percent complete: 100.0%; Average loss: 2.6168
</pre></div>
</div>
</section>
<section id="run-evaluation">
<h3>Run Evaluation<a class="headerlink" href="#run-evaluation" title="Link to this heading">#</a></h3>
<p>To chat with your model, run the following block.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set dropout layers to ``eval`` mode</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval"><span class="n">encoder</span><span class="o">.</span><span class="n">eval</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval"><span class="n">decoder</span><span class="o">.</span><span class="n">eval</span></a><span class="p">()</span>

<span class="c1"># Initialize search module</span>
<span class="n">searcher</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">GreedySearchDecoder</span></a><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>

<span class="c1"># Begin chatting (uncomment and run the following line to begin)</span>
<span class="c1"># evaluateInput(encoder, decoder, searcher, voc)</span>
</pre></div>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>That’s all for this one, folks. Congratulations, you now know the
fundamentals to building a generative chatbot model! If you’re
interested, you can try tailoring the chatbot’s behavior by tweaking the
model and training parameters and customizing the data that you train
the model on.</p>
<p>Check out the other tutorials for more cool deep learning applications
in PyTorch!</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (2 minutes 23.402 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-chatbot-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/44a84f8c1764dbf61662d306ff9ed43a/chatbot_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">chatbot_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/d90127b0deeb355be3a350521d770206/chatbot_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">chatbot_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/38f7ebe164dfda80cf255d96796423f1/chatbot_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">chatbot_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparations">Preparations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-preprocess-data">Load &amp; Preprocess Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-formatted-data-file">Create formatted data file</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-and-trim-data">Load and trim data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-data-for-models">Prepare Data for Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-models">Define Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seq2seq-model">Seq2Seq Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-training-procedure">Define Training Procedure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-loss">Masked loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-training-iteration">Single training iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-iterations">Training iterations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-evaluation">Define Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-decoding">Greedy decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-my-text">Evaluate my text</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-model">Run Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training">Run Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-evaluation">Run Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Chatbot Tutorial",
       "headline": "Chatbot Tutorial",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/chatbot_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. Chatbot Tutorial# Author: Matthew Inkawhich In this tutorial, we explore a fun and interesting use-case of recurrent sequence-to-sequence models. We will train a simple chatbot using movie scripts from the Cornell Movie-Dialogs Corpus. Conversational models are a hot topic in artificial intelligence research. Chatbots can be found in a variety of settings, including customer service applications and online helpdesks. These bots are often powered by retrieval-based models, which output predefined responses to questions of certain forms. In a highly restricted domain like a company\u2019s IT helpdesk, these models may be sufficient, however, they are not robust enough for more general use-cases. Teaching a machine to carry out a meaningful conversation with a human in multiple domains is a research question that is far from solved. Recently, the deep learning boom has allowed for powerful generative models like Google\u2019s Neural Conversational Model, which marks a large step towards multi-domain generative conversational models. In this tutorial, we will implement this kind of model in PyTorch. \u003e hello? Bot: hello . \u003e where am I? Bot: you re in a hospital . \u003e who are you? Bot: i m a lawyer . \u003e how are you doing? Bot: i m fine . \u003e are you my friend? Bot: no . \u003e you\u0027re under arrest Bot: i m trying to help you ! \u003e i\u0027m just kidding Bot: i m sorry . \u003e where are you from? Bot: san francisco . \u003e it\u0027s time for me to leave Bot: i know . \u003e goodbye Bot: goodbye . Tutorial Highlights Handle loading and preprocessing of Cornell Movie-Dialogs Corpus dataset Implement a sequence-to-sequence model with Luong attention mechanism(s) Jointly train encoder and decoder models using mini-batches Implement greedy-search decoding module Interact with trained chatbot Acknowledgments This tutorial borrows code from the following sources: Yuan-Kuei Wu\u2019s pytorch-chatbot implementation: ywk991112/pytorch-chatbot Sean Robertson\u2019s practical-pytorch seq2seq-translation example: spro/practical-pytorch FloydHub Cornell Movie Corpus preprocessing code: floydhub/textutil-preprocess-cornell-movie-corpus Preparations# To get started, download the Movie-Dialogs Corpus zip file. # and put in a ``data/`` directory under the current directory. # # After that, let\u2019s import some necessities. # import torch from torch.jit import script, trace import torch.nn as nn from torch import optim import torch.nn.functional as F import csv import random import re import os import unicodedata import codecs from io import open import itertools import math import json # If the current `accelerator \u003chttps://pytorch.org/docs/stable/torch.html#accelerators\u003e`__ is available, # we will use it. Otherwise, we use the CPU. device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\" print(f\"Using {device} device\") Using cuda device Load \u0026 Preprocess Data# The next step is to reformat our data file and load the data into structures that we can work with. The Cornell Movie-Dialogs Corpus is a rich dataset of movie character dialog: 220,579 conversational exchanges between 10,292 pairs of movie characters 9,035 characters from 617 movies 304,713 total utterances This dataset is large and diverse, and there is a great variation of language formality, time periods, sentiment, etc. Our hope is that this diversity makes our model robust to many forms of inputs and queries. First, we\u2019ll take a look at some lines of our datafile to see the original format. corpus_name = \"movie-corpus\" corpus = os.path.join(\"data\", corpus_name) def printLines(file, n=10): with open(file, \u0027rb\u0027) as datafile: lines = datafile.readlines() for line in lines[:n]: print(line) printLines(os.path.join(corpus, \"utterances.jsonl\")) b\u0027{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\\u0027s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\\u0027s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\\u0027re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\\u0027re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\\u0027m kidding. You know how sometimes you just become this \\\\\"persona\\\\\"? And you don\\\u0027t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\\u0027m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\\u0027\\\u0027\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\\u0027t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n\u0027 b\u0027{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n\u0027 Create formatted data file# For convenience, we\u2019ll create a nicely formatted data file in which each line contains a tab-separated query sentence and a response sentence pair. The following functions facilitate the parsing of the raw utterances.jsonl data file. loadLinesAndConversations splits each line of the file into a dictionary of lines with fields: lineID, characterID, and text and then groups them into conversations with fields: conversationID, movieID, and lines. extractSentencePairs extracts pairs of sentences from conversations # Splits each line of the file to create lines and conversations def loadLinesAndConversations(fileName): lines = {} conversations = {} with open(fileName, \u0027r\u0027, encoding=\u0027iso-8859-1\u0027) as f: for line in f: lineJson = json.loads(line) # Extract fields for line object lineObj = {} lineObj[\"lineID\"] = lineJson[\"id\"] lineObj[\"characterID\"] = lineJson[\"speaker\"] lineObj[\"text\"] = lineJson[\"text\"] lines[lineObj[\u0027lineID\u0027]] = lineObj # Extract fields for conversation object if lineJson[\"conversation_id\"] not in conversations: convObj = {} convObj[\"conversationID\"] = lineJson[\"conversation_id\"] convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"] convObj[\"lines\"] = [lineObj] else: convObj = conversations[lineJson[\"conversation_id\"]] convObj[\"lines\"].insert(0, lineObj) conversations[convObj[\"conversationID\"]] = convObj return lines, conversations # Extracts pairs of sentences from conversations def extractSentencePairs(conversations): qa_pairs = [] for conversation in conversations.values(): # Iterate over all the lines of the conversation for i in range(len(conversation[\"lines\"]) - 1): # We ignore the last line (no answer for it) inputLine = conversation[\"lines\"][i][\"text\"].strip() targetLine = conversation[\"lines\"][i+1][\"text\"].strip() # Filter wrong samples (if one of the lists is empty) if inputLine and targetLine: qa_pairs.append([inputLine, targetLine]) return qa_pairs Now we\u2019ll call these functions and create the file. We\u2019ll call it formatted_movie_lines.txt. # Define path to new file datafile = os.path.join(corpus, \"formatted_movie_lines.txt\") delimiter = \u0027\\t\u0027 # Unescape the delimiter delimiter = str(codecs.decode(delimiter, \"unicode_escape\")) # Initialize lines dict and conversations dict lines = {} conversations = {} # Load lines and conversations print(\"\\nProcessing corpus into lines and conversations...\") lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\")) # Write new csv file print(\"\\nWriting newly formatted file...\") with open(datafile, \u0027w\u0027, encoding=\u0027utf-8\u0027) as outputfile: writer = csv.writer(outputfile, delimiter=delimiter, lineterminator=\u0027\\n\u0027) for pair in extractSentencePairs(conversations): writer.writerow(pair) # Print a sample of lines print(\"\\nSample lines from file:\") printLines(datafile) Processing corpus into lines and conversations... Writing newly formatted file... Sample lines from file: b\u0027They do to!\\tThey do not!\\n\u0027 b\u0027She okay?\\tI hope so.\\n\u0027 b\"Wow\\tLet\u0027s go.\\n\" b\u0027\"I\\\u0027m kidding. You know how sometimes you just become this \"\"persona\"\"? And you don\\\u0027t know how to quit?\"\\tNo\\n\u0027 b\"No\\tOkay -- you\u0027re gonna need to learn how to lie.\\n\" b\"I figured you\u0027d get to the good stuff eventually.\\tWhat good stuff?\\n\" b\u0027What good stuff?\\t\"The \"\"real you\"\".\"\\n\u0027 b\u0027\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n\u0027 b\u0027do you listen to this crap?\\tWhat crap?\\n\u0027 b\"What crap?\\tMe. This endless ...blonde babble. I\u0027m like, boring myself.\\n\" Load and trim data# Our next order of business is to create a vocabulary and load query/response sentence pairs into memory. Note that we are dealing with sequences of words, which do not have an implicit mapping to a discrete numerical space. Thus, we must create one by mapping each unique word that we encounter in our dataset to an index value. For this we define a Voc class, which keeps a mapping from words to indexes, a reverse mapping of indexes to words, a count of each word and a total word count. The class provides methods for adding a word to the vocabulary (addWord), adding all words in a sentence (addSentence) and trimming infrequently seen words (trim). More on trimming later. # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token class Voc: def __init__(self, name): self.name = name self.trimmed = False self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count SOS, EOS, PAD def addSentence(self, sentence): for word in sentence.split(\u0027 \u0027): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.num_words self.word2count[word] = 1 self.index2word[self.num_words] = word self.num_words += 1 else: self.word2count[word] += 1 # Remove words below a certain count threshold def trim(self, min_count): if self.trimmed: return self.trimmed = True keep_words = [] for k, v in self.word2count.items(): if v \u003e= min_count: keep_words.append(k) print(\u0027keep_words {} / {} = {:.4f}\u0027.format( len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) )) # Reinitialize dictionaries self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count default tokens for word in keep_words: self.addWord(word) Now we can assemble our vocabulary and query/response sentence pairs. Before we are ready to use this data, we must perform some preprocessing. First, we must convert the Unicode strings to ASCII using unicodeToAscii. Next, we should convert all letters to lowercase and trim all non-letter characters except for basic punctuation (normalizeString). Finally, to aid in training convergence, we will filter out sentences with length greater than the MAX_LENGTH threshold (filterPairs). MAX_LENGTH = 10 # Maximum sentence length to consider # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return \u0027\u0027.join( c for c in unicodedata.normalize(\u0027NFD\u0027, s) if unicodedata.category(c) != \u0027Mn\u0027 ) # Lowercase, trim, and remove non-letter characters def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) s = re.sub(r\"\\s+\", r\" \", s).strip() return s # Read query/response pairs and return a voc object def readVocs(datafile, corpus_name): print(\"Reading lines...\") # Read the file and split into lines lines = open(datafile, encoding=\u0027utf-8\u0027).\\ read().strip().split(\u0027\\n\u0027) # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split(\u0027\\t\u0027)] for l in lines] voc = Voc(corpus_name) return voc, pairs # Returns True if both sentences in a pair \u0027p\u0027 are under the MAX_LENGTH threshold def filterPair(p): # Input sequences need to preserve the last word for EOS token return len(p[0].split(\u0027 \u0027)) \u003c MAX_LENGTH and len(p[1].split(\u0027 \u0027)) \u003c MAX_LENGTH # Filter pairs using the ``filterPair`` condition def filterPairs(pairs): return [pair for pair in pairs if filterPair(pair)] # Using the functions defined above, return a populated voc object and pairs list def loadPrepareData(corpus, corpus_name, datafile, save_dir): print(\"Start preparing training data ...\") voc, pairs = readVocs(datafile, corpus_name) print(\"Read {!s} sentence pairs\".format(len(pairs))) pairs = filterPairs(pairs) print(\"Trimmed to {!s} sentence pairs\".format(len(pairs))) print(\"Counting words...\") for pair in pairs: voc.addSentence(pair[0]) voc.addSentence(pair[1]) print(\"Counted words:\", voc.num_words) return voc, pairs # Load/Assemble voc and pairs save_dir = os.path.join(\"data\", \"save\") voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir) # Print some pairs to validate print(\"\\npairs:\") for pair in pairs[:10]: print(pair) Start preparing training data ... Reading lines... Read 221282 sentence pairs Trimmed to 64313 sentence pairs Counting words... Counted words: 18082 pairs: [\u0027they do to !\u0027, \u0027they do not !\u0027] [\u0027she okay ?\u0027, \u0027i hope so .\u0027] [\u0027wow\u0027, \u0027let s go .\u0027] [\u0027what good stuff ?\u0027, \u0027the real you .\u0027] [\u0027the real you .\u0027, \u0027like my fear of wearing pastels ?\u0027] [\u0027do you listen to this crap ?\u0027, \u0027what crap ?\u0027] [\u0027well no . . .\u0027, \u0027then that s all you had to say .\u0027] [\u0027then that s all you had to say .\u0027, \u0027but\u0027] [\u0027but\u0027, \u0027you always been this selfish ?\u0027] [\u0027have fun tonight ?\u0027, \u0027tons\u0027] Another tactic that is beneficial to achieving faster convergence during training is trimming rarely used words out of our vocabulary. Decreasing the feature space will also soften the difficulty of the function that the model must learn to approximate. We will do this as a two-step process: Trim words used under MIN_COUNT threshold using the voc.trim function. Filter out pairs with trimmed words. MIN_COUNT = 3 # Minimum word count threshold for trimming def trimRareWords(voc, pairs, MIN_COUNT): # Trim words used under the MIN_COUNT from the voc voc.trim(MIN_COUNT) # Filter out pairs with trimmed words keep_pairs = [] for pair in pairs: input_sentence = pair[0] output_sentence = pair[1] keep_input = True keep_output = True # Check input sentence for word in input_sentence.split(\u0027 \u0027): if word not in voc.word2index: keep_input = False break # Check output sentence for word in output_sentence.split(\u0027 \u0027): if word not in voc.word2index: keep_output = False break # Only keep pairs that do not contain trimmed word(s) in their input or output sentence if keep_input and keep_output: keep_pairs.append(pair) print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs))) return keep_pairs # Trim voc and pairs pairs = trimRareWords(voc, pairs, MIN_COUNT) keep_words 7833 / 18079 = 0.4333 Trimmed from 64313 pairs to 53131, 0.8261 of total Prepare Data for Models# Although we have put a great deal of effort into preparing and massaging our data into a nice vocabulary object and list of sentence pairs, our models will ultimately expect numerical torch tensors as inputs. One way to prepare the processed data for the models can be found in the seq2seq translation tutorial. In that tutorial, we use a batch size of 1, meaning that all we have to do is convert the words in our sentence pairs to their corresponding indexes from the vocabulary and feed this to the models. However, if you\u2019re interested in speeding up training and/or would like to leverage GPU parallelization capabilities, you will need to train with mini-batches. Using mini-batches also means that we must be mindful of the variation of sentence length in our batches. To accommodate sentences of different sizes in the same batch, we will make our batched input tensor of shape (max_length, batch_size), where sentences shorter than the max_length are zero padded after an EOS_token. If we simply convert our English sentences to tensors by converting words to their indexes(indexesFromSentence) and zero-pad, our tensor would have shape (batch_size, max_length) and indexing the first dimension would return a full sequence across all time-steps. However, we need to be able to index our batch along time, and across all sequences in the batch. Therefore, we transpose our input batch shape to (max_length, batch_size), so that indexing across the first dimension returns a time step across all sentences in the batch. We handle this transpose implicitly in the zeroPadding function. The inputVar function handles the process of converting sentences to tensor, ultimately creating a correctly shaped zero-padded tensor. It also returns a tensor of lengths for each of the sequences in the batch which will be passed to our decoder later. The outputVar function performs a similar function to inputVar, but instead of returning a lengths tensor, it returns a binary mask tensor and a maximum target sentence length. The binary mask tensor has the same shape as the output target tensor, but every element that is a PAD_token is 0 and all others are 1. batch2TrainData simply takes a bunch of pairs and returns the input and target tensors using the aforementioned functions. def indexesFromSentence(voc, sentence): return [voc.word2index[word] for word in sentence.split(\u0027 \u0027)] + [EOS_token] def zeroPadding(l, fillvalue=PAD_token): return list(itertools.zip_longest(*l, fillvalue=fillvalue)) def binaryMatrix(l, value=PAD_token): m = [] for i, seq in enumerate(l): m.append([]) for token in seq: if token == PAD_token: m[i].append(0) else: m[i].append(1) return m # Returns padded input sequence tensor and lengths def inputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) padVar = torch.LongTensor(padList) return padVar, lengths # Returns padded target sequence tensor, padding mask, and max target length def outputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] max_target_len = max([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) mask = binaryMatrix(padList) mask = torch.BoolTensor(mask) padVar = torch.LongTensor(padList) return padVar, mask, max_target_len # Returns all items for a given batch of pairs def batch2TrainData(voc, pair_batch): pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True) input_batch, output_batch = [], [] for pair in pair_batch: input_batch.append(pair[0]) output_batch.append(pair[1]) inp, lengths = inputVar(input_batch, voc) output, mask, max_target_len = outputVar(output_batch, voc) return inp, lengths, output, mask, max_target_len # Example for validation small_batch_size = 5 batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)]) input_variable, lengths, target_variable, mask, max_target_len = batches print(\"input_variable:\", input_variable) print(\"lengths:\", lengths) print(\"target_variable:\", target_variable) print(\"mask:\", mask) print(\"max_target_len:\", max_target_len) input_variable: tensor([[ 162, 35, 19, 1810, 280], [ 14, 19, 10, 495, 6], [ 40, 17, 34, 14, 2], [ 104, 14, 6, 2, 0], [ 717, 14, 2, 0, 0], [ 135, 14, 0, 0, 0], [ 187, 2, 0, 0, 0], [ 112, 0, 0, 0, 0], [ 14, 0, 0, 0, 0], [ 2, 0, 0, 0, 0]]) lengths: tensor([10, 7, 5, 4, 3]) target_variable: tensor([[ 48, 85, 39, 34, 1618], [ 67, 17, 24, 495, 7664], [ 104, 62, 64, 14, 10], [ 90, 1324, 397, 2, 2], [ 10, 1179, 1011, 0, 0], [ 2, 14, 22, 0, 0], [ 0, 2, 1059, 0, 0], [ 0, 0, 14, 0, 0], [ 0, 0, 2, 0, 0]]) mask: tensor([[ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, False, False], [ True, True, True, False, False], [False, True, True, False, False], [False, False, True, False, False], [False, False, True, False, False]]) max_target_len: 9 Define Models# Seq2Seq Model# The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The goal of a seq2seq model is to take a variable-length sequence as an input, and return a variable-length sequence as an output using a fixed-sized model. Sutskever et al. discovered that by using two separate recurrent neural nets together, we can accomplish this task. One RNN acts as an encoder, which encodes a variable length input sequence to a fixed-length context vector. In theory, this context vector (the final hidden layer of the RNN) will contain semantic information about the query sentence that is input to the bot. The second RNN is a decoder, which takes an input word and the context vector, and returns a guess for the next word in the sequence and a hidden state to use in the next iteration. Image source: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/ Encoder# The encoder RNN iterates through the input sentence one token (e.g. word) at a time, at each time step outputting an \u201coutput\u201d vector and a \u201chidden state\u201d vector. The hidden state vector is then passed to the next time step, while the output vector is recorded. The encoder transforms the context it saw at each point in the sequence into a set of points in a high-dimensional space, which the decoder will use to generate a meaningful output for the given task. At the heart of our encoder is a multi-layered Gated Recurrent Unit, invented by Cho et al. in 2014. We will use a bidirectional variant of the GRU, meaning that there are essentially two independent RNNs: one that is fed the input sequence in normal sequential order, and one that is fed the input sequence in reverse order. The outputs of each network are summed at each time step. Using a bidirectional GRU will give us the advantage of encoding both past and future contexts. Bidirectional RNN: Image source: https://colah.github.io/posts/2015-09-NN-Types-FP/ Note that an embedding layer is used to encode our word indices in an arbitrarily sized feature space. For our models, this layer will map each word to a feature space of size hidden_size. When trained, these values should encode semantic similarity between similar meaning words. Finally, if passing a padded batch of sequences to an RNN module, we must pack and unpack padding around the RNN pass using nn.utils.rnn.pack_padded_sequence and nn.utils.rnn.pad_packed_sequence respectively. Computation Graph: Convert word indexes to embeddings. Pack padded batch of sequences for RNN module. Forward pass through GRU. Unpack padding. Sum bidirectional GRU outputs. Return output and final hidden state. Inputs: input_seq: batch of input sentences; shape=(max_length, batch_size) input_lengths: list of sentence lengths corresponding to each sentence in the batch; shape=(batch_size) hidden: hidden state; shape=(n_layers x num_directions, batch_size, hidden_size) Outputs: outputs: output features from the last hidden layer of the GRU (sum of bidirectional outputs); shape=(max_length, batch_size, hidden_size) hidden: updated hidden state from GRU; shape=(n_layers x num_directions, batch_size, hidden_size) class EncoderRNN(nn.Module): def __init__(self, hidden_size, embedding, n_layers=1, dropout=0): super(EncoderRNN, self).__init__() self.n_layers = n_layers self.hidden_size = hidden_size self.embedding = embedding # Initialize GRU; the input_size and hidden_size parameters are both set to \u0027hidden_size\u0027 # because our input size is a word embedding with number of features == hidden_size self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True) def forward(self, input_seq, input_lengths, hidden=None): # Convert word indexes to embeddings embedded = self.embedding(input_seq) # Pack padded batch of sequences for RNN module packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # Forward pass through GRU outputs, hidden = self.gru(packed, hidden) # Unpack padding outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) # Sum bidirectional GRU outputs outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Return output and final hidden state return outputs, hidden Decoder# The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder\u2019s context vectors, and internal hidden states to generate the next word in the sequence. It continues generating words until it outputs an EOS_token, representing the end of the sentence. A common problem with a vanilla seq2seq decoder is that if we rely solely on the context vector to encode the entire input sequence\u2019s meaning, it is likely that we will have information loss. This is especially the case when dealing with long input sequences, greatly limiting the capability of our decoder. To combat this, Bahdanau et al. created an \u201cattention mechanism\u201d that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step. At a high level, attention is calculated using the decoder\u2019s current hidden state and the encoder\u2019s outputs. The output attention weights have the same shape as the input sequence, allowing us to multiply them by the encoder outputs, giving us a weighted sum which indicates the parts of encoder output to pay attention to. Sean Robertson\u2019s figure describes this very well: Luong et al. improved upon Bahdanau et al.\u2019s groundwork by creating \u201cGlobal attention\u201d. The key difference is that with \u201cGlobal attention\u201d, we consider all of the encoder\u2019s hidden states, as opposed to Bahdanau et al.\u2019s \u201cLocal attention\u201d, which only considers the encoder\u2019s hidden state from the current time step. Another difference is that with \u201cGlobal attention\u201d, we calculate attention weights, or energies, using the hidden state of the decoder from the current time step only. Bahdanau et al.\u2019s attention calculation requires knowledge of the decoder\u2019s state from the previous time step. Also, Luong et al. provides various methods to calculate the attention energies between the encoder output and decoder output which are called \u201cscore functions\u201d: where \\(h_t\\) = current target decoder state and \\(\\bar{h}_s\\) = all encoder states. Overall, the Global attention mechanism can be summarized by the following figure. Note that we will implement the \u201cAttention Layer\u201d as a separate nn.Module called Attn. The output of this module is a softmax normalized weights tensor of shape (batch_size, 1, max_length). # Luong attention layer class Attn(nn.Module): def __init__(self, method, hidden_size): super(Attn, self).__init__() self.method = method if self.method not in [\u0027dot\u0027, \u0027general\u0027, \u0027concat\u0027]: raise ValueError(self.method, \"is not an appropriate attention method.\") self.hidden_size = hidden_size if self.method == \u0027general\u0027: self.attn = nn.Linear(self.hidden_size, hidden_size) elif self.method == \u0027concat\u0027: self.attn = nn.Linear(self.hidden_size * 2, hidden_size) self.v = nn.Parameter(torch.FloatTensor(hidden_size)) def dot_score(self, hidden, encoder_output): return torch.sum(hidden * encoder_output, dim=2) def general_score(self, hidden, encoder_output): energy = self.attn(encoder_output) return torch.sum(hidden * energy, dim=2) def concat_score(self, hidden, encoder_output): energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh() return torch.sum(self.v * energy, dim=2) def forward(self, hidden, encoder_outputs): # Calculate the attention weights (energies) based on the given method if self.method == \u0027general\u0027: attn_energies = self.general_score(hidden, encoder_outputs) elif self.method == \u0027concat\u0027: attn_energies = self.concat_score(hidden, encoder_outputs) elif self.method == \u0027dot\u0027: attn_energies = self.dot_score(hidden, encoder_outputs) # Transpose max_length and batch_size dimensions attn_energies = attn_energies.t() # Return the softmax normalized probability scores (with added dimension) return F.softmax(attn_energies, dim=1).unsqueeze(1) Now that we have defined our attention submodule, we can implement the actual decoder model. For the decoder, we will manually feed our batch one time step at a time. This means that our embedded word tensor and GRU output will both have shape (1, batch_size, hidden_size). Computation Graph: Get embedding of current input word. Forward through unidirectional GRU. Calculate attention weights from the current GRU output from (2). Multiply attention weights to encoder outputs to get new \u201cweighted sum\u201d context vector. Concatenate weighted context vector and GRU output using Luong eq. 5. Predict next word using Luong eq. 6 (without softmax). Return output and final hidden state. Inputs: input_step: one time step (one word) of input sequence batch; shape=(1, batch_size) last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size) encoder_outputs: encoder model\u2019s output; shape=(max_length, batch_size, hidden_size) Outputs: output: softmax normalized tensor giving probabilities of each word being the correct next word in the decoded sequence; shape=(batch_size, voc.num_words) hidden: final hidden state of GRU; shape=(n_layers x num_directions, batch_size, hidden_size) class LuongAttnDecoderRNN(nn.Module): def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1): super(LuongAttnDecoderRNN, self).__init__() # Keep for reference self.attn_model = attn_model self.hidden_size = hidden_size self.output_size = output_size self.n_layers = n_layers self.dropout = dropout # Define layers self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(hidden_size * 2, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(attn_model, hidden_size) def forward(self, input_step, last_hidden, encoder_outputs): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self.embedding(input_step) embedded = self.embedding_dropout(embedded) # Forward through unidirectional GRU rnn_output, hidden = self.gru(embedded, last_hidden) # Calculate attention weights from the current GRU output attn_weights = self.attn(rnn_output, encoder_outputs) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output.squeeze(0) context = context.squeeze(1) concat_input = torch.cat((rnn_output, context), 1) concat_output = torch.tanh(self.concat(concat_input)) # Predict next word using Luong eq. 6 output = self.out(concat_output) output = F.softmax(output, dim=1) # Return output and final hidden state return output, hidden Define Training Procedure# Masked loss# Since we are dealing with batches of padded sequences, we cannot simply consider all elements of the tensor when calculating loss. We define maskNLLLoss to calculate our loss based on our decoder\u2019s output tensor, the target tensor, and a binary mask tensor describing the padding of the target tensor. This loss function calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor. def maskNLLLoss(inp, target, mask): nTotal = mask.sum() crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)) loss = crossEntropy.masked_select(mask).mean() loss = loss.to(device) return loss, nTotal.item() Single training iteration# The train function contains the algorithm for a single training iteration (a single batch of inputs). We will use a couple of clever tricks to aid in convergence: The first trick is using teacher forcing. This means that at some probability, set by teacher_forcing_ratio, we use the current target word as the decoder\u2019s next input rather than using the decoder\u2019s current guess. This technique acts as training wheels for the decoder, aiding in more efficient training. However, teacher forcing can lead to model instability during inference, as the decoder may not have a sufficient chance to truly craft its own output sequences during training. Thus, we must be mindful of how we are setting the teacher_forcing_ratio, and not be fooled by fast convergence. The second trick that we implement is gradient clipping. This is a commonly used technique for countering the \u201cexploding gradient\u201d problem. In essence, by clipping or thresholding gradients to a maximum value, we prevent the gradients from growing exponentially and either overflow (NaN), or overshoot steep cliffs in the cost function. Image source: Goodfellow et al. Deep Learning. 2016. https://www.deeplearningbook.org/ Sequence of Operations: Forward pass entire input batch through encoder. Initialize decoder inputs as SOS_token, and hidden state as the encoder\u2019s final hidden state. Forward input batch sequence through decoder one time step at a time. If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output. Calculate and accumulate loss. Perform backpropagation. Clip gradients. Update encoder and decoder model parameters. Note PyTorch\u2019s RNN modules (RNN, LSTM, GRU) can be used like any other non-recurrent layers by simply passing them the entire input sequence (or batch of sequences). We use the GRU layer like this in the encoder. The reality is that under the hood, there is an iterative process looping over each time step calculating hidden states. Alternatively, you can run these modules one time-step at a time. In this case, we manually loop over the sequences during the training process like we must do for the decoder model. As long as you maintain the correct conceptual model of these modules, implementing sequential models can be very straightforward. def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH): # Zero gradients encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() # Set device options input_variable = input_variable.to(device) target_variable = target_variable.to(device) mask = mask.to(device) # Lengths for RNN packing should always be on the CPU lengths = lengths.to(\"cpu\") # Initialize variables loss = 0 print_losses = [] n_totals = 0 # Forward pass through encoder encoder_outputs, encoder_hidden = encoder(input_variable, lengths) # Create initial decoder input (start with SOS tokens for each sentence) decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]) decoder_input = decoder_input.to(device) # Set initial decoder hidden state to the encoder\u0027s final hidden state decoder_hidden = encoder_hidden[:decoder.n_layers] # Determine if we are using teacher forcing this iteration use_teacher_forcing = True if random.random() \u003c teacher_forcing_ratio else False # Forward batch of sequences through decoder one time step at a time if use_teacher_forcing: for t in range(max_target_len): decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden, encoder_outputs ) # Teacher forcing: next input is current target decoder_input = target_variable[t].view(1, -1) # Calculate and accumulate loss mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t]) loss += mask_loss print_losses.append(mask_loss.item() * nTotal) n_totals += nTotal else: for t in range(max_target_len): decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden, encoder_outputs ) # No teacher forcing: next input is decoder\u0027s own current output _, topi = decoder_output.topk(1) decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]) decoder_input = decoder_input.to(device) # Calculate and accumulate loss mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t]) loss += mask_loss print_losses.append(mask_loss.item() * nTotal) n_totals += nTotal # Perform backpropagation loss.backward() # Clip gradients: gradients are modified in place _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip) _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip) # Adjust model weights encoder_optimizer.step() decoder_optimizer.step() return sum(print_losses) / n_totals Training iterations# It is finally time to tie the full training procedure together with the data. The trainIters function is responsible for running n_iterations of training given the passed models, optimizers, data, etc. This function is quite self explanatory, as we have done the heavy lifting with the train function. One thing to note is that when we save our model, we save a tarball containing the encoder and decoder state_dicts (parameters), the optimizers\u2019 state_dicts, the loss, the iteration, etc. Saving the model in this way will give us the ultimate flexibility with the checkpoint. After loading a checkpoint, we will be able to use the model parameters to run inference, or we can continue training right where we left off. def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename): # Load batches for each iteration training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)] # Initializations print(\u0027Initializing ...\u0027) start_iteration = 1 print_loss = 0 if loadFilename: start_iteration = checkpoint[\u0027iteration\u0027] + 1 # Training loop print(\"Training...\") for iteration in range(start_iteration, n_iteration + 1): training_batch = training_batches[iteration - 1] # Extract fields from batch input_variable, lengths, target_variable, mask, max_target_len = training_batch # Run a training iteration with batch loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip) print_loss += loss # Print progress if iteration % print_every == 0: print_loss_avg = print_loss / print_every print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg)) print_loss = 0 # Save checkpoint if (iteration % save_every == 0): directory = os.path.join(save_dir, model_name, corpus_name, \u0027{}-{}_{}\u0027.format(encoder_n_layers, decoder_n_layers, hidden_size)) if not os.path.exists(directory): os.makedirs(directory) torch.save({ \u0027iteration\u0027: iteration, \u0027en\u0027: encoder.state_dict(), \u0027de\u0027: decoder.state_dict(), \u0027en_opt\u0027: encoder_optimizer.state_dict(), \u0027de_opt\u0027: decoder_optimizer.state_dict(), \u0027loss\u0027: loss, \u0027voc_dict\u0027: voc.__dict__, \u0027embedding\u0027: embedding.state_dict() }, os.path.join(directory, \u0027{}_{}.tar\u0027.format(iteration, \u0027checkpoint\u0027))) Define Evaluation# After training a model, we want to be able to talk to the bot ourselves. First, we must define how we want the model to decode the encoded input. Greedy decoding# Greedy decoding is the decoding method that we use during training when we are NOT using teacher forcing. In other words, for each time step, we simply choose the word from decoder_output with the highest softmax value. This decoding method is optimal on a single time-step level. To facilitate the greedy decoding operation, we define a GreedySearchDecoder class. When run, an object of this class takes an input sequence (input_seq) of shape (input_seq length, 1), a scalar input length (input_length) tensor, and a max_length to bound the response sentence length. The input sentence is evaluated using the following computational graph: Computation Graph: Forward input through encoder model. Prepare encoder\u2019s final hidden layer to be first hidden input to the decoder. Initialize decoder\u2019s first input as SOS_token. Initialize tensors to append decoded words to. Iteratively decode one word token at a time: Forward pass through decoder. Obtain most likely word token and its softmax score. Record token and score. Prepare current token to be next decoder input. Return collections of word tokens and scores. class GreedySearchDecoder(nn.Module): def __init__(self, encoder, decoder): super(GreedySearchDecoder, self).__init__() self.encoder = encoder self.decoder = decoder def forward(self, input_seq, input_length, max_length): # Forward input through encoder model encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length) # Prepare encoder\u0027s final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden[:self.decoder.n_layers] # Initialize decoder input with SOS_token decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token # Initialize tensors to append decoded words to all_tokens = torch.zeros([0], device=device, dtype=torch.long) all_scores = torch.zeros([0], device=device) # Iteratively decode one word token at a time for _ in range(max_length): # Forward pass through decoder decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs) # Obtain most likely word token and its softmax score decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # Record token and score all_tokens = torch.cat((all_tokens, decoder_input), dim=0) all_scores = torch.cat((all_scores, decoder_scores), dim=0) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch.unsqueeze(decoder_input, 0) # Return collections of word tokens and scores return all_tokens, all_scores Evaluate my text# Now that we have our decoding method defined, we can write functions for evaluating a string input sentence. The evaluate function manages the low-level process of handling the input sentence. We first format the sentence as an input batch of word indexes with batch_size==1. We do this by converting the words of the sentence to their corresponding indexes, and transposing the dimensions to prepare the tensor for our models. We also create a lengths tensor which contains the length of our input sentence. In this case, lengths is scalar because we are only evaluating one sentence at a time (batch_size==1). Next, we obtain the decoded response sentence tensor using our GreedySearchDecoder object (searcher). Finally, we convert the response\u2019s indexes to words and return the list of decoded words. evaluateInput acts as the user interface for our chatbot. When called, an input text field will spawn in which we can enter our query sentence. After typing our input sentence and pressing Enter, our text is normalized in the same way as our training data, and is ultimately fed to the evaluate function to obtain a decoded output sentence. We loop this process, so we can keep chatting with our bot until we enter either \u201cq\u201d or \u201cquit\u201d. Finally, if a sentence is entered that contains a word that is not in the vocabulary, we handle this gracefully by printing an error message and prompting the user to enter another sentence. def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH): ### Format input sentence as a batch # words -\u003e indexes indexes_batch = [indexesFromSentence(voc, sentence)] # Create lengths tensor lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Transpose dimensions of batch to match models\u0027 expectations input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) # Use appropriate device input_batch = input_batch.to(device) lengths = lengths.to(\"cpu\") # Decode sentence with searcher tokens, scores = searcher(input_batch, lengths, max_length) # indexes -\u003e words decoded_words = [voc.index2word[token.item()] for token in tokens] return decoded_words def evaluateInput(encoder, decoder, searcher, voc): input_sentence = \u0027\u0027 while(1): try: # Get input sentence input_sentence = input(\u0027\u003e \u0027) # Check if it is quit case if input_sentence == \u0027q\u0027 or input_sentence == \u0027quit\u0027: break # Normalize sentence input_sentence = normalizeString(input_sentence) # Evaluate sentence output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) # Format and print response sentence output_words[:] = [x for x in output_words if not (x == \u0027EOS\u0027 or x == \u0027PAD\u0027)] print(\u0027Bot:\u0027, \u0027 \u0027.join(output_words)) except KeyError: print(\"Error: Encountered unknown word.\") Run Model# Finally, it is time to run our model! Regardless of whether we want to train or test the chatbot model, we must initialize the individual encoder and decoder models. In the following block, we set our desired configurations, choose to start from scratch or set a checkpoint to load from, and build and initialize the models. Feel free to play with different model configurations to optimize performance. # Configure models model_name = \u0027cb_model\u0027 attn_model = \u0027dot\u0027 #``attn_model = \u0027general\u0027`` #``attn_model = \u0027concat\u0027`` hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # Set checkpoint to load from; set to None if starting from scratch loadFilename = None checkpoint_iter = 4000 Sample code to load from a checkpoint: loadFilename = os.path.join(save_dir, model_name, corpus_name, \u0027{}-{}_{}\u0027.format(encoder_n_layers, decoder_n_layers, hidden_size), \u0027{}_checkpoint.tar\u0027.format(checkpoint_iter)) # Load model if a ``loadFilename`` is provided if loadFilename: # If loading on same machine the model was trained on checkpoint = torch.load(loadFilename) # If loading a model trained on GPU to CPU #checkpoint = torch.load(loadFilename, map_location=torch.device(\u0027cpu\u0027)) encoder_sd = checkpoint[\u0027en\u0027] decoder_sd = checkpoint[\u0027de\u0027] encoder_optimizer_sd = checkpoint[\u0027en_opt\u0027] decoder_optimizer_sd = checkpoint[\u0027de_opt\u0027] embedding_sd = checkpoint[\u0027embedding\u0027] voc.__dict__ = checkpoint[\u0027voc_dict\u0027] print(\u0027Building encoder and decoder ...\u0027) # Initialize word embeddings embedding = nn.Embedding(voc.num_words, hidden_size) if loadFilename: embedding.load_state_dict(embedding_sd) # Initialize encoder \u0026 decoder models encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout) decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout) if loadFilename: encoder.load_state_dict(encoder_sd) decoder.load_state_dict(decoder_sd) # Use appropriate device encoder = encoder.to(device) decoder = decoder.to(device) print(\u0027Models built and ready to go!\u0027) Building encoder and decoder ... Models built and ready to go! Run Training# Run the following block if you want to train the model. First we set training parameters, then we initialize our optimizers, and finally we call the trainIters function to run our training iterations. # Configure training/optimization clip = 50.0 teacher_forcing_ratio = 1.0 learning_rate = 0.0001 decoder_learning_ratio = 5.0 n_iteration = 4000 print_every = 1 save_every = 500 # Ensure dropout layers are in train mode encoder.train() decoder.train() # Initialize optimizers print(\u0027Building optimizers ...\u0027) encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio) if loadFilename: encoder_optimizer.load_state_dict(encoder_optimizer_sd) decoder_optimizer.load_state_dict(decoder_optimizer_sd) # If you have an accelerator, configure it to call for state in encoder_optimizer.state.values(): for k, v in state.items(): if isinstance(v, torch.Tensor): state[k] = v.to(device) for state in decoder_optimizer.state.values(): for k, v in state.items(): if isinstance(v, torch.Tensor): state[k] = v.to(device) # Run training iterations print(\"Starting Training!\") trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename) Building optimizers ... Starting Training! Initializing ... Training... Iteration: 1; Percent complete: 0.0%; Average loss: 8.9756 Iteration: 2; Percent complete: 0.1%; Average loss: 8.8667 Iteration: 3; Percent complete: 0.1%; Average loss: 8.6607 Iteration: 4; Percent complete: 0.1%; Average loss: 8.4327 Iteration: 5; Percent complete: 0.1%; Average loss: 8.0761 Iteration: 6; Percent complete: 0.1%; Average loss: 7.5822 Iteration: 7; Percent complete: 0.2%; Average loss: 7.1297 Iteration: 8; Percent complete: 0.2%; Average loss: 6.6842 Iteration: 9; Percent complete: 0.2%; Average loss: 6.7659 Iteration: 10; Percent complete: 0.2%; Average loss: 6.7540 Iteration: 11; Percent complete: 0.3%; Average loss: 6.2143 Iteration: 12; Percent complete: 0.3%; Average loss: 5.9623 Iteration: 13; Percent complete: 0.3%; Average loss: 5.6665 Iteration: 14; Percent complete: 0.4%; Average loss: 5.4779 Iteration: 15; Percent complete: 0.4%; Average loss: 5.5233 Iteration: 16; Percent complete: 0.4%; Average loss: 5.5385 Iteration: 17; Percent complete: 0.4%; Average loss: 5.0754 Iteration: 18; Percent complete: 0.4%; Average loss: 5.1344 Iteration: 19; Percent complete: 0.5%; Average loss: 5.1203 Iteration: 20; Percent complete: 0.5%; Average loss: 4.9330 Iteration: 21; Percent complete: 0.5%; Average loss: 4.9703 Iteration: 22; Percent complete: 0.5%; Average loss: 4.8878 Iteration: 23; Percent complete: 0.6%; Average loss: 5.1144 Iteration: 24; Percent complete: 0.6%; Average loss: 4.9549 Iteration: 25; Percent complete: 0.6%; Average loss: 4.9659 Iteration: 26; Percent complete: 0.7%; Average loss: 4.5693 Iteration: 27; Percent complete: 0.7%; Average loss: 4.8597 Iteration: 28; Percent complete: 0.7%; Average loss: 4.8144 Iteration: 29; Percent complete: 0.7%; Average loss: 4.9829 Iteration: 30; Percent complete: 0.8%; Average loss: 5.1429 Iteration: 31; Percent complete: 0.8%; Average loss: 4.5494 Iteration: 32; Percent complete: 0.8%; Average loss: 4.7390 Iteration: 33; Percent complete: 0.8%; Average loss: 4.7331 Iteration: 34; Percent complete: 0.9%; Average loss: 4.7900 Iteration: 35; Percent complete: 0.9%; Average loss: 4.8527 Iteration: 36; Percent complete: 0.9%; Average loss: 4.9294 Iteration: 37; Percent complete: 0.9%; Average loss: 4.6744 Iteration: 38; Percent complete: 0.9%; Average loss: 4.6147 Iteration: 39; Percent complete: 1.0%; Average loss: 4.8135 Iteration: 40; Percent complete: 1.0%; Average loss: 4.5181 Iteration: 41; Percent complete: 1.0%; Average loss: 4.6252 Iteration: 42; Percent complete: 1.1%; Average loss: 4.7419 Iteration: 43; Percent complete: 1.1%; Average loss: 4.5685 Iteration: 44; Percent complete: 1.1%; Average loss: 4.6670 Iteration: 45; Percent complete: 1.1%; Average loss: 4.5784 Iteration: 46; Percent complete: 1.1%; Average loss: 4.8193 Iteration: 47; Percent complete: 1.2%; Average loss: 4.9056 Iteration: 48; Percent complete: 1.2%; Average loss: 4.4678 Iteration: 49; Percent complete: 1.2%; Average loss: 4.7022 Iteration: 50; Percent complete: 1.2%; Average loss: 4.6481 Iteration: 51; Percent complete: 1.3%; Average loss: 4.6209 Iteration: 52; Percent complete: 1.3%; Average loss: 4.6002 Iteration: 53; Percent complete: 1.3%; Average loss: 4.6281 Iteration: 54; Percent complete: 1.4%; Average loss: 4.7856 Iteration: 55; Percent complete: 1.4%; Average loss: 4.6961 Iteration: 56; Percent complete: 1.4%; Average loss: 4.6516 Iteration: 57; Percent complete: 1.4%; Average loss: 4.6600 Iteration: 58; Percent complete: 1.5%; Average loss: 4.6082 Iteration: 59; Percent complete: 1.5%; Average loss: 4.4427 Iteration: 60; Percent complete: 1.5%; Average loss: 4.7418 Iteration: 61; Percent complete: 1.5%; Average loss: 4.7729 Iteration: 62; Percent complete: 1.6%; Average loss: 4.6912 Iteration: 63; Percent complete: 1.6%; Average loss: 4.4042 Iteration: 64; Percent complete: 1.6%; Average loss: 4.6691 Iteration: 65; Percent complete: 1.6%; Average loss: 4.6227 Iteration: 66; Percent complete: 1.7%; Average loss: 4.4480 Iteration: 67; Percent complete: 1.7%; Average loss: 4.7046 Iteration: 68; Percent complete: 1.7%; Average loss: 4.5236 Iteration: 69; Percent complete: 1.7%; Average loss: 4.6502 Iteration: 70; Percent complete: 1.8%; Average loss: 4.7270 Iteration: 71; Percent complete: 1.8%; Average loss: 4.4789 Iteration: 72; Percent complete: 1.8%; Average loss: 4.5091 Iteration: 73; Percent complete: 1.8%; Average loss: 4.4509 Iteration: 74; Percent complete: 1.8%; Average loss: 4.5240 Iteration: 75; Percent complete: 1.9%; Average loss: 4.1346 Iteration: 76; Percent complete: 1.9%; Average loss: 4.7585 Iteration: 77; Percent complete: 1.9%; Average loss: 4.3322 Iteration: 78; Percent complete: 1.9%; Average loss: 4.4739 Iteration: 79; Percent complete: 2.0%; Average loss: 4.1729 Iteration: 80; Percent complete: 2.0%; Average loss: 4.5390 Iteration: 81; Percent complete: 2.0%; Average loss: 4.5473 Iteration: 82; Percent complete: 2.1%; Average loss: 4.2703 Iteration: 83; Percent complete: 2.1%; Average loss: 4.4995 Iteration: 84; Percent complete: 2.1%; Average loss: 4.4703 Iteration: 85; Percent complete: 2.1%; Average loss: 4.3485 Iteration: 86; Percent complete: 2.1%; Average loss: 4.1946 Iteration: 87; Percent complete: 2.2%; Average loss: 4.3863 Iteration: 88; Percent complete: 2.2%; Average loss: 4.5126 Iteration: 89; Percent complete: 2.2%; Average loss: 4.6809 Iteration: 90; Percent complete: 2.2%; Average loss: 4.5945 Iteration: 91; Percent complete: 2.3%; Average loss: 4.5111 Iteration: 92; Percent complete: 2.3%; Average loss: 4.5442 Iteration: 93; Percent complete: 2.3%; Average loss: 4.4153 Iteration: 94; Percent complete: 2.4%; Average loss: 4.3059 Iteration: 95; Percent complete: 2.4%; Average loss: 4.4178 Iteration: 96; Percent complete: 2.4%; Average loss: 4.3698 Iteration: 97; Percent complete: 2.4%; Average loss: 4.5720 Iteration: 98; Percent complete: 2.5%; Average loss: 4.3105 Iteration: 99; Percent complete: 2.5%; Average loss: 4.3704 Iteration: 100; Percent complete: 2.5%; Average loss: 4.5521 Iteration: 101; Percent complete: 2.5%; Average loss: 4.4161 Iteration: 102; Percent complete: 2.5%; Average loss: 4.3238 Iteration: 103; Percent complete: 2.6%; Average loss: 4.3940 Iteration: 104; Percent complete: 2.6%; Average loss: 4.4251 Iteration: 105; Percent complete: 2.6%; Average loss: 4.3681 Iteration: 106; Percent complete: 2.6%; Average loss: 4.3026 Iteration: 107; Percent complete: 2.7%; Average loss: 4.7332 Iteration: 108; Percent complete: 2.7%; Average loss: 4.6414 Iteration: 109; Percent complete: 2.7%; Average loss: 4.5147 Iteration: 110; Percent complete: 2.8%; Average loss: 4.2995 Iteration: 111; Percent complete: 2.8%; Average loss: 4.4154 Iteration: 112; Percent complete: 2.8%; Average loss: 4.2703 Iteration: 113; Percent complete: 2.8%; Average loss: 4.3424 Iteration: 114; Percent complete: 2.9%; Average loss: 4.3429 Iteration: 115; Percent complete: 2.9%; Average loss: 4.2511 Iteration: 116; Percent complete: 2.9%; Average loss: 4.2147 Iteration: 117; Percent complete: 2.9%; Average loss: 4.2876 Iteration: 118; Percent complete: 2.9%; Average loss: 4.5823 Iteration: 119; Percent complete: 3.0%; Average loss: 4.3462 Iteration: 120; Percent complete: 3.0%; Average loss: 4.4426 Iteration: 121; Percent complete: 3.0%; Average loss: 4.3820 Iteration: 122; Percent complete: 3.0%; Average loss: 4.0776 Iteration: 123; Percent complete: 3.1%; Average loss: 4.3654 Iteration: 124; Percent complete: 3.1%; Average loss: 4.3790 Iteration: 125; Percent complete: 3.1%; Average loss: 4.3151 Iteration: 126; Percent complete: 3.1%; Average loss: 4.4324 Iteration: 127; Percent complete: 3.2%; Average loss: 4.3096 Iteration: 128; Percent complete: 3.2%; Average loss: 4.3883 Iteration: 129; Percent complete: 3.2%; Average loss: 4.0664 Iteration: 130; Percent complete: 3.2%; Average loss: 4.1046 Iteration: 131; Percent complete: 3.3%; Average loss: 4.2476 Iteration: 132; Percent complete: 3.3%; Average loss: 4.3515 Iteration: 133; Percent complete: 3.3%; Average loss: 4.3821 Iteration: 134; Percent complete: 3.4%; Average loss: 4.5355 Iteration: 135; Percent complete: 3.4%; Average loss: 4.3854 Iteration: 136; Percent complete: 3.4%; Average loss: 4.4099 Iteration: 137; Percent complete: 3.4%; Average loss: 4.4142 Iteration: 138; Percent complete: 3.5%; Average loss: 4.0639 Iteration: 139; Percent complete: 3.5%; Average loss: 4.3268 Iteration: 140; Percent complete: 3.5%; Average loss: 4.6308 Iteration: 141; Percent complete: 3.5%; Average loss: 4.2231 Iteration: 142; Percent complete: 3.5%; Average loss: 4.2517 Iteration: 143; Percent complete: 3.6%; Average loss: 4.2467 Iteration: 144; Percent complete: 3.6%; Average loss: 4.2183 Iteration: 145; Percent complete: 3.6%; Average loss: 4.0980 Iteration: 146; Percent complete: 3.6%; Average loss: 4.4000 Iteration: 147; Percent complete: 3.7%; Average loss: 4.3491 Iteration: 148; Percent complete: 3.7%; Average loss: 4.4067 Iteration: 149; Percent complete: 3.7%; Average loss: 4.0079 Iteration: 150; Percent complete: 3.8%; Average loss: 4.3088 Iteration: 151; Percent complete: 3.8%; Average loss: 4.4050 Iteration: 152; Percent complete: 3.8%; Average loss: 4.2857 Iteration: 153; Percent complete: 3.8%; Average loss: 4.1196 Iteration: 154; Percent complete: 3.9%; Average loss: 4.3951 Iteration: 155; Percent complete: 3.9%; Average loss: 4.1116 Iteration: 156; Percent complete: 3.9%; Average loss: 4.2435 Iteration: 157; Percent complete: 3.9%; Average loss: 4.3154 Iteration: 158; Percent complete: 4.0%; Average loss: 4.0113 Iteration: 159; Percent complete: 4.0%; Average loss: 4.3031 Iteration: 160; Percent complete: 4.0%; Average loss: 4.3604 Iteration: 161; Percent complete: 4.0%; Average loss: 3.9337 Iteration: 162; Percent complete: 4.0%; Average loss: 4.0941 Iteration: 163; Percent complete: 4.1%; Average loss: 4.1448 Iteration: 164; Percent complete: 4.1%; Average loss: 4.4160 Iteration: 165; Percent complete: 4.1%; Average loss: 4.0635 Iteration: 166; Percent complete: 4.2%; Average loss: 4.0130 Iteration: 167; Percent complete: 4.2%; Average loss: 4.0126 Iteration: 168; Percent complete: 4.2%; Average loss: 4.1332 Iteration: 169; Percent complete: 4.2%; Average loss: 4.1200 Iteration: 170; Percent complete: 4.2%; Average loss: 4.3998 Iteration: 171; Percent complete: 4.3%; Average loss: 4.0235 Iteration: 172; Percent complete: 4.3%; Average loss: 4.2458 Iteration: 173; Percent complete: 4.3%; Average loss: 4.3408 Iteration: 174; Percent complete: 4.3%; Average loss: 4.0092 Iteration: 175; Percent complete: 4.4%; Average loss: 4.1289 Iteration: 176; Percent complete: 4.4%; Average loss: 4.0282 Iteration: 177; Percent complete: 4.4%; Average loss: 3.9678 Iteration: 178; Percent complete: 4.5%; Average loss: 4.1102 Iteration: 179; Percent complete: 4.5%; Average loss: 4.3429 Iteration: 180; Percent complete: 4.5%; Average loss: 4.2584 Iteration: 181; Percent complete: 4.5%; Average loss: 3.9500 Iteration: 182; Percent complete: 4.5%; Average loss: 4.2582 Iteration: 183; Percent complete: 4.6%; Average loss: 4.2197 Iteration: 184; Percent complete: 4.6%; Average loss: 4.1177 Iteration: 185; Percent complete: 4.6%; Average loss: 3.9369 Iteration: 186; Percent complete: 4.7%; Average loss: 3.9190 Iteration: 187; Percent complete: 4.7%; Average loss: 4.1578 Iteration: 188; Percent complete: 4.7%; Average loss: 3.9495 Iteration: 189; Percent complete: 4.7%; Average loss: 3.9539 Iteration: 190; Percent complete: 4.8%; Average loss: 4.1388 Iteration: 191; Percent complete: 4.8%; Average loss: 4.2341 Iteration: 192; Percent complete: 4.8%; Average loss: 4.2562 Iteration: 193; Percent complete: 4.8%; Average loss: 4.1978 Iteration: 194; Percent complete: 4.9%; Average loss: 4.1372 Iteration: 195; Percent complete: 4.9%; Average loss: 4.2581 Iteration: 196; Percent complete: 4.9%; Average loss: 4.2444 Iteration: 197; Percent complete: 4.9%; Average loss: 4.0698 Iteration: 198; Percent complete: 5.0%; Average loss: 4.2402 Iteration: 199; Percent complete: 5.0%; Average loss: 4.0469 Iteration: 200; Percent complete: 5.0%; Average loss: 4.0102 Iteration: 201; Percent complete: 5.0%; Average loss: 4.2119 Iteration: 202; Percent complete: 5.1%; Average loss: 3.8905 Iteration: 203; Percent complete: 5.1%; Average loss: 4.0130 Iteration: 204; Percent complete: 5.1%; Average loss: 4.1105 Iteration: 205; Percent complete: 5.1%; Average loss: 4.0650 Iteration: 206; Percent complete: 5.1%; Average loss: 3.8371 Iteration: 207; Percent complete: 5.2%; Average loss: 4.1023 Iteration: 208; Percent complete: 5.2%; Average loss: 3.9663 Iteration: 209; Percent complete: 5.2%; Average loss: 4.1744 Iteration: 210; Percent complete: 5.2%; Average loss: 3.9062 Iteration: 211; Percent complete: 5.3%; Average loss: 3.9315 Iteration: 212; Percent complete: 5.3%; Average loss: 3.9648 Iteration: 213; Percent complete: 5.3%; Average loss: 4.1374 Iteration: 214; Percent complete: 5.3%; Average loss: 4.2198 Iteration: 215; Percent complete: 5.4%; Average loss: 3.8338 Iteration: 216; Percent complete: 5.4%; Average loss: 4.1694 Iteration: 217; Percent complete: 5.4%; Average loss: 4.2545 Iteration: 218; Percent complete: 5.5%; Average loss: 4.0823 Iteration: 219; Percent complete: 5.5%; Average loss: 3.8987 Iteration: 220; Percent complete: 5.5%; Average loss: 4.3054 Iteration: 221; Percent complete: 5.5%; Average loss: 4.2519 Iteration: 222; Percent complete: 5.5%; Average loss: 4.1531 Iteration: 223; Percent complete: 5.6%; Average loss: 4.2179 Iteration: 224; Percent complete: 5.6%; Average loss: 3.9710 Iteration: 225; Percent complete: 5.6%; Average loss: 4.2665 Iteration: 226; Percent complete: 5.7%; Average loss: 4.0739 Iteration: 227; Percent complete: 5.7%; Average loss: 3.8656 Iteration: 228; Percent complete: 5.7%; Average loss: 4.3188 Iteration: 229; Percent complete: 5.7%; Average loss: 3.9715 Iteration: 230; Percent complete: 5.8%; Average loss: 3.8609 Iteration: 231; Percent complete: 5.8%; Average loss: 4.3733 Iteration: 232; Percent complete: 5.8%; Average loss: 4.0167 Iteration: 233; Percent complete: 5.8%; Average loss: 3.9044 Iteration: 234; Percent complete: 5.9%; Average loss: 3.9252 Iteration: 235; Percent complete: 5.9%; Average loss: 3.9319 Iteration: 236; Percent complete: 5.9%; Average loss: 4.0813 Iteration: 237; Percent complete: 5.9%; Average loss: 3.8553 Iteration: 238; Percent complete: 5.9%; Average loss: 4.2141 Iteration: 239; Percent complete: 6.0%; Average loss: 4.2337 Iteration: 240; Percent complete: 6.0%; Average loss: 4.2982 Iteration: 241; Percent complete: 6.0%; Average loss: 3.7722 Iteration: 242; Percent complete: 6.0%; Average loss: 4.2424 Iteration: 243; Percent complete: 6.1%; Average loss: 3.9604 Iteration: 244; Percent complete: 6.1%; Average loss: 3.8432 Iteration: 245; Percent complete: 6.1%; Average loss: 3.9931 Iteration: 246; Percent complete: 6.2%; Average loss: 4.1019 Iteration: 247; Percent complete: 6.2%; Average loss: 4.0875 Iteration: 248; Percent complete: 6.2%; Average loss: 3.9039 Iteration: 249; Percent complete: 6.2%; Average loss: 4.0813 Iteration: 250; Percent complete: 6.2%; Average loss: 3.9606 Iteration: 251; Percent complete: 6.3%; Average loss: 4.0392 Iteration: 252; Percent complete: 6.3%; Average loss: 4.1931 Iteration: 253; Percent complete: 6.3%; Average loss: 4.0117 Iteration: 254; Percent complete: 6.3%; Average loss: 4.1002 Iteration: 255; Percent complete: 6.4%; Average loss: 3.8141 Iteration: 256; Percent complete: 6.4%; Average loss: 3.9038 Iteration: 257; Percent complete: 6.4%; Average loss: 4.0396 Iteration: 258; Percent complete: 6.5%; Average loss: 3.9385 Iteration: 259; Percent complete: 6.5%; Average loss: 3.8174 Iteration: 260; Percent complete: 6.5%; Average loss: 4.0905 Iteration: 261; Percent complete: 6.5%; Average loss: 3.8126 Iteration: 262; Percent complete: 6.6%; Average loss: 4.0326 Iteration: 263; Percent complete: 6.6%; Average loss: 3.8222 Iteration: 264; Percent complete: 6.6%; Average loss: 3.9978 Iteration: 265; Percent complete: 6.6%; Average loss: 3.8937 Iteration: 266; Percent complete: 6.7%; Average loss: 4.0828 Iteration: 267; Percent complete: 6.7%; Average loss: 4.0608 Iteration: 268; Percent complete: 6.7%; Average loss: 4.0653 Iteration: 269; Percent complete: 6.7%; Average loss: 4.0254 Iteration: 270; Percent complete: 6.8%; Average loss: 3.7896 Iteration: 271; Percent complete: 6.8%; Average loss: 4.0030 Iteration: 272; Percent complete: 6.8%; Average loss: 4.0295 Iteration: 273; Percent complete: 6.8%; Average loss: 4.1786 Iteration: 274; Percent complete: 6.9%; Average loss: 4.2398 Iteration: 275; Percent complete: 6.9%; Average loss: 4.0740 Iteration: 276; Percent complete: 6.9%; Average loss: 4.1029 Iteration: 277; Percent complete: 6.9%; Average loss: 3.9468 Iteration: 278; Percent complete: 7.0%; Average loss: 3.8306 Iteration: 279; Percent complete: 7.0%; Average loss: 3.8863 Iteration: 280; Percent complete: 7.0%; Average loss: 4.0676 Iteration: 281; Percent complete: 7.0%; Average loss: 4.1471 Iteration: 282; Percent complete: 7.0%; Average loss: 3.7536 Iteration: 283; Percent complete: 7.1%; Average loss: 3.7565 Iteration: 284; Percent complete: 7.1%; Average loss: 4.1690 Iteration: 285; Percent complete: 7.1%; Average loss: 3.9237 Iteration: 286; Percent complete: 7.1%; Average loss: 3.8931 Iteration: 287; Percent complete: 7.2%; Average loss: 4.0724 Iteration: 288; Percent complete: 7.2%; Average loss: 3.6871 Iteration: 289; Percent complete: 7.2%; Average loss: 3.7048 Iteration: 290; Percent complete: 7.2%; Average loss: 4.0547 Iteration: 291; Percent complete: 7.3%; Average loss: 4.0709 Iteration: 292; Percent complete: 7.3%; Average loss: 3.8564 Iteration: 293; Percent complete: 7.3%; Average loss: 3.8243 Iteration: 294; Percent complete: 7.3%; Average loss: 3.8689 Iteration: 295; Percent complete: 7.4%; Average loss: 4.1418 Iteration: 296; Percent complete: 7.4%; Average loss: 4.1690 Iteration: 297; Percent complete: 7.4%; Average loss: 4.2614 Iteration: 298; Percent complete: 7.4%; Average loss: 3.9150 Iteration: 299; Percent complete: 7.5%; Average loss: 3.7848 Iteration: 300; Percent complete: 7.5%; Average loss: 3.8905 Iteration: 301; Percent complete: 7.5%; Average loss: 3.7434 Iteration: 302; Percent complete: 7.5%; Average loss: 3.7664 Iteration: 303; Percent complete: 7.6%; Average loss: 3.8246 Iteration: 304; Percent complete: 7.6%; Average loss: 3.9367 Iteration: 305; Percent complete: 7.6%; Average loss: 4.1597 Iteration: 306; Percent complete: 7.6%; Average loss: 4.0126 Iteration: 307; Percent complete: 7.7%; Average loss: 3.9040 Iteration: 308; Percent complete: 7.7%; Average loss: 3.6686 Iteration: 309; Percent complete: 7.7%; Average loss: 3.9476 Iteration: 310; Percent complete: 7.8%; Average loss: 3.5270 Iteration: 311; Percent complete: 7.8%; Average loss: 3.9742 Iteration: 312; Percent complete: 7.8%; Average loss: 3.9565 Iteration: 313; Percent complete: 7.8%; Average loss: 3.7915 Iteration: 314; Percent complete: 7.8%; Average loss: 3.8458 Iteration: 315; Percent complete: 7.9%; Average loss: 3.8274 Iteration: 316; Percent complete: 7.9%; Average loss: 4.0187 Iteration: 317; Percent complete: 7.9%; Average loss: 3.9187 Iteration: 318; Percent complete: 8.0%; Average loss: 3.9373 Iteration: 319; Percent complete: 8.0%; Average loss: 3.7506 Iteration: 320; Percent complete: 8.0%; Average loss: 4.1697 Iteration: 321; Percent complete: 8.0%; Average loss: 3.9130 Iteration: 322; Percent complete: 8.1%; Average loss: 3.7959 Iteration: 323; Percent complete: 8.1%; Average loss: 3.6426 Iteration: 324; Percent complete: 8.1%; Average loss: 3.6684 Iteration: 325; Percent complete: 8.1%; Average loss: 3.7139 Iteration: 326; Percent complete: 8.2%; Average loss: 3.9993 Iteration: 327; Percent complete: 8.2%; Average loss: 3.8237 Iteration: 328; Percent complete: 8.2%; Average loss: 3.9764 Iteration: 329; Percent complete: 8.2%; Average loss: 3.6514 Iteration: 330; Percent complete: 8.2%; Average loss: 4.0228 Iteration: 331; Percent complete: 8.3%; Average loss: 3.8762 Iteration: 332; Percent complete: 8.3%; Average loss: 4.0881 Iteration: 333; Percent complete: 8.3%; Average loss: 3.7423 Iteration: 334; Percent complete: 8.3%; Average loss: 3.7220 Iteration: 335; Percent complete: 8.4%; Average loss: 4.0425 Iteration: 336; Percent complete: 8.4%; Average loss: 3.7790 Iteration: 337; Percent complete: 8.4%; Average loss: 3.7951 Iteration: 338; Percent complete: 8.5%; Average loss: 3.9750 Iteration: 339; Percent complete: 8.5%; Average loss: 4.1157 Iteration: 340; Percent complete: 8.5%; Average loss: 3.9190 Iteration: 341; Percent complete: 8.5%; Average loss: 3.7664 Iteration: 342; Percent complete: 8.6%; Average loss: 3.8566 Iteration: 343; Percent complete: 8.6%; Average loss: 3.8795 Iteration: 344; Percent complete: 8.6%; Average loss: 3.8601 Iteration: 345; Percent complete: 8.6%; Average loss: 4.0740 Iteration: 346; Percent complete: 8.6%; Average loss: 3.8743 Iteration: 347; Percent complete: 8.7%; Average loss: 3.9843 Iteration: 348; Percent complete: 8.7%; Average loss: 3.7432 Iteration: 349; Percent complete: 8.7%; Average loss: 3.9001 Iteration: 350; Percent complete: 8.8%; Average loss: 3.9852 Iteration: 351; Percent complete: 8.8%; Average loss: 3.8096 Iteration: 352; Percent complete: 8.8%; Average loss: 4.0090 Iteration: 353; Percent complete: 8.8%; Average loss: 3.7908 Iteration: 354; Percent complete: 8.8%; Average loss: 4.0879 Iteration: 355; Percent complete: 8.9%; Average loss: 3.9825 Iteration: 356; Percent complete: 8.9%; Average loss: 3.8816 Iteration: 357; Percent complete: 8.9%; Average loss: 3.6711 Iteration: 358; Percent complete: 8.9%; Average loss: 3.9173 Iteration: 359; Percent complete: 9.0%; Average loss: 3.7453 Iteration: 360; Percent complete: 9.0%; Average loss: 3.9662 Iteration: 361; Percent complete: 9.0%; Average loss: 3.6735 Iteration: 362; Percent complete: 9.0%; Average loss: 3.9023 Iteration: 363; Percent complete: 9.1%; Average loss: 3.8795 Iteration: 364; Percent complete: 9.1%; Average loss: 3.7925 Iteration: 365; Percent complete: 9.1%; Average loss: 3.6494 Iteration: 366; Percent complete: 9.2%; Average loss: 3.6937 Iteration: 367; Percent complete: 9.2%; Average loss: 3.6153 Iteration: 368; Percent complete: 9.2%; Average loss: 4.0357 Iteration: 369; Percent complete: 9.2%; Average loss: 3.8784 Iteration: 370; Percent complete: 9.2%; Average loss: 4.0499 Iteration: 371; Percent complete: 9.3%; Average loss: 3.9273 Iteration: 372; Percent complete: 9.3%; Average loss: 3.7436 Iteration: 373; Percent complete: 9.3%; Average loss: 3.9384 Iteration: 374; Percent complete: 9.3%; Average loss: 3.9976 Iteration: 375; Percent complete: 9.4%; Average loss: 3.9210 Iteration: 376; Percent complete: 9.4%; Average loss: 3.6776 Iteration: 377; Percent complete: 9.4%; Average loss: 3.7840 Iteration: 378; Percent complete: 9.4%; Average loss: 4.1935 Iteration: 379; Percent complete: 9.5%; Average loss: 3.8242 Iteration: 380; Percent complete: 9.5%; Average loss: 3.8671 Iteration: 381; Percent complete: 9.5%; Average loss: 3.6727 Iteration: 382; Percent complete: 9.6%; Average loss: 3.8424 Iteration: 383; Percent complete: 9.6%; Average loss: 4.2503 Iteration: 384; Percent complete: 9.6%; Average loss: 4.0939 Iteration: 385; Percent complete: 9.6%; Average loss: 3.6899 Iteration: 386; Percent complete: 9.7%; Average loss: 3.7289 Iteration: 387; Percent complete: 9.7%; Average loss: 3.9006 Iteration: 388; Percent complete: 9.7%; Average loss: 3.8941 Iteration: 389; Percent complete: 9.7%; Average loss: 3.9536 Iteration: 390; Percent complete: 9.8%; Average loss: 3.6524 Iteration: 391; Percent complete: 9.8%; Average loss: 3.6636 Iteration: 392; Percent complete: 9.8%; Average loss: 4.0040 Iteration: 393; Percent complete: 9.8%; Average loss: 4.1662 Iteration: 394; Percent complete: 9.8%; Average loss: 3.6631 Iteration: 395; Percent complete: 9.9%; Average loss: 3.8412 Iteration: 396; Percent complete: 9.9%; Average loss: 3.6785 Iteration: 397; Percent complete: 9.9%; Average loss: 3.7956 Iteration: 398; Percent complete: 10.0%; Average loss: 3.9529 Iteration: 399; Percent complete: 10.0%; Average loss: 3.8220 Iteration: 400; Percent complete: 10.0%; Average loss: 3.7838 Iteration: 401; Percent complete: 10.0%; Average loss: 3.7761 Iteration: 402; Percent complete: 10.1%; Average loss: 3.6371 Iteration: 403; Percent complete: 10.1%; Average loss: 3.5735 Iteration: 404; Percent complete: 10.1%; Average loss: 3.6639 Iteration: 405; Percent complete: 10.1%; Average loss: 3.8436 Iteration: 406; Percent complete: 10.2%; Average loss: 3.9763 Iteration: 407; Percent complete: 10.2%; Average loss: 3.8025 Iteration: 408; Percent complete: 10.2%; Average loss: 3.7171 Iteration: 409; Percent complete: 10.2%; Average loss: 3.6045 Iteration: 410; Percent complete: 10.2%; Average loss: 3.9035 Iteration: 411; Percent complete: 10.3%; Average loss: 3.7347 Iteration: 412; Percent complete: 10.3%; Average loss: 3.7916 Iteration: 413; Percent complete: 10.3%; Average loss: 3.8472 Iteration: 414; Percent complete: 10.3%; Average loss: 4.0208 Iteration: 415; Percent complete: 10.4%; Average loss: 3.7478 Iteration: 416; Percent complete: 10.4%; Average loss: 3.8706 Iteration: 417; Percent complete: 10.4%; Average loss: 3.8347 Iteration: 418; Percent complete: 10.4%; Average loss: 3.9374 Iteration: 419; Percent complete: 10.5%; Average loss: 3.5887 Iteration: 420; Percent complete: 10.5%; Average loss: 3.9890 Iteration: 421; Percent complete: 10.5%; Average loss: 3.8349 Iteration: 422; Percent complete: 10.5%; Average loss: 3.9375 Iteration: 423; Percent complete: 10.6%; Average loss: 3.8246 Iteration: 424; Percent complete: 10.6%; Average loss: 4.0132 Iteration: 425; Percent complete: 10.6%; Average loss: 3.6273 Iteration: 426; Percent complete: 10.7%; Average loss: 3.6084 Iteration: 427; Percent complete: 10.7%; Average loss: 3.6830 Iteration: 428; Percent complete: 10.7%; Average loss: 4.0477 Iteration: 429; Percent complete: 10.7%; Average loss: 3.5428 Iteration: 430; Percent complete: 10.8%; Average loss: 3.8053 Iteration: 431; Percent complete: 10.8%; Average loss: 4.1039 Iteration: 432; Percent complete: 10.8%; Average loss: 3.9015 Iteration: 433; Percent complete: 10.8%; Average loss: 3.7782 Iteration: 434; Percent complete: 10.8%; Average loss: 3.8073 Iteration: 435; Percent complete: 10.9%; Average loss: 3.8359 Iteration: 436; Percent complete: 10.9%; Average loss: 3.7974 Iteration: 437; Percent complete: 10.9%; Average loss: 3.5793 Iteration: 438; Percent complete: 10.9%; Average loss: 3.8747 Iteration: 439; Percent complete: 11.0%; Average loss: 3.7048 Iteration: 440; Percent complete: 11.0%; Average loss: 3.8742 Iteration: 441; Percent complete: 11.0%; Average loss: 3.8835 Iteration: 442; Percent complete: 11.1%; Average loss: 3.7671 Iteration: 443; Percent complete: 11.1%; Average loss: 3.8183 Iteration: 444; Percent complete: 11.1%; Average loss: 3.5914 Iteration: 445; Percent complete: 11.1%; Average loss: 3.6467 Iteration: 446; Percent complete: 11.2%; Average loss: 4.1770 Iteration: 447; Percent complete: 11.2%; Average loss: 3.5894 Iteration: 448; Percent complete: 11.2%; Average loss: 3.9268 Iteration: 449; Percent complete: 11.2%; Average loss: 3.6881 Iteration: 450; Percent complete: 11.2%; Average loss: 3.6542 Iteration: 451; Percent complete: 11.3%; Average loss: 3.6058 Iteration: 452; Percent complete: 11.3%; Average loss: 3.7535 Iteration: 453; Percent complete: 11.3%; Average loss: 4.0341 Iteration: 454; Percent complete: 11.3%; Average loss: 3.5259 Iteration: 455; Percent complete: 11.4%; Average loss: 3.5237 Iteration: 456; Percent complete: 11.4%; Average loss: 3.6494 Iteration: 457; Percent complete: 11.4%; Average loss: 3.8239 Iteration: 458; Percent complete: 11.5%; Average loss: 3.6876 Iteration: 459; Percent complete: 11.5%; Average loss: 3.6188 Iteration: 460; Percent complete: 11.5%; Average loss: 3.6964 Iteration: 461; Percent complete: 11.5%; Average loss: 3.7003 Iteration: 462; Percent complete: 11.6%; Average loss: 3.5728 Iteration: 463; Percent complete: 11.6%; Average loss: 3.7111 Iteration: 464; Percent complete: 11.6%; Average loss: 3.9350 Iteration: 465; Percent complete: 11.6%; Average loss: 3.9611 Iteration: 466; Percent complete: 11.7%; Average loss: 3.9878 Iteration: 467; Percent complete: 11.7%; Average loss: 3.7157 Iteration: 468; Percent complete: 11.7%; Average loss: 3.6982 Iteration: 469; Percent complete: 11.7%; Average loss: 3.9292 Iteration: 470; Percent complete: 11.8%; Average loss: 3.6340 Iteration: 471; Percent complete: 11.8%; Average loss: 3.5767 Iteration: 472; Percent complete: 11.8%; Average loss: 3.7061 Iteration: 473; Percent complete: 11.8%; Average loss: 3.4571 Iteration: 474; Percent complete: 11.8%; Average loss: 4.0826 Iteration: 475; Percent complete: 11.9%; Average loss: 3.9763 Iteration: 476; Percent complete: 11.9%; Average loss: 3.7742 Iteration: 477; Percent complete: 11.9%; Average loss: 3.6747 Iteration: 478; Percent complete: 11.9%; Average loss: 3.7080 Iteration: 479; Percent complete: 12.0%; Average loss: 3.7840 Iteration: 480; Percent complete: 12.0%; Average loss: 4.0200 Iteration: 481; Percent complete: 12.0%; Average loss: 3.9979 Iteration: 482; Percent complete: 12.0%; Average loss: 3.8290 Iteration: 483; Percent complete: 12.1%; Average loss: 3.9083 Iteration: 484; Percent complete: 12.1%; Average loss: 3.7496 Iteration: 485; Percent complete: 12.1%; Average loss: 3.6685 Iteration: 486; Percent complete: 12.2%; Average loss: 3.6077 Iteration: 487; Percent complete: 12.2%; Average loss: 3.6229 Iteration: 488; Percent complete: 12.2%; Average loss: 3.7313 Iteration: 489; Percent complete: 12.2%; Average loss: 3.4573 Iteration: 490; Percent complete: 12.2%; Average loss: 3.9110 Iteration: 491; Percent complete: 12.3%; Average loss: 3.5708 Iteration: 492; Percent complete: 12.3%; Average loss: 3.4428 Iteration: 493; Percent complete: 12.3%; Average loss: 3.6250 Iteration: 494; Percent complete: 12.3%; Average loss: 3.8401 Iteration: 495; Percent complete: 12.4%; Average loss: 3.5267 Iteration: 496; Percent complete: 12.4%; Average loss: 3.4872 Iteration: 497; Percent complete: 12.4%; Average loss: 3.8448 Iteration: 498; Percent complete: 12.4%; Average loss: 3.8414 Iteration: 499; Percent complete: 12.5%; Average loss: 3.6201 Iteration: 500; Percent complete: 12.5%; Average loss: 3.7278 Iteration: 501; Percent complete: 12.5%; Average loss: 3.6465 Iteration: 502; Percent complete: 12.6%; Average loss: 3.7115 Iteration: 503; Percent complete: 12.6%; Average loss: 3.5751 Iteration: 504; Percent complete: 12.6%; Average loss: 3.7741 Iteration: 505; Percent complete: 12.6%; Average loss: 3.8538 Iteration: 506; Percent complete: 12.7%; Average loss: 3.9116 Iteration: 507; Percent complete: 12.7%; Average loss: 3.6859 Iteration: 508; Percent complete: 12.7%; Average loss: 3.8692 Iteration: 509; Percent complete: 12.7%; Average loss: 3.7456 Iteration: 510; Percent complete: 12.8%; Average loss: 3.8843 Iteration: 511; Percent complete: 12.8%; Average loss: 3.7346 Iteration: 512; Percent complete: 12.8%; Average loss: 3.6633 Iteration: 513; Percent complete: 12.8%; Average loss: 3.6086 Iteration: 514; Percent complete: 12.8%; Average loss: 4.0130 Iteration: 515; Percent complete: 12.9%; Average loss: 3.8067 Iteration: 516; Percent complete: 12.9%; Average loss: 3.4940 Iteration: 517; Percent complete: 12.9%; Average loss: 3.7906 Iteration: 518; Percent complete: 13.0%; Average loss: 3.7453 Iteration: 519; Percent complete: 13.0%; Average loss: 3.7317 Iteration: 520; Percent complete: 13.0%; Average loss: 3.4098 Iteration: 521; Percent complete: 13.0%; Average loss: 3.8869 Iteration: 522; Percent complete: 13.1%; Average loss: 3.9227 Iteration: 523; Percent complete: 13.1%; Average loss: 3.9113 Iteration: 524; Percent complete: 13.1%; Average loss: 3.6720 Iteration: 525; Percent complete: 13.1%; Average loss: 3.8332 Iteration: 526; Percent complete: 13.2%; Average loss: 3.9244 Iteration: 527; Percent complete: 13.2%; Average loss: 3.7948 Iteration: 528; Percent complete: 13.2%; Average loss: 3.6549 Iteration: 529; Percent complete: 13.2%; Average loss: 3.7200 Iteration: 530; Percent complete: 13.2%; Average loss: 3.7915 Iteration: 531; Percent complete: 13.3%; Average loss: 3.6985 Iteration: 532; Percent complete: 13.3%; Average loss: 3.6895 Iteration: 533; Percent complete: 13.3%; Average loss: 3.5698 Iteration: 534; Percent complete: 13.4%; Average loss: 3.5777 Iteration: 535; Percent complete: 13.4%; Average loss: 3.5592 Iteration: 536; Percent complete: 13.4%; Average loss: 3.7459 Iteration: 537; Percent complete: 13.4%; Average loss: 3.5808 Iteration: 538; Percent complete: 13.5%; Average loss: 3.6476 Iteration: 539; Percent complete: 13.5%; Average loss: 4.0026 Iteration: 540; Percent complete: 13.5%; Average loss: 3.7889 Iteration: 541; Percent complete: 13.5%; Average loss: 3.9443 Iteration: 542; Percent complete: 13.6%; Average loss: 3.7708 Iteration: 543; Percent complete: 13.6%; Average loss: 3.7578 Iteration: 544; Percent complete: 13.6%; Average loss: 3.8539 Iteration: 545; Percent complete: 13.6%; Average loss: 3.5869 Iteration: 546; Percent complete: 13.7%; Average loss: 3.7823 Iteration: 547; Percent complete: 13.7%; Average loss: 4.0667 Iteration: 548; Percent complete: 13.7%; Average loss: 3.5022 Iteration: 549; Percent complete: 13.7%; Average loss: 3.8831 Iteration: 550; Percent complete: 13.8%; Average loss: 3.6687 Iteration: 551; Percent complete: 13.8%; Average loss: 3.7143 Iteration: 552; Percent complete: 13.8%; Average loss: 4.0907 Iteration: 553; Percent complete: 13.8%; Average loss: 3.6730 Iteration: 554; Percent complete: 13.9%; Average loss: 3.8940 Iteration: 555; Percent complete: 13.9%; Average loss: 3.7960 Iteration: 556; Percent complete: 13.9%; Average loss: 3.6539 Iteration: 557; Percent complete: 13.9%; Average loss: 3.4448 Iteration: 558; Percent complete: 14.0%; Average loss: 3.8843 Iteration: 559; Percent complete: 14.0%; Average loss: 3.5870 Iteration: 560; Percent complete: 14.0%; Average loss: 3.8990 Iteration: 561; Percent complete: 14.0%; Average loss: 3.7573 Iteration: 562; Percent complete: 14.1%; Average loss: 3.6309 Iteration: 563; Percent complete: 14.1%; Average loss: 3.4695 Iteration: 564; Percent complete: 14.1%; Average loss: 3.6592 Iteration: 565; Percent complete: 14.1%; Average loss: 3.5284 Iteration: 566; Percent complete: 14.1%; Average loss: 3.6090 Iteration: 567; Percent complete: 14.2%; Average loss: 3.6157 Iteration: 568; Percent complete: 14.2%; Average loss: 3.6231 Iteration: 569; Percent complete: 14.2%; Average loss: 3.5885 Iteration: 570; Percent complete: 14.2%; Average loss: 3.7504 Iteration: 571; Percent complete: 14.3%; Average loss: 3.5600 Iteration: 572; Percent complete: 14.3%; Average loss: 3.5519 Iteration: 573; Percent complete: 14.3%; Average loss: 3.6517 Iteration: 574; Percent complete: 14.3%; Average loss: 3.6215 Iteration: 575; Percent complete: 14.4%; Average loss: 3.6275 Iteration: 576; Percent complete: 14.4%; Average loss: 3.4602 Iteration: 577; Percent complete: 14.4%; Average loss: 3.5683 Iteration: 578; Percent complete: 14.4%; Average loss: 3.7060 Iteration: 579; Percent complete: 14.5%; Average loss: 3.5885 Iteration: 580; Percent complete: 14.5%; Average loss: 3.8502 Iteration: 581; Percent complete: 14.5%; Average loss: 3.4425 Iteration: 582; Percent complete: 14.5%; Average loss: 3.8642 Iteration: 583; Percent complete: 14.6%; Average loss: 3.7559 Iteration: 584; Percent complete: 14.6%; Average loss: 3.4155 Iteration: 585; Percent complete: 14.6%; Average loss: 3.6386 Iteration: 586; Percent complete: 14.6%; Average loss: 3.7383 Iteration: 587; Percent complete: 14.7%; Average loss: 3.6886 Iteration: 588; Percent complete: 14.7%; Average loss: 3.9131 Iteration: 589; Percent complete: 14.7%; Average loss: 3.6342 Iteration: 590; Percent complete: 14.8%; Average loss: 3.8054 Iteration: 591; Percent complete: 14.8%; Average loss: 3.8479 Iteration: 592; Percent complete: 14.8%; Average loss: 3.5324 Iteration: 593; Percent complete: 14.8%; Average loss: 3.8679 Iteration: 594; Percent complete: 14.8%; Average loss: 3.5635 Iteration: 595; Percent complete: 14.9%; Average loss: 3.6341 Iteration: 596; Percent complete: 14.9%; Average loss: 3.5701 Iteration: 597; Percent complete: 14.9%; Average loss: 3.5757 Iteration: 598; Percent complete: 14.9%; Average loss: 3.8664 Iteration: 599; Percent complete: 15.0%; Average loss: 3.8431 Iteration: 600; Percent complete: 15.0%; Average loss: 3.6757 Iteration: 601; Percent complete: 15.0%; Average loss: 3.5568 Iteration: 602; Percent complete: 15.0%; Average loss: 3.5255 Iteration: 603; Percent complete: 15.1%; Average loss: 3.6832 Iteration: 604; Percent complete: 15.1%; Average loss: 3.5041 Iteration: 605; Percent complete: 15.1%; Average loss: 3.4693 Iteration: 606; Percent complete: 15.2%; Average loss: 3.7391 Iteration: 607; Percent complete: 15.2%; Average loss: 3.5533 Iteration: 608; Percent complete: 15.2%; Average loss: 3.6217 Iteration: 609; Percent complete: 15.2%; Average loss: 3.6252 Iteration: 610; Percent complete: 15.2%; Average loss: 3.6748 Iteration: 611; Percent complete: 15.3%; Average loss: 3.4140 Iteration: 612; Percent complete: 15.3%; Average loss: 4.0225 Iteration: 613; Percent complete: 15.3%; Average loss: 3.7063 Iteration: 614; Percent complete: 15.3%; Average loss: 3.4902 Iteration: 615; Percent complete: 15.4%; Average loss: 3.7020 Iteration: 616; Percent complete: 15.4%; Average loss: 3.6972 Iteration: 617; Percent complete: 15.4%; Average loss: 3.4077 Iteration: 618; Percent complete: 15.4%; Average loss: 3.5445 Iteration: 619; Percent complete: 15.5%; Average loss: 3.5710 Iteration: 620; Percent complete: 15.5%; Average loss: 3.6647 Iteration: 621; Percent complete: 15.5%; Average loss: 3.8920 Iteration: 622; Percent complete: 15.6%; Average loss: 3.8137 Iteration: 623; Percent complete: 15.6%; Average loss: 3.5768 Iteration: 624; Percent complete: 15.6%; Average loss: 3.8745 Iteration: 625; Percent complete: 15.6%; Average loss: 3.4053 Iteration: 626; Percent complete: 15.7%; Average loss: 3.5182 Iteration: 627; Percent complete: 15.7%; Average loss: 4.0964 Iteration: 628; Percent complete: 15.7%; Average loss: 3.5067 Iteration: 629; Percent complete: 15.7%; Average loss: 3.7664 Iteration: 630; Percent complete: 15.8%; Average loss: 3.5864 Iteration: 631; Percent complete: 15.8%; Average loss: 3.7993 Iteration: 632; Percent complete: 15.8%; Average loss: 3.5678 Iteration: 633; Percent complete: 15.8%; Average loss: 3.7831 Iteration: 634; Percent complete: 15.8%; Average loss: 3.5835 Iteration: 635; Percent complete: 15.9%; Average loss: 3.5797 Iteration: 636; Percent complete: 15.9%; Average loss: 3.5247 Iteration: 637; Percent complete: 15.9%; Average loss: 3.8920 Iteration: 638; Percent complete: 16.0%; Average loss: 3.5004 Iteration: 639; Percent complete: 16.0%; Average loss: 3.6167 Iteration: 640; Percent complete: 16.0%; Average loss: 3.6565 Iteration: 641; Percent complete: 16.0%; Average loss: 3.7118 Iteration: 642; Percent complete: 16.1%; Average loss: 3.3290 Iteration: 643; Percent complete: 16.1%; Average loss: 3.8205 Iteration: 644; Percent complete: 16.1%; Average loss: 3.8775 Iteration: 645; Percent complete: 16.1%; Average loss: 3.6254 Iteration: 646; Percent complete: 16.2%; Average loss: 3.6253 Iteration: 647; Percent complete: 16.2%; Average loss: 3.6529 Iteration: 648; Percent complete: 16.2%; Average loss: 3.4786 Iteration: 649; Percent complete: 16.2%; Average loss: 3.7553 Iteration: 650; Percent complete: 16.2%; Average loss: 3.7958 Iteration: 651; Percent complete: 16.3%; Average loss: 3.7888 Iteration: 652; Percent complete: 16.3%; Average loss: 3.4761 Iteration: 653; Percent complete: 16.3%; Average loss: 3.4374 Iteration: 654; Percent complete: 16.4%; Average loss: 3.5033 Iteration: 655; Percent complete: 16.4%; Average loss: 3.9741 Iteration: 656; Percent complete: 16.4%; Average loss: 3.8172 Iteration: 657; Percent complete: 16.4%; Average loss: 3.6626 Iteration: 658; Percent complete: 16.4%; Average loss: 3.4722 Iteration: 659; Percent complete: 16.5%; Average loss: 3.8056 Iteration: 660; Percent complete: 16.5%; Average loss: 3.7402 Iteration: 661; Percent complete: 16.5%; Average loss: 3.6150 Iteration: 662; Percent complete: 16.6%; Average loss: 3.6006 Iteration: 663; Percent complete: 16.6%; Average loss: 3.4548 Iteration: 664; Percent complete: 16.6%; Average loss: 3.6442 Iteration: 665; Percent complete: 16.6%; Average loss: 3.6635 Iteration: 666; Percent complete: 16.7%; Average loss: 3.4318 Iteration: 667; Percent complete: 16.7%; Average loss: 3.4845 Iteration: 668; Percent complete: 16.7%; Average loss: 3.4367 Iteration: 669; Percent complete: 16.7%; Average loss: 3.8478 Iteration: 670; Percent complete: 16.8%; Average loss: 3.7855 Iteration: 671; Percent complete: 16.8%; Average loss: 3.4413 Iteration: 672; Percent complete: 16.8%; Average loss: 3.6745 Iteration: 673; Percent complete: 16.8%; Average loss: 3.4916 Iteration: 674; Percent complete: 16.9%; Average loss: 3.5772 Iteration: 675; Percent complete: 16.9%; Average loss: 3.5467 Iteration: 676; Percent complete: 16.9%; Average loss: 3.5360 Iteration: 677; Percent complete: 16.9%; Average loss: 3.7590 Iteration: 678; Percent complete: 17.0%; Average loss: 3.6524 Iteration: 679; Percent complete: 17.0%; Average loss: 3.3031 Iteration: 680; Percent complete: 17.0%; Average loss: 3.7814 Iteration: 681; Percent complete: 17.0%; Average loss: 3.8696 Iteration: 682; Percent complete: 17.1%; Average loss: 3.7246 Iteration: 683; Percent complete: 17.1%; Average loss: 3.4913 Iteration: 684; Percent complete: 17.1%; Average loss: 3.8657 Iteration: 685; Percent complete: 17.1%; Average loss: 3.6651 Iteration: 686; Percent complete: 17.2%; Average loss: 3.4994 Iteration: 687; Percent complete: 17.2%; Average loss: 3.7060 Iteration: 688; Percent complete: 17.2%; Average loss: 3.5047 Iteration: 689; Percent complete: 17.2%; Average loss: 3.7251 Iteration: 690; Percent complete: 17.2%; Average loss: 3.7566 Iteration: 691; Percent complete: 17.3%; Average loss: 3.7596 Iteration: 692; Percent complete: 17.3%; Average loss: 3.4761 Iteration: 693; Percent complete: 17.3%; Average loss: 3.8354 Iteration: 694; Percent complete: 17.3%; Average loss: 3.7101 Iteration: 695; Percent complete: 17.4%; Average loss: 3.5857 Iteration: 696; Percent complete: 17.4%; Average loss: 3.6699 Iteration: 697; Percent complete: 17.4%; Average loss: 3.4157 Iteration: 698; Percent complete: 17.4%; Average loss: 3.7081 Iteration: 699; Percent complete: 17.5%; Average loss: 3.7905 Iteration: 700; Percent complete: 17.5%; Average loss: 3.6708 Iteration: 701; Percent complete: 17.5%; Average loss: 3.5294 Iteration: 702; Percent complete: 17.5%; Average loss: 3.5501 Iteration: 703; Percent complete: 17.6%; Average loss: 3.3321 Iteration: 704; Percent complete: 17.6%; Average loss: 3.6214 Iteration: 705; Percent complete: 17.6%; Average loss: 3.8308 Iteration: 706; Percent complete: 17.6%; Average loss: 3.5605 Iteration: 707; Percent complete: 17.7%; Average loss: 3.7015 Iteration: 708; Percent complete: 17.7%; Average loss: 3.7471 Iteration: 709; Percent complete: 17.7%; Average loss: 3.4249 Iteration: 710; Percent complete: 17.8%; Average loss: 3.7454 Iteration: 711; Percent complete: 17.8%; Average loss: 3.7104 Iteration: 712; Percent complete: 17.8%; Average loss: 3.4503 Iteration: 713; Percent complete: 17.8%; Average loss: 3.4711 Iteration: 714; Percent complete: 17.8%; Average loss: 3.9769 Iteration: 715; Percent complete: 17.9%; Average loss: 3.6794 Iteration: 716; Percent complete: 17.9%; Average loss: 3.4821 Iteration: 717; Percent complete: 17.9%; Average loss: 3.5484 Iteration: 718; Percent complete: 17.9%; Average loss: 3.4589 Iteration: 719; Percent complete: 18.0%; Average loss: 3.4465 Iteration: 720; Percent complete: 18.0%; Average loss: 3.5725 Iteration: 721; Percent complete: 18.0%; Average loss: 3.7296 Iteration: 722; Percent complete: 18.1%; Average loss: 3.8700 Iteration: 723; Percent complete: 18.1%; Average loss: 3.3565 Iteration: 724; Percent complete: 18.1%; Average loss: 3.6770 Iteration: 725; Percent complete: 18.1%; Average loss: 3.4511 Iteration: 726; Percent complete: 18.1%; Average loss: 3.5928 Iteration: 727; Percent complete: 18.2%; Average loss: 3.3567 Iteration: 728; Percent complete: 18.2%; Average loss: 3.5285 Iteration: 729; Percent complete: 18.2%; Average loss: 3.7483 Iteration: 730; Percent complete: 18.2%; Average loss: 3.6571 Iteration: 731; Percent complete: 18.3%; Average loss: 3.5836 Iteration: 732; Percent complete: 18.3%; Average loss: 3.5767 Iteration: 733; Percent complete: 18.3%; Average loss: 3.8608 Iteration: 734; Percent complete: 18.4%; Average loss: 3.7554 Iteration: 735; Percent complete: 18.4%; Average loss: 3.4166 Iteration: 736; Percent complete: 18.4%; Average loss: 3.6720 Iteration: 737; Percent complete: 18.4%; Average loss: 3.5104 Iteration: 738; Percent complete: 18.4%; Average loss: 3.7443 Iteration: 739; Percent complete: 18.5%; Average loss: 3.5940 Iteration: 740; Percent complete: 18.5%; Average loss: 3.6642 Iteration: 741; Percent complete: 18.5%; Average loss: 3.6126 Iteration: 742; Percent complete: 18.6%; Average loss: 3.8553 Iteration: 743; Percent complete: 18.6%; Average loss: 3.4120 Iteration: 744; Percent complete: 18.6%; Average loss: 3.7858 Iteration: 745; Percent complete: 18.6%; Average loss: 3.5265 Iteration: 746; Percent complete: 18.6%; Average loss: 3.5867 Iteration: 747; Percent complete: 18.7%; Average loss: 3.6414 Iteration: 748; Percent complete: 18.7%; Average loss: 3.7228 Iteration: 749; Percent complete: 18.7%; Average loss: 3.6811 Iteration: 750; Percent complete: 18.8%; Average loss: 3.6291 Iteration: 751; Percent complete: 18.8%; Average loss: 3.9091 Iteration: 752; Percent complete: 18.8%; Average loss: 3.7639 Iteration: 753; Percent complete: 18.8%; Average loss: 3.6157 Iteration: 754; Percent complete: 18.9%; Average loss: 3.4204 Iteration: 755; Percent complete: 18.9%; Average loss: 3.5056 Iteration: 756; Percent complete: 18.9%; Average loss: 3.4939 Iteration: 757; Percent complete: 18.9%; Average loss: 3.5918 Iteration: 758; Percent complete: 18.9%; Average loss: 3.5301 Iteration: 759; Percent complete: 19.0%; Average loss: 3.6288 Iteration: 760; Percent complete: 19.0%; Average loss: 3.6519 Iteration: 761; Percent complete: 19.0%; Average loss: 3.6578 Iteration: 762; Percent complete: 19.1%; Average loss: 3.5480 Iteration: 763; Percent complete: 19.1%; Average loss: 4.1786 Iteration: 764; Percent complete: 19.1%; Average loss: 3.4167 Iteration: 765; Percent complete: 19.1%; Average loss: 3.6874 Iteration: 766; Percent complete: 19.1%; Average loss: 3.6532 Iteration: 767; Percent complete: 19.2%; Average loss: 3.5401 Iteration: 768; Percent complete: 19.2%; Average loss: 3.3460 Iteration: 769; Percent complete: 19.2%; Average loss: 3.6328 Iteration: 770; Percent complete: 19.2%; Average loss: 3.7002 Iteration: 771; Percent complete: 19.3%; Average loss: 3.5127 Iteration: 772; Percent complete: 19.3%; Average loss: 3.6769 Iteration: 773; Percent complete: 19.3%; Average loss: 3.5349 Iteration: 774; Percent complete: 19.4%; Average loss: 3.4672 Iteration: 775; Percent complete: 19.4%; Average loss: 3.5078 Iteration: 776; Percent complete: 19.4%; Average loss: 3.1873 Iteration: 777; Percent complete: 19.4%; Average loss: 3.6004 Iteration: 778; Percent complete: 19.4%; Average loss: 3.5847 Iteration: 779; Percent complete: 19.5%; Average loss: 3.6367 Iteration: 780; Percent complete: 19.5%; Average loss: 3.3701 Iteration: 781; Percent complete: 19.5%; Average loss: 3.3984 Iteration: 782; Percent complete: 19.6%; Average loss: 3.4242 Iteration: 783; Percent complete: 19.6%; Average loss: 3.5546 Iteration: 784; Percent complete: 19.6%; Average loss: 3.4181 Iteration: 785; Percent complete: 19.6%; Average loss: 3.6756 Iteration: 786; Percent complete: 19.7%; Average loss: 3.5761 Iteration: 787; Percent complete: 19.7%; Average loss: 3.5875 Iteration: 788; Percent complete: 19.7%; Average loss: 3.5991 Iteration: 789; Percent complete: 19.7%; Average loss: 3.4455 Iteration: 790; Percent complete: 19.8%; Average loss: 3.8295 Iteration: 791; Percent complete: 19.8%; Average loss: 3.6048 Iteration: 792; Percent complete: 19.8%; Average loss: 3.8313 Iteration: 793; Percent complete: 19.8%; Average loss: 3.5996 Iteration: 794; Percent complete: 19.9%; Average loss: 3.7168 Iteration: 795; Percent complete: 19.9%; Average loss: 3.4881 Iteration: 796; Percent complete: 19.9%; Average loss: 3.8928 Iteration: 797; Percent complete: 19.9%; Average loss: 3.8045 Iteration: 798; Percent complete: 20.0%; Average loss: 3.6000 Iteration: 799; Percent complete: 20.0%; Average loss: 3.6082 Iteration: 800; Percent complete: 20.0%; Average loss: 3.4202 Iteration: 801; Percent complete: 20.0%; Average loss: 3.3625 Iteration: 802; Percent complete: 20.1%; Average loss: 3.6204 Iteration: 803; Percent complete: 20.1%; Average loss: 3.7309 Iteration: 804; Percent complete: 20.1%; Average loss: 3.4829 Iteration: 805; Percent complete: 20.1%; Average loss: 3.7324 Iteration: 806; Percent complete: 20.2%; Average loss: 3.3346 Iteration: 807; Percent complete: 20.2%; Average loss: 3.4380 Iteration: 808; Percent complete: 20.2%; Average loss: 3.5204 Iteration: 809; Percent complete: 20.2%; Average loss: 3.5121 Iteration: 810; Percent complete: 20.2%; Average loss: 3.9458 Iteration: 811; Percent complete: 20.3%; Average loss: 3.6748 Iteration: 812; Percent complete: 20.3%; Average loss: 3.6910 Iteration: 813; Percent complete: 20.3%; Average loss: 3.5063 Iteration: 814; Percent complete: 20.3%; Average loss: 3.7836 Iteration: 815; Percent complete: 20.4%; Average loss: 3.6605 Iteration: 816; Percent complete: 20.4%; Average loss: 3.7650 Iteration: 817; Percent complete: 20.4%; Average loss: 3.5724 Iteration: 818; Percent complete: 20.4%; Average loss: 3.7947 Iteration: 819; Percent complete: 20.5%; Average loss: 3.8254 Iteration: 820; Percent complete: 20.5%; Average loss: 3.6078 Iteration: 821; Percent complete: 20.5%; Average loss: 3.4210 Iteration: 822; Percent complete: 20.5%; Average loss: 3.5031 Iteration: 823; Percent complete: 20.6%; Average loss: 3.5266 Iteration: 824; Percent complete: 20.6%; Average loss: 3.5579 Iteration: 825; Percent complete: 20.6%; Average loss: 3.7550 Iteration: 826; Percent complete: 20.6%; Average loss: 3.5722 Iteration: 827; Percent complete: 20.7%; Average loss: 3.4871 Iteration: 828; Percent complete: 20.7%; Average loss: 3.4543 Iteration: 829; Percent complete: 20.7%; Average loss: 3.4841 Iteration: 830; Percent complete: 20.8%; Average loss: 3.6156 Iteration: 831; Percent complete: 20.8%; Average loss: 3.4467 Iteration: 832; Percent complete: 20.8%; Average loss: 3.8080 Iteration: 833; Percent complete: 20.8%; Average loss: 3.5139 Iteration: 834; Percent complete: 20.8%; Average loss: 3.4338 Iteration: 835; Percent complete: 20.9%; Average loss: 3.4961 Iteration: 836; Percent complete: 20.9%; Average loss: 3.6224 Iteration: 837; Percent complete: 20.9%; Average loss: 3.6306 Iteration: 838; Percent complete: 20.9%; Average loss: 3.6733 Iteration: 839; Percent complete: 21.0%; Average loss: 3.5079 Iteration: 840; Percent complete: 21.0%; Average loss: 3.7091 Iteration: 841; Percent complete: 21.0%; Average loss: 3.5388 Iteration: 842; Percent complete: 21.1%; Average loss: 3.7554 Iteration: 843; Percent complete: 21.1%; Average loss: 3.5814 Iteration: 844; Percent complete: 21.1%; Average loss: 3.4944 Iteration: 845; Percent complete: 21.1%; Average loss: 3.6068 Iteration: 846; Percent complete: 21.1%; Average loss: 3.4408 Iteration: 847; Percent complete: 21.2%; Average loss: 3.4530 Iteration: 848; Percent complete: 21.2%; Average loss: 3.7785 Iteration: 849; Percent complete: 21.2%; Average loss: 3.5396 Iteration: 850; Percent complete: 21.2%; Average loss: 3.4701 Iteration: 851; Percent complete: 21.3%; Average loss: 3.5384 Iteration: 852; Percent complete: 21.3%; Average loss: 3.3757 Iteration: 853; Percent complete: 21.3%; Average loss: 3.6732 Iteration: 854; Percent complete: 21.3%; Average loss: 3.4488 Iteration: 855; Percent complete: 21.4%; Average loss: 3.3713 Iteration: 856; Percent complete: 21.4%; Average loss: 3.6021 Iteration: 857; Percent complete: 21.4%; Average loss: 3.8309 Iteration: 858; Percent complete: 21.4%; Average loss: 3.8395 Iteration: 859; Percent complete: 21.5%; Average loss: 3.7310 Iteration: 860; Percent complete: 21.5%; Average loss: 3.4144 Iteration: 861; Percent complete: 21.5%; Average loss: 3.3466 Iteration: 862; Percent complete: 21.6%; Average loss: 3.6603 Iteration: 863; Percent complete: 21.6%; Average loss: 3.5511 Iteration: 864; Percent complete: 21.6%; Average loss: 3.5495 Iteration: 865; Percent complete: 21.6%; Average loss: 3.6713 Iteration: 866; Percent complete: 21.6%; Average loss: 3.5399 Iteration: 867; Percent complete: 21.7%; Average loss: 3.7692 Iteration: 868; Percent complete: 21.7%; Average loss: 3.6146 Iteration: 869; Percent complete: 21.7%; Average loss: 3.6035 Iteration: 870; Percent complete: 21.8%; Average loss: 3.4795 Iteration: 871; Percent complete: 21.8%; Average loss: 3.6295 Iteration: 872; Percent complete: 21.8%; Average loss: 3.4730 Iteration: 873; Percent complete: 21.8%; Average loss: 3.3979 Iteration: 874; Percent complete: 21.9%; Average loss: 3.6712 Iteration: 875; Percent complete: 21.9%; Average loss: 3.6100 Iteration: 876; Percent complete: 21.9%; Average loss: 3.4999 Iteration: 877; Percent complete: 21.9%; Average loss: 3.7838 Iteration: 878; Percent complete: 21.9%; Average loss: 3.4534 Iteration: 879; Percent complete: 22.0%; Average loss: 3.6838 Iteration: 880; Percent complete: 22.0%; Average loss: 3.7096 Iteration: 881; Percent complete: 22.0%; Average loss: 3.7338 Iteration: 882; Percent complete: 22.1%; Average loss: 3.4327 Iteration: 883; Percent complete: 22.1%; Average loss: 3.4220 Iteration: 884; Percent complete: 22.1%; Average loss: 3.5996 Iteration: 885; Percent complete: 22.1%; Average loss: 3.6561 Iteration: 886; Percent complete: 22.1%; Average loss: 3.4134 Iteration: 887; Percent complete: 22.2%; Average loss: 3.3858 Iteration: 888; Percent complete: 22.2%; Average loss: 3.5435 Iteration: 889; Percent complete: 22.2%; Average loss: 3.6374 Iteration: 890; Percent complete: 22.2%; Average loss: 3.6111 Iteration: 891; Percent complete: 22.3%; Average loss: 3.6029 Iteration: 892; Percent complete: 22.3%; Average loss: 3.5496 Iteration: 893; Percent complete: 22.3%; Average loss: 3.6360 Iteration: 894; Percent complete: 22.4%; Average loss: 3.6071 Iteration: 895; Percent complete: 22.4%; Average loss: 3.6390 Iteration: 896; Percent complete: 22.4%; Average loss: 3.3238 Iteration: 897; Percent complete: 22.4%; Average loss: 3.7024 Iteration: 898; Percent complete: 22.4%; Average loss: 3.7487 Iteration: 899; Percent complete: 22.5%; Average loss: 3.4458 Iteration: 900; Percent complete: 22.5%; Average loss: 3.5874 Iteration: 901; Percent complete: 22.5%; Average loss: 3.6236 Iteration: 902; Percent complete: 22.6%; Average loss: 3.5138 Iteration: 903; Percent complete: 22.6%; Average loss: 3.3747 Iteration: 904; Percent complete: 22.6%; Average loss: 3.6796 Iteration: 905; Percent complete: 22.6%; Average loss: 3.5164 Iteration: 906; Percent complete: 22.7%; Average loss: 3.5000 Iteration: 907; Percent complete: 22.7%; Average loss: 3.5040 Iteration: 908; Percent complete: 22.7%; Average loss: 3.6096 Iteration: 909; Percent complete: 22.7%; Average loss: 3.3654 Iteration: 910; Percent complete: 22.8%; Average loss: 3.5613 Iteration: 911; Percent complete: 22.8%; Average loss: 3.3179 Iteration: 912; Percent complete: 22.8%; Average loss: 3.3383 Iteration: 913; Percent complete: 22.8%; Average loss: 3.4643 Iteration: 914; Percent complete: 22.9%; Average loss: 3.6939 Iteration: 915; Percent complete: 22.9%; Average loss: 3.5031 Iteration: 916; Percent complete: 22.9%; Average loss: 3.5192 Iteration: 917; Percent complete: 22.9%; Average loss: 3.6442 Iteration: 918; Percent complete: 22.9%; Average loss: 3.6219 Iteration: 919; Percent complete: 23.0%; Average loss: 3.2370 Iteration: 920; Percent complete: 23.0%; Average loss: 3.6036 Iteration: 921; Percent complete: 23.0%; Average loss: 3.5306 Iteration: 922; Percent complete: 23.1%; Average loss: 3.9722 Iteration: 923; Percent complete: 23.1%; Average loss: 3.6352 Iteration: 924; Percent complete: 23.1%; Average loss: 3.5550 Iteration: 925; Percent complete: 23.1%; Average loss: 3.3815 Iteration: 926; Percent complete: 23.2%; Average loss: 3.4231 Iteration: 927; Percent complete: 23.2%; Average loss: 3.5543 Iteration: 928; Percent complete: 23.2%; Average loss: 3.5012 Iteration: 929; Percent complete: 23.2%; Average loss: 3.3187 Iteration: 930; Percent complete: 23.2%; Average loss: 3.6258 Iteration: 931; Percent complete: 23.3%; Average loss: 3.5970 Iteration: 932; Percent complete: 23.3%; Average loss: 3.5319 Iteration: 933; Percent complete: 23.3%; Average loss: 3.5063 Iteration: 934; Percent complete: 23.4%; Average loss: 3.2207 Iteration: 935; Percent complete: 23.4%; Average loss: 3.6131 Iteration: 936; Percent complete: 23.4%; Average loss: 3.7260 Iteration: 937; Percent complete: 23.4%; Average loss: 3.4936 Iteration: 938; Percent complete: 23.4%; Average loss: 3.7430 Iteration: 939; Percent complete: 23.5%; Average loss: 3.3320 Iteration: 940; Percent complete: 23.5%; Average loss: 3.4399 Iteration: 941; Percent complete: 23.5%; Average loss: 3.2794 Iteration: 942; Percent complete: 23.5%; Average loss: 3.5269 Iteration: 943; Percent complete: 23.6%; Average loss: 3.5199 Iteration: 944; Percent complete: 23.6%; Average loss: 3.3009 Iteration: 945; Percent complete: 23.6%; Average loss: 3.2852 Iteration: 946; Percent complete: 23.6%; Average loss: 3.3087 Iteration: 947; Percent complete: 23.7%; Average loss: 3.3618 Iteration: 948; Percent complete: 23.7%; Average loss: 3.4958 Iteration: 949; Percent complete: 23.7%; Average loss: 3.5799 Iteration: 950; Percent complete: 23.8%; Average loss: 3.1719 Iteration: 951; Percent complete: 23.8%; Average loss: 3.6137 Iteration: 952; Percent complete: 23.8%; Average loss: 3.6082 Iteration: 953; Percent complete: 23.8%; Average loss: 3.5706 Iteration: 954; Percent complete: 23.8%; Average loss: 3.4365 Iteration: 955; Percent complete: 23.9%; Average loss: 3.5402 Iteration: 956; Percent complete: 23.9%; Average loss: 3.5633 Iteration: 957; Percent complete: 23.9%; Average loss: 3.5311 Iteration: 958; Percent complete: 23.9%; Average loss: 3.2248 Iteration: 959; Percent complete: 24.0%; Average loss: 3.5902 Iteration: 960; Percent complete: 24.0%; Average loss: 3.4654 Iteration: 961; Percent complete: 24.0%; Average loss: 3.3947 Iteration: 962; Percent complete: 24.1%; Average loss: 3.5248 Iteration: 963; Percent complete: 24.1%; Average loss: 3.3059 Iteration: 964; Percent complete: 24.1%; Average loss: 3.4158 Iteration: 965; Percent complete: 24.1%; Average loss: 3.3753 Iteration: 966; Percent complete: 24.1%; Average loss: 3.6563 Iteration: 967; Percent complete: 24.2%; Average loss: 3.5264 Iteration: 968; Percent complete: 24.2%; Average loss: 3.6272 Iteration: 969; Percent complete: 24.2%; Average loss: 3.5403 Iteration: 970; Percent complete: 24.2%; Average loss: 3.5453 Iteration: 971; Percent complete: 24.3%; Average loss: 3.5868 Iteration: 972; Percent complete: 24.3%; Average loss: 3.4309 Iteration: 973; Percent complete: 24.3%; Average loss: 3.4027 Iteration: 974; Percent complete: 24.3%; Average loss: 3.3117 Iteration: 975; Percent complete: 24.4%; Average loss: 3.3891 Iteration: 976; Percent complete: 24.4%; Average loss: 3.4421 Iteration: 977; Percent complete: 24.4%; Average loss: 3.6162 Iteration: 978; Percent complete: 24.4%; Average loss: 3.3439 Iteration: 979; Percent complete: 24.5%; Average loss: 3.4579 Iteration: 980; Percent complete: 24.5%; Average loss: 3.5925 Iteration: 981; Percent complete: 24.5%; Average loss: 3.6405 Iteration: 982; Percent complete: 24.6%; Average loss: 3.4974 Iteration: 983; Percent complete: 24.6%; Average loss: 3.5548 Iteration: 984; Percent complete: 24.6%; Average loss: 3.6539 Iteration: 985; Percent complete: 24.6%; Average loss: 3.5643 Iteration: 986; Percent complete: 24.6%; Average loss: 3.5989 Iteration: 987; Percent complete: 24.7%; Average loss: 3.5971 Iteration: 988; Percent complete: 24.7%; Average loss: 3.5695 Iteration: 989; Percent complete: 24.7%; Average loss: 3.7366 Iteration: 990; Percent complete: 24.8%; Average loss: 3.4818 Iteration: 991; Percent complete: 24.8%; Average loss: 3.6065 Iteration: 992; Percent complete: 24.8%; Average loss: 3.4019 Iteration: 993; Percent complete: 24.8%; Average loss: 3.6955 Iteration: 994; Percent complete: 24.9%; Average loss: 3.4423 Iteration: 995; Percent complete: 24.9%; Average loss: 3.6290 Iteration: 996; Percent complete: 24.9%; Average loss: 3.3760 Iteration: 997; Percent complete: 24.9%; Average loss: 3.2708 Iteration: 998; Percent complete: 24.9%; Average loss: 3.5385 Iteration: 999; Percent complete: 25.0%; Average loss: 3.5764 Iteration: 1000; Percent complete: 25.0%; Average loss: 3.5835 Iteration: 1001; Percent complete: 25.0%; Average loss: 3.6666 Iteration: 1002; Percent complete: 25.1%; Average loss: 3.5882 Iteration: 1003; Percent complete: 25.1%; Average loss: 3.4410 Iteration: 1004; Percent complete: 25.1%; Average loss: 3.2638 Iteration: 1005; Percent complete: 25.1%; Average loss: 3.3953 Iteration: 1006; Percent complete: 25.1%; Average loss: 3.7288 Iteration: 1007; Percent complete: 25.2%; Average loss: 3.3369 Iteration: 1008; Percent complete: 25.2%; Average loss: 3.4223 Iteration: 1009; Percent complete: 25.2%; Average loss: 3.6371 Iteration: 1010; Percent complete: 25.2%; Average loss: 3.5037 Iteration: 1011; Percent complete: 25.3%; Average loss: 3.5227 Iteration: 1012; Percent complete: 25.3%; Average loss: 3.5478 Iteration: 1013; Percent complete: 25.3%; Average loss: 3.6536 Iteration: 1014; Percent complete: 25.4%; Average loss: 3.4728 Iteration: 1015; Percent complete: 25.4%; Average loss: 3.3962 Iteration: 1016; Percent complete: 25.4%; Average loss: 3.3546 Iteration: 1017; Percent complete: 25.4%; Average loss: 3.4085 Iteration: 1018; Percent complete: 25.4%; Average loss: 3.6940 Iteration: 1019; Percent complete: 25.5%; Average loss: 3.2618 Iteration: 1020; Percent complete: 25.5%; Average loss: 3.6615 Iteration: 1021; Percent complete: 25.5%; Average loss: 3.6274 Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4881 Iteration: 1023; Percent complete: 25.6%; Average loss: 3.3866 Iteration: 1024; Percent complete: 25.6%; Average loss: 3.2793 Iteration: 1025; Percent complete: 25.6%; Average loss: 3.7651 Iteration: 1026; Percent complete: 25.7%; Average loss: 3.4508 Iteration: 1027; Percent complete: 25.7%; Average loss: 3.3056 Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5260 Iteration: 1029; Percent complete: 25.7%; Average loss: 3.4646 Iteration: 1030; Percent complete: 25.8%; Average loss: 3.3311 Iteration: 1031; Percent complete: 25.8%; Average loss: 3.4757 Iteration: 1032; Percent complete: 25.8%; Average loss: 3.6143 Iteration: 1033; Percent complete: 25.8%; Average loss: 3.4232 Iteration: 1034; Percent complete: 25.9%; Average loss: 3.3160 Iteration: 1035; Percent complete: 25.9%; Average loss: 3.5201 Iteration: 1036; Percent complete: 25.9%; Average loss: 3.6992 Iteration: 1037; Percent complete: 25.9%; Average loss: 3.5522 Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3460 Iteration: 1039; Percent complete: 26.0%; Average loss: 3.6363 Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3753 Iteration: 1041; Percent complete: 26.0%; Average loss: 3.4394 Iteration: 1042; Percent complete: 26.1%; Average loss: 3.6315 Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4188 Iteration: 1044; Percent complete: 26.1%; Average loss: 3.4129 Iteration: 1045; Percent complete: 26.1%; Average loss: 3.2494 Iteration: 1046; Percent complete: 26.2%; Average loss: 3.2902 Iteration: 1047; Percent complete: 26.2%; Average loss: 2.8987 Iteration: 1048; Percent complete: 26.2%; Average loss: 3.6109 Iteration: 1049; Percent complete: 26.2%; Average loss: 3.3000 Iteration: 1050; Percent complete: 26.2%; Average loss: 3.5121 Iteration: 1051; Percent complete: 26.3%; Average loss: 3.5420 Iteration: 1052; Percent complete: 26.3%; Average loss: 3.4491 Iteration: 1053; Percent complete: 26.3%; Average loss: 3.4365 Iteration: 1054; Percent complete: 26.4%; Average loss: 4.0352 Iteration: 1055; Percent complete: 26.4%; Average loss: 3.1702 Iteration: 1056; Percent complete: 26.4%; Average loss: 3.1852 Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4867 Iteration: 1058; Percent complete: 26.5%; Average loss: 3.6960 Iteration: 1059; Percent complete: 26.5%; Average loss: 3.4307 Iteration: 1060; Percent complete: 26.5%; Average loss: 3.2801 Iteration: 1061; Percent complete: 26.5%; Average loss: 3.6296 Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4889 Iteration: 1063; Percent complete: 26.6%; Average loss: 3.6195 Iteration: 1064; Percent complete: 26.6%; Average loss: 3.4649 Iteration: 1065; Percent complete: 26.6%; Average loss: 3.1247 Iteration: 1066; Percent complete: 26.7%; Average loss: 3.2391 Iteration: 1067; Percent complete: 26.7%; Average loss: 3.4222 Iteration: 1068; Percent complete: 26.7%; Average loss: 3.3815 Iteration: 1069; Percent complete: 26.7%; Average loss: 3.2545 Iteration: 1070; Percent complete: 26.8%; Average loss: 3.4986 Iteration: 1071; Percent complete: 26.8%; Average loss: 3.2708 Iteration: 1072; Percent complete: 26.8%; Average loss: 3.2492 Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5521 Iteration: 1074; Percent complete: 26.9%; Average loss: 3.2817 Iteration: 1075; Percent complete: 26.9%; Average loss: 3.4295 Iteration: 1076; Percent complete: 26.9%; Average loss: 3.2755 Iteration: 1077; Percent complete: 26.9%; Average loss: 3.1587 Iteration: 1078; Percent complete: 27.0%; Average loss: 3.2524 Iteration: 1079; Percent complete: 27.0%; Average loss: 3.1584 Iteration: 1080; Percent complete: 27.0%; Average loss: 3.4187 Iteration: 1081; Percent complete: 27.0%; Average loss: 3.2773 Iteration: 1082; Percent complete: 27.1%; Average loss: 3.3619 Iteration: 1083; Percent complete: 27.1%; Average loss: 3.6978 Iteration: 1084; Percent complete: 27.1%; Average loss: 3.4924 Iteration: 1085; Percent complete: 27.1%; Average loss: 3.4806 Iteration: 1086; Percent complete: 27.2%; Average loss: 3.5957 Iteration: 1087; Percent complete: 27.2%; Average loss: 3.6100 Iteration: 1088; Percent complete: 27.2%; Average loss: 3.4899 Iteration: 1089; Percent complete: 27.2%; Average loss: 3.6349 Iteration: 1090; Percent complete: 27.3%; Average loss: 3.4053 Iteration: 1091; Percent complete: 27.3%; Average loss: 3.3123 Iteration: 1092; Percent complete: 27.3%; Average loss: 3.0027 Iteration: 1093; Percent complete: 27.3%; Average loss: 3.5749 Iteration: 1094; Percent complete: 27.4%; Average loss: 3.2798 Iteration: 1095; Percent complete: 27.4%; Average loss: 3.4246 Iteration: 1096; Percent complete: 27.4%; Average loss: 3.5275 Iteration: 1097; Percent complete: 27.4%; Average loss: 3.3511 Iteration: 1098; Percent complete: 27.5%; Average loss: 3.4239 Iteration: 1099; Percent complete: 27.5%; Average loss: 3.4000 Iteration: 1100; Percent complete: 27.5%; Average loss: 3.3448 Iteration: 1101; Percent complete: 27.5%; Average loss: 3.2467 Iteration: 1102; Percent complete: 27.6%; Average loss: 3.4202 Iteration: 1103; Percent complete: 27.6%; Average loss: 3.4471 Iteration: 1104; Percent complete: 27.6%; Average loss: 3.5045 Iteration: 1105; Percent complete: 27.6%; Average loss: 3.3867 Iteration: 1106; Percent complete: 27.7%; Average loss: 3.2456 Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5195 Iteration: 1108; Percent complete: 27.7%; Average loss: 3.5303 Iteration: 1109; Percent complete: 27.7%; Average loss: 3.5356 Iteration: 1110; Percent complete: 27.8%; Average loss: 3.6261 Iteration: 1111; Percent complete: 27.8%; Average loss: 3.1564 Iteration: 1112; Percent complete: 27.8%; Average loss: 3.6672 Iteration: 1113; Percent complete: 27.8%; Average loss: 3.7637 Iteration: 1114; Percent complete: 27.9%; Average loss: 3.4584 Iteration: 1115; Percent complete: 27.9%; Average loss: 3.5285 Iteration: 1116; Percent complete: 27.9%; Average loss: 3.3918 Iteration: 1117; Percent complete: 27.9%; Average loss: 3.2234 Iteration: 1118; Percent complete: 28.0%; Average loss: 3.4087 Iteration: 1119; Percent complete: 28.0%; Average loss: 3.4003 Iteration: 1120; Percent complete: 28.0%; Average loss: 3.2559 Iteration: 1121; Percent complete: 28.0%; Average loss: 3.1856 Iteration: 1122; Percent complete: 28.1%; Average loss: 3.4303 Iteration: 1123; Percent complete: 28.1%; Average loss: 3.4626 Iteration: 1124; Percent complete: 28.1%; Average loss: 3.3345 Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5955 Iteration: 1126; Percent complete: 28.1%; Average loss: 3.3973 Iteration: 1127; Percent complete: 28.2%; Average loss: 3.4468 Iteration: 1128; Percent complete: 28.2%; Average loss: 3.3445 Iteration: 1129; Percent complete: 28.2%; Average loss: 3.3969 Iteration: 1130; Percent complete: 28.2%; Average loss: 3.0674 Iteration: 1131; Percent complete: 28.3%; Average loss: 3.5602 Iteration: 1132; Percent complete: 28.3%; Average loss: 3.4168 Iteration: 1133; Percent complete: 28.3%; Average loss: 3.3123 Iteration: 1134; Percent complete: 28.3%; Average loss: 3.6757 Iteration: 1135; Percent complete: 28.4%; Average loss: 3.4056 Iteration: 1136; Percent complete: 28.4%; Average loss: 3.4888 Iteration: 1137; Percent complete: 28.4%; Average loss: 3.4755 Iteration: 1138; Percent complete: 28.4%; Average loss: 3.7109 Iteration: 1139; Percent complete: 28.5%; Average loss: 3.4640 Iteration: 1140; Percent complete: 28.5%; Average loss: 3.6623 Iteration: 1141; Percent complete: 28.5%; Average loss: 3.4532 Iteration: 1142; Percent complete: 28.5%; Average loss: 3.3533 Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2811 Iteration: 1144; Percent complete: 28.6%; Average loss: 3.4230 Iteration: 1145; Percent complete: 28.6%; Average loss: 3.6672 Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3249 Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5826 Iteration: 1148; Percent complete: 28.7%; Average loss: 3.2909 Iteration: 1149; Percent complete: 28.7%; Average loss: 3.2574 Iteration: 1150; Percent complete: 28.7%; Average loss: 3.4375 Iteration: 1151; Percent complete: 28.8%; Average loss: 2.9891 Iteration: 1152; Percent complete: 28.8%; Average loss: 3.6208 Iteration: 1153; Percent complete: 28.8%; Average loss: 3.3724 Iteration: 1154; Percent complete: 28.8%; Average loss: 3.2111 Iteration: 1155; Percent complete: 28.9%; Average loss: 3.4181 Iteration: 1156; Percent complete: 28.9%; Average loss: 3.4257 Iteration: 1157; Percent complete: 28.9%; Average loss: 3.4218 Iteration: 1158; Percent complete: 28.9%; Average loss: 3.2635 Iteration: 1159; Percent complete: 29.0%; Average loss: 3.4570 Iteration: 1160; Percent complete: 29.0%; Average loss: 3.2414 Iteration: 1161; Percent complete: 29.0%; Average loss: 3.4035 Iteration: 1162; Percent complete: 29.0%; Average loss: 3.4728 Iteration: 1163; Percent complete: 29.1%; Average loss: 3.3862 Iteration: 1164; Percent complete: 29.1%; Average loss: 3.5708 Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4590 Iteration: 1166; Percent complete: 29.1%; Average loss: 3.3614 Iteration: 1167; Percent complete: 29.2%; Average loss: 3.3953 Iteration: 1168; Percent complete: 29.2%; Average loss: 3.4563 Iteration: 1169; Percent complete: 29.2%; Average loss: 3.1485 Iteration: 1170; Percent complete: 29.2%; Average loss: 3.1245 Iteration: 1171; Percent complete: 29.3%; Average loss: 3.3549 Iteration: 1172; Percent complete: 29.3%; Average loss: 3.3154 Iteration: 1173; Percent complete: 29.3%; Average loss: 3.5246 Iteration: 1174; Percent complete: 29.3%; Average loss: 3.3889 Iteration: 1175; Percent complete: 29.4%; Average loss: 3.5563 Iteration: 1176; Percent complete: 29.4%; Average loss: 3.2295 Iteration: 1177; Percent complete: 29.4%; Average loss: 3.4436 Iteration: 1178; Percent complete: 29.4%; Average loss: 3.4054 Iteration: 1179; Percent complete: 29.5%; Average loss: 3.3573 Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5811 Iteration: 1181; Percent complete: 29.5%; Average loss: 3.5508 Iteration: 1182; Percent complete: 29.5%; Average loss: 3.3261 Iteration: 1183; Percent complete: 29.6%; Average loss: 3.1573 Iteration: 1184; Percent complete: 29.6%; Average loss: 3.2140 Iteration: 1185; Percent complete: 29.6%; Average loss: 3.2972 Iteration: 1186; Percent complete: 29.6%; Average loss: 3.5942 Iteration: 1187; Percent complete: 29.7%; Average loss: 3.4566 Iteration: 1188; Percent complete: 29.7%; Average loss: 3.2896 Iteration: 1189; Percent complete: 29.7%; Average loss: 3.5416 Iteration: 1190; Percent complete: 29.8%; Average loss: 3.3749 Iteration: 1191; Percent complete: 29.8%; Average loss: 3.3892 Iteration: 1192; Percent complete: 29.8%; Average loss: 3.4891 Iteration: 1193; Percent complete: 29.8%; Average loss: 3.1602 Iteration: 1194; Percent complete: 29.8%; Average loss: 3.6090 Iteration: 1195; Percent complete: 29.9%; Average loss: 3.5056 Iteration: 1196; Percent complete: 29.9%; Average loss: 3.2549 Iteration: 1197; Percent complete: 29.9%; Average loss: 3.2926 Iteration: 1198; Percent complete: 29.9%; Average loss: 3.4504 Iteration: 1199; Percent complete: 30.0%; Average loss: 3.1391 Iteration: 1200; Percent complete: 30.0%; Average loss: 3.5963 Iteration: 1201; Percent complete: 30.0%; Average loss: 3.3830 Iteration: 1202; Percent complete: 30.0%; Average loss: 3.2398 Iteration: 1203; Percent complete: 30.1%; Average loss: 3.4291 Iteration: 1204; Percent complete: 30.1%; Average loss: 3.2550 Iteration: 1205; Percent complete: 30.1%; Average loss: 3.4283 Iteration: 1206; Percent complete: 30.1%; Average loss: 3.4440 Iteration: 1207; Percent complete: 30.2%; Average loss: 3.3955 Iteration: 1208; Percent complete: 30.2%; Average loss: 3.3856 Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3696 Iteration: 1210; Percent complete: 30.2%; Average loss: 3.5425 Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2798 Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4320 Iteration: 1213; Percent complete: 30.3%; Average loss: 3.4009 Iteration: 1214; Percent complete: 30.3%; Average loss: 3.5388 Iteration: 1215; Percent complete: 30.4%; Average loss: 3.4524 Iteration: 1216; Percent complete: 30.4%; Average loss: 3.2551 Iteration: 1217; Percent complete: 30.4%; Average loss: 3.4475 Iteration: 1218; Percent complete: 30.4%; Average loss: 3.5178 Iteration: 1219; Percent complete: 30.5%; Average loss: 3.4757 Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5385 Iteration: 1221; Percent complete: 30.5%; Average loss: 3.6644 Iteration: 1222; Percent complete: 30.6%; Average loss: 3.4217 Iteration: 1223; Percent complete: 30.6%; Average loss: 3.2523 Iteration: 1224; Percent complete: 30.6%; Average loss: 3.0176 Iteration: 1225; Percent complete: 30.6%; Average loss: 3.3162 Iteration: 1226; Percent complete: 30.6%; Average loss: 3.3668 Iteration: 1227; Percent complete: 30.7%; Average loss: 3.2712 Iteration: 1228; Percent complete: 30.7%; Average loss: 3.3755 Iteration: 1229; Percent complete: 30.7%; Average loss: 3.3903 Iteration: 1230; Percent complete: 30.8%; Average loss: 3.4267 Iteration: 1231; Percent complete: 30.8%; Average loss: 3.4983 Iteration: 1232; Percent complete: 30.8%; Average loss: 3.4982 Iteration: 1233; Percent complete: 30.8%; Average loss: 3.3616 Iteration: 1234; Percent complete: 30.9%; Average loss: 3.4407 Iteration: 1235; Percent complete: 30.9%; Average loss: 3.3135 Iteration: 1236; Percent complete: 30.9%; Average loss: 3.5517 Iteration: 1237; Percent complete: 30.9%; Average loss: 3.3171 Iteration: 1238; Percent complete: 30.9%; Average loss: 3.2207 Iteration: 1239; Percent complete: 31.0%; Average loss: 3.2958 Iteration: 1240; Percent complete: 31.0%; Average loss: 3.4932 Iteration: 1241; Percent complete: 31.0%; Average loss: 3.6039 Iteration: 1242; Percent complete: 31.1%; Average loss: 3.3654 Iteration: 1243; Percent complete: 31.1%; Average loss: 3.4679 Iteration: 1244; Percent complete: 31.1%; Average loss: 3.4936 Iteration: 1245; Percent complete: 31.1%; Average loss: 3.3719 Iteration: 1246; Percent complete: 31.1%; Average loss: 3.2561 Iteration: 1247; Percent complete: 31.2%; Average loss: 3.4829 Iteration: 1248; Percent complete: 31.2%; Average loss: 3.3445 Iteration: 1249; Percent complete: 31.2%; Average loss: 3.5326 Iteration: 1250; Percent complete: 31.2%; Average loss: 3.2863 Iteration: 1251; Percent complete: 31.3%; Average loss: 3.6407 Iteration: 1252; Percent complete: 31.3%; Average loss: 3.4096 Iteration: 1253; Percent complete: 31.3%; Average loss: 3.2648 Iteration: 1254; Percent complete: 31.4%; Average loss: 3.4306 Iteration: 1255; Percent complete: 31.4%; Average loss: 3.5320 Iteration: 1256; Percent complete: 31.4%; Average loss: 3.2160 Iteration: 1257; Percent complete: 31.4%; Average loss: 3.5834 Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2080 Iteration: 1259; Percent complete: 31.5%; Average loss: 3.3095 Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2808 Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3778 Iteration: 1262; Percent complete: 31.6%; Average loss: 3.3737 Iteration: 1263; Percent complete: 31.6%; Average loss: 3.2947 Iteration: 1264; Percent complete: 31.6%; Average loss: 3.1727 Iteration: 1265; Percent complete: 31.6%; Average loss: 3.2412 Iteration: 1266; Percent complete: 31.6%; Average loss: 3.2721 Iteration: 1267; Percent complete: 31.7%; Average loss: 3.3056 Iteration: 1268; Percent complete: 31.7%; Average loss: 3.2298 Iteration: 1269; Percent complete: 31.7%; Average loss: 3.7730 Iteration: 1270; Percent complete: 31.8%; Average loss: 3.5188 Iteration: 1271; Percent complete: 31.8%; Average loss: 3.4896 Iteration: 1272; Percent complete: 31.8%; Average loss: 3.4604 Iteration: 1273; Percent complete: 31.8%; Average loss: 3.1022 Iteration: 1274; Percent complete: 31.9%; Average loss: 3.3908 Iteration: 1275; Percent complete: 31.9%; Average loss: 3.3941 Iteration: 1276; Percent complete: 31.9%; Average loss: 3.5349 Iteration: 1277; Percent complete: 31.9%; Average loss: 3.2741 Iteration: 1278; Percent complete: 31.9%; Average loss: 3.2191 Iteration: 1279; Percent complete: 32.0%; Average loss: 3.1909 Iteration: 1280; Percent complete: 32.0%; Average loss: 3.3359 Iteration: 1281; Percent complete: 32.0%; Average loss: 3.7551 Iteration: 1282; Percent complete: 32.0%; Average loss: 3.1264 Iteration: 1283; Percent complete: 32.1%; Average loss: 3.5457 Iteration: 1284; Percent complete: 32.1%; Average loss: 3.4926 Iteration: 1285; Percent complete: 32.1%; Average loss: 3.6239 Iteration: 1286; Percent complete: 32.1%; Average loss: 3.3105 Iteration: 1287; Percent complete: 32.2%; Average loss: 3.3636 Iteration: 1288; Percent complete: 32.2%; Average loss: 3.4786 Iteration: 1289; Percent complete: 32.2%; Average loss: 3.3796 Iteration: 1290; Percent complete: 32.2%; Average loss: 3.4699 Iteration: 1291; Percent complete: 32.3%; Average loss: 3.4117 Iteration: 1292; Percent complete: 32.3%; Average loss: 3.4235 Iteration: 1293; Percent complete: 32.3%; Average loss: 3.4031 Iteration: 1294; Percent complete: 32.4%; Average loss: 3.0346 Iteration: 1295; Percent complete: 32.4%; Average loss: 3.5452 Iteration: 1296; Percent complete: 32.4%; Average loss: 3.3560 Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2351 Iteration: 1298; Percent complete: 32.5%; Average loss: 3.2569 Iteration: 1299; Percent complete: 32.5%; Average loss: 3.5985 Iteration: 1300; Percent complete: 32.5%; Average loss: 3.5154 Iteration: 1301; Percent complete: 32.5%; Average loss: 3.2262 Iteration: 1302; Percent complete: 32.6%; Average loss: 3.3006 Iteration: 1303; Percent complete: 32.6%; Average loss: 3.2717 Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4018 Iteration: 1305; Percent complete: 32.6%; Average loss: 3.5098 Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4878 Iteration: 1307; Percent complete: 32.7%; Average loss: 3.3512 Iteration: 1308; Percent complete: 32.7%; Average loss: 3.2762 Iteration: 1309; Percent complete: 32.7%; Average loss: 3.5118 Iteration: 1310; Percent complete: 32.8%; Average loss: 3.2859 Iteration: 1311; Percent complete: 32.8%; Average loss: 3.5237 Iteration: 1312; Percent complete: 32.8%; Average loss: 3.4307 Iteration: 1313; Percent complete: 32.8%; Average loss: 3.4123 Iteration: 1314; Percent complete: 32.9%; Average loss: 3.4541 Iteration: 1315; Percent complete: 32.9%; Average loss: 3.3647 Iteration: 1316; Percent complete: 32.9%; Average loss: 3.2860 Iteration: 1317; Percent complete: 32.9%; Average loss: 3.3292 Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3472 Iteration: 1319; Percent complete: 33.0%; Average loss: 3.1754 Iteration: 1320; Percent complete: 33.0%; Average loss: 3.3058 Iteration: 1321; Percent complete: 33.0%; Average loss: 3.4378 Iteration: 1322; Percent complete: 33.1%; Average loss: 3.4060 Iteration: 1323; Percent complete: 33.1%; Average loss: 3.4366 Iteration: 1324; Percent complete: 33.1%; Average loss: 3.6099 Iteration: 1325; Percent complete: 33.1%; Average loss: 3.2110 Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3512 Iteration: 1327; Percent complete: 33.2%; Average loss: 3.4975 Iteration: 1328; Percent complete: 33.2%; Average loss: 3.2937 Iteration: 1329; Percent complete: 33.2%; Average loss: 3.3910 Iteration: 1330; Percent complete: 33.2%; Average loss: 3.1525 Iteration: 1331; Percent complete: 33.3%; Average loss: 3.2483 Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3157 Iteration: 1333; Percent complete: 33.3%; Average loss: 3.5312 Iteration: 1334; Percent complete: 33.4%; Average loss: 3.2872 Iteration: 1335; Percent complete: 33.4%; Average loss: 3.3198 Iteration: 1336; Percent complete: 33.4%; Average loss: 3.3977 Iteration: 1337; Percent complete: 33.4%; Average loss: 3.4129 Iteration: 1338; Percent complete: 33.5%; Average loss: 3.4102 Iteration: 1339; Percent complete: 33.5%; Average loss: 3.1361 Iteration: 1340; Percent complete: 33.5%; Average loss: 3.2423 Iteration: 1341; Percent complete: 33.5%; Average loss: 3.2877 Iteration: 1342; Percent complete: 33.6%; Average loss: 3.1018 Iteration: 1343; Percent complete: 33.6%; Average loss: 3.2981 Iteration: 1344; Percent complete: 33.6%; Average loss: 3.4417 Iteration: 1345; Percent complete: 33.6%; Average loss: 3.5177 Iteration: 1346; Percent complete: 33.7%; Average loss: 3.4550 Iteration: 1347; Percent complete: 33.7%; Average loss: 3.4000 Iteration: 1348; Percent complete: 33.7%; Average loss: 3.2927 Iteration: 1349; Percent complete: 33.7%; Average loss: 3.3140 Iteration: 1350; Percent complete: 33.8%; Average loss: 3.3301 Iteration: 1351; Percent complete: 33.8%; Average loss: 3.1570 Iteration: 1352; Percent complete: 33.8%; Average loss: 3.1186 Iteration: 1353; Percent complete: 33.8%; Average loss: 3.5372 Iteration: 1354; Percent complete: 33.9%; Average loss: 3.0916 Iteration: 1355; Percent complete: 33.9%; Average loss: 3.4463 Iteration: 1356; Percent complete: 33.9%; Average loss: 3.2154 Iteration: 1357; Percent complete: 33.9%; Average loss: 3.3875 Iteration: 1358; Percent complete: 34.0%; Average loss: 3.1615 Iteration: 1359; Percent complete: 34.0%; Average loss: 3.1588 Iteration: 1360; Percent complete: 34.0%; Average loss: 3.4078 Iteration: 1361; Percent complete: 34.0%; Average loss: 3.6468 Iteration: 1362; Percent complete: 34.1%; Average loss: 3.3011 Iteration: 1363; Percent complete: 34.1%; Average loss: 3.0562 Iteration: 1364; Percent complete: 34.1%; Average loss: 3.6795 Iteration: 1365; Percent complete: 34.1%; Average loss: 3.5559 Iteration: 1366; Percent complete: 34.2%; Average loss: 3.4192 Iteration: 1367; Percent complete: 34.2%; Average loss: 3.1223 Iteration: 1368; Percent complete: 34.2%; Average loss: 3.1133 Iteration: 1369; Percent complete: 34.2%; Average loss: 3.6308 Iteration: 1370; Percent complete: 34.2%; Average loss: 3.2064 Iteration: 1371; Percent complete: 34.3%; Average loss: 3.2045 Iteration: 1372; Percent complete: 34.3%; Average loss: 3.4136 Iteration: 1373; Percent complete: 34.3%; Average loss: 3.3787 Iteration: 1374; Percent complete: 34.4%; Average loss: 3.2713 Iteration: 1375; Percent complete: 34.4%; Average loss: 3.4048 Iteration: 1376; Percent complete: 34.4%; Average loss: 3.5985 Iteration: 1377; Percent complete: 34.4%; Average loss: 3.6378 Iteration: 1378; Percent complete: 34.4%; Average loss: 3.0926 Iteration: 1379; Percent complete: 34.5%; Average loss: 3.3930 Iteration: 1380; Percent complete: 34.5%; Average loss: 3.4076 Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4102 Iteration: 1382; Percent complete: 34.5%; Average loss: 3.2385 Iteration: 1383; Percent complete: 34.6%; Average loss: 3.2605 Iteration: 1384; Percent complete: 34.6%; Average loss: 3.2787 Iteration: 1385; Percent complete: 34.6%; Average loss: 3.2901 Iteration: 1386; Percent complete: 34.6%; Average loss: 3.3305 Iteration: 1387; Percent complete: 34.7%; Average loss: 3.2499 Iteration: 1388; Percent complete: 34.7%; Average loss: 3.5784 Iteration: 1389; Percent complete: 34.7%; Average loss: 3.3248 Iteration: 1390; Percent complete: 34.8%; Average loss: 3.2589 Iteration: 1391; Percent complete: 34.8%; Average loss: 3.3062 Iteration: 1392; Percent complete: 34.8%; Average loss: 3.2074 Iteration: 1393; Percent complete: 34.8%; Average loss: 3.2260 Iteration: 1394; Percent complete: 34.8%; Average loss: 3.2535 Iteration: 1395; Percent complete: 34.9%; Average loss: 3.3730 Iteration: 1396; Percent complete: 34.9%; Average loss: 3.0788 Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3010 Iteration: 1398; Percent complete: 34.9%; Average loss: 3.3369 Iteration: 1399; Percent complete: 35.0%; Average loss: 3.0594 Iteration: 1400; Percent complete: 35.0%; Average loss: 3.4156 Iteration: 1401; Percent complete: 35.0%; Average loss: 3.3990 Iteration: 1402; Percent complete: 35.0%; Average loss: 3.2819 Iteration: 1403; Percent complete: 35.1%; Average loss: 3.6316 Iteration: 1404; Percent complete: 35.1%; Average loss: 3.3607 Iteration: 1405; Percent complete: 35.1%; Average loss: 3.3051 Iteration: 1406; Percent complete: 35.1%; Average loss: 3.3546 Iteration: 1407; Percent complete: 35.2%; Average loss: 3.5810 Iteration: 1408; Percent complete: 35.2%; Average loss: 3.4460 Iteration: 1409; Percent complete: 35.2%; Average loss: 3.1311 Iteration: 1410; Percent complete: 35.2%; Average loss: 3.1871 Iteration: 1411; Percent complete: 35.3%; Average loss: 3.0655 Iteration: 1412; Percent complete: 35.3%; Average loss: 3.5743 Iteration: 1413; Percent complete: 35.3%; Average loss: 3.3125 Iteration: 1414; Percent complete: 35.4%; Average loss: 3.1450 Iteration: 1415; Percent complete: 35.4%; Average loss: 3.4077 Iteration: 1416; Percent complete: 35.4%; Average loss: 3.5340 Iteration: 1417; Percent complete: 35.4%; Average loss: 3.3795 Iteration: 1418; Percent complete: 35.4%; Average loss: 3.2985 Iteration: 1419; Percent complete: 35.5%; Average loss: 3.6224 Iteration: 1420; Percent complete: 35.5%; Average loss: 3.3417 Iteration: 1421; Percent complete: 35.5%; Average loss: 3.4217 Iteration: 1422; Percent complete: 35.5%; Average loss: 3.5704 Iteration: 1423; Percent complete: 35.6%; Average loss: 3.4412 Iteration: 1424; Percent complete: 35.6%; Average loss: 3.3127 Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2959 Iteration: 1426; Percent complete: 35.6%; Average loss: 3.1869 Iteration: 1427; Percent complete: 35.7%; Average loss: 3.0883 Iteration: 1428; Percent complete: 35.7%; Average loss: 3.4662 Iteration: 1429; Percent complete: 35.7%; Average loss: 3.1438 Iteration: 1430; Percent complete: 35.8%; Average loss: 3.2759 Iteration: 1431; Percent complete: 35.8%; Average loss: 3.0650 Iteration: 1432; Percent complete: 35.8%; Average loss: 3.1093 Iteration: 1433; Percent complete: 35.8%; Average loss: 3.4859 Iteration: 1434; Percent complete: 35.9%; Average loss: 3.2363 Iteration: 1435; Percent complete: 35.9%; Average loss: 3.2696 Iteration: 1436; Percent complete: 35.9%; Average loss: 3.3600 Iteration: 1437; Percent complete: 35.9%; Average loss: 3.4008 Iteration: 1438; Percent complete: 35.9%; Average loss: 3.1511 Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2601 Iteration: 1440; Percent complete: 36.0%; Average loss: 3.4012 Iteration: 1441; Percent complete: 36.0%; Average loss: 3.1501 Iteration: 1442; Percent complete: 36.0%; Average loss: 3.5515 Iteration: 1443; Percent complete: 36.1%; Average loss: 3.0292 Iteration: 1444; Percent complete: 36.1%; Average loss: 3.2407 Iteration: 1445; Percent complete: 36.1%; Average loss: 3.3892 Iteration: 1446; Percent complete: 36.1%; Average loss: 3.0944 Iteration: 1447; Percent complete: 36.2%; Average loss: 3.4417 Iteration: 1448; Percent complete: 36.2%; Average loss: 3.4977 Iteration: 1449; Percent complete: 36.2%; Average loss: 3.2666 Iteration: 1450; Percent complete: 36.2%; Average loss: 3.0903 Iteration: 1451; Percent complete: 36.3%; Average loss: 3.3159 Iteration: 1452; Percent complete: 36.3%; Average loss: 3.3400 Iteration: 1453; Percent complete: 36.3%; Average loss: 3.4265 Iteration: 1454; Percent complete: 36.4%; Average loss: 3.2540 Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4771 Iteration: 1456; Percent complete: 36.4%; Average loss: 3.2859 Iteration: 1457; Percent complete: 36.4%; Average loss: 3.4423 Iteration: 1458; Percent complete: 36.4%; Average loss: 3.5000 Iteration: 1459; Percent complete: 36.5%; Average loss: 3.3131 Iteration: 1460; Percent complete: 36.5%; Average loss: 3.1602 Iteration: 1461; Percent complete: 36.5%; Average loss: 3.0817 Iteration: 1462; Percent complete: 36.5%; Average loss: 3.3780 Iteration: 1463; Percent complete: 36.6%; Average loss: 3.1346 Iteration: 1464; Percent complete: 36.6%; Average loss: 3.4133 Iteration: 1465; Percent complete: 36.6%; Average loss: 3.3356 Iteration: 1466; Percent complete: 36.6%; Average loss: 3.0293 Iteration: 1467; Percent complete: 36.7%; Average loss: 3.2114 Iteration: 1468; Percent complete: 36.7%; Average loss: 3.2568 Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2487 Iteration: 1470; Percent complete: 36.8%; Average loss: 3.3245 Iteration: 1471; Percent complete: 36.8%; Average loss: 3.3014 Iteration: 1472; Percent complete: 36.8%; Average loss: 3.3982 Iteration: 1473; Percent complete: 36.8%; Average loss: 3.2846 Iteration: 1474; Percent complete: 36.9%; Average loss: 3.2199 Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3856 Iteration: 1476; Percent complete: 36.9%; Average loss: 3.4653 Iteration: 1477; Percent complete: 36.9%; Average loss: 3.3666 Iteration: 1478; Percent complete: 37.0%; Average loss: 3.0966 Iteration: 1479; Percent complete: 37.0%; Average loss: 3.5325 Iteration: 1480; Percent complete: 37.0%; Average loss: 3.4514 Iteration: 1481; Percent complete: 37.0%; Average loss: 3.4329 Iteration: 1482; Percent complete: 37.0%; Average loss: 3.2273 Iteration: 1483; Percent complete: 37.1%; Average loss: 3.6875 Iteration: 1484; Percent complete: 37.1%; Average loss: 3.1712 Iteration: 1485; Percent complete: 37.1%; Average loss: 3.1798 Iteration: 1486; Percent complete: 37.1%; Average loss: 3.4597 Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1170 Iteration: 1488; Percent complete: 37.2%; Average loss: 3.3318 Iteration: 1489; Percent complete: 37.2%; Average loss: 3.4839 Iteration: 1490; Percent complete: 37.2%; Average loss: 3.4545 Iteration: 1491; Percent complete: 37.3%; Average loss: 3.2632 Iteration: 1492; Percent complete: 37.3%; Average loss: 3.4525 Iteration: 1493; Percent complete: 37.3%; Average loss: 3.4058 Iteration: 1494; Percent complete: 37.4%; Average loss: 3.3444 Iteration: 1495; Percent complete: 37.4%; Average loss: 3.3328 Iteration: 1496; Percent complete: 37.4%; Average loss: 3.1337 Iteration: 1497; Percent complete: 37.4%; Average loss: 3.2866 Iteration: 1498; Percent complete: 37.5%; Average loss: 3.0240 Iteration: 1499; Percent complete: 37.5%; Average loss: 3.2799 Iteration: 1500; Percent complete: 37.5%; Average loss: 3.2164 Iteration: 1501; Percent complete: 37.5%; Average loss: 3.3125 Iteration: 1502; Percent complete: 37.5%; Average loss: 3.2946 Iteration: 1503; Percent complete: 37.6%; Average loss: 3.2588 Iteration: 1504; Percent complete: 37.6%; Average loss: 3.1946 Iteration: 1505; Percent complete: 37.6%; Average loss: 3.5868 Iteration: 1506; Percent complete: 37.6%; Average loss: 3.1659 Iteration: 1507; Percent complete: 37.7%; Average loss: 3.2829 Iteration: 1508; Percent complete: 37.7%; Average loss: 3.4228 Iteration: 1509; Percent complete: 37.7%; Average loss: 3.1732 Iteration: 1510; Percent complete: 37.8%; Average loss: 2.9865 Iteration: 1511; Percent complete: 37.8%; Average loss: 3.7110 Iteration: 1512; Percent complete: 37.8%; Average loss: 3.3741 Iteration: 1513; Percent complete: 37.8%; Average loss: 3.2747 Iteration: 1514; Percent complete: 37.9%; Average loss: 3.1599 Iteration: 1515; Percent complete: 37.9%; Average loss: 3.4713 Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2873 Iteration: 1517; Percent complete: 37.9%; Average loss: 3.2530 Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3592 Iteration: 1519; Percent complete: 38.0%; Average loss: 3.4801 Iteration: 1520; Percent complete: 38.0%; Average loss: 3.3616 Iteration: 1521; Percent complete: 38.0%; Average loss: 3.3834 Iteration: 1522; Percent complete: 38.0%; Average loss: 3.1273 Iteration: 1523; Percent complete: 38.1%; Average loss: 3.5313 Iteration: 1524; Percent complete: 38.1%; Average loss: 3.1972 Iteration: 1525; Percent complete: 38.1%; Average loss: 3.1213 Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1717 Iteration: 1527; Percent complete: 38.2%; Average loss: 3.2082 Iteration: 1528; Percent complete: 38.2%; Average loss: 3.3992 Iteration: 1529; Percent complete: 38.2%; Average loss: 3.3326 Iteration: 1530; Percent complete: 38.2%; Average loss: 3.0717 Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3488 Iteration: 1532; Percent complete: 38.3%; Average loss: 3.1780 Iteration: 1533; Percent complete: 38.3%; Average loss: 3.3625 Iteration: 1534; Percent complete: 38.4%; Average loss: 3.3810 Iteration: 1535; Percent complete: 38.4%; Average loss: 3.4223 Iteration: 1536; Percent complete: 38.4%; Average loss: 3.4768 Iteration: 1537; Percent complete: 38.4%; Average loss: 3.1695 Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2979 Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2310 Iteration: 1540; Percent complete: 38.5%; Average loss: 3.3374 Iteration: 1541; Percent complete: 38.5%; Average loss: 3.2722 Iteration: 1542; Percent complete: 38.6%; Average loss: 3.4297 Iteration: 1543; Percent complete: 38.6%; Average loss: 3.1907 Iteration: 1544; Percent complete: 38.6%; Average loss: 3.4096 Iteration: 1545; Percent complete: 38.6%; Average loss: 3.1113 Iteration: 1546; Percent complete: 38.6%; Average loss: 3.4330 Iteration: 1547; Percent complete: 38.7%; Average loss: 3.2288 Iteration: 1548; Percent complete: 38.7%; Average loss: 3.3236 Iteration: 1549; Percent complete: 38.7%; Average loss: 3.4752 Iteration: 1550; Percent complete: 38.8%; Average loss: 3.3948 Iteration: 1551; Percent complete: 38.8%; Average loss: 3.5097 Iteration: 1552; Percent complete: 38.8%; Average loss: 3.6375 Iteration: 1553; Percent complete: 38.8%; Average loss: 3.3665 Iteration: 1554; Percent complete: 38.9%; Average loss: 3.3911 Iteration: 1555; Percent complete: 38.9%; Average loss: 3.3434 Iteration: 1556; Percent complete: 38.9%; Average loss: 3.4880 Iteration: 1557; Percent complete: 38.9%; Average loss: 3.2318 Iteration: 1558; Percent complete: 39.0%; Average loss: 3.3511 Iteration: 1559; Percent complete: 39.0%; Average loss: 3.3005 Iteration: 1560; Percent complete: 39.0%; Average loss: 3.1619 Iteration: 1561; Percent complete: 39.0%; Average loss: 3.2699 Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4281 Iteration: 1563; Percent complete: 39.1%; Average loss: 3.2585 Iteration: 1564; Percent complete: 39.1%; Average loss: 3.2603 Iteration: 1565; Percent complete: 39.1%; Average loss: 3.4680 Iteration: 1566; Percent complete: 39.1%; Average loss: 3.1595 Iteration: 1567; Percent complete: 39.2%; Average loss: 3.4048 Iteration: 1568; Percent complete: 39.2%; Average loss: 3.4642 Iteration: 1569; Percent complete: 39.2%; Average loss: 3.1179 Iteration: 1570; Percent complete: 39.2%; Average loss: 3.4441 Iteration: 1571; Percent complete: 39.3%; Average loss: 3.3405 Iteration: 1572; Percent complete: 39.3%; Average loss: 3.3930 Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4923 Iteration: 1574; Percent complete: 39.4%; Average loss: 3.2645 Iteration: 1575; Percent complete: 39.4%; Average loss: 3.3127 Iteration: 1576; Percent complete: 39.4%; Average loss: 3.0435 Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3554 Iteration: 1578; Percent complete: 39.5%; Average loss: 3.1275 Iteration: 1579; Percent complete: 39.5%; Average loss: 3.0764 Iteration: 1580; Percent complete: 39.5%; Average loss: 3.1599 Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4129 Iteration: 1582; Percent complete: 39.6%; Average loss: 3.3901 Iteration: 1583; Percent complete: 39.6%; Average loss: 3.1290 Iteration: 1584; Percent complete: 39.6%; Average loss: 3.0889 Iteration: 1585; Percent complete: 39.6%; Average loss: 3.7038 Iteration: 1586; Percent complete: 39.6%; Average loss: 2.9876 Iteration: 1587; Percent complete: 39.7%; Average loss: 3.1982 Iteration: 1588; Percent complete: 39.7%; Average loss: 3.4515 Iteration: 1589; Percent complete: 39.7%; Average loss: 3.3631 Iteration: 1590; Percent complete: 39.8%; Average loss: 2.9818 Iteration: 1591; Percent complete: 39.8%; Average loss: 3.2659 Iteration: 1592; Percent complete: 39.8%; Average loss: 3.2922 Iteration: 1593; Percent complete: 39.8%; Average loss: 3.3134 Iteration: 1594; Percent complete: 39.9%; Average loss: 3.1412 Iteration: 1595; Percent complete: 39.9%; Average loss: 3.2426 Iteration: 1596; Percent complete: 39.9%; Average loss: 3.2817 Iteration: 1597; Percent complete: 39.9%; Average loss: 3.2492 Iteration: 1598; Percent complete: 40.0%; Average loss: 3.2902 Iteration: 1599; Percent complete: 40.0%; Average loss: 3.2677 Iteration: 1600; Percent complete: 40.0%; Average loss: 3.2467 Iteration: 1601; Percent complete: 40.0%; Average loss: 3.3063 Iteration: 1602; Percent complete: 40.1%; Average loss: 3.1434 Iteration: 1603; Percent complete: 40.1%; Average loss: 3.0706 Iteration: 1604; Percent complete: 40.1%; Average loss: 3.1218 Iteration: 1605; Percent complete: 40.1%; Average loss: 3.1164 Iteration: 1606; Percent complete: 40.2%; Average loss: 3.1038 Iteration: 1607; Percent complete: 40.2%; Average loss: 3.0928 Iteration: 1608; Percent complete: 40.2%; Average loss: 3.1569 Iteration: 1609; Percent complete: 40.2%; Average loss: 3.1271 Iteration: 1610; Percent complete: 40.2%; Average loss: 3.4363 Iteration: 1611; Percent complete: 40.3%; Average loss: 3.2735 Iteration: 1612; Percent complete: 40.3%; Average loss: 3.2958 Iteration: 1613; Percent complete: 40.3%; Average loss: 3.2095 Iteration: 1614; Percent complete: 40.4%; Average loss: 3.1391 Iteration: 1615; Percent complete: 40.4%; Average loss: 3.1524 Iteration: 1616; Percent complete: 40.4%; Average loss: 3.5148 Iteration: 1617; Percent complete: 40.4%; Average loss: 3.1639 Iteration: 1618; Percent complete: 40.5%; Average loss: 3.2185 Iteration: 1619; Percent complete: 40.5%; Average loss: 3.4011 Iteration: 1620; Percent complete: 40.5%; Average loss: 3.5213 Iteration: 1621; Percent complete: 40.5%; Average loss: 3.4618 Iteration: 1622; Percent complete: 40.6%; Average loss: 3.4498 Iteration: 1623; Percent complete: 40.6%; Average loss: 2.9532 Iteration: 1624; Percent complete: 40.6%; Average loss: 3.3195 Iteration: 1625; Percent complete: 40.6%; Average loss: 3.4536 Iteration: 1626; Percent complete: 40.6%; Average loss: 3.2901 Iteration: 1627; Percent complete: 40.7%; Average loss: 3.5448 Iteration: 1628; Percent complete: 40.7%; Average loss: 3.2011 Iteration: 1629; Percent complete: 40.7%; Average loss: 3.4767 Iteration: 1630; Percent complete: 40.8%; Average loss: 3.4475 Iteration: 1631; Percent complete: 40.8%; Average loss: 3.0126 Iteration: 1632; Percent complete: 40.8%; Average loss: 3.3468 Iteration: 1633; Percent complete: 40.8%; Average loss: 3.4301 Iteration: 1634; Percent complete: 40.8%; Average loss: 3.2859 Iteration: 1635; Percent complete: 40.9%; Average loss: 3.1498 Iteration: 1636; Percent complete: 40.9%; Average loss: 3.3057 Iteration: 1637; Percent complete: 40.9%; Average loss: 3.5481 Iteration: 1638; Percent complete: 40.9%; Average loss: 3.2757 Iteration: 1639; Percent complete: 41.0%; Average loss: 3.2519 Iteration: 1640; Percent complete: 41.0%; Average loss: 3.4574 Iteration: 1641; Percent complete: 41.0%; Average loss: 3.0593 Iteration: 1642; Percent complete: 41.0%; Average loss: 3.1927 Iteration: 1643; Percent complete: 41.1%; Average loss: 3.1699 Iteration: 1644; Percent complete: 41.1%; Average loss: 3.2359 Iteration: 1645; Percent complete: 41.1%; Average loss: 3.2573 Iteration: 1646; Percent complete: 41.1%; Average loss: 3.3166 Iteration: 1647; Percent complete: 41.2%; Average loss: 3.4670 Iteration: 1648; Percent complete: 41.2%; Average loss: 3.2071 Iteration: 1649; Percent complete: 41.2%; Average loss: 3.3730 Iteration: 1650; Percent complete: 41.2%; Average loss: 3.1438 Iteration: 1651; Percent complete: 41.3%; Average loss: 3.1626 Iteration: 1652; Percent complete: 41.3%; Average loss: 3.4830 Iteration: 1653; Percent complete: 41.3%; Average loss: 3.2268 Iteration: 1654; Percent complete: 41.3%; Average loss: 3.3280 Iteration: 1655; Percent complete: 41.4%; Average loss: 3.3214 Iteration: 1656; Percent complete: 41.4%; Average loss: 3.3766 Iteration: 1657; Percent complete: 41.4%; Average loss: 3.2628 Iteration: 1658; Percent complete: 41.4%; Average loss: 3.3462 Iteration: 1659; Percent complete: 41.5%; Average loss: 3.2713 Iteration: 1660; Percent complete: 41.5%; Average loss: 3.1532 Iteration: 1661; Percent complete: 41.5%; Average loss: 3.2225 Iteration: 1662; Percent complete: 41.5%; Average loss: 3.4193 Iteration: 1663; Percent complete: 41.6%; Average loss: 3.2750 Iteration: 1664; Percent complete: 41.6%; Average loss: 3.1409 Iteration: 1665; Percent complete: 41.6%; Average loss: 3.1378 Iteration: 1666; Percent complete: 41.6%; Average loss: 3.2707 Iteration: 1667; Percent complete: 41.7%; Average loss: 3.5263 Iteration: 1668; Percent complete: 41.7%; Average loss: 3.3075 Iteration: 1669; Percent complete: 41.7%; Average loss: 3.1993 Iteration: 1670; Percent complete: 41.8%; Average loss: 3.2695 Iteration: 1671; Percent complete: 41.8%; Average loss: 3.0414 Iteration: 1672; Percent complete: 41.8%; Average loss: 3.2188 Iteration: 1673; Percent complete: 41.8%; Average loss: 3.1233 Iteration: 1674; Percent complete: 41.9%; Average loss: 3.4019 Iteration: 1675; Percent complete: 41.9%; Average loss: 3.1182 Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1970 Iteration: 1677; Percent complete: 41.9%; Average loss: 3.1060 Iteration: 1678; Percent complete: 41.9%; Average loss: 3.2728 Iteration: 1679; Percent complete: 42.0%; Average loss: 3.4736 Iteration: 1680; Percent complete: 42.0%; Average loss: 3.2797 Iteration: 1681; Percent complete: 42.0%; Average loss: 3.1724 Iteration: 1682; Percent complete: 42.0%; Average loss: 3.1412 Iteration: 1683; Percent complete: 42.1%; Average loss: 3.2605 Iteration: 1684; Percent complete: 42.1%; Average loss: 3.2501 Iteration: 1685; Percent complete: 42.1%; Average loss: 3.4020 Iteration: 1686; Percent complete: 42.1%; Average loss: 3.3215 Iteration: 1687; Percent complete: 42.2%; Average loss: 3.4016 Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2471 Iteration: 1689; Percent complete: 42.2%; Average loss: 3.3043 Iteration: 1690; Percent complete: 42.2%; Average loss: 3.4693 Iteration: 1691; Percent complete: 42.3%; Average loss: 3.1482 Iteration: 1692; Percent complete: 42.3%; Average loss: 3.4002 Iteration: 1693; Percent complete: 42.3%; Average loss: 3.5696 Iteration: 1694; Percent complete: 42.4%; Average loss: 3.0822 Iteration: 1695; Percent complete: 42.4%; Average loss: 3.2774 Iteration: 1696; Percent complete: 42.4%; Average loss: 3.1763 Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1843 Iteration: 1698; Percent complete: 42.4%; Average loss: 3.2078 Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2551 Iteration: 1700; Percent complete: 42.5%; Average loss: 3.4726 Iteration: 1701; Percent complete: 42.5%; Average loss: 3.0031 Iteration: 1702; Percent complete: 42.5%; Average loss: 3.2619 Iteration: 1703; Percent complete: 42.6%; Average loss: 3.1689 Iteration: 1704; Percent complete: 42.6%; Average loss: 3.3563 Iteration: 1705; Percent complete: 42.6%; Average loss: 3.2487 Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2766 Iteration: 1707; Percent complete: 42.7%; Average loss: 3.3415 Iteration: 1708; Percent complete: 42.7%; Average loss: 3.3225 Iteration: 1709; Percent complete: 42.7%; Average loss: 3.3163 Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2116 Iteration: 1711; Percent complete: 42.8%; Average loss: 3.4488 Iteration: 1712; Percent complete: 42.8%; Average loss: 3.2190 Iteration: 1713; Percent complete: 42.8%; Average loss: 3.1963 Iteration: 1714; Percent complete: 42.9%; Average loss: 3.0645 Iteration: 1715; Percent complete: 42.9%; Average loss: 3.2170 Iteration: 1716; Percent complete: 42.9%; Average loss: 3.0569 Iteration: 1717; Percent complete: 42.9%; Average loss: 3.3507 Iteration: 1718; Percent complete: 43.0%; Average loss: 3.1546 Iteration: 1719; Percent complete: 43.0%; Average loss: 3.0325 Iteration: 1720; Percent complete: 43.0%; Average loss: 3.0731 Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3000 Iteration: 1722; Percent complete: 43.0%; Average loss: 3.6698 Iteration: 1723; Percent complete: 43.1%; Average loss: 2.9787 Iteration: 1724; Percent complete: 43.1%; Average loss: 3.2896 Iteration: 1725; Percent complete: 43.1%; Average loss: 3.3481 Iteration: 1726; Percent complete: 43.1%; Average loss: 3.4355 Iteration: 1727; Percent complete: 43.2%; Average loss: 3.1458 Iteration: 1728; Percent complete: 43.2%; Average loss: 3.3915 Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0075 Iteration: 1730; Percent complete: 43.2%; Average loss: 3.3863 Iteration: 1731; Percent complete: 43.3%; Average loss: 3.1421 Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2612 Iteration: 1733; Percent complete: 43.3%; Average loss: 3.4238 Iteration: 1734; Percent complete: 43.4%; Average loss: 3.5788 Iteration: 1735; Percent complete: 43.4%; Average loss: 3.3611 Iteration: 1736; Percent complete: 43.4%; Average loss: 3.1344 Iteration: 1737; Percent complete: 43.4%; Average loss: 3.4480 Iteration: 1738; Percent complete: 43.5%; Average loss: 3.2246 Iteration: 1739; Percent complete: 43.5%; Average loss: 2.9605 Iteration: 1740; Percent complete: 43.5%; Average loss: 3.3506 Iteration: 1741; Percent complete: 43.5%; Average loss: 3.2539 Iteration: 1742; Percent complete: 43.5%; Average loss: 3.1053 Iteration: 1743; Percent complete: 43.6%; Average loss: 3.0102 Iteration: 1744; Percent complete: 43.6%; Average loss: 3.6755 Iteration: 1745; Percent complete: 43.6%; Average loss: 3.2601 Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3843 Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2221 Iteration: 1748; Percent complete: 43.7%; Average loss: 3.4334 Iteration: 1749; Percent complete: 43.7%; Average loss: 3.0772 Iteration: 1750; Percent complete: 43.8%; Average loss: 3.0923 Iteration: 1751; Percent complete: 43.8%; Average loss: 3.1830 Iteration: 1752; Percent complete: 43.8%; Average loss: 3.2208 Iteration: 1753; Percent complete: 43.8%; Average loss: 3.5371 Iteration: 1754; Percent complete: 43.9%; Average loss: 3.1931 Iteration: 1755; Percent complete: 43.9%; Average loss: 3.1259 Iteration: 1756; Percent complete: 43.9%; Average loss: 3.2537 Iteration: 1757; Percent complete: 43.9%; Average loss: 3.1955 Iteration: 1758; Percent complete: 44.0%; Average loss: 3.1781 Iteration: 1759; Percent complete: 44.0%; Average loss: 2.9669 Iteration: 1760; Percent complete: 44.0%; Average loss: 3.2454 Iteration: 1761; Percent complete: 44.0%; Average loss: 2.9332 Iteration: 1762; Percent complete: 44.0%; Average loss: 3.1077 Iteration: 1763; Percent complete: 44.1%; Average loss: 3.1092 Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2088 Iteration: 1765; Percent complete: 44.1%; Average loss: 3.2224 Iteration: 1766; Percent complete: 44.1%; Average loss: 3.2454 Iteration: 1767; Percent complete: 44.2%; Average loss: 3.4496 Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2256 Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1094 Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2458 Iteration: 1771; Percent complete: 44.3%; Average loss: 3.1819 Iteration: 1772; Percent complete: 44.3%; Average loss: 3.2736 Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2426 Iteration: 1774; Percent complete: 44.4%; Average loss: 3.0377 Iteration: 1775; Percent complete: 44.4%; Average loss: 3.2220 Iteration: 1776; Percent complete: 44.4%; Average loss: 3.1462 Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9775 Iteration: 1778; Percent complete: 44.5%; Average loss: 3.3032 Iteration: 1779; Percent complete: 44.5%; Average loss: 3.2855 Iteration: 1780; Percent complete: 44.5%; Average loss: 3.3948 Iteration: 1781; Percent complete: 44.5%; Average loss: 3.1779 Iteration: 1782; Percent complete: 44.5%; Average loss: 3.2676 Iteration: 1783; Percent complete: 44.6%; Average loss: 3.0810 Iteration: 1784; Percent complete: 44.6%; Average loss: 3.2845 Iteration: 1785; Percent complete: 44.6%; Average loss: 3.2761 Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2958 Iteration: 1787; Percent complete: 44.7%; Average loss: 3.2853 Iteration: 1788; Percent complete: 44.7%; Average loss: 3.1711 Iteration: 1789; Percent complete: 44.7%; Average loss: 3.3819 Iteration: 1790; Percent complete: 44.8%; Average loss: 3.2027 Iteration: 1791; Percent complete: 44.8%; Average loss: 3.0985 Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2615 Iteration: 1793; Percent complete: 44.8%; Average loss: 3.1524 Iteration: 1794; Percent complete: 44.9%; Average loss: 3.1559 Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1663 Iteration: 1796; Percent complete: 44.9%; Average loss: 3.0649 Iteration: 1797; Percent complete: 44.9%; Average loss: 3.2108 Iteration: 1798; Percent complete: 45.0%; Average loss: 3.0422 Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1826 Iteration: 1800; Percent complete: 45.0%; Average loss: 3.0963 Iteration: 1801; Percent complete: 45.0%; Average loss: 3.0560 Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1991 Iteration: 1803; Percent complete: 45.1%; Average loss: 2.9104 Iteration: 1804; Percent complete: 45.1%; Average loss: 3.2447 Iteration: 1805; Percent complete: 45.1%; Average loss: 3.1976 Iteration: 1806; Percent complete: 45.1%; Average loss: 3.2907 Iteration: 1807; Percent complete: 45.2%; Average loss: 3.1917 Iteration: 1808; Percent complete: 45.2%; Average loss: 3.1141 Iteration: 1809; Percent complete: 45.2%; Average loss: 3.1918 Iteration: 1810; Percent complete: 45.2%; Average loss: 3.2792 Iteration: 1811; Percent complete: 45.3%; Average loss: 3.2783 Iteration: 1812; Percent complete: 45.3%; Average loss: 3.1454 Iteration: 1813; Percent complete: 45.3%; Average loss: 3.1015 Iteration: 1814; Percent complete: 45.4%; Average loss: 3.1420 Iteration: 1815; Percent complete: 45.4%; Average loss: 2.7707 Iteration: 1816; Percent complete: 45.4%; Average loss: 3.1911 Iteration: 1817; Percent complete: 45.4%; Average loss: 3.1716 Iteration: 1818; Percent complete: 45.5%; Average loss: 2.9869 Iteration: 1819; Percent complete: 45.5%; Average loss: 3.2687 Iteration: 1820; Percent complete: 45.5%; Average loss: 3.2327 Iteration: 1821; Percent complete: 45.5%; Average loss: 3.3191 Iteration: 1822; Percent complete: 45.6%; Average loss: 3.2466 Iteration: 1823; Percent complete: 45.6%; Average loss: 3.4279 Iteration: 1824; Percent complete: 45.6%; Average loss: 3.4130 Iteration: 1825; Percent complete: 45.6%; Average loss: 3.2090 Iteration: 1826; Percent complete: 45.6%; Average loss: 3.2403 Iteration: 1827; Percent complete: 45.7%; Average loss: 3.1620 Iteration: 1828; Percent complete: 45.7%; Average loss: 3.6215 Iteration: 1829; Percent complete: 45.7%; Average loss: 3.1014 Iteration: 1830; Percent complete: 45.8%; Average loss: 3.0118 Iteration: 1831; Percent complete: 45.8%; Average loss: 2.9912 Iteration: 1832; Percent complete: 45.8%; Average loss: 3.1008 Iteration: 1833; Percent complete: 45.8%; Average loss: 3.1427 Iteration: 1834; Percent complete: 45.9%; Average loss: 3.0986 Iteration: 1835; Percent complete: 45.9%; Average loss: 2.9797 Iteration: 1836; Percent complete: 45.9%; Average loss: 3.1775 Iteration: 1837; Percent complete: 45.9%; Average loss: 3.2430 Iteration: 1838; Percent complete: 46.0%; Average loss: 3.1108 Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2073 Iteration: 1840; Percent complete: 46.0%; Average loss: 3.2350 Iteration: 1841; Percent complete: 46.0%; Average loss: 3.3845 Iteration: 1842; Percent complete: 46.1%; Average loss: 3.2045 Iteration: 1843; Percent complete: 46.1%; Average loss: 3.2833 Iteration: 1844; Percent complete: 46.1%; Average loss: 3.2261 Iteration: 1845; Percent complete: 46.1%; Average loss: 3.2602 Iteration: 1846; Percent complete: 46.2%; Average loss: 3.1464 Iteration: 1847; Percent complete: 46.2%; Average loss: 3.1704 Iteration: 1848; Percent complete: 46.2%; Average loss: 3.1765 Iteration: 1849; Percent complete: 46.2%; Average loss: 3.3754 Iteration: 1850; Percent complete: 46.2%; Average loss: 3.1926 Iteration: 1851; Percent complete: 46.3%; Average loss: 3.2741 Iteration: 1852; Percent complete: 46.3%; Average loss: 3.2449 Iteration: 1853; Percent complete: 46.3%; Average loss: 3.1001 Iteration: 1854; Percent complete: 46.4%; Average loss: 3.2708 Iteration: 1855; Percent complete: 46.4%; Average loss: 3.5114 Iteration: 1856; Percent complete: 46.4%; Average loss: 3.0112 Iteration: 1857; Percent complete: 46.4%; Average loss: 3.2055 Iteration: 1858; Percent complete: 46.5%; Average loss: 3.6327 Iteration: 1859; Percent complete: 46.5%; Average loss: 3.1245 Iteration: 1860; Percent complete: 46.5%; Average loss: 3.0938 Iteration: 1861; Percent complete: 46.5%; Average loss: 3.1358 Iteration: 1862; Percent complete: 46.6%; Average loss: 3.2882 Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2291 Iteration: 1864; Percent complete: 46.6%; Average loss: 3.1844 Iteration: 1865; Percent complete: 46.6%; Average loss: 3.3168 Iteration: 1866; Percent complete: 46.7%; Average loss: 3.3370 Iteration: 1867; Percent complete: 46.7%; Average loss: 3.0526 Iteration: 1868; Percent complete: 46.7%; Average loss: 3.2473 Iteration: 1869; Percent complete: 46.7%; Average loss: 3.0285 Iteration: 1870; Percent complete: 46.8%; Average loss: 3.0706 Iteration: 1871; Percent complete: 46.8%; Average loss: 3.0571 Iteration: 1872; Percent complete: 46.8%; Average loss: 3.2842 Iteration: 1873; Percent complete: 46.8%; Average loss: 2.9786 Iteration: 1874; Percent complete: 46.9%; Average loss: 3.3603 Iteration: 1875; Percent complete: 46.9%; Average loss: 3.0579 Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0983 Iteration: 1877; Percent complete: 46.9%; Average loss: 3.4031 Iteration: 1878; Percent complete: 46.9%; Average loss: 3.0677 Iteration: 1879; Percent complete: 47.0%; Average loss: 2.9285 Iteration: 1880; Percent complete: 47.0%; Average loss: 3.2671 Iteration: 1881; Percent complete: 47.0%; Average loss: 3.2148 Iteration: 1882; Percent complete: 47.0%; Average loss: 2.9378 Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2267 Iteration: 1884; Percent complete: 47.1%; Average loss: 3.3522 Iteration: 1885; Percent complete: 47.1%; Average loss: 3.2891 Iteration: 1886; Percent complete: 47.1%; Average loss: 2.9964 Iteration: 1887; Percent complete: 47.2%; Average loss: 3.0176 Iteration: 1888; Percent complete: 47.2%; Average loss: 3.1454 Iteration: 1889; Percent complete: 47.2%; Average loss: 3.1915 Iteration: 1890; Percent complete: 47.2%; Average loss: 3.2359 Iteration: 1891; Percent complete: 47.3%; Average loss: 3.0962 Iteration: 1892; Percent complete: 47.3%; Average loss: 3.3692 Iteration: 1893; Percent complete: 47.3%; Average loss: 3.3912 Iteration: 1894; Percent complete: 47.3%; Average loss: 3.0405 Iteration: 1895; Percent complete: 47.4%; Average loss: 3.1106 Iteration: 1896; Percent complete: 47.4%; Average loss: 3.0846 Iteration: 1897; Percent complete: 47.4%; Average loss: 3.4277 Iteration: 1898; Percent complete: 47.4%; Average loss: 2.9799 Iteration: 1899; Percent complete: 47.5%; Average loss: 3.1453 Iteration: 1900; Percent complete: 47.5%; Average loss: 3.1121 Iteration: 1901; Percent complete: 47.5%; Average loss: 3.1978 Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1718 Iteration: 1903; Percent complete: 47.6%; Average loss: 3.1191 Iteration: 1904; Percent complete: 47.6%; Average loss: 2.8197 Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1070 Iteration: 1906; Percent complete: 47.6%; Average loss: 3.1851 Iteration: 1907; Percent complete: 47.7%; Average loss: 3.3788 Iteration: 1908; Percent complete: 47.7%; Average loss: 2.8880 Iteration: 1909; Percent complete: 47.7%; Average loss: 2.9629 Iteration: 1910; Percent complete: 47.8%; Average loss: 3.3981 Iteration: 1911; Percent complete: 47.8%; Average loss: 3.0858 Iteration: 1912; Percent complete: 47.8%; Average loss: 3.4882 Iteration: 1913; Percent complete: 47.8%; Average loss: 3.0333 Iteration: 1914; Percent complete: 47.9%; Average loss: 3.2414 Iteration: 1915; Percent complete: 47.9%; Average loss: 2.8341 Iteration: 1916; Percent complete: 47.9%; Average loss: 3.3463 Iteration: 1917; Percent complete: 47.9%; Average loss: 3.0890 Iteration: 1918; Percent complete: 47.9%; Average loss: 2.9568 Iteration: 1919; Percent complete: 48.0%; Average loss: 3.1573 Iteration: 1920; Percent complete: 48.0%; Average loss: 3.2544 Iteration: 1921; Percent complete: 48.0%; Average loss: 3.2395 Iteration: 1922; Percent complete: 48.0%; Average loss: 3.1368 Iteration: 1923; Percent complete: 48.1%; Average loss: 3.2152 Iteration: 1924; Percent complete: 48.1%; Average loss: 3.1945 Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2614 Iteration: 1926; Percent complete: 48.1%; Average loss: 2.9677 Iteration: 1927; Percent complete: 48.2%; Average loss: 2.9873 Iteration: 1928; Percent complete: 48.2%; Average loss: 3.2643 Iteration: 1929; Percent complete: 48.2%; Average loss: 3.2558 Iteration: 1930; Percent complete: 48.2%; Average loss: 3.3733 Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1466 Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2971 Iteration: 1933; Percent complete: 48.3%; Average loss: 3.2172 Iteration: 1934; Percent complete: 48.4%; Average loss: 3.3892 Iteration: 1935; Percent complete: 48.4%; Average loss: 3.1875 Iteration: 1936; Percent complete: 48.4%; Average loss: 3.1404 Iteration: 1937; Percent complete: 48.4%; Average loss: 3.1504 Iteration: 1938; Percent complete: 48.4%; Average loss: 3.3288 Iteration: 1939; Percent complete: 48.5%; Average loss: 3.0991 Iteration: 1940; Percent complete: 48.5%; Average loss: 3.2125 Iteration: 1941; Percent complete: 48.5%; Average loss: 3.0157 Iteration: 1942; Percent complete: 48.5%; Average loss: 3.3990 Iteration: 1943; Percent complete: 48.6%; Average loss: 3.4287 Iteration: 1944; Percent complete: 48.6%; Average loss: 3.3431 Iteration: 1945; Percent complete: 48.6%; Average loss: 3.1859 Iteration: 1946; Percent complete: 48.6%; Average loss: 3.2922 Iteration: 1947; Percent complete: 48.7%; Average loss: 2.9843 Iteration: 1948; Percent complete: 48.7%; Average loss: 3.0806 Iteration: 1949; Percent complete: 48.7%; Average loss: 3.1620 Iteration: 1950; Percent complete: 48.8%; Average loss: 3.0785 Iteration: 1951; Percent complete: 48.8%; Average loss: 3.3131 Iteration: 1952; Percent complete: 48.8%; Average loss: 3.1016 Iteration: 1953; Percent complete: 48.8%; Average loss: 3.0700 Iteration: 1954; Percent complete: 48.9%; Average loss: 3.2372 Iteration: 1955; Percent complete: 48.9%; Average loss: 3.1492 Iteration: 1956; Percent complete: 48.9%; Average loss: 3.3137 Iteration: 1957; Percent complete: 48.9%; Average loss: 3.2153 Iteration: 1958; Percent complete: 48.9%; Average loss: 3.2165 Iteration: 1959; Percent complete: 49.0%; Average loss: 2.8605 Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2427 Iteration: 1961; Percent complete: 49.0%; Average loss: 3.0256 Iteration: 1962; Percent complete: 49.0%; Average loss: 3.2262 Iteration: 1963; Percent complete: 49.1%; Average loss: 3.0678 Iteration: 1964; Percent complete: 49.1%; Average loss: 3.1483 Iteration: 1965; Percent complete: 49.1%; Average loss: 3.4390 Iteration: 1966; Percent complete: 49.1%; Average loss: 3.1905 Iteration: 1967; Percent complete: 49.2%; Average loss: 3.3930 Iteration: 1968; Percent complete: 49.2%; Average loss: 2.9543 Iteration: 1969; Percent complete: 49.2%; Average loss: 2.9691 Iteration: 1970; Percent complete: 49.2%; Average loss: 3.1899 Iteration: 1971; Percent complete: 49.3%; Average loss: 3.3687 Iteration: 1972; Percent complete: 49.3%; Average loss: 3.0602 Iteration: 1973; Percent complete: 49.3%; Average loss: 3.2373 Iteration: 1974; Percent complete: 49.4%; Average loss: 3.0415 Iteration: 1975; Percent complete: 49.4%; Average loss: 3.0197 Iteration: 1976; Percent complete: 49.4%; Average loss: 3.0326 Iteration: 1977; Percent complete: 49.4%; Average loss: 3.1913 Iteration: 1978; Percent complete: 49.5%; Average loss: 2.9980 Iteration: 1979; Percent complete: 49.5%; Average loss: 2.8606 Iteration: 1980; Percent complete: 49.5%; Average loss: 3.2751 Iteration: 1981; Percent complete: 49.5%; Average loss: 3.1523 Iteration: 1982; Percent complete: 49.5%; Average loss: 3.0918 Iteration: 1983; Percent complete: 49.6%; Average loss: 2.9924 Iteration: 1984; Percent complete: 49.6%; Average loss: 3.3509 Iteration: 1985; Percent complete: 49.6%; Average loss: 3.2010 Iteration: 1986; Percent complete: 49.6%; Average loss: 3.0353 Iteration: 1987; Percent complete: 49.7%; Average loss: 3.2421 Iteration: 1988; Percent complete: 49.7%; Average loss: 3.1356 Iteration: 1989; Percent complete: 49.7%; Average loss: 3.4368 Iteration: 1990; Percent complete: 49.8%; Average loss: 3.2295 Iteration: 1991; Percent complete: 49.8%; Average loss: 3.3265 Iteration: 1992; Percent complete: 49.8%; Average loss: 3.3066 Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2507 Iteration: 1994; Percent complete: 49.9%; Average loss: 3.1183 Iteration: 1995; Percent complete: 49.9%; Average loss: 3.2044 Iteration: 1996; Percent complete: 49.9%; Average loss: 3.2723 Iteration: 1997; Percent complete: 49.9%; Average loss: 3.2805 Iteration: 1998; Percent complete: 50.0%; Average loss: 3.3458 Iteration: 1999; Percent complete: 50.0%; Average loss: 3.1202 Iteration: 2000; Percent complete: 50.0%; Average loss: 3.3546 Iteration: 2001; Percent complete: 50.0%; Average loss: 3.1956 Iteration: 2002; Percent complete: 50.0%; Average loss: 3.3476 Iteration: 2003; Percent complete: 50.1%; Average loss: 3.1374 Iteration: 2004; Percent complete: 50.1%; Average loss: 3.0831 Iteration: 2005; Percent complete: 50.1%; Average loss: 2.9801 Iteration: 2006; Percent complete: 50.1%; Average loss: 3.2901 Iteration: 2007; Percent complete: 50.2%; Average loss: 3.1725 Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0565 Iteration: 2009; Percent complete: 50.2%; Average loss: 3.1183 Iteration: 2010; Percent complete: 50.2%; Average loss: 2.9414 Iteration: 2011; Percent complete: 50.3%; Average loss: 3.2771 Iteration: 2012; Percent complete: 50.3%; Average loss: 3.2008 Iteration: 2013; Percent complete: 50.3%; Average loss: 2.9653 Iteration: 2014; Percent complete: 50.3%; Average loss: 3.1646 Iteration: 2015; Percent complete: 50.4%; Average loss: 2.9365 Iteration: 2016; Percent complete: 50.4%; Average loss: 3.1802 Iteration: 2017; Percent complete: 50.4%; Average loss: 3.2576 Iteration: 2018; Percent complete: 50.4%; Average loss: 3.3129 Iteration: 2019; Percent complete: 50.5%; Average loss: 3.2613 Iteration: 2020; Percent complete: 50.5%; Average loss: 3.2725 Iteration: 2021; Percent complete: 50.5%; Average loss: 2.9849 Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0380 Iteration: 2023; Percent complete: 50.6%; Average loss: 3.0434 Iteration: 2024; Percent complete: 50.6%; Average loss: 3.2042 Iteration: 2025; Percent complete: 50.6%; Average loss: 3.2327 Iteration: 2026; Percent complete: 50.6%; Average loss: 3.0675 Iteration: 2027; Percent complete: 50.7%; Average loss: 3.3589 Iteration: 2028; Percent complete: 50.7%; Average loss: 3.0197 Iteration: 2029; Percent complete: 50.7%; Average loss: 3.0023 Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1059 Iteration: 2031; Percent complete: 50.8%; Average loss: 2.8385 Iteration: 2032; Percent complete: 50.8%; Average loss: 2.9412 Iteration: 2033; Percent complete: 50.8%; Average loss: 3.1979 Iteration: 2034; Percent complete: 50.8%; Average loss: 2.8318 Iteration: 2035; Percent complete: 50.9%; Average loss: 3.1291 Iteration: 2036; Percent complete: 50.9%; Average loss: 3.2190 Iteration: 2037; Percent complete: 50.9%; Average loss: 3.3984 Iteration: 2038; Percent complete: 50.9%; Average loss: 3.1209 Iteration: 2039; Percent complete: 51.0%; Average loss: 2.9138 Iteration: 2040; Percent complete: 51.0%; Average loss: 3.1049 Iteration: 2041; Percent complete: 51.0%; Average loss: 3.2126 Iteration: 2042; Percent complete: 51.0%; Average loss: 3.0747 Iteration: 2043; Percent complete: 51.1%; Average loss: 3.2095 Iteration: 2044; Percent complete: 51.1%; Average loss: 2.9979 Iteration: 2045; Percent complete: 51.1%; Average loss: 3.1341 Iteration: 2046; Percent complete: 51.1%; Average loss: 3.1161 Iteration: 2047; Percent complete: 51.2%; Average loss: 3.3410 Iteration: 2048; Percent complete: 51.2%; Average loss: 3.1131 Iteration: 2049; Percent complete: 51.2%; Average loss: 3.0937 Iteration: 2050; Percent complete: 51.2%; Average loss: 3.1124 Iteration: 2051; Percent complete: 51.3%; Average loss: 3.0688 Iteration: 2052; Percent complete: 51.3%; Average loss: 2.9768 Iteration: 2053; Percent complete: 51.3%; Average loss: 3.0976 Iteration: 2054; Percent complete: 51.3%; Average loss: 3.0278 Iteration: 2055; Percent complete: 51.4%; Average loss: 3.3135 Iteration: 2056; Percent complete: 51.4%; Average loss: 3.2416 Iteration: 2057; Percent complete: 51.4%; Average loss: 2.9672 Iteration: 2058; Percent complete: 51.4%; Average loss: 3.0429 Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0737 Iteration: 2060; Percent complete: 51.5%; Average loss: 3.4118 Iteration: 2061; Percent complete: 51.5%; Average loss: 3.1033 Iteration: 2062; Percent complete: 51.5%; Average loss: 3.1453 Iteration: 2063; Percent complete: 51.6%; Average loss: 3.2424 Iteration: 2064; Percent complete: 51.6%; Average loss: 3.2154 Iteration: 2065; Percent complete: 51.6%; Average loss: 3.0748 Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1009 Iteration: 2067; Percent complete: 51.7%; Average loss: 2.8797 Iteration: 2068; Percent complete: 51.7%; Average loss: 3.0679 Iteration: 2069; Percent complete: 51.7%; Average loss: 2.9289 Iteration: 2070; Percent complete: 51.7%; Average loss: 3.2909 Iteration: 2071; Percent complete: 51.8%; Average loss: 3.0431 Iteration: 2072; Percent complete: 51.8%; Average loss: 3.2676 Iteration: 2073; Percent complete: 51.8%; Average loss: 3.0464 Iteration: 2074; Percent complete: 51.8%; Average loss: 3.0591 Iteration: 2075; Percent complete: 51.9%; Average loss: 3.3184 Iteration: 2076; Percent complete: 51.9%; Average loss: 3.2609 Iteration: 2077; Percent complete: 51.9%; Average loss: 3.1005 Iteration: 2078; Percent complete: 51.9%; Average loss: 3.4461 Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2502 Iteration: 2080; Percent complete: 52.0%; Average loss: 3.0639 Iteration: 2081; Percent complete: 52.0%; Average loss: 3.1261 Iteration: 2082; Percent complete: 52.0%; Average loss: 3.1780 Iteration: 2083; Percent complete: 52.1%; Average loss: 3.2539 Iteration: 2084; Percent complete: 52.1%; Average loss: 3.0667 Iteration: 2085; Percent complete: 52.1%; Average loss: 3.4732 Iteration: 2086; Percent complete: 52.1%; Average loss: 3.2303 Iteration: 2087; Percent complete: 52.2%; Average loss: 3.1046 Iteration: 2088; Percent complete: 52.2%; Average loss: 2.9764 Iteration: 2089; Percent complete: 52.2%; Average loss: 3.4462 Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0562 Iteration: 2091; Percent complete: 52.3%; Average loss: 2.9813 Iteration: 2092; Percent complete: 52.3%; Average loss: 3.2426 Iteration: 2093; Percent complete: 52.3%; Average loss: 3.2314 Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0830 Iteration: 2095; Percent complete: 52.4%; Average loss: 3.0841 Iteration: 2096; Percent complete: 52.4%; Average loss: 3.1752 Iteration: 2097; Percent complete: 52.4%; Average loss: 3.2856 Iteration: 2098; Percent complete: 52.4%; Average loss: 3.1171 Iteration: 2099; Percent complete: 52.5%; Average loss: 3.3201 Iteration: 2100; Percent complete: 52.5%; Average loss: 3.2419 Iteration: 2101; Percent complete: 52.5%; Average loss: 3.1215 Iteration: 2102; Percent complete: 52.5%; Average loss: 3.0835 Iteration: 2103; Percent complete: 52.6%; Average loss: 3.1122 Iteration: 2104; Percent complete: 52.6%; Average loss: 3.2119 Iteration: 2105; Percent complete: 52.6%; Average loss: 3.2200 Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0409 Iteration: 2107; Percent complete: 52.7%; Average loss: 3.0825 Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0013 Iteration: 2109; Percent complete: 52.7%; Average loss: 3.0907 Iteration: 2110; Percent complete: 52.8%; Average loss: 3.0096 Iteration: 2111; Percent complete: 52.8%; Average loss: 2.7031 Iteration: 2112; Percent complete: 52.8%; Average loss: 3.1733 Iteration: 2113; Percent complete: 52.8%; Average loss: 3.0530 Iteration: 2114; Percent complete: 52.8%; Average loss: 3.0050 Iteration: 2115; Percent complete: 52.9%; Average loss: 3.2045 Iteration: 2116; Percent complete: 52.9%; Average loss: 3.0263 Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0305 Iteration: 2118; Percent complete: 52.9%; Average loss: 3.0058 Iteration: 2119; Percent complete: 53.0%; Average loss: 3.2442 Iteration: 2120; Percent complete: 53.0%; Average loss: 2.9049 Iteration: 2121; Percent complete: 53.0%; Average loss: 3.1599 Iteration: 2122; Percent complete: 53.0%; Average loss: 3.2436 Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2942 Iteration: 2124; Percent complete: 53.1%; Average loss: 3.2995 Iteration: 2125; Percent complete: 53.1%; Average loss: 3.3042 Iteration: 2126; Percent complete: 53.1%; Average loss: 3.2598 Iteration: 2127; Percent complete: 53.2%; Average loss: 3.1431 Iteration: 2128; Percent complete: 53.2%; Average loss: 2.9628 Iteration: 2129; Percent complete: 53.2%; Average loss: 2.9348 Iteration: 2130; Percent complete: 53.2%; Average loss: 3.0923 Iteration: 2131; Percent complete: 53.3%; Average loss: 3.2826 Iteration: 2132; Percent complete: 53.3%; Average loss: 3.2938 Iteration: 2133; Percent complete: 53.3%; Average loss: 3.3821 Iteration: 2134; Percent complete: 53.3%; Average loss: 2.9238 Iteration: 2135; Percent complete: 53.4%; Average loss: 3.0247 Iteration: 2136; Percent complete: 53.4%; Average loss: 3.2990 Iteration: 2137; Percent complete: 53.4%; Average loss: 2.9808 Iteration: 2138; Percent complete: 53.4%; Average loss: 3.2112 Iteration: 2139; Percent complete: 53.5%; Average loss: 3.2955 Iteration: 2140; Percent complete: 53.5%; Average loss: 3.1112 Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0548 Iteration: 2142; Percent complete: 53.5%; Average loss: 3.1217 Iteration: 2143; Percent complete: 53.6%; Average loss: 3.2373 Iteration: 2144; Percent complete: 53.6%; Average loss: 3.0418 Iteration: 2145; Percent complete: 53.6%; Average loss: 3.1832 Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1107 Iteration: 2147; Percent complete: 53.7%; Average loss: 2.8719 Iteration: 2148; Percent complete: 53.7%; Average loss: 3.1498 Iteration: 2149; Percent complete: 53.7%; Average loss: 3.2259 Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0889 Iteration: 2151; Percent complete: 53.8%; Average loss: 3.0834 Iteration: 2152; Percent complete: 53.8%; Average loss: 3.1860 Iteration: 2153; Percent complete: 53.8%; Average loss: 3.2135 Iteration: 2154; Percent complete: 53.8%; Average loss: 2.9301 Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1562 Iteration: 2156; Percent complete: 53.9%; Average loss: 3.1032 Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2843 Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1409 Iteration: 2159; Percent complete: 54.0%; Average loss: 3.1804 Iteration: 2160; Percent complete: 54.0%; Average loss: 3.0017 Iteration: 2161; Percent complete: 54.0%; Average loss: 3.2979 Iteration: 2162; Percent complete: 54.0%; Average loss: 2.9884 Iteration: 2163; Percent complete: 54.1%; Average loss: 2.9322 Iteration: 2164; Percent complete: 54.1%; Average loss: 3.3154 Iteration: 2165; Percent complete: 54.1%; Average loss: 3.3131 Iteration: 2166; Percent complete: 54.1%; Average loss: 3.0377 Iteration: 2167; Percent complete: 54.2%; Average loss: 3.1361 Iteration: 2168; Percent complete: 54.2%; Average loss: 3.0827 Iteration: 2169; Percent complete: 54.2%; Average loss: 3.0727 Iteration: 2170; Percent complete: 54.2%; Average loss: 3.0143 Iteration: 2171; Percent complete: 54.3%; Average loss: 3.1060 Iteration: 2172; Percent complete: 54.3%; Average loss: 3.2135 Iteration: 2173; Percent complete: 54.3%; Average loss: 2.9126 Iteration: 2174; Percent complete: 54.4%; Average loss: 3.1115 Iteration: 2175; Percent complete: 54.4%; Average loss: 3.1472 Iteration: 2176; Percent complete: 54.4%; Average loss: 3.2552 Iteration: 2177; Percent complete: 54.4%; Average loss: 3.2198 Iteration: 2178; Percent complete: 54.4%; Average loss: 3.2614 Iteration: 2179; Percent complete: 54.5%; Average loss: 3.3390 Iteration: 2180; Percent complete: 54.5%; Average loss: 3.1390 Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9987 Iteration: 2182; Percent complete: 54.5%; Average loss: 2.9912 Iteration: 2183; Percent complete: 54.6%; Average loss: 3.0339 Iteration: 2184; Percent complete: 54.6%; Average loss: 3.1301 Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1500 Iteration: 2186; Percent complete: 54.6%; Average loss: 3.0220 Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1923 Iteration: 2188; Percent complete: 54.7%; Average loss: 2.8926 Iteration: 2189; Percent complete: 54.7%; Average loss: 3.4892 Iteration: 2190; Percent complete: 54.8%; Average loss: 3.0132 Iteration: 2191; Percent complete: 54.8%; Average loss: 3.1026 Iteration: 2192; Percent complete: 54.8%; Average loss: 3.1462 Iteration: 2193; Percent complete: 54.8%; Average loss: 2.8901 Iteration: 2194; Percent complete: 54.9%; Average loss: 2.8679 Iteration: 2195; Percent complete: 54.9%; Average loss: 3.0384 Iteration: 2196; Percent complete: 54.9%; Average loss: 3.2498 Iteration: 2197; Percent complete: 54.9%; Average loss: 3.4152 Iteration: 2198; Percent complete: 54.9%; Average loss: 3.2113 Iteration: 2199; Percent complete: 55.0%; Average loss: 2.8610 Iteration: 2200; Percent complete: 55.0%; Average loss: 3.0128 Iteration: 2201; Percent complete: 55.0%; Average loss: 2.9122 Iteration: 2202; Percent complete: 55.0%; Average loss: 3.2180 Iteration: 2203; Percent complete: 55.1%; Average loss: 2.9439 Iteration: 2204; Percent complete: 55.1%; Average loss: 3.3303 Iteration: 2205; Percent complete: 55.1%; Average loss: 3.0232 Iteration: 2206; Percent complete: 55.1%; Average loss: 3.1805 Iteration: 2207; Percent complete: 55.2%; Average loss: 3.0913 Iteration: 2208; Percent complete: 55.2%; Average loss: 2.9517 Iteration: 2209; Percent complete: 55.2%; Average loss: 3.2063 Iteration: 2210; Percent complete: 55.2%; Average loss: 3.1697 Iteration: 2211; Percent complete: 55.3%; Average loss: 3.1781 Iteration: 2212; Percent complete: 55.3%; Average loss: 3.2732 Iteration: 2213; Percent complete: 55.3%; Average loss: 2.8888 Iteration: 2214; Percent complete: 55.4%; Average loss: 2.8368 Iteration: 2215; Percent complete: 55.4%; Average loss: 2.8400 Iteration: 2216; Percent complete: 55.4%; Average loss: 3.0987 Iteration: 2217; Percent complete: 55.4%; Average loss: 3.1582 Iteration: 2218; Percent complete: 55.5%; Average loss: 2.7920 Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0000 Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9638 Iteration: 2221; Percent complete: 55.5%; Average loss: 3.0302 Iteration: 2222; Percent complete: 55.5%; Average loss: 3.0181 Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0998 Iteration: 2224; Percent complete: 55.6%; Average loss: 3.0554 Iteration: 2225; Percent complete: 55.6%; Average loss: 2.8464 Iteration: 2226; Percent complete: 55.6%; Average loss: 3.0448 Iteration: 2227; Percent complete: 55.7%; Average loss: 3.0630 Iteration: 2228; Percent complete: 55.7%; Average loss: 3.1950 Iteration: 2229; Percent complete: 55.7%; Average loss: 3.1169 Iteration: 2230; Percent complete: 55.8%; Average loss: 3.1213 Iteration: 2231; Percent complete: 55.8%; Average loss: 3.1055 Iteration: 2232; Percent complete: 55.8%; Average loss: 2.8968 Iteration: 2233; Percent complete: 55.8%; Average loss: 3.2895 Iteration: 2234; Percent complete: 55.9%; Average loss: 2.8392 Iteration: 2235; Percent complete: 55.9%; Average loss: 3.0391 Iteration: 2236; Percent complete: 55.9%; Average loss: 2.7693 Iteration: 2237; Percent complete: 55.9%; Average loss: 3.0824 Iteration: 2238; Percent complete: 56.0%; Average loss: 3.0933 Iteration: 2239; Percent complete: 56.0%; Average loss: 3.3870 Iteration: 2240; Percent complete: 56.0%; Average loss: 3.0724 Iteration: 2241; Percent complete: 56.0%; Average loss: 3.1873 Iteration: 2242; Percent complete: 56.0%; Average loss: 3.1191 Iteration: 2243; Percent complete: 56.1%; Average loss: 3.2350 Iteration: 2244; Percent complete: 56.1%; Average loss: 3.1224 Iteration: 2245; Percent complete: 56.1%; Average loss: 2.9236 Iteration: 2246; Percent complete: 56.1%; Average loss: 3.2879 Iteration: 2247; Percent complete: 56.2%; Average loss: 3.1348 Iteration: 2248; Percent complete: 56.2%; Average loss: 3.3120 Iteration: 2249; Percent complete: 56.2%; Average loss: 2.8640 Iteration: 2250; Percent complete: 56.2%; Average loss: 2.8977 Iteration: 2251; Percent complete: 56.3%; Average loss: 3.1312 Iteration: 2252; Percent complete: 56.3%; Average loss: 3.1883 Iteration: 2253; Percent complete: 56.3%; Average loss: 2.9763 Iteration: 2254; Percent complete: 56.4%; Average loss: 3.0954 Iteration: 2255; Percent complete: 56.4%; Average loss: 3.1808 Iteration: 2256; Percent complete: 56.4%; Average loss: 2.9695 Iteration: 2257; Percent complete: 56.4%; Average loss: 3.1071 Iteration: 2258; Percent complete: 56.5%; Average loss: 3.2404 Iteration: 2259; Percent complete: 56.5%; Average loss: 2.8706 Iteration: 2260; Percent complete: 56.5%; Average loss: 3.0331 Iteration: 2261; Percent complete: 56.5%; Average loss: 3.2041 Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0222 Iteration: 2263; Percent complete: 56.6%; Average loss: 3.1625 Iteration: 2264; Percent complete: 56.6%; Average loss: 3.0137 Iteration: 2265; Percent complete: 56.6%; Average loss: 3.0775 Iteration: 2266; Percent complete: 56.6%; Average loss: 3.0243 Iteration: 2267; Percent complete: 56.7%; Average loss: 3.0034 Iteration: 2268; Percent complete: 56.7%; Average loss: 3.2269 Iteration: 2269; Percent complete: 56.7%; Average loss: 3.2067 Iteration: 2270; Percent complete: 56.8%; Average loss: 2.7770 Iteration: 2271; Percent complete: 56.8%; Average loss: 2.9114 Iteration: 2272; Percent complete: 56.8%; Average loss: 3.1411 Iteration: 2273; Percent complete: 56.8%; Average loss: 2.7882 Iteration: 2274; Percent complete: 56.9%; Average loss: 3.2345 Iteration: 2275; Percent complete: 56.9%; Average loss: 2.9891 Iteration: 2276; Percent complete: 56.9%; Average loss: 3.1682 Iteration: 2277; Percent complete: 56.9%; Average loss: 3.0280 Iteration: 2278; Percent complete: 57.0%; Average loss: 3.1017 Iteration: 2279; Percent complete: 57.0%; Average loss: 3.0785 Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0376 Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1108 Iteration: 2282; Percent complete: 57.0%; Average loss: 2.9810 Iteration: 2283; Percent complete: 57.1%; Average loss: 3.1257 Iteration: 2284; Percent complete: 57.1%; Average loss: 3.0359 Iteration: 2285; Percent complete: 57.1%; Average loss: 3.0268 Iteration: 2286; Percent complete: 57.1%; Average loss: 3.0498 Iteration: 2287; Percent complete: 57.2%; Average loss: 3.0149 Iteration: 2288; Percent complete: 57.2%; Average loss: 3.0173 Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0555 Iteration: 2290; Percent complete: 57.2%; Average loss: 3.0348 Iteration: 2291; Percent complete: 57.3%; Average loss: 2.9604 Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0491 Iteration: 2293; Percent complete: 57.3%; Average loss: 3.1823 Iteration: 2294; Percent complete: 57.4%; Average loss: 2.9983 Iteration: 2295; Percent complete: 57.4%; Average loss: 2.9055 Iteration: 2296; Percent complete: 57.4%; Average loss: 3.0790 Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0457 Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0458 Iteration: 2299; Percent complete: 57.5%; Average loss: 2.9167 Iteration: 2300; Percent complete: 57.5%; Average loss: 3.0950 Iteration: 2301; Percent complete: 57.5%; Average loss: 2.9468 Iteration: 2302; Percent complete: 57.6%; Average loss: 3.2159 Iteration: 2303; Percent complete: 57.6%; Average loss: 3.1529 Iteration: 2304; Percent complete: 57.6%; Average loss: 3.0623 Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1036 Iteration: 2306; Percent complete: 57.6%; Average loss: 3.2615 Iteration: 2307; Percent complete: 57.7%; Average loss: 3.1033 Iteration: 2308; Percent complete: 57.7%; Average loss: 2.8780 Iteration: 2309; Percent complete: 57.7%; Average loss: 3.1406 Iteration: 2310; Percent complete: 57.8%; Average loss: 3.3305 Iteration: 2311; Percent complete: 57.8%; Average loss: 2.8320 Iteration: 2312; Percent complete: 57.8%; Average loss: 3.0199 Iteration: 2313; Percent complete: 57.8%; Average loss: 3.1209 Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2403 Iteration: 2315; Percent complete: 57.9%; Average loss: 3.1784 Iteration: 2316; Percent complete: 57.9%; Average loss: 3.0789 Iteration: 2317; Percent complete: 57.9%; Average loss: 3.1977 Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0165 Iteration: 2319; Percent complete: 58.0%; Average loss: 2.8454 Iteration: 2320; Percent complete: 58.0%; Average loss: 3.0900 Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0062 Iteration: 2322; Percent complete: 58.1%; Average loss: 3.1715 Iteration: 2323; Percent complete: 58.1%; Average loss: 3.0566 Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9597 Iteration: 2325; Percent complete: 58.1%; Average loss: 2.8965 Iteration: 2326; Percent complete: 58.1%; Average loss: 2.9583 Iteration: 2327; Percent complete: 58.2%; Average loss: 3.1536 Iteration: 2328; Percent complete: 58.2%; Average loss: 2.8782 Iteration: 2329; Percent complete: 58.2%; Average loss: 3.0814 Iteration: 2330; Percent complete: 58.2%; Average loss: 2.8340 Iteration: 2331; Percent complete: 58.3%; Average loss: 3.0194 Iteration: 2332; Percent complete: 58.3%; Average loss: 3.0427 Iteration: 2333; Percent complete: 58.3%; Average loss: 3.1660 Iteration: 2334; Percent complete: 58.4%; Average loss: 2.9728 Iteration: 2335; Percent complete: 58.4%; Average loss: 3.2884 Iteration: 2336; Percent complete: 58.4%; Average loss: 2.8075 Iteration: 2337; Percent complete: 58.4%; Average loss: 3.0595 Iteration: 2338; Percent complete: 58.5%; Average loss: 3.3721 Iteration: 2339; Percent complete: 58.5%; Average loss: 2.9347 Iteration: 2340; Percent complete: 58.5%; Average loss: 3.0833 Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9608 Iteration: 2342; Percent complete: 58.6%; Average loss: 3.1427 Iteration: 2343; Percent complete: 58.6%; Average loss: 2.9137 Iteration: 2344; Percent complete: 58.6%; Average loss: 3.2405 Iteration: 2345; Percent complete: 58.6%; Average loss: 3.1545 Iteration: 2346; Percent complete: 58.7%; Average loss: 3.0198 Iteration: 2347; Percent complete: 58.7%; Average loss: 3.2179 Iteration: 2348; Percent complete: 58.7%; Average loss: 2.9848 Iteration: 2349; Percent complete: 58.7%; Average loss: 3.0862 Iteration: 2350; Percent complete: 58.8%; Average loss: 3.1892 Iteration: 2351; Percent complete: 58.8%; Average loss: 3.0031 Iteration: 2352; Percent complete: 58.8%; Average loss: 2.9914 Iteration: 2353; Percent complete: 58.8%; Average loss: 3.0637 Iteration: 2354; Percent complete: 58.9%; Average loss: 3.1568 Iteration: 2355; Percent complete: 58.9%; Average loss: 3.1258 Iteration: 2356; Percent complete: 58.9%; Average loss: 3.2422 Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0489 Iteration: 2358; Percent complete: 59.0%; Average loss: 3.1715 Iteration: 2359; Percent complete: 59.0%; Average loss: 3.0309 Iteration: 2360; Percent complete: 59.0%; Average loss: 3.1805 Iteration: 2361; Percent complete: 59.0%; Average loss: 3.1988 Iteration: 2362; Percent complete: 59.1%; Average loss: 3.4646 Iteration: 2363; Percent complete: 59.1%; Average loss: 2.9572 Iteration: 2364; Percent complete: 59.1%; Average loss: 3.2112 Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0356 Iteration: 2366; Percent complete: 59.2%; Average loss: 3.0455 Iteration: 2367; Percent complete: 59.2%; Average loss: 2.9878 Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0219 Iteration: 2369; Percent complete: 59.2%; Average loss: 3.1053 Iteration: 2370; Percent complete: 59.2%; Average loss: 3.1336 Iteration: 2371; Percent complete: 59.3%; Average loss: 2.9981 Iteration: 2372; Percent complete: 59.3%; Average loss: 2.8602 Iteration: 2373; Percent complete: 59.3%; Average loss: 2.8818 Iteration: 2374; Percent complete: 59.4%; Average loss: 2.8658 Iteration: 2375; Percent complete: 59.4%; Average loss: 3.1041 Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0129 Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9055 Iteration: 2378; Percent complete: 59.5%; Average loss: 3.0099 Iteration: 2379; Percent complete: 59.5%; Average loss: 2.8627 Iteration: 2380; Percent complete: 59.5%; Average loss: 3.2245 Iteration: 2381; Percent complete: 59.5%; Average loss: 2.9531 Iteration: 2382; Percent complete: 59.6%; Average loss: 3.1522 Iteration: 2383; Percent complete: 59.6%; Average loss: 2.9352 Iteration: 2384; Percent complete: 59.6%; Average loss: 2.9468 Iteration: 2385; Percent complete: 59.6%; Average loss: 3.0468 Iteration: 2386; Percent complete: 59.7%; Average loss: 3.3041 Iteration: 2387; Percent complete: 59.7%; Average loss: 3.1390 Iteration: 2388; Percent complete: 59.7%; Average loss: 3.0712 Iteration: 2389; Percent complete: 59.7%; Average loss: 3.2613 Iteration: 2390; Percent complete: 59.8%; Average loss: 3.1104 Iteration: 2391; Percent complete: 59.8%; Average loss: 3.1027 Iteration: 2392; Percent complete: 59.8%; Average loss: 2.9220 Iteration: 2393; Percent complete: 59.8%; Average loss: 2.7582 Iteration: 2394; Percent complete: 59.9%; Average loss: 2.9519 Iteration: 2395; Percent complete: 59.9%; Average loss: 3.0700 Iteration: 2396; Percent complete: 59.9%; Average loss: 2.9421 Iteration: 2397; Percent complete: 59.9%; Average loss: 3.1277 Iteration: 2398; Percent complete: 60.0%; Average loss: 3.1273 Iteration: 2399; Percent complete: 60.0%; Average loss: 2.8617 Iteration: 2400; Percent complete: 60.0%; Average loss: 3.2461 Iteration: 2401; Percent complete: 60.0%; Average loss: 2.9264 Iteration: 2402; Percent complete: 60.1%; Average loss: 3.0161 Iteration: 2403; Percent complete: 60.1%; Average loss: 3.1109 Iteration: 2404; Percent complete: 60.1%; Average loss: 3.2606 Iteration: 2405; Percent complete: 60.1%; Average loss: 2.8673 Iteration: 2406; Percent complete: 60.2%; Average loss: 2.9993 Iteration: 2407; Percent complete: 60.2%; Average loss: 3.2134 Iteration: 2408; Percent complete: 60.2%; Average loss: 2.8263 Iteration: 2409; Percent complete: 60.2%; Average loss: 2.7882 Iteration: 2410; Percent complete: 60.2%; Average loss: 3.0431 Iteration: 2411; Percent complete: 60.3%; Average loss: 3.0518 Iteration: 2412; Percent complete: 60.3%; Average loss: 3.0085 Iteration: 2413; Percent complete: 60.3%; Average loss: 3.1448 Iteration: 2414; Percent complete: 60.4%; Average loss: 3.1769 Iteration: 2415; Percent complete: 60.4%; Average loss: 2.9600 Iteration: 2416; Percent complete: 60.4%; Average loss: 3.2283 Iteration: 2417; Percent complete: 60.4%; Average loss: 3.0538 Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9939 Iteration: 2419; Percent complete: 60.5%; Average loss: 3.1233 Iteration: 2420; Percent complete: 60.5%; Average loss: 2.7492 Iteration: 2421; Percent complete: 60.5%; Average loss: 3.0136 Iteration: 2422; Percent complete: 60.6%; Average loss: 2.7565 Iteration: 2423; Percent complete: 60.6%; Average loss: 3.0735 Iteration: 2424; Percent complete: 60.6%; Average loss: 3.1032 Iteration: 2425; Percent complete: 60.6%; Average loss: 2.8280 Iteration: 2426; Percent complete: 60.7%; Average loss: 3.1329 Iteration: 2427; Percent complete: 60.7%; Average loss: 2.9362 Iteration: 2428; Percent complete: 60.7%; Average loss: 3.1600 Iteration: 2429; Percent complete: 60.7%; Average loss: 2.9634 Iteration: 2430; Percent complete: 60.8%; Average loss: 3.2153 Iteration: 2431; Percent complete: 60.8%; Average loss: 3.1281 Iteration: 2432; Percent complete: 60.8%; Average loss: 3.0623 Iteration: 2433; Percent complete: 60.8%; Average loss: 2.9320 Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0338 Iteration: 2435; Percent complete: 60.9%; Average loss: 2.9103 Iteration: 2436; Percent complete: 60.9%; Average loss: 3.0746 Iteration: 2437; Percent complete: 60.9%; Average loss: 3.3086 Iteration: 2438; Percent complete: 61.0%; Average loss: 3.0874 Iteration: 2439; Percent complete: 61.0%; Average loss: 3.1493 Iteration: 2440; Percent complete: 61.0%; Average loss: 3.0229 Iteration: 2441; Percent complete: 61.0%; Average loss: 3.0438 Iteration: 2442; Percent complete: 61.1%; Average loss: 2.9483 Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1269 Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0764 Iteration: 2445; Percent complete: 61.1%; Average loss: 3.2643 Iteration: 2446; Percent complete: 61.2%; Average loss: 2.9247 Iteration: 2447; Percent complete: 61.2%; Average loss: 3.0072 Iteration: 2448; Percent complete: 61.2%; Average loss: 2.8210 Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0877 Iteration: 2450; Percent complete: 61.3%; Average loss: 3.1041 Iteration: 2451; Percent complete: 61.3%; Average loss: 2.9320 Iteration: 2452; Percent complete: 61.3%; Average loss: 3.0115 Iteration: 2453; Percent complete: 61.3%; Average loss: 2.8060 Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0587 Iteration: 2455; Percent complete: 61.4%; Average loss: 3.0052 Iteration: 2456; Percent complete: 61.4%; Average loss: 3.2530 Iteration: 2457; Percent complete: 61.4%; Average loss: 3.0144 Iteration: 2458; Percent complete: 61.5%; Average loss: 3.3349 Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8693 Iteration: 2460; Percent complete: 61.5%; Average loss: 2.9783 Iteration: 2461; Percent complete: 61.5%; Average loss: 2.8159 Iteration: 2462; Percent complete: 61.6%; Average loss: 3.0540 Iteration: 2463; Percent complete: 61.6%; Average loss: 3.0719 Iteration: 2464; Percent complete: 61.6%; Average loss: 3.3003 Iteration: 2465; Percent complete: 61.6%; Average loss: 3.0105 Iteration: 2466; Percent complete: 61.7%; Average loss: 3.0307 Iteration: 2467; Percent complete: 61.7%; Average loss: 3.0135 Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0549 Iteration: 2469; Percent complete: 61.7%; Average loss: 2.9122 Iteration: 2470; Percent complete: 61.8%; Average loss: 2.8348 Iteration: 2471; Percent complete: 61.8%; Average loss: 3.0166 Iteration: 2472; Percent complete: 61.8%; Average loss: 2.9974 Iteration: 2473; Percent complete: 61.8%; Average loss: 2.9090 Iteration: 2474; Percent complete: 61.9%; Average loss: 2.8073 Iteration: 2475; Percent complete: 61.9%; Average loss: 3.0480 Iteration: 2476; Percent complete: 61.9%; Average loss: 3.1765 Iteration: 2477; Percent complete: 61.9%; Average loss: 3.1683 Iteration: 2478; Percent complete: 62.0%; Average loss: 2.9080 Iteration: 2479; Percent complete: 62.0%; Average loss: 2.9812 Iteration: 2480; Percent complete: 62.0%; Average loss: 2.9407 Iteration: 2481; Percent complete: 62.0%; Average loss: 3.0896 Iteration: 2482; Percent complete: 62.1%; Average loss: 2.7915 Iteration: 2483; Percent complete: 62.1%; Average loss: 2.9394 Iteration: 2484; Percent complete: 62.1%; Average loss: 3.2039 Iteration: 2485; Percent complete: 62.1%; Average loss: 2.7345 Iteration: 2486; Percent complete: 62.2%; Average loss: 2.7746 Iteration: 2487; Percent complete: 62.2%; Average loss: 2.9798 Iteration: 2488; Percent complete: 62.2%; Average loss: 3.0595 Iteration: 2489; Percent complete: 62.2%; Average loss: 3.1850 Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1692 Iteration: 2491; Percent complete: 62.3%; Average loss: 3.3004 Iteration: 2492; Percent complete: 62.3%; Average loss: 2.9401 Iteration: 2493; Percent complete: 62.3%; Average loss: 2.9413 Iteration: 2494; Percent complete: 62.4%; Average loss: 2.7718 Iteration: 2495; Percent complete: 62.4%; Average loss: 3.0345 Iteration: 2496; Percent complete: 62.4%; Average loss: 2.8252 Iteration: 2497; Percent complete: 62.4%; Average loss: 2.9005 Iteration: 2498; Percent complete: 62.5%; Average loss: 3.2856 Iteration: 2499; Percent complete: 62.5%; Average loss: 2.8441 Iteration: 2500; Percent complete: 62.5%; Average loss: 2.8413 Iteration: 2501; Percent complete: 62.5%; Average loss: 3.1272 Iteration: 2502; Percent complete: 62.5%; Average loss: 2.7544 Iteration: 2503; Percent complete: 62.6%; Average loss: 2.8886 Iteration: 2504; Percent complete: 62.6%; Average loss: 3.0993 Iteration: 2505; Percent complete: 62.6%; Average loss: 3.2615 Iteration: 2506; Percent complete: 62.6%; Average loss: 2.9981 Iteration: 2507; Percent complete: 62.7%; Average loss: 3.2945 Iteration: 2508; Percent complete: 62.7%; Average loss: 2.8503 Iteration: 2509; Percent complete: 62.7%; Average loss: 2.9599 Iteration: 2510; Percent complete: 62.7%; Average loss: 2.9801 Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0135 Iteration: 2512; Percent complete: 62.8%; Average loss: 3.0740 Iteration: 2513; Percent complete: 62.8%; Average loss: 3.1955 Iteration: 2514; Percent complete: 62.8%; Average loss: 2.9684 Iteration: 2515; Percent complete: 62.9%; Average loss: 2.9019 Iteration: 2516; Percent complete: 62.9%; Average loss: 3.0028 Iteration: 2517; Percent complete: 62.9%; Average loss: 2.8783 Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8856 Iteration: 2519; Percent complete: 63.0%; Average loss: 3.0284 Iteration: 2520; Percent complete: 63.0%; Average loss: 2.9636 Iteration: 2521; Percent complete: 63.0%; Average loss: 2.7766 Iteration: 2522; Percent complete: 63.0%; Average loss: 2.7459 Iteration: 2523; Percent complete: 63.1%; Average loss: 3.1688 Iteration: 2524; Percent complete: 63.1%; Average loss: 2.9331 Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9748 Iteration: 2526; Percent complete: 63.1%; Average loss: 3.2294 Iteration: 2527; Percent complete: 63.2%; Average loss: 3.1383 Iteration: 2528; Percent complete: 63.2%; Average loss: 2.9339 Iteration: 2529; Percent complete: 63.2%; Average loss: 3.0296 Iteration: 2530; Percent complete: 63.2%; Average loss: 3.1753 Iteration: 2531; Percent complete: 63.3%; Average loss: 2.6389 Iteration: 2532; Percent complete: 63.3%; Average loss: 2.7770 Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1313 Iteration: 2534; Percent complete: 63.3%; Average loss: 2.7986 Iteration: 2535; Percent complete: 63.4%; Average loss: 3.3283 Iteration: 2536; Percent complete: 63.4%; Average loss: 2.8831 Iteration: 2537; Percent complete: 63.4%; Average loss: 3.2484 Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1242 Iteration: 2539; Percent complete: 63.5%; Average loss: 3.0232 Iteration: 2540; Percent complete: 63.5%; Average loss: 3.0750 Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0290 Iteration: 2542; Percent complete: 63.5%; Average loss: 3.1119 Iteration: 2543; Percent complete: 63.6%; Average loss: 3.1334 Iteration: 2544; Percent complete: 63.6%; Average loss: 3.2672 Iteration: 2545; Percent complete: 63.6%; Average loss: 3.1018 Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0342 Iteration: 2547; Percent complete: 63.7%; Average loss: 3.0437 Iteration: 2548; Percent complete: 63.7%; Average loss: 2.8805 Iteration: 2549; Percent complete: 63.7%; Average loss: 2.9823 Iteration: 2550; Percent complete: 63.7%; Average loss: 2.8398 Iteration: 2551; Percent complete: 63.8%; Average loss: 3.1862 Iteration: 2552; Percent complete: 63.8%; Average loss: 2.8505 Iteration: 2553; Percent complete: 63.8%; Average loss: 2.8340 Iteration: 2554; Percent complete: 63.8%; Average loss: 3.2693 Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0484 Iteration: 2556; Percent complete: 63.9%; Average loss: 3.1435 Iteration: 2557; Percent complete: 63.9%; Average loss: 3.0810 Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0952 Iteration: 2559; Percent complete: 64.0%; Average loss: 2.8666 Iteration: 2560; Percent complete: 64.0%; Average loss: 3.0456 Iteration: 2561; Percent complete: 64.0%; Average loss: 2.8361 Iteration: 2562; Percent complete: 64.0%; Average loss: 2.8853 Iteration: 2563; Percent complete: 64.1%; Average loss: 3.0061 Iteration: 2564; Percent complete: 64.1%; Average loss: 2.7737 Iteration: 2565; Percent complete: 64.1%; Average loss: 3.1567 Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9995 Iteration: 2567; Percent complete: 64.2%; Average loss: 3.1011 Iteration: 2568; Percent complete: 64.2%; Average loss: 2.9193 Iteration: 2569; Percent complete: 64.2%; Average loss: 3.0728 Iteration: 2570; Percent complete: 64.2%; Average loss: 2.9686 Iteration: 2571; Percent complete: 64.3%; Average loss: 2.9707 Iteration: 2572; Percent complete: 64.3%; Average loss: 2.9283 Iteration: 2573; Percent complete: 64.3%; Average loss: 2.8691 Iteration: 2574; Percent complete: 64.3%; Average loss: 2.8737 Iteration: 2575; Percent complete: 64.4%; Average loss: 3.0436 Iteration: 2576; Percent complete: 64.4%; Average loss: 3.0803 Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9889 Iteration: 2578; Percent complete: 64.5%; Average loss: 2.7886 Iteration: 2579; Percent complete: 64.5%; Average loss: 3.1133 Iteration: 2580; Percent complete: 64.5%; Average loss: 3.1995 Iteration: 2581; Percent complete: 64.5%; Average loss: 2.8235 Iteration: 2582; Percent complete: 64.5%; Average loss: 2.8938 Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8342 Iteration: 2584; Percent complete: 64.6%; Average loss: 3.0567 Iteration: 2585; Percent complete: 64.6%; Average loss: 2.9049 Iteration: 2586; Percent complete: 64.6%; Average loss: 3.0516 Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0693 Iteration: 2588; Percent complete: 64.7%; Average loss: 3.0267 Iteration: 2589; Percent complete: 64.7%; Average loss: 2.9125 Iteration: 2590; Percent complete: 64.8%; Average loss: 3.0484 Iteration: 2591; Percent complete: 64.8%; Average loss: 2.8200 Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9754 Iteration: 2593; Percent complete: 64.8%; Average loss: 2.7906 Iteration: 2594; Percent complete: 64.8%; Average loss: 2.9763 Iteration: 2595; Percent complete: 64.9%; Average loss: 3.0039 Iteration: 2596; Percent complete: 64.9%; Average loss: 2.8341 Iteration: 2597; Percent complete: 64.9%; Average loss: 2.8217 Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0818 Iteration: 2599; Percent complete: 65.0%; Average loss: 2.8517 Iteration: 2600; Percent complete: 65.0%; Average loss: 3.0641 Iteration: 2601; Percent complete: 65.0%; Average loss: 2.8008 Iteration: 2602; Percent complete: 65.0%; Average loss: 2.9656 Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0249 Iteration: 2604; Percent complete: 65.1%; Average loss: 3.0578 Iteration: 2605; Percent complete: 65.1%; Average loss: 3.0076 Iteration: 2606; Percent complete: 65.1%; Average loss: 3.1911 Iteration: 2607; Percent complete: 65.2%; Average loss: 2.9554 Iteration: 2608; Percent complete: 65.2%; Average loss: 3.3508 Iteration: 2609; Percent complete: 65.2%; Average loss: 2.7313 Iteration: 2610; Percent complete: 65.2%; Average loss: 2.8613 Iteration: 2611; Percent complete: 65.3%; Average loss: 2.9278 Iteration: 2612; Percent complete: 65.3%; Average loss: 2.8899 Iteration: 2613; Percent complete: 65.3%; Average loss: 2.9105 Iteration: 2614; Percent complete: 65.3%; Average loss: 2.9889 Iteration: 2615; Percent complete: 65.4%; Average loss: 3.2389 Iteration: 2616; Percent complete: 65.4%; Average loss: 3.1052 Iteration: 2617; Percent complete: 65.4%; Average loss: 2.8186 Iteration: 2618; Percent complete: 65.5%; Average loss: 2.7755 Iteration: 2619; Percent complete: 65.5%; Average loss: 2.9713 Iteration: 2620; Percent complete: 65.5%; Average loss: 2.8084 Iteration: 2621; Percent complete: 65.5%; Average loss: 3.0791 Iteration: 2622; Percent complete: 65.5%; Average loss: 3.1711 Iteration: 2623; Percent complete: 65.6%; Average loss: 2.8928 Iteration: 2624; Percent complete: 65.6%; Average loss: 2.8690 Iteration: 2625; Percent complete: 65.6%; Average loss: 3.1626 Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0258 Iteration: 2627; Percent complete: 65.7%; Average loss: 3.0813 Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0679 Iteration: 2629; Percent complete: 65.7%; Average loss: 3.2515 Iteration: 2630; Percent complete: 65.8%; Average loss: 3.0112 Iteration: 2631; Percent complete: 65.8%; Average loss: 3.2863 Iteration: 2632; Percent complete: 65.8%; Average loss: 2.7774 Iteration: 2633; Percent complete: 65.8%; Average loss: 3.0006 Iteration: 2634; Percent complete: 65.8%; Average loss: 2.9435 Iteration: 2635; Percent complete: 65.9%; Average loss: 2.6353 Iteration: 2636; Percent complete: 65.9%; Average loss: 2.9537 Iteration: 2637; Percent complete: 65.9%; Average loss: 2.9645 Iteration: 2638; Percent complete: 66.0%; Average loss: 2.9122 Iteration: 2639; Percent complete: 66.0%; Average loss: 2.7983 Iteration: 2640; Percent complete: 66.0%; Average loss: 3.1623 Iteration: 2641; Percent complete: 66.0%; Average loss: 2.8879 Iteration: 2642; Percent complete: 66.0%; Average loss: 2.8149 Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8597 Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9114 Iteration: 2645; Percent complete: 66.1%; Average loss: 2.9978 Iteration: 2646; Percent complete: 66.1%; Average loss: 3.0980 Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9715 Iteration: 2648; Percent complete: 66.2%; Average loss: 3.1596 Iteration: 2649; Percent complete: 66.2%; Average loss: 2.7925 Iteration: 2650; Percent complete: 66.2%; Average loss: 2.8568 Iteration: 2651; Percent complete: 66.3%; Average loss: 3.0559 Iteration: 2652; Percent complete: 66.3%; Average loss: 2.7448 Iteration: 2653; Percent complete: 66.3%; Average loss: 3.1605 Iteration: 2654; Percent complete: 66.3%; Average loss: 2.9927 Iteration: 2655; Percent complete: 66.4%; Average loss: 2.7089 Iteration: 2656; Percent complete: 66.4%; Average loss: 2.9251 Iteration: 2657; Percent complete: 66.4%; Average loss: 2.8872 Iteration: 2658; Percent complete: 66.5%; Average loss: 2.9504 Iteration: 2659; Percent complete: 66.5%; Average loss: 3.1010 Iteration: 2660; Percent complete: 66.5%; Average loss: 2.8557 Iteration: 2661; Percent complete: 66.5%; Average loss: 2.8999 Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0074 Iteration: 2663; Percent complete: 66.6%; Average loss: 2.8513 Iteration: 2664; Percent complete: 66.6%; Average loss: 3.0145 Iteration: 2665; Percent complete: 66.6%; Average loss: 3.0367 Iteration: 2666; Percent complete: 66.6%; Average loss: 2.8531 Iteration: 2667; Percent complete: 66.7%; Average loss: 3.0589 Iteration: 2668; Percent complete: 66.7%; Average loss: 3.0710 Iteration: 2669; Percent complete: 66.7%; Average loss: 2.7680 Iteration: 2670; Percent complete: 66.8%; Average loss: 3.0574 Iteration: 2671; Percent complete: 66.8%; Average loss: 3.0735 Iteration: 2672; Percent complete: 66.8%; Average loss: 3.0834 Iteration: 2673; Percent complete: 66.8%; Average loss: 2.8882 Iteration: 2674; Percent complete: 66.8%; Average loss: 2.9548 Iteration: 2675; Percent complete: 66.9%; Average loss: 2.9422 Iteration: 2676; Percent complete: 66.9%; Average loss: 2.7966 Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9648 Iteration: 2678; Percent complete: 67.0%; Average loss: 3.0058 Iteration: 2679; Percent complete: 67.0%; Average loss: 2.8208 Iteration: 2680; Percent complete: 67.0%; Average loss: 3.1237 Iteration: 2681; Percent complete: 67.0%; Average loss: 3.0942 Iteration: 2682; Percent complete: 67.0%; Average loss: 2.8342 Iteration: 2683; Percent complete: 67.1%; Average loss: 2.7847 Iteration: 2684; Percent complete: 67.1%; Average loss: 2.8463 Iteration: 2685; Percent complete: 67.1%; Average loss: 3.0027 Iteration: 2686; Percent complete: 67.2%; Average loss: 3.0846 Iteration: 2687; Percent complete: 67.2%; Average loss: 3.1774 Iteration: 2688; Percent complete: 67.2%; Average loss: 2.9649 Iteration: 2689; Percent complete: 67.2%; Average loss: 3.0302 Iteration: 2690; Percent complete: 67.2%; Average loss: 2.7577 Iteration: 2691; Percent complete: 67.3%; Average loss: 3.1749 Iteration: 2692; Percent complete: 67.3%; Average loss: 3.0016 Iteration: 2693; Percent complete: 67.3%; Average loss: 2.8782 Iteration: 2694; Percent complete: 67.3%; Average loss: 2.9790 Iteration: 2695; Percent complete: 67.4%; Average loss: 2.9258 Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9982 Iteration: 2697; Percent complete: 67.4%; Average loss: 3.3170 Iteration: 2698; Percent complete: 67.5%; Average loss: 2.9693 Iteration: 2699; Percent complete: 67.5%; Average loss: 3.0606 Iteration: 2700; Percent complete: 67.5%; Average loss: 2.9834 Iteration: 2701; Percent complete: 67.5%; Average loss: 3.2088 Iteration: 2702; Percent complete: 67.5%; Average loss: 2.9910 Iteration: 2703; Percent complete: 67.6%; Average loss: 2.8734 Iteration: 2704; Percent complete: 67.6%; Average loss: 2.9153 Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0608 Iteration: 2706; Percent complete: 67.7%; Average loss: 2.9478 Iteration: 2707; Percent complete: 67.7%; Average loss: 3.3227 Iteration: 2708; Percent complete: 67.7%; Average loss: 2.8279 Iteration: 2709; Percent complete: 67.7%; Average loss: 2.8431 Iteration: 2710; Percent complete: 67.8%; Average loss: 2.9521 Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8186 Iteration: 2712; Percent complete: 67.8%; Average loss: 3.0365 Iteration: 2713; Percent complete: 67.8%; Average loss: 2.7710 Iteration: 2714; Percent complete: 67.8%; Average loss: 3.0324 Iteration: 2715; Percent complete: 67.9%; Average loss: 2.8389 Iteration: 2716; Percent complete: 67.9%; Average loss: 3.0831 Iteration: 2717; Percent complete: 67.9%; Average loss: 2.8422 Iteration: 2718; Percent complete: 68.0%; Average loss: 3.0787 Iteration: 2719; Percent complete: 68.0%; Average loss: 3.1670 Iteration: 2720; Percent complete: 68.0%; Average loss: 2.8523 Iteration: 2721; Percent complete: 68.0%; Average loss: 2.9262 Iteration: 2722; Percent complete: 68.0%; Average loss: 3.1981 Iteration: 2723; Percent complete: 68.1%; Average loss: 3.1106 Iteration: 2724; Percent complete: 68.1%; Average loss: 2.9482 Iteration: 2725; Percent complete: 68.1%; Average loss: 3.0815 Iteration: 2726; Percent complete: 68.2%; Average loss: 3.2107 Iteration: 2727; Percent complete: 68.2%; Average loss: 2.9726 Iteration: 2728; Percent complete: 68.2%; Average loss: 3.0498 Iteration: 2729; Percent complete: 68.2%; Average loss: 3.0838 Iteration: 2730; Percent complete: 68.2%; Average loss: 2.8946 Iteration: 2731; Percent complete: 68.3%; Average loss: 2.9872 Iteration: 2732; Percent complete: 68.3%; Average loss: 3.0012 Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9639 Iteration: 2734; Percent complete: 68.3%; Average loss: 2.8706 Iteration: 2735; Percent complete: 68.4%; Average loss: 2.8216 Iteration: 2736; Percent complete: 68.4%; Average loss: 3.0474 Iteration: 2737; Percent complete: 68.4%; Average loss: 3.0928 Iteration: 2738; Percent complete: 68.5%; Average loss: 2.8906 Iteration: 2739; Percent complete: 68.5%; Average loss: 3.0172 Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9404 Iteration: 2741; Percent complete: 68.5%; Average loss: 2.6217 Iteration: 2742; Percent complete: 68.5%; Average loss: 2.9364 Iteration: 2743; Percent complete: 68.6%; Average loss: 3.0814 Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0591 Iteration: 2745; Percent complete: 68.6%; Average loss: 3.1256 Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0071 Iteration: 2747; Percent complete: 68.7%; Average loss: 2.7152 Iteration: 2748; Percent complete: 68.7%; Average loss: 3.2410 Iteration: 2749; Percent complete: 68.7%; Average loss: 3.0299 Iteration: 2750; Percent complete: 68.8%; Average loss: 2.9470 Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0017 Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8554 Iteration: 2753; Percent complete: 68.8%; Average loss: 3.0169 Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8472 Iteration: 2755; Percent complete: 68.9%; Average loss: 2.6713 Iteration: 2756; Percent complete: 68.9%; Average loss: 2.9615 Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0460 Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0288 Iteration: 2759; Percent complete: 69.0%; Average loss: 2.8848 Iteration: 2760; Percent complete: 69.0%; Average loss: 2.9872 Iteration: 2761; Percent complete: 69.0%; Average loss: 3.0629 Iteration: 2762; Percent complete: 69.0%; Average loss: 2.8128 Iteration: 2763; Percent complete: 69.1%; Average loss: 2.8483 Iteration: 2764; Percent complete: 69.1%; Average loss: 2.8363 Iteration: 2765; Percent complete: 69.1%; Average loss: 2.9523 Iteration: 2766; Percent complete: 69.2%; Average loss: 2.9454 Iteration: 2767; Percent complete: 69.2%; Average loss: 3.3464 Iteration: 2768; Percent complete: 69.2%; Average loss: 3.1293 Iteration: 2769; Percent complete: 69.2%; Average loss: 2.9229 Iteration: 2770; Percent complete: 69.2%; Average loss: 3.1116 Iteration: 2771; Percent complete: 69.3%; Average loss: 2.9889 Iteration: 2772; Percent complete: 69.3%; Average loss: 2.8270 Iteration: 2773; Percent complete: 69.3%; Average loss: 2.9177 Iteration: 2774; Percent complete: 69.3%; Average loss: 2.8812 Iteration: 2775; Percent complete: 69.4%; Average loss: 2.5925 Iteration: 2776; Percent complete: 69.4%; Average loss: 2.7181 Iteration: 2777; Percent complete: 69.4%; Average loss: 2.8853 Iteration: 2778; Percent complete: 69.5%; Average loss: 3.0394 Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9626 Iteration: 2780; Percent complete: 69.5%; Average loss: 2.7945 Iteration: 2781; Percent complete: 69.5%; Average loss: 3.0437 Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9119 Iteration: 2783; Percent complete: 69.6%; Average loss: 2.6435 Iteration: 2784; Percent complete: 69.6%; Average loss: 2.8534 Iteration: 2785; Percent complete: 69.6%; Average loss: 3.1532 Iteration: 2786; Percent complete: 69.7%; Average loss: 2.7463 Iteration: 2787; Percent complete: 69.7%; Average loss: 2.8628 Iteration: 2788; Percent complete: 69.7%; Average loss: 2.9051 Iteration: 2789; Percent complete: 69.7%; Average loss: 3.0059 Iteration: 2790; Percent complete: 69.8%; Average loss: 2.9914 Iteration: 2791; Percent complete: 69.8%; Average loss: 2.9405 Iteration: 2792; Percent complete: 69.8%; Average loss: 2.8637 Iteration: 2793; Percent complete: 69.8%; Average loss: 2.8020 Iteration: 2794; Percent complete: 69.8%; Average loss: 2.6122 Iteration: 2795; Percent complete: 69.9%; Average loss: 2.8357 Iteration: 2796; Percent complete: 69.9%; Average loss: 2.9496 Iteration: 2797; Percent complete: 69.9%; Average loss: 2.9483 Iteration: 2798; Percent complete: 70.0%; Average loss: 2.9574 Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9151 Iteration: 2800; Percent complete: 70.0%; Average loss: 3.1191 Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8555 Iteration: 2802; Percent complete: 70.0%; Average loss: 2.6340 Iteration: 2803; Percent complete: 70.1%; Average loss: 2.8717 Iteration: 2804; Percent complete: 70.1%; Average loss: 2.8891 Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9483 Iteration: 2806; Percent complete: 70.2%; Average loss: 3.0727 Iteration: 2807; Percent complete: 70.2%; Average loss: 3.1897 Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0563 Iteration: 2809; Percent complete: 70.2%; Average loss: 2.9623 Iteration: 2810; Percent complete: 70.2%; Average loss: 3.1789 Iteration: 2811; Percent complete: 70.3%; Average loss: 3.0903 Iteration: 2812; Percent complete: 70.3%; Average loss: 2.9646 Iteration: 2813; Percent complete: 70.3%; Average loss: 2.9356 Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8996 Iteration: 2815; Percent complete: 70.4%; Average loss: 2.7105 Iteration: 2816; Percent complete: 70.4%; Average loss: 2.7669 Iteration: 2817; Percent complete: 70.4%; Average loss: 2.8119 Iteration: 2818; Percent complete: 70.5%; Average loss: 3.2486 Iteration: 2819; Percent complete: 70.5%; Average loss: 2.9912 Iteration: 2820; Percent complete: 70.5%; Average loss: 2.7983 Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8762 Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0260 Iteration: 2823; Percent complete: 70.6%; Average loss: 3.0546 Iteration: 2824; Percent complete: 70.6%; Average loss: 2.8984 Iteration: 2825; Percent complete: 70.6%; Average loss: 2.9298 Iteration: 2826; Percent complete: 70.7%; Average loss: 2.9173 Iteration: 2827; Percent complete: 70.7%; Average loss: 2.9132 Iteration: 2828; Percent complete: 70.7%; Average loss: 2.8236 Iteration: 2829; Percent complete: 70.7%; Average loss: 3.1194 Iteration: 2830; Percent complete: 70.8%; Average loss: 2.8900 Iteration: 2831; Percent complete: 70.8%; Average loss: 3.0743 Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9295 Iteration: 2833; Percent complete: 70.8%; Average loss: 2.8008 Iteration: 2834; Percent complete: 70.9%; Average loss: 2.8604 Iteration: 2835; Percent complete: 70.9%; Average loss: 2.9667 Iteration: 2836; Percent complete: 70.9%; Average loss: 2.9627 Iteration: 2837; Percent complete: 70.9%; Average loss: 2.7512 Iteration: 2838; Percent complete: 71.0%; Average loss: 2.7317 Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8027 Iteration: 2840; Percent complete: 71.0%; Average loss: 3.0991 Iteration: 2841; Percent complete: 71.0%; Average loss: 2.8449 Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9406 Iteration: 2843; Percent complete: 71.1%; Average loss: 2.8649 Iteration: 2844; Percent complete: 71.1%; Average loss: 2.8318 Iteration: 2845; Percent complete: 71.1%; Average loss: 3.0933 Iteration: 2846; Percent complete: 71.2%; Average loss: 2.9027 Iteration: 2847; Percent complete: 71.2%; Average loss: 2.8930 Iteration: 2848; Percent complete: 71.2%; Average loss: 2.8110 Iteration: 2849; Percent complete: 71.2%; Average loss: 2.8697 Iteration: 2850; Percent complete: 71.2%; Average loss: 2.7753 Iteration: 2851; Percent complete: 71.3%; Average loss: 2.8739 Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9882 Iteration: 2853; Percent complete: 71.3%; Average loss: 2.8291 Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9852 Iteration: 2855; Percent complete: 71.4%; Average loss: 2.8480 Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8427 Iteration: 2857; Percent complete: 71.4%; Average loss: 3.0807 Iteration: 2858; Percent complete: 71.5%; Average loss: 3.0335 Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8332 Iteration: 2860; Percent complete: 71.5%; Average loss: 2.9118 Iteration: 2861; Percent complete: 71.5%; Average loss: 2.7608 Iteration: 2862; Percent complete: 71.5%; Average loss: 2.7904 Iteration: 2863; Percent complete: 71.6%; Average loss: 3.0003 Iteration: 2864; Percent complete: 71.6%; Average loss: 2.8170 Iteration: 2865; Percent complete: 71.6%; Average loss: 3.0308 Iteration: 2866; Percent complete: 71.7%; Average loss: 2.8075 Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9253 Iteration: 2868; Percent complete: 71.7%; Average loss: 2.6983 Iteration: 2869; Percent complete: 71.7%; Average loss: 2.8471 Iteration: 2870; Percent complete: 71.8%; Average loss: 2.9655 Iteration: 2871; Percent complete: 71.8%; Average loss: 3.1801 Iteration: 2872; Percent complete: 71.8%; Average loss: 2.8851 Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0887 Iteration: 2874; Percent complete: 71.9%; Average loss: 2.9433 Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9169 Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9673 Iteration: 2877; Percent complete: 71.9%; Average loss: 2.8460 Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8527 Iteration: 2879; Percent complete: 72.0%; Average loss: 2.8740 Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8514 Iteration: 2881; Percent complete: 72.0%; Average loss: 2.8554 Iteration: 2882; Percent complete: 72.0%; Average loss: 2.9390 Iteration: 2883; Percent complete: 72.1%; Average loss: 2.9010 Iteration: 2884; Percent complete: 72.1%; Average loss: 2.6885 Iteration: 2885; Percent complete: 72.1%; Average loss: 3.0749 Iteration: 2886; Percent complete: 72.2%; Average loss: 2.8276 Iteration: 2887; Percent complete: 72.2%; Average loss: 2.7556 Iteration: 2888; Percent complete: 72.2%; Average loss: 3.0712 Iteration: 2889; Percent complete: 72.2%; Average loss: 2.7802 Iteration: 2890; Percent complete: 72.2%; Average loss: 2.8922 Iteration: 2891; Percent complete: 72.3%; Average loss: 3.0274 Iteration: 2892; Percent complete: 72.3%; Average loss: 2.8782 Iteration: 2893; Percent complete: 72.3%; Average loss: 2.7411 Iteration: 2894; Percent complete: 72.4%; Average loss: 2.6468 Iteration: 2895; Percent complete: 72.4%; Average loss: 3.0694 Iteration: 2896; Percent complete: 72.4%; Average loss: 2.7082 Iteration: 2897; Percent complete: 72.4%; Average loss: 2.7636 Iteration: 2898; Percent complete: 72.5%; Average loss: 2.6953 Iteration: 2899; Percent complete: 72.5%; Average loss: 2.5704 Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9935 Iteration: 2901; Percent complete: 72.5%; Average loss: 2.6925 Iteration: 2902; Percent complete: 72.5%; Average loss: 3.0798 Iteration: 2903; Percent complete: 72.6%; Average loss: 2.7649 Iteration: 2904; Percent complete: 72.6%; Average loss: 2.9993 Iteration: 2905; Percent complete: 72.6%; Average loss: 2.9260 Iteration: 2906; Percent complete: 72.7%; Average loss: 2.9192 Iteration: 2907; Percent complete: 72.7%; Average loss: 2.9466 Iteration: 2908; Percent complete: 72.7%; Average loss: 2.8860 Iteration: 2909; Percent complete: 72.7%; Average loss: 2.9092 Iteration: 2910; Percent complete: 72.8%; Average loss: 2.8376 Iteration: 2911; Percent complete: 72.8%; Average loss: 2.7872 Iteration: 2912; Percent complete: 72.8%; Average loss: 3.0203 Iteration: 2913; Percent complete: 72.8%; Average loss: 2.9417 Iteration: 2914; Percent complete: 72.9%; Average loss: 2.8927 Iteration: 2915; Percent complete: 72.9%; Average loss: 2.7659 Iteration: 2916; Percent complete: 72.9%; Average loss: 3.0578 Iteration: 2917; Percent complete: 72.9%; Average loss: 2.7945 Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9576 Iteration: 2919; Percent complete: 73.0%; Average loss: 3.1665 Iteration: 2920; Percent complete: 73.0%; Average loss: 2.6441 Iteration: 2921; Percent complete: 73.0%; Average loss: 3.1765 Iteration: 2922; Percent complete: 73.0%; Average loss: 3.0600 Iteration: 2923; Percent complete: 73.1%; Average loss: 3.1879 Iteration: 2924; Percent complete: 73.1%; Average loss: 2.9281 Iteration: 2925; Percent complete: 73.1%; Average loss: 2.8572 Iteration: 2926; Percent complete: 73.2%; Average loss: 2.7520 Iteration: 2927; Percent complete: 73.2%; Average loss: 2.8416 Iteration: 2928; Percent complete: 73.2%; Average loss: 2.5569 Iteration: 2929; Percent complete: 73.2%; Average loss: 2.9574 Iteration: 2930; Percent complete: 73.2%; Average loss: 2.9354 Iteration: 2931; Percent complete: 73.3%; Average loss: 2.6870 Iteration: 2932; Percent complete: 73.3%; Average loss: 2.9385 Iteration: 2933; Percent complete: 73.3%; Average loss: 2.9433 Iteration: 2934; Percent complete: 73.4%; Average loss: 2.8217 Iteration: 2935; Percent complete: 73.4%; Average loss: 2.8426 Iteration: 2936; Percent complete: 73.4%; Average loss: 2.8986 Iteration: 2937; Percent complete: 73.4%; Average loss: 2.8919 Iteration: 2938; Percent complete: 73.5%; Average loss: 2.8699 Iteration: 2939; Percent complete: 73.5%; Average loss: 3.0618 Iteration: 2940; Percent complete: 73.5%; Average loss: 2.9268 Iteration: 2941; Percent complete: 73.5%; Average loss: 2.8913 Iteration: 2942; Percent complete: 73.6%; Average loss: 2.9999 Iteration: 2943; Percent complete: 73.6%; Average loss: 2.8531 Iteration: 2944; Percent complete: 73.6%; Average loss: 3.0234 Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8584 Iteration: 2946; Percent complete: 73.7%; Average loss: 3.0159 Iteration: 2947; Percent complete: 73.7%; Average loss: 2.9612 Iteration: 2948; Percent complete: 73.7%; Average loss: 2.8266 Iteration: 2949; Percent complete: 73.7%; Average loss: 2.7707 Iteration: 2950; Percent complete: 73.8%; Average loss: 2.9449 Iteration: 2951; Percent complete: 73.8%; Average loss: 2.9393 Iteration: 2952; Percent complete: 73.8%; Average loss: 2.9744 Iteration: 2953; Percent complete: 73.8%; Average loss: 2.9331 Iteration: 2954; Percent complete: 73.9%; Average loss: 2.7976 Iteration: 2955; Percent complete: 73.9%; Average loss: 3.2895 Iteration: 2956; Percent complete: 73.9%; Average loss: 2.8712 Iteration: 2957; Percent complete: 73.9%; Average loss: 2.6149 Iteration: 2958; Percent complete: 74.0%; Average loss: 3.1430 Iteration: 2959; Percent complete: 74.0%; Average loss: 2.8189 Iteration: 2960; Percent complete: 74.0%; Average loss: 3.0931 Iteration: 2961; Percent complete: 74.0%; Average loss: 3.0536 Iteration: 2962; Percent complete: 74.1%; Average loss: 3.0741 Iteration: 2963; Percent complete: 74.1%; Average loss: 3.0255 Iteration: 2964; Percent complete: 74.1%; Average loss: 3.0595 Iteration: 2965; Percent complete: 74.1%; Average loss: 3.1002 Iteration: 2966; Percent complete: 74.2%; Average loss: 2.8155 Iteration: 2967; Percent complete: 74.2%; Average loss: 2.9351 Iteration: 2968; Percent complete: 74.2%; Average loss: 2.7739 Iteration: 2969; Percent complete: 74.2%; Average loss: 2.6940 Iteration: 2970; Percent complete: 74.2%; Average loss: 3.0183 Iteration: 2971; Percent complete: 74.3%; Average loss: 2.9247 Iteration: 2972; Percent complete: 74.3%; Average loss: 2.8965 Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0855 Iteration: 2974; Percent complete: 74.4%; Average loss: 3.0527 Iteration: 2975; Percent complete: 74.4%; Average loss: 2.9082 Iteration: 2976; Percent complete: 74.4%; Average loss: 2.9744 Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9103 Iteration: 2978; Percent complete: 74.5%; Average loss: 2.9555 Iteration: 2979; Percent complete: 74.5%; Average loss: 2.8775 Iteration: 2980; Percent complete: 74.5%; Average loss: 2.5978 Iteration: 2981; Percent complete: 74.5%; Average loss: 2.7261 Iteration: 2982; Percent complete: 74.6%; Average loss: 2.8320 Iteration: 2983; Percent complete: 74.6%; Average loss: 2.9820 Iteration: 2984; Percent complete: 74.6%; Average loss: 2.8926 Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9800 Iteration: 2986; Percent complete: 74.7%; Average loss: 2.8141 Iteration: 2987; Percent complete: 74.7%; Average loss: 3.1539 Iteration: 2988; Percent complete: 74.7%; Average loss: 2.9164 Iteration: 2989; Percent complete: 74.7%; Average loss: 2.7686 Iteration: 2990; Percent complete: 74.8%; Average loss: 3.0157 Iteration: 2991; Percent complete: 74.8%; Average loss: 2.5433 Iteration: 2992; Percent complete: 74.8%; Average loss: 3.0755 Iteration: 2993; Percent complete: 74.8%; Average loss: 2.9134 Iteration: 2994; Percent complete: 74.9%; Average loss: 3.1198 Iteration: 2995; Percent complete: 74.9%; Average loss: 2.8238 Iteration: 2996; Percent complete: 74.9%; Average loss: 2.9042 Iteration: 2997; Percent complete: 74.9%; Average loss: 2.8500 Iteration: 2998; Percent complete: 75.0%; Average loss: 2.7370 Iteration: 2999; Percent complete: 75.0%; Average loss: 2.7632 Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9042 Iteration: 3001; Percent complete: 75.0%; Average loss: 2.9104 Iteration: 3002; Percent complete: 75.0%; Average loss: 2.9315 Iteration: 3003; Percent complete: 75.1%; Average loss: 2.5538 Iteration: 3004; Percent complete: 75.1%; Average loss: 2.7653 Iteration: 3005; Percent complete: 75.1%; Average loss: 2.8091 Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8280 Iteration: 3007; Percent complete: 75.2%; Average loss: 2.8973 Iteration: 3008; Percent complete: 75.2%; Average loss: 2.9303 Iteration: 3009; Percent complete: 75.2%; Average loss: 2.8992 Iteration: 3010; Percent complete: 75.2%; Average loss: 2.9465 Iteration: 3011; Percent complete: 75.3%; Average loss: 2.7476 Iteration: 3012; Percent complete: 75.3%; Average loss: 2.8692 Iteration: 3013; Percent complete: 75.3%; Average loss: 3.0576 Iteration: 3014; Percent complete: 75.3%; Average loss: 2.9363 Iteration: 3015; Percent complete: 75.4%; Average loss: 2.8506 Iteration: 3016; Percent complete: 75.4%; Average loss: 3.0690 Iteration: 3017; Percent complete: 75.4%; Average loss: 3.0610 Iteration: 3018; Percent complete: 75.4%; Average loss: 2.7825 Iteration: 3019; Percent complete: 75.5%; Average loss: 2.5459 Iteration: 3020; Percent complete: 75.5%; Average loss: 3.0053 Iteration: 3021; Percent complete: 75.5%; Average loss: 2.9335 Iteration: 3022; Percent complete: 75.5%; Average loss: 3.0949 Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8999 Iteration: 3024; Percent complete: 75.6%; Average loss: 3.0097 Iteration: 3025; Percent complete: 75.6%; Average loss: 2.8900 Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8316 Iteration: 3027; Percent complete: 75.7%; Average loss: 2.8314 Iteration: 3028; Percent complete: 75.7%; Average loss: 2.7724 Iteration: 3029; Percent complete: 75.7%; Average loss: 2.7451 Iteration: 3030; Percent complete: 75.8%; Average loss: 2.8051 Iteration: 3031; Percent complete: 75.8%; Average loss: 2.8484 Iteration: 3032; Percent complete: 75.8%; Average loss: 2.7615 Iteration: 3033; Percent complete: 75.8%; Average loss: 2.9300 Iteration: 3034; Percent complete: 75.8%; Average loss: 3.0415 Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7075 Iteration: 3036; Percent complete: 75.9%; Average loss: 2.8187 Iteration: 3037; Percent complete: 75.9%; Average loss: 2.7239 Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8551 Iteration: 3039; Percent complete: 76.0%; Average loss: 2.8072 Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9392 Iteration: 3041; Percent complete: 76.0%; Average loss: 2.8911 Iteration: 3042; Percent complete: 76.0%; Average loss: 2.9741 Iteration: 3043; Percent complete: 76.1%; Average loss: 2.8365 Iteration: 3044; Percent complete: 76.1%; Average loss: 2.9648 Iteration: 3045; Percent complete: 76.1%; Average loss: 2.6674 Iteration: 3046; Percent complete: 76.1%; Average loss: 2.8556 Iteration: 3047; Percent complete: 76.2%; Average loss: 3.2320 Iteration: 3048; Percent complete: 76.2%; Average loss: 2.6103 Iteration: 3049; Percent complete: 76.2%; Average loss: 2.9026 Iteration: 3050; Percent complete: 76.2%; Average loss: 2.7588 Iteration: 3051; Percent complete: 76.3%; Average loss: 2.7987 Iteration: 3052; Percent complete: 76.3%; Average loss: 3.0229 Iteration: 3053; Percent complete: 76.3%; Average loss: 2.9316 Iteration: 3054; Percent complete: 76.3%; Average loss: 2.8221 Iteration: 3055; Percent complete: 76.4%; Average loss: 2.7022 Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8500 Iteration: 3057; Percent complete: 76.4%; Average loss: 2.8739 Iteration: 3058; Percent complete: 76.4%; Average loss: 2.8017 Iteration: 3059; Percent complete: 76.5%; Average loss: 2.8212 Iteration: 3060; Percent complete: 76.5%; Average loss: 2.9076 Iteration: 3061; Percent complete: 76.5%; Average loss: 3.1309 Iteration: 3062; Percent complete: 76.5%; Average loss: 2.9417 Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8713 Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8195 Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9570 Iteration: 3066; Percent complete: 76.6%; Average loss: 2.8774 Iteration: 3067; Percent complete: 76.7%; Average loss: 2.8918 Iteration: 3068; Percent complete: 76.7%; Average loss: 2.8781 Iteration: 3069; Percent complete: 76.7%; Average loss: 2.7337 Iteration: 3070; Percent complete: 76.8%; Average loss: 3.1019 Iteration: 3071; Percent complete: 76.8%; Average loss: 2.9705 Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8116 Iteration: 3073; Percent complete: 76.8%; Average loss: 2.8756 Iteration: 3074; Percent complete: 76.8%; Average loss: 2.7602 Iteration: 3075; Percent complete: 76.9%; Average loss: 2.7899 Iteration: 3076; Percent complete: 76.9%; Average loss: 2.8827 Iteration: 3077; Percent complete: 76.9%; Average loss: 2.9504 Iteration: 3078; Percent complete: 77.0%; Average loss: 2.8038 Iteration: 3079; Percent complete: 77.0%; Average loss: 3.0024 Iteration: 3080; Percent complete: 77.0%; Average loss: 2.7422 Iteration: 3081; Percent complete: 77.0%; Average loss: 2.7029 Iteration: 3082; Percent complete: 77.0%; Average loss: 2.8107 Iteration: 3083; Percent complete: 77.1%; Average loss: 2.6870 Iteration: 3084; Percent complete: 77.1%; Average loss: 2.5395 Iteration: 3085; Percent complete: 77.1%; Average loss: 2.8710 Iteration: 3086; Percent complete: 77.1%; Average loss: 2.7820 Iteration: 3087; Percent complete: 77.2%; Average loss: 2.9683 Iteration: 3088; Percent complete: 77.2%; Average loss: 2.6662 Iteration: 3089; Percent complete: 77.2%; Average loss: 2.8119 Iteration: 3090; Percent complete: 77.2%; Average loss: 3.0061 Iteration: 3091; Percent complete: 77.3%; Average loss: 2.6946 Iteration: 3092; Percent complete: 77.3%; Average loss: 2.9894 Iteration: 3093; Percent complete: 77.3%; Average loss: 3.0249 Iteration: 3094; Percent complete: 77.3%; Average loss: 2.8570 Iteration: 3095; Percent complete: 77.4%; Average loss: 2.8867 Iteration: 3096; Percent complete: 77.4%; Average loss: 2.6720 Iteration: 3097; Percent complete: 77.4%; Average loss: 2.9286 Iteration: 3098; Percent complete: 77.5%; Average loss: 2.6378 Iteration: 3099; Percent complete: 77.5%; Average loss: 2.7723 Iteration: 3100; Percent complete: 77.5%; Average loss: 2.9296 Iteration: 3101; Percent complete: 77.5%; Average loss: 2.7979 Iteration: 3102; Percent complete: 77.5%; Average loss: 2.8227 Iteration: 3103; Percent complete: 77.6%; Average loss: 2.7246 Iteration: 3104; Percent complete: 77.6%; Average loss: 2.5275 Iteration: 3105; Percent complete: 77.6%; Average loss: 2.8894 Iteration: 3106; Percent complete: 77.6%; Average loss: 2.9235 Iteration: 3107; Percent complete: 77.7%; Average loss: 2.8868 Iteration: 3108; Percent complete: 77.7%; Average loss: 2.8655 Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8612 Iteration: 3110; Percent complete: 77.8%; Average loss: 3.0113 Iteration: 3111; Percent complete: 77.8%; Average loss: 3.0005 Iteration: 3112; Percent complete: 77.8%; Average loss: 3.0154 Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9004 Iteration: 3114; Percent complete: 77.8%; Average loss: 2.8492 Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7946 Iteration: 3116; Percent complete: 77.9%; Average loss: 2.7277 Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8577 Iteration: 3118; Percent complete: 78.0%; Average loss: 2.6949 Iteration: 3119; Percent complete: 78.0%; Average loss: 2.8490 Iteration: 3120; Percent complete: 78.0%; Average loss: 2.9207 Iteration: 3121; Percent complete: 78.0%; Average loss: 2.6902 Iteration: 3122; Percent complete: 78.0%; Average loss: 2.8039 Iteration: 3123; Percent complete: 78.1%; Average loss: 2.9765 Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7022 Iteration: 3125; Percent complete: 78.1%; Average loss: 2.6665 Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8983 Iteration: 3127; Percent complete: 78.2%; Average loss: 2.9523 Iteration: 3128; Percent complete: 78.2%; Average loss: 2.9094 Iteration: 3129; Percent complete: 78.2%; Average loss: 2.7803 Iteration: 3130; Percent complete: 78.2%; Average loss: 2.7735 Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7078 Iteration: 3132; Percent complete: 78.3%; Average loss: 2.6066 Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8564 Iteration: 3134; Percent complete: 78.3%; Average loss: 2.8057 Iteration: 3135; Percent complete: 78.4%; Average loss: 2.9009 Iteration: 3136; Percent complete: 78.4%; Average loss: 2.8712 Iteration: 3137; Percent complete: 78.4%; Average loss: 2.8816 Iteration: 3138; Percent complete: 78.5%; Average loss: 2.6431 Iteration: 3139; Percent complete: 78.5%; Average loss: 2.7677 Iteration: 3140; Percent complete: 78.5%; Average loss: 2.8159 Iteration: 3141; Percent complete: 78.5%; Average loss: 2.7317 Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8099 Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8721 Iteration: 3144; Percent complete: 78.6%; Average loss: 2.9329 Iteration: 3145; Percent complete: 78.6%; Average loss: 2.9142 Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7942 Iteration: 3147; Percent complete: 78.7%; Average loss: 3.3282 Iteration: 3148; Percent complete: 78.7%; Average loss: 2.8843 Iteration: 3149; Percent complete: 78.7%; Average loss: 2.8390 Iteration: 3150; Percent complete: 78.8%; Average loss: 2.8359 Iteration: 3151; Percent complete: 78.8%; Average loss: 2.9266 Iteration: 3152; Percent complete: 78.8%; Average loss: 2.6332 Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6443 Iteration: 3154; Percent complete: 78.8%; Average loss: 2.8584 Iteration: 3155; Percent complete: 78.9%; Average loss: 2.9708 Iteration: 3156; Percent complete: 78.9%; Average loss: 2.7314 Iteration: 3157; Percent complete: 78.9%; Average loss: 2.6603 Iteration: 3158; Percent complete: 79.0%; Average loss: 3.1238 Iteration: 3159; Percent complete: 79.0%; Average loss: 2.7714 Iteration: 3160; Percent complete: 79.0%; Average loss: 2.9591 Iteration: 3161; Percent complete: 79.0%; Average loss: 3.1092 Iteration: 3162; Percent complete: 79.0%; Average loss: 2.7033 Iteration: 3163; Percent complete: 79.1%; Average loss: 2.5335 Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8166 Iteration: 3165; Percent complete: 79.1%; Average loss: 2.9550 Iteration: 3166; Percent complete: 79.1%; Average loss: 2.9861 Iteration: 3167; Percent complete: 79.2%; Average loss: 2.7951 Iteration: 3168; Percent complete: 79.2%; Average loss: 2.9197 Iteration: 3169; Percent complete: 79.2%; Average loss: 2.8039 Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9605 Iteration: 3171; Percent complete: 79.3%; Average loss: 2.8932 Iteration: 3172; Percent complete: 79.3%; Average loss: 2.8407 Iteration: 3173; Percent complete: 79.3%; Average loss: 3.0987 Iteration: 3174; Percent complete: 79.3%; Average loss: 3.0832 Iteration: 3175; Percent complete: 79.4%; Average loss: 2.7120 Iteration: 3176; Percent complete: 79.4%; Average loss: 2.8610 Iteration: 3177; Percent complete: 79.4%; Average loss: 2.6021 Iteration: 3178; Percent complete: 79.5%; Average loss: 2.9299 Iteration: 3179; Percent complete: 79.5%; Average loss: 2.9136 Iteration: 3180; Percent complete: 79.5%; Average loss: 3.1010 Iteration: 3181; Percent complete: 79.5%; Average loss: 3.0585 Iteration: 3182; Percent complete: 79.5%; Average loss: 3.0169 Iteration: 3183; Percent complete: 79.6%; Average loss: 2.9386 Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8119 Iteration: 3185; Percent complete: 79.6%; Average loss: 2.6383 Iteration: 3186; Percent complete: 79.7%; Average loss: 3.0301 Iteration: 3187; Percent complete: 79.7%; Average loss: 2.7493 Iteration: 3188; Percent complete: 79.7%; Average loss: 2.8811 Iteration: 3189; Percent complete: 79.7%; Average loss: 2.8886 Iteration: 3190; Percent complete: 79.8%; Average loss: 2.7996 Iteration: 3191; Percent complete: 79.8%; Average loss: 2.8282 Iteration: 3192; Percent complete: 79.8%; Average loss: 2.6497 Iteration: 3193; Percent complete: 79.8%; Average loss: 2.8673 Iteration: 3194; Percent complete: 79.8%; Average loss: 2.7617 Iteration: 3195; Percent complete: 79.9%; Average loss: 3.0431 Iteration: 3196; Percent complete: 79.9%; Average loss: 2.9346 Iteration: 3197; Percent complete: 79.9%; Average loss: 2.6489 Iteration: 3198; Percent complete: 80.0%; Average loss: 2.7939 Iteration: 3199; Percent complete: 80.0%; Average loss: 2.8033 Iteration: 3200; Percent complete: 80.0%; Average loss: 2.7293 Iteration: 3201; Percent complete: 80.0%; Average loss: 2.9392 Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8895 Iteration: 3203; Percent complete: 80.1%; Average loss: 2.9887 Iteration: 3204; Percent complete: 80.1%; Average loss: 2.7517 Iteration: 3205; Percent complete: 80.1%; Average loss: 2.7560 Iteration: 3206; Percent complete: 80.2%; Average loss: 2.6589 Iteration: 3207; Percent complete: 80.2%; Average loss: 2.7717 Iteration: 3208; Percent complete: 80.2%; Average loss: 2.8106 Iteration: 3209; Percent complete: 80.2%; Average loss: 2.8480 Iteration: 3210; Percent complete: 80.2%; Average loss: 2.8923 Iteration: 3211; Percent complete: 80.3%; Average loss: 2.6894 Iteration: 3212; Percent complete: 80.3%; Average loss: 2.7134 Iteration: 3213; Percent complete: 80.3%; Average loss: 2.6193 Iteration: 3214; Percent complete: 80.3%; Average loss: 3.0627 Iteration: 3215; Percent complete: 80.4%; Average loss: 3.0382 Iteration: 3216; Percent complete: 80.4%; Average loss: 2.8695 Iteration: 3217; Percent complete: 80.4%; Average loss: 2.7243 Iteration: 3218; Percent complete: 80.5%; Average loss: 2.8867 Iteration: 3219; Percent complete: 80.5%; Average loss: 3.1501 Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8389 Iteration: 3221; Percent complete: 80.5%; Average loss: 2.7975 Iteration: 3222; Percent complete: 80.5%; Average loss: 2.6297 Iteration: 3223; Percent complete: 80.6%; Average loss: 2.7997 Iteration: 3224; Percent complete: 80.6%; Average loss: 2.7528 Iteration: 3225; Percent complete: 80.6%; Average loss: 2.7327 Iteration: 3226; Percent complete: 80.7%; Average loss: 2.8161 Iteration: 3227; Percent complete: 80.7%; Average loss: 2.7669 Iteration: 3228; Percent complete: 80.7%; Average loss: 2.9076 Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7975 Iteration: 3230; Percent complete: 80.8%; Average loss: 2.4685 Iteration: 3231; Percent complete: 80.8%; Average loss: 2.9595 Iteration: 3232; Percent complete: 80.8%; Average loss: 2.7549 Iteration: 3233; Percent complete: 80.8%; Average loss: 2.8397 Iteration: 3234; Percent complete: 80.8%; Average loss: 2.6217 Iteration: 3235; Percent complete: 80.9%; Average loss: 2.7724 Iteration: 3236; Percent complete: 80.9%; Average loss: 2.9681 Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7615 Iteration: 3238; Percent complete: 81.0%; Average loss: 2.9355 Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8458 Iteration: 3240; Percent complete: 81.0%; Average loss: 2.9161 Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7090 Iteration: 3242; Percent complete: 81.0%; Average loss: 2.9326 Iteration: 3243; Percent complete: 81.1%; Average loss: 2.8715 Iteration: 3244; Percent complete: 81.1%; Average loss: 2.6919 Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7020 Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9264 Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7968 Iteration: 3248; Percent complete: 81.2%; Average loss: 2.6390 Iteration: 3249; Percent complete: 81.2%; Average loss: 2.6668 Iteration: 3250; Percent complete: 81.2%; Average loss: 2.7627 Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8597 Iteration: 3252; Percent complete: 81.3%; Average loss: 2.9135 Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9317 Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8430 Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9661 Iteration: 3256; Percent complete: 81.4%; Average loss: 3.3176 Iteration: 3257; Percent complete: 81.4%; Average loss: 2.6542 Iteration: 3258; Percent complete: 81.5%; Average loss: 2.7711 Iteration: 3259; Percent complete: 81.5%; Average loss: 2.9601 Iteration: 3260; Percent complete: 81.5%; Average loss: 2.8149 Iteration: 3261; Percent complete: 81.5%; Average loss: 2.8393 Iteration: 3262; Percent complete: 81.5%; Average loss: 2.6799 Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9596 Iteration: 3264; Percent complete: 81.6%; Average loss: 2.8757 Iteration: 3265; Percent complete: 81.6%; Average loss: 2.6411 Iteration: 3266; Percent complete: 81.7%; Average loss: 2.7922 Iteration: 3267; Percent complete: 81.7%; Average loss: 2.7385 Iteration: 3268; Percent complete: 81.7%; Average loss: 2.9999 Iteration: 3269; Percent complete: 81.7%; Average loss: 2.8503 Iteration: 3270; Percent complete: 81.8%; Average loss: 2.9187 Iteration: 3271; Percent complete: 81.8%; Average loss: 2.6218 Iteration: 3272; Percent complete: 81.8%; Average loss: 3.1177 Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7214 Iteration: 3274; Percent complete: 81.8%; Average loss: 2.7754 Iteration: 3275; Percent complete: 81.9%; Average loss: 2.9491 Iteration: 3276; Percent complete: 81.9%; Average loss: 2.8814 Iteration: 3277; Percent complete: 81.9%; Average loss: 2.8941 Iteration: 3278; Percent complete: 82.0%; Average loss: 2.8560 Iteration: 3279; Percent complete: 82.0%; Average loss: 2.7694 Iteration: 3280; Percent complete: 82.0%; Average loss: 2.9491 Iteration: 3281; Percent complete: 82.0%; Average loss: 2.7715 Iteration: 3282; Percent complete: 82.0%; Average loss: 2.8141 Iteration: 3283; Percent complete: 82.1%; Average loss: 2.6711 Iteration: 3284; Percent complete: 82.1%; Average loss: 2.8983 Iteration: 3285; Percent complete: 82.1%; Average loss: 2.9840 Iteration: 3286; Percent complete: 82.2%; Average loss: 2.4865 Iteration: 3287; Percent complete: 82.2%; Average loss: 2.8675 Iteration: 3288; Percent complete: 82.2%; Average loss: 2.7325 Iteration: 3289; Percent complete: 82.2%; Average loss: 2.6046 Iteration: 3290; Percent complete: 82.2%; Average loss: 2.8153 Iteration: 3291; Percent complete: 82.3%; Average loss: 2.6478 Iteration: 3292; Percent complete: 82.3%; Average loss: 2.6390 Iteration: 3293; Percent complete: 82.3%; Average loss: 2.6301 Iteration: 3294; Percent complete: 82.3%; Average loss: 2.7821 Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8347 Iteration: 3296; Percent complete: 82.4%; Average loss: 2.7727 Iteration: 3297; Percent complete: 82.4%; Average loss: 3.1048 Iteration: 3298; Percent complete: 82.5%; Average loss: 2.7168 Iteration: 3299; Percent complete: 82.5%; Average loss: 2.6872 Iteration: 3300; Percent complete: 82.5%; Average loss: 2.6663 Iteration: 3301; Percent complete: 82.5%; Average loss: 2.8109 Iteration: 3302; Percent complete: 82.5%; Average loss: 2.6053 Iteration: 3303; Percent complete: 82.6%; Average loss: 2.7403 Iteration: 3304; Percent complete: 82.6%; Average loss: 2.6275 Iteration: 3305; Percent complete: 82.6%; Average loss: 2.8244 Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8806 Iteration: 3307; Percent complete: 82.7%; Average loss: 3.0202 Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7257 Iteration: 3309; Percent complete: 82.7%; Average loss: 2.8465 Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7214 Iteration: 3311; Percent complete: 82.8%; Average loss: 2.6771 Iteration: 3312; Percent complete: 82.8%; Average loss: 2.9289 Iteration: 3313; Percent complete: 82.8%; Average loss: 2.7344 Iteration: 3314; Percent complete: 82.8%; Average loss: 2.6457 Iteration: 3315; Percent complete: 82.9%; Average loss: 2.7354 Iteration: 3316; Percent complete: 82.9%; Average loss: 2.9240 Iteration: 3317; Percent complete: 82.9%; Average loss: 2.8632 Iteration: 3318; Percent complete: 83.0%; Average loss: 2.6166 Iteration: 3319; Percent complete: 83.0%; Average loss: 3.1565 Iteration: 3320; Percent complete: 83.0%; Average loss: 2.9158 Iteration: 3321; Percent complete: 83.0%; Average loss: 2.8173 Iteration: 3322; Percent complete: 83.0%; Average loss: 2.6573 Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7586 Iteration: 3324; Percent complete: 83.1%; Average loss: 2.6959 Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7122 Iteration: 3326; Percent complete: 83.2%; Average loss: 2.8687 Iteration: 3327; Percent complete: 83.2%; Average loss: 2.8104 Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6543 Iteration: 3329; Percent complete: 83.2%; Average loss: 2.5764 Iteration: 3330; Percent complete: 83.2%; Average loss: 2.8283 Iteration: 3331; Percent complete: 83.3%; Average loss: 2.7068 Iteration: 3332; Percent complete: 83.3%; Average loss: 2.6352 Iteration: 3333; Percent complete: 83.3%; Average loss: 2.7618 Iteration: 3334; Percent complete: 83.4%; Average loss: 2.7234 Iteration: 3335; Percent complete: 83.4%; Average loss: 2.7991 Iteration: 3336; Percent complete: 83.4%; Average loss: 3.2406 Iteration: 3337; Percent complete: 83.4%; Average loss: 2.7847 Iteration: 3338; Percent complete: 83.5%; Average loss: 2.9362 Iteration: 3339; Percent complete: 83.5%; Average loss: 2.7474 Iteration: 3340; Percent complete: 83.5%; Average loss: 2.7756 Iteration: 3341; Percent complete: 83.5%; Average loss: 2.9787 Iteration: 3342; Percent complete: 83.5%; Average loss: 2.7549 Iteration: 3343; Percent complete: 83.6%; Average loss: 2.5395 Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7748 Iteration: 3345; Percent complete: 83.6%; Average loss: 2.5706 Iteration: 3346; Percent complete: 83.7%; Average loss: 2.6230 Iteration: 3347; Percent complete: 83.7%; Average loss: 2.9153 Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6769 Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8751 Iteration: 3350; Percent complete: 83.8%; Average loss: 2.6116 Iteration: 3351; Percent complete: 83.8%; Average loss: 3.0434 Iteration: 3352; Percent complete: 83.8%; Average loss: 2.8971 Iteration: 3353; Percent complete: 83.8%; Average loss: 2.8728 Iteration: 3354; Percent complete: 83.9%; Average loss: 2.7236 Iteration: 3355; Percent complete: 83.9%; Average loss: 2.8872 Iteration: 3356; Percent complete: 83.9%; Average loss: 2.9297 Iteration: 3357; Percent complete: 83.9%; Average loss: 2.4624 Iteration: 3358; Percent complete: 84.0%; Average loss: 2.6156 Iteration: 3359; Percent complete: 84.0%; Average loss: 2.6399 Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6900 Iteration: 3361; Percent complete: 84.0%; Average loss: 2.8120 Iteration: 3362; Percent complete: 84.0%; Average loss: 2.7338 Iteration: 3363; Percent complete: 84.1%; Average loss: 2.8510 Iteration: 3364; Percent complete: 84.1%; Average loss: 2.8404 Iteration: 3365; Percent complete: 84.1%; Average loss: 2.6501 Iteration: 3366; Percent complete: 84.2%; Average loss: 2.8302 Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7096 Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9962 Iteration: 3369; Percent complete: 84.2%; Average loss: 2.6763 Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6098 Iteration: 3371; Percent complete: 84.3%; Average loss: 2.8035 Iteration: 3372; Percent complete: 84.3%; Average loss: 2.4390 Iteration: 3373; Percent complete: 84.3%; Average loss: 3.0294 Iteration: 3374; Percent complete: 84.4%; Average loss: 2.4713 Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8440 Iteration: 3376; Percent complete: 84.4%; Average loss: 2.8451 Iteration: 3377; Percent complete: 84.4%; Average loss: 2.7753 Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7155 Iteration: 3379; Percent complete: 84.5%; Average loss: 2.8847 Iteration: 3380; Percent complete: 84.5%; Average loss: 2.9252 Iteration: 3381; Percent complete: 84.5%; Average loss: 2.8666 Iteration: 3382; Percent complete: 84.5%; Average loss: 2.8278 Iteration: 3383; Percent complete: 84.6%; Average loss: 2.8084 Iteration: 3384; Percent complete: 84.6%; Average loss: 2.7055 Iteration: 3385; Percent complete: 84.6%; Average loss: 2.8228 Iteration: 3386; Percent complete: 84.7%; Average loss: 2.4837 Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7085 Iteration: 3388; Percent complete: 84.7%; Average loss: 2.6598 Iteration: 3389; Percent complete: 84.7%; Average loss: 2.5747 Iteration: 3390; Percent complete: 84.8%; Average loss: 2.8001 Iteration: 3391; Percent complete: 84.8%; Average loss: 2.8125 Iteration: 3392; Percent complete: 84.8%; Average loss: 2.8607 Iteration: 3393; Percent complete: 84.8%; Average loss: 2.7869 Iteration: 3394; Percent complete: 84.9%; Average loss: 2.8262 Iteration: 3395; Percent complete: 84.9%; Average loss: 2.7201 Iteration: 3396; Percent complete: 84.9%; Average loss: 2.6119 Iteration: 3397; Percent complete: 84.9%; Average loss: 2.8198 Iteration: 3398; Percent complete: 85.0%; Average loss: 2.7679 Iteration: 3399; Percent complete: 85.0%; Average loss: 2.6924 Iteration: 3400; Percent complete: 85.0%; Average loss: 3.0022 Iteration: 3401; Percent complete: 85.0%; Average loss: 2.9101 Iteration: 3402; Percent complete: 85.0%; Average loss: 2.9995 Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8642 Iteration: 3404; Percent complete: 85.1%; Average loss: 2.7667 Iteration: 3405; Percent complete: 85.1%; Average loss: 2.9222 Iteration: 3406; Percent complete: 85.2%; Average loss: 2.8285 Iteration: 3407; Percent complete: 85.2%; Average loss: 2.7064 Iteration: 3408; Percent complete: 85.2%; Average loss: 2.8224 Iteration: 3409; Percent complete: 85.2%; Average loss: 2.7011 Iteration: 3410; Percent complete: 85.2%; Average loss: 2.8177 Iteration: 3411; Percent complete: 85.3%; Average loss: 2.7281 Iteration: 3412; Percent complete: 85.3%; Average loss: 2.5660 Iteration: 3413; Percent complete: 85.3%; Average loss: 2.7405 Iteration: 3414; Percent complete: 85.4%; Average loss: 2.6388 Iteration: 3415; Percent complete: 85.4%; Average loss: 2.8238 Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8022 Iteration: 3417; Percent complete: 85.4%; Average loss: 2.9583 Iteration: 3418; Percent complete: 85.5%; Average loss: 2.7848 Iteration: 3419; Percent complete: 85.5%; Average loss: 3.0869 Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8273 Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7476 Iteration: 3422; Percent complete: 85.5%; Average loss: 3.0241 Iteration: 3423; Percent complete: 85.6%; Average loss: 2.6781 Iteration: 3424; Percent complete: 85.6%; Average loss: 2.7106 Iteration: 3425; Percent complete: 85.6%; Average loss: 3.0663 Iteration: 3426; Percent complete: 85.7%; Average loss: 2.9754 Iteration: 3427; Percent complete: 85.7%; Average loss: 2.8757 Iteration: 3428; Percent complete: 85.7%; Average loss: 2.9953 Iteration: 3429; Percent complete: 85.7%; Average loss: 2.7984 Iteration: 3430; Percent complete: 85.8%; Average loss: 2.9903 Iteration: 3431; Percent complete: 85.8%; Average loss: 2.4823 Iteration: 3432; Percent complete: 85.8%; Average loss: 2.7056 Iteration: 3433; Percent complete: 85.8%; Average loss: 2.7967 Iteration: 3434; Percent complete: 85.9%; Average loss: 2.7027 Iteration: 3435; Percent complete: 85.9%; Average loss: 3.0630 Iteration: 3436; Percent complete: 85.9%; Average loss: 2.8327 Iteration: 3437; Percent complete: 85.9%; Average loss: 2.7289 Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8060 Iteration: 3439; Percent complete: 86.0%; Average loss: 2.7837 Iteration: 3440; Percent complete: 86.0%; Average loss: 2.7605 Iteration: 3441; Percent complete: 86.0%; Average loss: 2.7223 Iteration: 3442; Percent complete: 86.1%; Average loss: 2.9967 Iteration: 3443; Percent complete: 86.1%; Average loss: 2.7990 Iteration: 3444; Percent complete: 86.1%; Average loss: 2.6517 Iteration: 3445; Percent complete: 86.1%; Average loss: 2.7093 Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5278 Iteration: 3447; Percent complete: 86.2%; Average loss: 3.1060 Iteration: 3448; Percent complete: 86.2%; Average loss: 2.6275 Iteration: 3449; Percent complete: 86.2%; Average loss: 2.7917 Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8431 Iteration: 3451; Percent complete: 86.3%; Average loss: 2.6315 Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8888 Iteration: 3453; Percent complete: 86.3%; Average loss: 2.8647 Iteration: 3454; Percent complete: 86.4%; Average loss: 2.4642 Iteration: 3455; Percent complete: 86.4%; Average loss: 2.7782 Iteration: 3456; Percent complete: 86.4%; Average loss: 2.8487 Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8076 Iteration: 3458; Percent complete: 86.5%; Average loss: 2.4438 Iteration: 3459; Percent complete: 86.5%; Average loss: 2.7463 Iteration: 3460; Percent complete: 86.5%; Average loss: 2.6891 Iteration: 3461; Percent complete: 86.5%; Average loss: 2.6447 Iteration: 3462; Percent complete: 86.6%; Average loss: 2.6058 Iteration: 3463; Percent complete: 86.6%; Average loss: 2.7323 Iteration: 3464; Percent complete: 86.6%; Average loss: 2.7739 Iteration: 3465; Percent complete: 86.6%; Average loss: 2.7082 Iteration: 3466; Percent complete: 86.7%; Average loss: 2.8581 Iteration: 3467; Percent complete: 86.7%; Average loss: 3.0025 Iteration: 3468; Percent complete: 86.7%; Average loss: 2.7160 Iteration: 3469; Percent complete: 86.7%; Average loss: 2.7803 Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7233 Iteration: 3471; Percent complete: 86.8%; Average loss: 2.5267 Iteration: 3472; Percent complete: 86.8%; Average loss: 2.7375 Iteration: 3473; Percent complete: 86.8%; Average loss: 2.7123 Iteration: 3474; Percent complete: 86.9%; Average loss: 2.8499 Iteration: 3475; Percent complete: 86.9%; Average loss: 2.5721 Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7998 Iteration: 3477; Percent complete: 86.9%; Average loss: 2.8672 Iteration: 3478; Percent complete: 87.0%; Average loss: 2.6062 Iteration: 3479; Percent complete: 87.0%; Average loss: 2.8151 Iteration: 3480; Percent complete: 87.0%; Average loss: 2.7301 Iteration: 3481; Percent complete: 87.0%; Average loss: 2.7585 Iteration: 3482; Percent complete: 87.1%; Average loss: 2.8795 Iteration: 3483; Percent complete: 87.1%; Average loss: 2.6545 Iteration: 3484; Percent complete: 87.1%; Average loss: 2.7672 Iteration: 3485; Percent complete: 87.1%; Average loss: 2.7513 Iteration: 3486; Percent complete: 87.2%; Average loss: 2.8752 Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8168 Iteration: 3488; Percent complete: 87.2%; Average loss: 2.9565 Iteration: 3489; Percent complete: 87.2%; Average loss: 2.7897 Iteration: 3490; Percent complete: 87.2%; Average loss: 2.8318 Iteration: 3491; Percent complete: 87.3%; Average loss: 2.4434 Iteration: 3492; Percent complete: 87.3%; Average loss: 2.7260 Iteration: 3493; Percent complete: 87.3%; Average loss: 2.6695 Iteration: 3494; Percent complete: 87.4%; Average loss: 2.9198 Iteration: 3495; Percent complete: 87.4%; Average loss: 2.6407 Iteration: 3496; Percent complete: 87.4%; Average loss: 3.1342 Iteration: 3497; Percent complete: 87.4%; Average loss: 2.6249 Iteration: 3498; Percent complete: 87.5%; Average loss: 3.0354 Iteration: 3499; Percent complete: 87.5%; Average loss: 2.6396 Iteration: 3500; Percent complete: 87.5%; Average loss: 2.7259 Iteration: 3501; Percent complete: 87.5%; Average loss: 2.7380 Iteration: 3502; Percent complete: 87.5%; Average loss: 2.9445 Iteration: 3503; Percent complete: 87.6%; Average loss: 2.5588 Iteration: 3504; Percent complete: 87.6%; Average loss: 2.8837 Iteration: 3505; Percent complete: 87.6%; Average loss: 2.7732 Iteration: 3506; Percent complete: 87.6%; Average loss: 2.7966 Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8140 Iteration: 3508; Percent complete: 87.7%; Average loss: 2.7502 Iteration: 3509; Percent complete: 87.7%; Average loss: 2.8737 Iteration: 3510; Percent complete: 87.8%; Average loss: 2.5366 Iteration: 3511; Percent complete: 87.8%; Average loss: 2.8272 Iteration: 3512; Percent complete: 87.8%; Average loss: 2.8242 Iteration: 3513; Percent complete: 87.8%; Average loss: 2.5206 Iteration: 3514; Percent complete: 87.8%; Average loss: 2.7685 Iteration: 3515; Percent complete: 87.9%; Average loss: 2.5191 Iteration: 3516; Percent complete: 87.9%; Average loss: 2.6273 Iteration: 3517; Percent complete: 87.9%; Average loss: 2.7113 Iteration: 3518; Percent complete: 87.9%; Average loss: 2.7691 Iteration: 3519; Percent complete: 88.0%; Average loss: 2.7507 Iteration: 3520; Percent complete: 88.0%; Average loss: 2.6292 Iteration: 3521; Percent complete: 88.0%; Average loss: 2.7815 Iteration: 3522; Percent complete: 88.0%; Average loss: 2.8682 Iteration: 3523; Percent complete: 88.1%; Average loss: 2.6226 Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7736 Iteration: 3525; Percent complete: 88.1%; Average loss: 2.5896 Iteration: 3526; Percent complete: 88.1%; Average loss: 2.7114 Iteration: 3527; Percent complete: 88.2%; Average loss: 2.7527 Iteration: 3528; Percent complete: 88.2%; Average loss: 2.9691 Iteration: 3529; Percent complete: 88.2%; Average loss: 2.7432 Iteration: 3530; Percent complete: 88.2%; Average loss: 2.7543 Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7214 Iteration: 3532; Percent complete: 88.3%; Average loss: 2.7186 Iteration: 3533; Percent complete: 88.3%; Average loss: 2.8295 Iteration: 3534; Percent complete: 88.3%; Average loss: 2.8928 Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8877 Iteration: 3536; Percent complete: 88.4%; Average loss: 2.8401 Iteration: 3537; Percent complete: 88.4%; Average loss: 2.6779 Iteration: 3538; Percent complete: 88.4%; Average loss: 2.7671 Iteration: 3539; Percent complete: 88.5%; Average loss: 2.9584 Iteration: 3540; Percent complete: 88.5%; Average loss: 2.5945 Iteration: 3541; Percent complete: 88.5%; Average loss: 2.5690 Iteration: 3542; Percent complete: 88.5%; Average loss: 2.5005 Iteration: 3543; Percent complete: 88.6%; Average loss: 2.8044 Iteration: 3544; Percent complete: 88.6%; Average loss: 2.5951 Iteration: 3545; Percent complete: 88.6%; Average loss: 2.5640 Iteration: 3546; Percent complete: 88.6%; Average loss: 2.8776 Iteration: 3547; Percent complete: 88.7%; Average loss: 2.7037 Iteration: 3548; Percent complete: 88.7%; Average loss: 2.7002 Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7734 Iteration: 3550; Percent complete: 88.8%; Average loss: 2.9073 Iteration: 3551; Percent complete: 88.8%; Average loss: 2.7140 Iteration: 3552; Percent complete: 88.8%; Average loss: 2.5015 Iteration: 3553; Percent complete: 88.8%; Average loss: 2.8978 Iteration: 3554; Percent complete: 88.8%; Average loss: 2.7488 Iteration: 3555; Percent complete: 88.9%; Average loss: 2.5834 Iteration: 3556; Percent complete: 88.9%; Average loss: 2.7427 Iteration: 3557; Percent complete: 88.9%; Average loss: 2.7349 Iteration: 3558; Percent complete: 88.9%; Average loss: 2.6462 Iteration: 3559; Percent complete: 89.0%; Average loss: 2.7126 Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6679 Iteration: 3561; Percent complete: 89.0%; Average loss: 3.0023 Iteration: 3562; Percent complete: 89.0%; Average loss: 2.8293 Iteration: 3563; Percent complete: 89.1%; Average loss: 2.9573 Iteration: 3564; Percent complete: 89.1%; Average loss: 2.8934 Iteration: 3565; Percent complete: 89.1%; Average loss: 3.0389 Iteration: 3566; Percent complete: 89.1%; Average loss: 2.5417 Iteration: 3567; Percent complete: 89.2%; Average loss: 2.4963 Iteration: 3568; Percent complete: 89.2%; Average loss: 2.5774 Iteration: 3569; Percent complete: 89.2%; Average loss: 2.7792 Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8621 Iteration: 3571; Percent complete: 89.3%; Average loss: 2.6069 Iteration: 3572; Percent complete: 89.3%; Average loss: 3.0751 Iteration: 3573; Percent complete: 89.3%; Average loss: 2.7948 Iteration: 3574; Percent complete: 89.3%; Average loss: 2.6695 Iteration: 3575; Percent complete: 89.4%; Average loss: 2.7968 Iteration: 3576; Percent complete: 89.4%; Average loss: 2.8746 Iteration: 3577; Percent complete: 89.4%; Average loss: 2.8912 Iteration: 3578; Percent complete: 89.5%; Average loss: 2.9147 Iteration: 3579; Percent complete: 89.5%; Average loss: 2.8887 Iteration: 3580; Percent complete: 89.5%; Average loss: 2.6602 Iteration: 3581; Percent complete: 89.5%; Average loss: 2.8228 Iteration: 3582; Percent complete: 89.5%; Average loss: 2.9505 Iteration: 3583; Percent complete: 89.6%; Average loss: 2.8533 Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7067 Iteration: 3585; Percent complete: 89.6%; Average loss: 2.7401 Iteration: 3586; Percent complete: 89.6%; Average loss: 2.6808 Iteration: 3587; Percent complete: 89.7%; Average loss: 2.6501 Iteration: 3588; Percent complete: 89.7%; Average loss: 2.4566 Iteration: 3589; Percent complete: 89.7%; Average loss: 2.8453 Iteration: 3590; Percent complete: 89.8%; Average loss: 2.9385 Iteration: 3591; Percent complete: 89.8%; Average loss: 2.5139 Iteration: 3592; Percent complete: 89.8%; Average loss: 2.6738 Iteration: 3593; Percent complete: 89.8%; Average loss: 2.7439 Iteration: 3594; Percent complete: 89.8%; Average loss: 2.6203 Iteration: 3595; Percent complete: 89.9%; Average loss: 2.7321 Iteration: 3596; Percent complete: 89.9%; Average loss: 2.8466 Iteration: 3597; Percent complete: 89.9%; Average loss: 2.8582 Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7680 Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6733 Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7236 Iteration: 3601; Percent complete: 90.0%; Average loss: 2.7208 Iteration: 3602; Percent complete: 90.0%; Average loss: 2.8232 Iteration: 3603; Percent complete: 90.1%; Average loss: 2.8498 Iteration: 3604; Percent complete: 90.1%; Average loss: 2.8375 Iteration: 3605; Percent complete: 90.1%; Average loss: 2.5860 Iteration: 3606; Percent complete: 90.1%; Average loss: 2.7870 Iteration: 3607; Percent complete: 90.2%; Average loss: 2.3556 Iteration: 3608; Percent complete: 90.2%; Average loss: 2.7868 Iteration: 3609; Percent complete: 90.2%; Average loss: 2.7471 Iteration: 3610; Percent complete: 90.2%; Average loss: 2.7859 Iteration: 3611; Percent complete: 90.3%; Average loss: 2.6757 Iteration: 3612; Percent complete: 90.3%; Average loss: 2.8288 Iteration: 3613; Percent complete: 90.3%; Average loss: 3.0294 Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6438 Iteration: 3615; Percent complete: 90.4%; Average loss: 2.9528 Iteration: 3616; Percent complete: 90.4%; Average loss: 2.6048 Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6991 Iteration: 3618; Percent complete: 90.5%; Average loss: 2.5222 Iteration: 3619; Percent complete: 90.5%; Average loss: 2.6258 Iteration: 3620; Percent complete: 90.5%; Average loss: 2.6860 Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7846 Iteration: 3622; Percent complete: 90.5%; Average loss: 2.7449 Iteration: 3623; Percent complete: 90.6%; Average loss: 2.7723 Iteration: 3624; Percent complete: 90.6%; Average loss: 2.5565 Iteration: 3625; Percent complete: 90.6%; Average loss: 2.4947 Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6439 Iteration: 3627; Percent complete: 90.7%; Average loss: 2.6985 Iteration: 3628; Percent complete: 90.7%; Average loss: 2.7331 Iteration: 3629; Percent complete: 90.7%; Average loss: 2.7844 Iteration: 3630; Percent complete: 90.8%; Average loss: 2.7134 Iteration: 3631; Percent complete: 90.8%; Average loss: 2.6845 Iteration: 3632; Percent complete: 90.8%; Average loss: 2.5318 Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7819 Iteration: 3634; Percent complete: 90.8%; Average loss: 2.8330 Iteration: 3635; Percent complete: 90.9%; Average loss: 2.6000 Iteration: 3636; Percent complete: 90.9%; Average loss: 2.4740 Iteration: 3637; Percent complete: 90.9%; Average loss: 2.8126 Iteration: 3638; Percent complete: 91.0%; Average loss: 2.7081 Iteration: 3639; Percent complete: 91.0%; Average loss: 2.4977 Iteration: 3640; Percent complete: 91.0%; Average loss: 2.5915 Iteration: 3641; Percent complete: 91.0%; Average loss: 2.3142 Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7087 Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7890 Iteration: 3644; Percent complete: 91.1%; Average loss: 2.5196 Iteration: 3645; Percent complete: 91.1%; Average loss: 2.6911 Iteration: 3646; Percent complete: 91.1%; Average loss: 2.7318 Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6789 Iteration: 3648; Percent complete: 91.2%; Average loss: 2.6548 Iteration: 3649; Percent complete: 91.2%; Average loss: 2.5602 Iteration: 3650; Percent complete: 91.2%; Average loss: 2.5718 Iteration: 3651; Percent complete: 91.3%; Average loss: 2.6904 Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7021 Iteration: 3653; Percent complete: 91.3%; Average loss: 2.8412 Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6709 Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6757 Iteration: 3656; Percent complete: 91.4%; Average loss: 2.7199 Iteration: 3657; Percent complete: 91.4%; Average loss: 2.5588 Iteration: 3658; Percent complete: 91.5%; Average loss: 2.7798 Iteration: 3659; Percent complete: 91.5%; Average loss: 2.7170 Iteration: 3660; Percent complete: 91.5%; Average loss: 2.1945 Iteration: 3661; Percent complete: 91.5%; Average loss: 2.6969 Iteration: 3662; Percent complete: 91.5%; Average loss: 2.7817 Iteration: 3663; Percent complete: 91.6%; Average loss: 2.4590 Iteration: 3664; Percent complete: 91.6%; Average loss: 2.5740 Iteration: 3665; Percent complete: 91.6%; Average loss: 2.5805 Iteration: 3666; Percent complete: 91.6%; Average loss: 2.6993 Iteration: 3667; Percent complete: 91.7%; Average loss: 2.6136 Iteration: 3668; Percent complete: 91.7%; Average loss: 2.6798 Iteration: 3669; Percent complete: 91.7%; Average loss: 2.6772 Iteration: 3670; Percent complete: 91.8%; Average loss: 2.6185 Iteration: 3671; Percent complete: 91.8%; Average loss: 2.9239 Iteration: 3672; Percent complete: 91.8%; Average loss: 2.4581 Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6778 Iteration: 3674; Percent complete: 91.8%; Average loss: 2.5263 Iteration: 3675; Percent complete: 91.9%; Average loss: 2.8935 Iteration: 3676; Percent complete: 91.9%; Average loss: 2.6322 Iteration: 3677; Percent complete: 91.9%; Average loss: 2.8365 Iteration: 3678; Percent complete: 92.0%; Average loss: 2.5844 Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7383 Iteration: 3680; Percent complete: 92.0%; Average loss: 2.5843 Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7317 Iteration: 3682; Percent complete: 92.0%; Average loss: 2.7277 Iteration: 3683; Percent complete: 92.1%; Average loss: 2.6092 Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6693 Iteration: 3685; Percent complete: 92.1%; Average loss: 2.4912 Iteration: 3686; Percent complete: 92.2%; Average loss: 2.6113 Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6892 Iteration: 3688; Percent complete: 92.2%; Average loss: 2.5918 Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5398 Iteration: 3690; Percent complete: 92.2%; Average loss: 2.7778 Iteration: 3691; Percent complete: 92.3%; Average loss: 2.6075 Iteration: 3692; Percent complete: 92.3%; Average loss: 2.6261 Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7985 Iteration: 3694; Percent complete: 92.3%; Average loss: 2.6538 Iteration: 3695; Percent complete: 92.4%; Average loss: 2.8084 Iteration: 3696; Percent complete: 92.4%; Average loss: 2.8812 Iteration: 3697; Percent complete: 92.4%; Average loss: 2.8325 Iteration: 3698; Percent complete: 92.5%; Average loss: 2.8862 Iteration: 3699; Percent complete: 92.5%; Average loss: 2.8058 Iteration: 3700; Percent complete: 92.5%; Average loss: 2.7812 Iteration: 3701; Percent complete: 92.5%; Average loss: 2.7479 Iteration: 3702; Percent complete: 92.5%; Average loss: 2.9007 Iteration: 3703; Percent complete: 92.6%; Average loss: 2.7062 Iteration: 3704; Percent complete: 92.6%; Average loss: 2.6978 Iteration: 3705; Percent complete: 92.6%; Average loss: 3.0413 Iteration: 3706; Percent complete: 92.7%; Average loss: 2.6656 Iteration: 3707; Percent complete: 92.7%; Average loss: 2.7610 Iteration: 3708; Percent complete: 92.7%; Average loss: 2.7962 Iteration: 3709; Percent complete: 92.7%; Average loss: 2.6843 Iteration: 3710; Percent complete: 92.8%; Average loss: 2.6062 Iteration: 3711; Percent complete: 92.8%; Average loss: 2.7765 Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6372 Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6883 Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5702 Iteration: 3715; Percent complete: 92.9%; Average loss: 2.7026 Iteration: 3716; Percent complete: 92.9%; Average loss: 2.8407 Iteration: 3717; Percent complete: 92.9%; Average loss: 2.6249 Iteration: 3718; Percent complete: 93.0%; Average loss: 2.7596 Iteration: 3719; Percent complete: 93.0%; Average loss: 2.8515 Iteration: 3720; Percent complete: 93.0%; Average loss: 2.5251 Iteration: 3721; Percent complete: 93.0%; Average loss: 2.8361 Iteration: 3722; Percent complete: 93.0%; Average loss: 2.3973 Iteration: 3723; Percent complete: 93.1%; Average loss: 2.7257 Iteration: 3724; Percent complete: 93.1%; Average loss: 2.7930 Iteration: 3725; Percent complete: 93.1%; Average loss: 2.7132 Iteration: 3726; Percent complete: 93.2%; Average loss: 2.8860 Iteration: 3727; Percent complete: 93.2%; Average loss: 2.5148 Iteration: 3728; Percent complete: 93.2%; Average loss: 2.9264 Iteration: 3729; Percent complete: 93.2%; Average loss: 2.7717 Iteration: 3730; Percent complete: 93.2%; Average loss: 2.4697 Iteration: 3731; Percent complete: 93.3%; Average loss: 2.8212 Iteration: 3732; Percent complete: 93.3%; Average loss: 2.6055 Iteration: 3733; Percent complete: 93.3%; Average loss: 2.7805 Iteration: 3734; Percent complete: 93.3%; Average loss: 2.6400 Iteration: 3735; Percent complete: 93.4%; Average loss: 2.8751 Iteration: 3736; Percent complete: 93.4%; Average loss: 2.9432 Iteration: 3737; Percent complete: 93.4%; Average loss: 2.8112 Iteration: 3738; Percent complete: 93.5%; Average loss: 2.6234 Iteration: 3739; Percent complete: 93.5%; Average loss: 2.6543 Iteration: 3740; Percent complete: 93.5%; Average loss: 2.7528 Iteration: 3741; Percent complete: 93.5%; Average loss: 2.5411 Iteration: 3742; Percent complete: 93.5%; Average loss: 2.4673 Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6613 Iteration: 3744; Percent complete: 93.6%; Average loss: 2.7095 Iteration: 3745; Percent complete: 93.6%; Average loss: 2.9082 Iteration: 3746; Percent complete: 93.7%; Average loss: 2.6593 Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6911 Iteration: 3748; Percent complete: 93.7%; Average loss: 2.5851 Iteration: 3749; Percent complete: 93.7%; Average loss: 2.6174 Iteration: 3750; Percent complete: 93.8%; Average loss: 2.7568 Iteration: 3751; Percent complete: 93.8%; Average loss: 2.7105 Iteration: 3752; Percent complete: 93.8%; Average loss: 2.7509 Iteration: 3753; Percent complete: 93.8%; Average loss: 2.6668 Iteration: 3754; Percent complete: 93.8%; Average loss: 2.6777 Iteration: 3755; Percent complete: 93.9%; Average loss: 2.7127 Iteration: 3756; Percent complete: 93.9%; Average loss: 2.7050 Iteration: 3757; Percent complete: 93.9%; Average loss: 2.9084 Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6841 Iteration: 3759; Percent complete: 94.0%; Average loss: 2.5651 Iteration: 3760; Percent complete: 94.0%; Average loss: 2.5326 Iteration: 3761; Percent complete: 94.0%; Average loss: 2.5820 Iteration: 3762; Percent complete: 94.0%; Average loss: 2.5241 Iteration: 3763; Percent complete: 94.1%; Average loss: 2.6565 Iteration: 3764; Percent complete: 94.1%; Average loss: 2.6826 Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6434 Iteration: 3766; Percent complete: 94.2%; Average loss: 2.6483 Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6925 Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5389 Iteration: 3769; Percent complete: 94.2%; Average loss: 2.6955 Iteration: 3770; Percent complete: 94.2%; Average loss: 2.7920 Iteration: 3771; Percent complete: 94.3%; Average loss: 2.7962 Iteration: 3772; Percent complete: 94.3%; Average loss: 2.7930 Iteration: 3773; Percent complete: 94.3%; Average loss: 2.6847 Iteration: 3774; Percent complete: 94.3%; Average loss: 2.7743 Iteration: 3775; Percent complete: 94.4%; Average loss: 2.6346 Iteration: 3776; Percent complete: 94.4%; Average loss: 2.6962 Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7107 Iteration: 3778; Percent complete: 94.5%; Average loss: 2.6187 Iteration: 3779; Percent complete: 94.5%; Average loss: 2.7352 Iteration: 3780; Percent complete: 94.5%; Average loss: 2.7330 Iteration: 3781; Percent complete: 94.5%; Average loss: 2.7142 Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6425 Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6270 Iteration: 3784; Percent complete: 94.6%; Average loss: 2.7350 Iteration: 3785; Percent complete: 94.6%; Average loss: 2.6032 Iteration: 3786; Percent complete: 94.7%; Average loss: 2.5346 Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7918 Iteration: 3788; Percent complete: 94.7%; Average loss: 2.6616 Iteration: 3789; Percent complete: 94.7%; Average loss: 2.6691 Iteration: 3790; Percent complete: 94.8%; Average loss: 2.5599 Iteration: 3791; Percent complete: 94.8%; Average loss: 2.6638 Iteration: 3792; Percent complete: 94.8%; Average loss: 2.6441 Iteration: 3793; Percent complete: 94.8%; Average loss: 2.6998 Iteration: 3794; Percent complete: 94.8%; Average loss: 2.7123 Iteration: 3795; Percent complete: 94.9%; Average loss: 2.7011 Iteration: 3796; Percent complete: 94.9%; Average loss: 2.5441 Iteration: 3797; Percent complete: 94.9%; Average loss: 2.4698 Iteration: 3798; Percent complete: 95.0%; Average loss: 2.6847 Iteration: 3799; Percent complete: 95.0%; Average loss: 2.7156 Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6954 Iteration: 3801; Percent complete: 95.0%; Average loss: 2.7549 Iteration: 3802; Percent complete: 95.0%; Average loss: 2.6027 Iteration: 3803; Percent complete: 95.1%; Average loss: 2.6849 Iteration: 3804; Percent complete: 95.1%; Average loss: 2.7157 Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6573 Iteration: 3806; Percent complete: 95.2%; Average loss: 2.6000 Iteration: 3807; Percent complete: 95.2%; Average loss: 2.8664 Iteration: 3808; Percent complete: 95.2%; Average loss: 2.5344 Iteration: 3809; Percent complete: 95.2%; Average loss: 2.7704 Iteration: 3810; Percent complete: 95.2%; Average loss: 2.6243 Iteration: 3811; Percent complete: 95.3%; Average loss: 2.2530 Iteration: 3812; Percent complete: 95.3%; Average loss: 2.6777 Iteration: 3813; Percent complete: 95.3%; Average loss: 2.5203 Iteration: 3814; Percent complete: 95.3%; Average loss: 2.7517 Iteration: 3815; Percent complete: 95.4%; Average loss: 2.4611 Iteration: 3816; Percent complete: 95.4%; Average loss: 2.5626 Iteration: 3817; Percent complete: 95.4%; Average loss: 2.2729 Iteration: 3818; Percent complete: 95.5%; Average loss: 2.7028 Iteration: 3819; Percent complete: 95.5%; Average loss: 2.6521 Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6974 Iteration: 3821; Percent complete: 95.5%; Average loss: 2.5632 Iteration: 3822; Percent complete: 95.5%; Average loss: 2.7989 Iteration: 3823; Percent complete: 95.6%; Average loss: 2.5610 Iteration: 3824; Percent complete: 95.6%; Average loss: 2.7097 Iteration: 3825; Percent complete: 95.6%; Average loss: 2.8630 Iteration: 3826; Percent complete: 95.7%; Average loss: 2.7525 Iteration: 3827; Percent complete: 95.7%; Average loss: 2.7541 Iteration: 3828; Percent complete: 95.7%; Average loss: 2.7641 Iteration: 3829; Percent complete: 95.7%; Average loss: 2.7419 Iteration: 3830; Percent complete: 95.8%; Average loss: 2.4675 Iteration: 3831; Percent complete: 95.8%; Average loss: 2.5662 Iteration: 3832; Percent complete: 95.8%; Average loss: 2.5485 Iteration: 3833; Percent complete: 95.8%; Average loss: 2.6425 Iteration: 3834; Percent complete: 95.9%; Average loss: 2.5751 Iteration: 3835; Percent complete: 95.9%; Average loss: 2.5921 Iteration: 3836; Percent complete: 95.9%; Average loss: 2.4374 Iteration: 3837; Percent complete: 95.9%; Average loss: 2.5669 Iteration: 3838; Percent complete: 96.0%; Average loss: 2.4904 Iteration: 3839; Percent complete: 96.0%; Average loss: 2.5498 Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6096 Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7638 Iteration: 3842; Percent complete: 96.0%; Average loss: 2.7508 Iteration: 3843; Percent complete: 96.1%; Average loss: 2.8292 Iteration: 3844; Percent complete: 96.1%; Average loss: 2.6815 Iteration: 3845; Percent complete: 96.1%; Average loss: 2.9526 Iteration: 3846; Percent complete: 96.2%; Average loss: 2.4789 Iteration: 3847; Percent complete: 96.2%; Average loss: 2.4900 Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6332 Iteration: 3849; Percent complete: 96.2%; Average loss: 2.5425 Iteration: 3850; Percent complete: 96.2%; Average loss: 2.5071 Iteration: 3851; Percent complete: 96.3%; Average loss: 2.6764 Iteration: 3852; Percent complete: 96.3%; Average loss: 2.6213 Iteration: 3853; Percent complete: 96.3%; Average loss: 2.6265 Iteration: 3854; Percent complete: 96.4%; Average loss: 2.4832 Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7230 Iteration: 3856; Percent complete: 96.4%; Average loss: 2.6200 Iteration: 3857; Percent complete: 96.4%; Average loss: 2.6516 Iteration: 3858; Percent complete: 96.5%; Average loss: 2.7206 Iteration: 3859; Percent complete: 96.5%; Average loss: 2.7094 Iteration: 3860; Percent complete: 96.5%; Average loss: 2.7111 Iteration: 3861; Percent complete: 96.5%; Average loss: 2.5660 Iteration: 3862; Percent complete: 96.5%; Average loss: 2.4195 Iteration: 3863; Percent complete: 96.6%; Average loss: 2.8211 Iteration: 3864; Percent complete: 96.6%; Average loss: 2.6808 Iteration: 3865; Percent complete: 96.6%; Average loss: 2.4598 Iteration: 3866; Percent complete: 96.7%; Average loss: 2.6108 Iteration: 3867; Percent complete: 96.7%; Average loss: 2.7470 Iteration: 3868; Percent complete: 96.7%; Average loss: 2.7210 Iteration: 3869; Percent complete: 96.7%; Average loss: 2.5721 Iteration: 3870; Percent complete: 96.8%; Average loss: 2.5732 Iteration: 3871; Percent complete: 96.8%; Average loss: 2.6927 Iteration: 3872; Percent complete: 96.8%; Average loss: 2.8439 Iteration: 3873; Percent complete: 96.8%; Average loss: 2.7675 Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6716 Iteration: 3875; Percent complete: 96.9%; Average loss: 2.8848 Iteration: 3876; Percent complete: 96.9%; Average loss: 2.5334 Iteration: 3877; Percent complete: 96.9%; Average loss: 2.6027 Iteration: 3878; Percent complete: 97.0%; Average loss: 2.5435 Iteration: 3879; Percent complete: 97.0%; Average loss: 2.6349 Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6351 Iteration: 3881; Percent complete: 97.0%; Average loss: 2.4971 Iteration: 3882; Percent complete: 97.0%; Average loss: 2.4575 Iteration: 3883; Percent complete: 97.1%; Average loss: 2.6995 Iteration: 3884; Percent complete: 97.1%; Average loss: 2.5582 Iteration: 3885; Percent complete: 97.1%; Average loss: 2.4982 Iteration: 3886; Percent complete: 97.2%; Average loss: 2.8728 Iteration: 3887; Percent complete: 97.2%; Average loss: 2.6808 Iteration: 3888; Percent complete: 97.2%; Average loss: 2.5871 Iteration: 3889; Percent complete: 97.2%; Average loss: 2.6470 Iteration: 3890; Percent complete: 97.2%; Average loss: 2.7127 Iteration: 3891; Percent complete: 97.3%; Average loss: 2.6149 Iteration: 3892; Percent complete: 97.3%; Average loss: 2.6328 Iteration: 3893; Percent complete: 97.3%; Average loss: 2.5805 Iteration: 3894; Percent complete: 97.4%; Average loss: 2.5563 Iteration: 3895; Percent complete: 97.4%; Average loss: 2.4224 Iteration: 3896; Percent complete: 97.4%; Average loss: 2.6262 Iteration: 3897; Percent complete: 97.4%; Average loss: 2.3236 Iteration: 3898; Percent complete: 97.5%; Average loss: 2.4078 Iteration: 3899; Percent complete: 97.5%; Average loss: 2.7367 Iteration: 3900; Percent complete: 97.5%; Average loss: 2.5749 Iteration: 3901; Percent complete: 97.5%; Average loss: 2.5296 Iteration: 3902; Percent complete: 97.5%; Average loss: 2.6909 Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6074 Iteration: 3904; Percent complete: 97.6%; Average loss: 2.6102 Iteration: 3905; Percent complete: 97.6%; Average loss: 2.5208 Iteration: 3906; Percent complete: 97.7%; Average loss: 2.7649 Iteration: 3907; Percent complete: 97.7%; Average loss: 2.3827 Iteration: 3908; Percent complete: 97.7%; Average loss: 2.6105 Iteration: 3909; Percent complete: 97.7%; Average loss: 2.7437 Iteration: 3910; Percent complete: 97.8%; Average loss: 2.6139 Iteration: 3911; Percent complete: 97.8%; Average loss: 2.5302 Iteration: 3912; Percent complete: 97.8%; Average loss: 2.7742 Iteration: 3913; Percent complete: 97.8%; Average loss: 2.6821 Iteration: 3914; Percent complete: 97.9%; Average loss: 2.9002 Iteration: 3915; Percent complete: 97.9%; Average loss: 2.6962 Iteration: 3916; Percent complete: 97.9%; Average loss: 2.5186 Iteration: 3917; Percent complete: 97.9%; Average loss: 2.4853 Iteration: 3918; Percent complete: 98.0%; Average loss: 2.5459 Iteration: 3919; Percent complete: 98.0%; Average loss: 2.5058 Iteration: 3920; Percent complete: 98.0%; Average loss: 2.5340 Iteration: 3921; Percent complete: 98.0%; Average loss: 2.6730 Iteration: 3922; Percent complete: 98.0%; Average loss: 2.4618 Iteration: 3923; Percent complete: 98.1%; Average loss: 2.6910 Iteration: 3924; Percent complete: 98.1%; Average loss: 2.5462 Iteration: 3925; Percent complete: 98.1%; Average loss: 2.7956 Iteration: 3926; Percent complete: 98.2%; Average loss: 2.6415 Iteration: 3927; Percent complete: 98.2%; Average loss: 2.8166 Iteration: 3928; Percent complete: 98.2%; Average loss: 2.7054 Iteration: 3929; Percent complete: 98.2%; Average loss: 2.5100 Iteration: 3930; Percent complete: 98.2%; Average loss: 2.6257 Iteration: 3931; Percent complete: 98.3%; Average loss: 2.5922 Iteration: 3932; Percent complete: 98.3%; Average loss: 2.5230 Iteration: 3933; Percent complete: 98.3%; Average loss: 2.6372 Iteration: 3934; Percent complete: 98.4%; Average loss: 2.5645 Iteration: 3935; Percent complete: 98.4%; Average loss: 2.6899 Iteration: 3936; Percent complete: 98.4%; Average loss: 2.7416 Iteration: 3937; Percent complete: 98.4%; Average loss: 2.7057 Iteration: 3938; Percent complete: 98.5%; Average loss: 2.8595 Iteration: 3939; Percent complete: 98.5%; Average loss: 2.6063 Iteration: 3940; Percent complete: 98.5%; Average loss: 2.7413 Iteration: 3941; Percent complete: 98.5%; Average loss: 2.4071 Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5229 Iteration: 3943; Percent complete: 98.6%; Average loss: 2.5431 Iteration: 3944; Percent complete: 98.6%; Average loss: 2.8482 Iteration: 3945; Percent complete: 98.6%; Average loss: 2.7996 Iteration: 3946; Percent complete: 98.7%; Average loss: 2.7710 Iteration: 3947; Percent complete: 98.7%; Average loss: 2.7102 Iteration: 3948; Percent complete: 98.7%; Average loss: 2.6340 Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6396 Iteration: 3950; Percent complete: 98.8%; Average loss: 2.7269 Iteration: 3951; Percent complete: 98.8%; Average loss: 2.5554 Iteration: 3952; Percent complete: 98.8%; Average loss: 2.5558 Iteration: 3953; Percent complete: 98.8%; Average loss: 2.2954 Iteration: 3954; Percent complete: 98.9%; Average loss: 2.7160 Iteration: 3955; Percent complete: 98.9%; Average loss: 2.7652 Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6656 Iteration: 3957; Percent complete: 98.9%; Average loss: 2.4357 Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4752 Iteration: 3959; Percent complete: 99.0%; Average loss: 2.4079 Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6974 Iteration: 3961; Percent complete: 99.0%; Average loss: 2.6180 Iteration: 3962; Percent complete: 99.1%; Average loss: 2.5828 Iteration: 3963; Percent complete: 99.1%; Average loss: 2.7850 Iteration: 3964; Percent complete: 99.1%; Average loss: 2.6524 Iteration: 3965; Percent complete: 99.1%; Average loss: 2.6048 Iteration: 3966; Percent complete: 99.2%; Average loss: 2.4392 Iteration: 3967; Percent complete: 99.2%; Average loss: 2.7728 Iteration: 3968; Percent complete: 99.2%; Average loss: 2.5519 Iteration: 3969; Percent complete: 99.2%; Average loss: 2.7046 Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5091 Iteration: 3971; Percent complete: 99.3%; Average loss: 2.5323 Iteration: 3972; Percent complete: 99.3%; Average loss: 2.5706 Iteration: 3973; Percent complete: 99.3%; Average loss: 2.5353 Iteration: 3974; Percent complete: 99.4%; Average loss: 2.8306 Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5334 Iteration: 3976; Percent complete: 99.4%; Average loss: 2.5948 Iteration: 3977; Percent complete: 99.4%; Average loss: 2.8497 Iteration: 3978; Percent complete: 99.5%; Average loss: 2.7015 Iteration: 3979; Percent complete: 99.5%; Average loss: 2.6840 Iteration: 3980; Percent complete: 99.5%; Average loss: 2.5775 Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5468 Iteration: 3982; Percent complete: 99.6%; Average loss: 2.4048 Iteration: 3983; Percent complete: 99.6%; Average loss: 2.4671 Iteration: 3984; Percent complete: 99.6%; Average loss: 2.6611 Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6338 Iteration: 3986; Percent complete: 99.7%; Average loss: 2.7020 Iteration: 3987; Percent complete: 99.7%; Average loss: 2.5422 Iteration: 3988; Percent complete: 99.7%; Average loss: 2.6078 Iteration: 3989; Percent complete: 99.7%; Average loss: 2.4650 Iteration: 3990; Percent complete: 99.8%; Average loss: 2.5320 Iteration: 3991; Percent complete: 99.8%; Average loss: 2.5551 Iteration: 3992; Percent complete: 99.8%; Average loss: 2.7428 Iteration: 3993; Percent complete: 99.8%; Average loss: 2.5203 Iteration: 3994; Percent complete: 99.9%; Average loss: 2.4324 Iteration: 3995; Percent complete: 99.9%; Average loss: 2.7006 Iteration: 3996; Percent complete: 99.9%; Average loss: 2.4844 Iteration: 3997; Percent complete: 99.9%; Average loss: 2.4931 Iteration: 3998; Percent complete: 100.0%; Average loss: 2.6130 Iteration: 3999; Percent complete: 100.0%; Average loss: 2.5353 Iteration: 4000; Percent complete: 100.0%; Average loss: 2.6168 Run Evaluation# To chat with your model, run the following block. # Set dropout layers to ``eval`` mode encoder.eval() decoder.eval() # Initialize search module searcher = GreedySearchDecoder(encoder, decoder) # Begin chatting (uncomment and run the following line to begin) # evaluateInput(encoder, decoder, searcher, voc) Conclusion# That\u2019s all for this one, folks. Congratulations, you now know the fundamentals to building a generative chatbot model! If you\u2019re interested, you can try tailoring the chatbot\u2019s behavior by tweaking the model and training parameters and customizing the data that you train the model on. Check out the other tutorials for more cool deep learning applications in PyTorch! Total running time of the script: (2 minutes 23.402 seconds) Download Jupyter notebook: chatbot_tutorial.ipynb Download Python source code: chatbot_tutorial.py Download zipped: chatbot_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/chatbot_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>