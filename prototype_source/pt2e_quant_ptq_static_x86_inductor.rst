PyTorch 2.0 Export Post Training Static Quantization with X86 Backend through Inductor
========================================================================================

**Author**: `Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen Xia <https://github.com/Xia-Weiwen>`_, `Jiong Gong <https://github.com/jgong5>`_, `Jerry Zhang <https://github.com/jerryzh168>`_

Prerequisites
^^^^^^^^^^^^^^^

-  `PyTorch 2.0 Export Post Training Static Quantization <https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_static.html>`_
-  `TorchInductor and torch.compile concepts in PyTorch <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_

Introduction
^^^^^^^^^^^^^^

This tutorial introduces the steps for utilizing the PyTorch 2.0 Export Quantization flow to generate a quantized model customized
for the x86 inductor backend and explains how to lower the quantized model into the inductor.

The new quantization 2.0 flow uses the PT2 Export to capture the model into a graph and perform quantization transformations on top of the ATen graph. This approach is expected to have significantly higher model coverage, better programmability, and a simplified UX.
TorchInductor is the new compiler backend that compiles the FX Graphs generated by TorchDynamo into optimized C++/Triton kernels.

This flow of quantization 2.0 with Inductor mainly includes three steps:

- Step 1: Capture the FX Graph from the eager Model based on the `torch export mechanism <https://pytorch.org/docs/main/export.html>`_.
- Step 2: Apply the Quantization flow based on the captured FX Graph, including defining the backend-specific quantizer, generating the prepared model with observers,
  performing the prepared model's calibration, and converting the prepared model into the reference quantized model.
- Step 3: Lower the reference quantized model into inductor with the API ``torch.compile``.

The high-level architecture of this flow could look like this:

::

    float_model(Python)                               Input
        \                                              /
         \                                            /
    —--------------------------------------------------------
    |                   Capture FX Graph                    |
    —--------------------------------------------------------
                                |
                        FX Graph in ATen     
                                |            X86InductorQuantizer
                                |                 /
    —--------------------------------------------------------
    |                      prepare_pt2e                     |
    |                           |                           |
    |                     Calibrate/Train                   |
    |                           |                           |
    |                      convert_pt2e                     |
    —--------------------------------------------------------
                                |
                    Reference Quantized Model
                                |
    —--------------------------------------------------------
    |                    Lower into Inductor                |
    —--------------------------------------------------------
                                |
                             Inductor

Combining Quantization in PyTorch 2.0 Export and TorchInductor, we have flexibility and productivity with the new Quantization frontend
and outstanding out-of-box performance with the compiler backend. Especially on Intel fourth generation (SPR) Xeon processors which can
further boost the models' performance by leveraging the
`advanced-matrix-extensions <https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html>`_ feature.

Now, we will walk you through a step-by-step tutorial for how to use it with `torchvision resnet18 model <https://download.pytorch.org/models/resnet18-f37072fd.pth>`_.

1. Capture FX Graph
---------------------

We will start by performing the necessary imports, capturing the FX Graph from the eager module.

::

    import torch
    import torchvision.models as models
    import copy
    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
    import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
    from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer
    from torch._export import capture_pre_autograd_graph

    # Create the Eager Model
    model_name = "resnet18"
    model = models.__dict__[model_name](pretrained=True)

    # Set the model to eval mode
    model = model.eval()

    # Create the data, using the dummy data here as an example
    traced_bs = 50
    x = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)
    example_inputs = (x,)

    # Capture the FX Graph to be quantized
    with torch.no_grad():
         # if you are using the PyTorch nightlies or building from source with the pytorch master,
        # use the API of `capture_pre_autograd_graph`
        # Note 1: `capture_pre_autograd_graph` is also a short-term API, it will be updated to use the official `torch.export` API when that is ready.
        exported_model = capture_pre_autograd_graph(
            model,
            example_inputs
        )
        # Note 2: if you are using the PyTorch 2.1 release binary or building from source with the PyTorch 2.1 release branch,
        # please use the API of `torch._dynamo.export` to capture the FX Graph.
        # exported_model, guards = torch._dynamo.export(
        #     model,
        #     *copy.deepcopy(example_inputs),
        #     aten_graph=True,
        # )


Next, we will have the FX Module to be quantized.

2. Apply Quantization
----------------------------

After we capture the FX Module to be quantized, we will import the Backend Quantizer for X86 CPU and configure how to
quantize the model.

::

    quantizer = X86InductorQuantizer()
    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())

.. note::

   The default quantization configuration in ``X86InductorQuantizer`` uses 8-bits for both activations and weights.
  When Vector Neural Network Instruction is not available, the oneDNN backend silently chooses kernels that assume
  `multiplications are 7-bit x 8-bit <https://oneapi-src.github.io/oneDNN/dev_guide_int8_computations.html#inputs-of-mixed-type-u8-and-s8>`_. In other words, potential
  numeric saturation and accuracy issue may happen when running on CPU without Vector Neural Network Instruction.

After we import the backend-specific Quantizer, we will prepare the model for post-training static quantization.
``prepare_pt2e`` folds BatchNorm operators into preceding Conv2d operators, and inserts observers in appropriate places in the model.

::

    prepared_model = prepare_pt2e(exported_model, quantizer)

Now, we will calibrate the ``prepared_model`` after the observers are inserted in the model.

::

    # We use the dummy data as an example here
    prepared_model(*example_inputs)

    # Alternatively: user can define the dataset to calibrate
    # def calibrate(model, data_loader):
    #     model.eval()
    #     with torch.no_grad():
    #         for image, target in data_loader:
    #             model(image)
    # calibrate(prepared_model, data_loader_test)  # run calibration on sample data

Finally, we will convert the calibrated Model to a quantized Model. ``convert_pt2e`` takes a calibrated model and produces a reference quantized model.

::

    converted_model = convert_pt2e(prepared_model)

After these steps, we finished running the quantization flow and we will get the reference quantized model.


3. Lower into Inductor
------------------------

After we get the reference quantized model, we will further lower it to the inductor backend.

::

    optimized_model = torch.compile(converted_model)

    # Running some benchmark
    optimized_model(*example_inputs)


Put all these codes together, we will have the toy example code.
Please note that since the Inductor ``freeze`` feature does not turn on by default yet, run your example code with ``TORCHINDUCTOR_FREEZING=1``.
For example:

::

    TORCHINDUCTOR_FREEZING=1 python example_x86inductorquantizer_pytorch_2_1.py

4. Conclusion
---------------

With this tutorial, we introduce how to use Inductor with X86 CPU in PyTorch 2.0 Quantization. Users can learn about
how to use ``X86InductorQuantizer`` to quantize a model and lower it into the inductor with X86 CPU devices. If you need new quantization
features for the ``X86InductorQuantizer``, please submit a new issue in PyTorch GitHub repository labeled as ``oncall: quantization`` and assign
it to `Leslie Fang <https://github.com/leslie-fang-intel>`_.
