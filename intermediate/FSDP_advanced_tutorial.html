
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Advanced Model Training with Fully Sharded Data Parallel (FSDP) — PyTorch Tutorials 2.9.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=047068a3" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=c2809cec"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/FSDP_advanced_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/intermediate/FSDP_advanced_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.9.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started/locally">Get Started</a>
</li>
<li>
<a href="https://docs.pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.9.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Advanced...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Advanced Model Training with Fully Sharded Data Parallel (FSDP)" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
</div>
<script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/FSDP_advanced_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="advanced-model-training-with-fully-sharded-data-parallel-fsdp">
<h1>Advanced Model Training with Fully Sharded Data Parallel (FSDP)<a class="headerlink" href="#advanced-model-training-with-fully-sharded-data-parallel-fsdp" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Oct 31, 2024 | Last Updated: Oct 31, 2024 | Last Verified: Nov 05, 2024</p>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/HamidShojanazeri">Hamid Shojanazeri</a>, <a class="reference external" href="https://github.com/lessw2020">Less
Wright</a>, <a class="reference external" href="https://github.com/rohan-varma/">Rohan Varma</a>, <a class="reference external" href="https://github.com/zhaojuanmao">Yanli Zhao</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch’s Fully Sharded Data Parallel Module: A wrapper for sharding module parameters across</p></li>
</ul>
<p class="sd-card-text">data parallel workers.</p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch 1.12 or later</p></li>
<li><p class="sd-card-text">Read about the <a class="reference external" href="https://pytorch.org/docs/main/fsdp.html">FSDP API</a>.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>This tutorial introduces more advanced features of Fully Sharded Data Parallel
(FSDP) as part of the PyTorch 1.12 release. To get familiar with FSDP, please
refer to the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">FSDP getting started tutorial</a>.</p>
<p>In this tutorial, we fine-tune a HuggingFace (HF) T5 model with FSDP for text
summarization as a working example.</p>
<p>The example uses Wikihow and for simplicity, we will showcase the training on a
single node, P4dn instance with 8 A100 GPUs. We now have several blog posts (
<a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">(link1),</a>
<a class="reference external" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">(link2)</a>)
and a <a class="reference external" href="https://arxiv.org/abs/2304.11277">paper</a> on
large scale FSDP training on a multi-node cluster.</p>
<p>FSDP is a production ready package with focus on ease of use, performance, and
long-term support.  One of the main benefits of FSDP is reducing the memory
footprint on each GPU. This enables training of larger models with lower total
memory vs DDP, and leverages the overlap of computation and communication to
train models efficiently.
This reduced memory pressure can be leveraged to either train larger models or
increase batch size, potentially helping overall training throughput.  You can
read more about PyTorch FSDP <a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">here</a>.</p>
<section id="fsdp-features-in-this-tutorial">
<h2>FSDP Features in This Tutorial<a class="headerlink" href="#fsdp-features-in-this-tutorial" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Transformer Auto Wrap Policy</p></li>
<li><p>Mixed Precision</p></li>
<li><p>Initializing FSDP Model on Device</p></li>
<li><p>Sharding Strategy</p></li>
<li><p>Backward Prefetch</p></li>
<li><p>Model Checkpoint Saving via Streaming to CPU</p></li>
</ul>
</section>
<section id="recap-on-how-fsdp-works">
<h2>Recap on How FSDP Works<a class="headerlink" href="#recap-on-how-fsdp-works" title="Link to this heading">#</a></h2>
<p>At a high level FDSP works as follow:</p>
<p><em>In the constructor</em></p>
<ul class="simple">
<li><p>Shard model parameters and each rank only keeps its own shard</p></li>
</ul>
<p><em>In the forward pass</em></p>
<ul class="simple">
<li><p>Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full
parameter for this FSDP unit and run the forward computation</p></li>
<li><p>Discard the non-owned parameter shards it has just collected to free memory</p></li>
</ul>
<p><em>In the backward pass</em></p>
<ul class="simple">
<li><p>Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full
parameter in this FSDP unit and run backward computation</p></li>
<li><p>Discard non-owned parameters to free memory.</p></li>
<li><p>Run reduce_scatter to sync gradients</p></li>
</ul>
</section>
<section id="fine-tuning-hf-t5">
<h2>Fine-tuning HF T5<a class="headerlink" href="#fine-tuning-hf-t5" title="Link to this heading">#</a></h2>
<p>HF T5 pre-trained models are available in four different sizes, ranging from
small with 60 Million parameters to XXL with 11 Billion parameters. In this
tutorial, we demonstrate the fine-tuning of a T5 3B with FSDP for text
summarization using WikiHow dataset.  The main focus of this tutorial is to
highlight different available features in FSDP that are helpful for training
large scale model above 3B parameters. Also, we cover specific features for
Transformer based models. The code for this tutorial is available in  <a class="reference external" href="https://github.com/pytorch/examples/tree/main/distributed/FSDP/">Pytorch
examples</a>.</p>
<p><em>Setup</em></p>
<p>1.1 Install the latest PyTorch</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio
</pre></div>
</div>
<p>1.2 Dataset Setup</p>
<p>Please create a <cite>data</cite> folder, download the WikiHow dataset from <a class="reference external" href="https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358">wikihowAll.csv</a>  and
<a class="reference external" href="https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag">wikihowSep.cs</a>,
and place them in the <cite>data</cite> folder.  We will use the wikihow dataset from
<a class="reference external" href="https://github.com/pytorch/examples/blob/main/distributed/FSDP/summarization_dataset.py">summarization_dataset</a>.</p>
<p>Next, we add the following code snippets to a Python script “T5_training.py”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The full source code for this tutorial is available in <a class="reference external" href="https://github.com/pytorch/examples/tree/main/distributed/FSDP/">PyTorch examples</a>.</p>
</div>
<p>1.3  Import necessary packages:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.lr_scheduler</span><span class="w"> </span><span class="kn">import</span> <span class="n">StepLR</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.t5.modeling_t5</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5Block</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
 <span class="n">checkpoint_wrapper</span><span class="p">,</span>
 <span class="n">CheckpointImpl</span><span class="p">,</span>
 <span class="n">apply_activation_checkpointing_wrapper</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.fsdp</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">FullStateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.fsdp.wrap</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
    <span class="n">enable_wrap</span><span class="p">,</span>
    <span class="n">wrap</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">summarization_dataset</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.t5.modeling_t5</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5Block</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Type</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>
</pre></div>
</div>
<p>1.4 Distributed training setup.
Here we use two helper functions to initialize the processes for distributed
training,  and then to clean up after training completion.  In this tutorial, we
are going to use torch elastic, using <a class="reference external" href="https://pytorch.org/docs/stable/elastic/run.html">torchrun</a> , which will set the
worker <cite>RANK</cite> and <cite>WORLD_SIZE</cite> automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">():</span>
    <span class="c1"># initialize the process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
</pre></div>
</div>
<p>2.1  Set up the HuggingFace T5 model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span>  <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>
</pre></div>
</div>
<p>We also, add couple of helper functions here for date and formatting memory
metrics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_date_of_run</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""create date and time for file save uniqueness</span>
<span class="sd">    example: 2022-05-07-08:31:12_PM'</span>
<span class="sd">    """</span>
    <span class="n">date_of_run</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%Y-%m-</span><span class="si">%d</span><span class="s2">-%I:%M:%S_%p"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; current date and time of run = </span><span class="si">{</span><span class="n">date_of_run</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">date_of_run</span>

<span class="k">def</span><span class="w"> </span><span class="nf">format_metrics_to_gb</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""quick function to format numbers to gigabyte and round to 4 digit precision"""</span>
    <span class="n">metric_num</span> <span class="o">=</span> <span class="n">item</span> <span class="o">/</span> <span class="n">g_gigabyte</span>
    <span class="n">metric_num</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">metric_num</span><span class="p">,</span> <span class="n">ndigits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric_num</span>
</pre></div>
</div>
<p>2.2 Define a train function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'LOCAL_RANK'</span><span class="p">])</span>
    <span class="n">fsdp_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sampler</span><span class="p">:</span>
        <span class="n">sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">inner_pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)),</span> <span class="n">colour</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"r0 Training Epoch"</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"source_ids"</span><span class="p">],</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"source_mask"</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"target_ids"</span><span class="p">]</span> <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">"loss"</span><span class="p">]</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">inner_pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">fsdp_loss</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">inner_pbar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Train Epoch: </span><span class="se">\t</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="se">\t</span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_accuracy</span>
</pre></div>
</div>
<p>2.3 Define a validation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">validation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'LOCAL_RANK'</span><span class="p">])</span>
    <span class="n">fsdp_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">inner_pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)),</span> <span class="n">colour</span><span class="o">=</span><span class="s2">"green"</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Validation Epoch"</span>
        <span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"source_ids"</span><span class="p">],</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"source_mask"</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">"target_ids"</span><span class="p">])</span>
            <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s2">"loss"</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># sum up batch loss</span>
            <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">inner_pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">fsdp_loss</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">fsdp_loss</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">inner_pbar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Validation Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val_loss</span>
</pre></div>
</div>
<p>2.4 Define a distributed train function that wraps the model in FSDP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">fsdp_main</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>

    <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">setup_model</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'LOCAL_RANK'</span><span class="p">])</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'RANK'</span><span class="p">])</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'WORLD_SIZE'</span><span class="p">])</span>


    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">'wikihow'</span><span class="p">,</span> <span class="s1">'all'</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="s1">'data/'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Size of train dataset: "</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Size of Validation dataset: "</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


    <span class="c1">#wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">wikihow</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">,</span> <span class="mi">1500</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">wikihow</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s1">'validation'</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">sampler1</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">sampler2</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="n">setup</span><span class="p">()</span>


    <span class="n">train_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'batch_size'</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="s1">'sampler'</span><span class="p">:</span> <span class="n">sampler1</span><span class="p">}</span>
    <span class="n">test_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'batch_size'</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">test_batch_size</span><span class="p">,</span> <span class="s1">'sampler'</span><span class="p">:</span> <span class="n">sampler2</span><span class="p">}</span>
    <span class="n">cuda_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'num_workers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="s1">'pin_memory'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="s1">'shuffle'</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
    <span class="n">train_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cuda_kwargs</span><span class="p">)</span>
    <span class="n">test_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cuda_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span><span class="o">**</span><span class="n">train_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">test_kwargs</span><span class="p">)</span>

    <span class="n">t5_auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span>
            <span class="n">T5Block</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">sharding_strategy</span><span class="p">:</span> <span class="n">ShardingStrategy</span> <span class="o">=</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">SHARD_GRAD_OP</span> <span class="c1">#for Zero2 and FULL_SHARD for Zero3</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


    <span class="c1">#init_start_event = torch.cuda.Event(enable_timing=True)</span>
    <span class="c1">#init_end_event = torch.cuda.Event(enable_timing=True)</span>

    <span class="c1">#init_start_event.record()</span>

    <span class="n">bf16_ready</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span>
    <span class="ow">and</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="s2">"11.0"</span>
    <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_nccl_available</span><span class="p">()</span>
    <span class="ow">and</span> <span class="n">nccl</span><span class="o">.</span><span class="n">version</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">bf16_ready</span><span class="p">:</span>
        <span class="n">mp_policy</span> <span class="o">=</span> <span class="n">bfSixteen</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mp_policy</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># defaults to fp32</span>

    <span class="c1"># model is on CPU before input to FSDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mp_policy</span><span class="p">,</span>
        <span class="c1">#sharding_strategy=sharding_strategy,</span>
        <span class="n">device_id</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="n">curr_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="n">file_save_name</span> <span class="o">=</span> <span class="s2">"T5-model-"</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">time_of_run</span> <span class="o">=</span> <span class="n">get_date_of_run</span><span class="p">()</span>
        <span class="n">dur</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">train_acc_tracking</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">val_acc_tracking</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">training_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">track_memory</span><span class="p">:</span>
        <span class="n">mem_alloc_tracker</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">mem_reserved_tracker</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_validation</span><span class="p">:</span>
            <span class="n">curr_val_loss</span> <span class="o">=</span> <span class="n">validation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> completed...entering save and stats zone"</span><span class="p">)</span>

            <span class="n">dur</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
            <span class="n">train_acc_tracking</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_validation</span><span class="p">:</span>
                <span class="n">val_acc_tracking</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curr_val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">track_memory</span><span class="p">:</span>
                <span class="n">mem_alloc_tracker</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">format_metrics_to_gb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">mem_reserved_tracker</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">format_metrics_to_gb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">())</span>
                <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"completed save and stats zone..."</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">save_model</span> <span class="ow">and</span> <span class="n">curr_val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>

            <span class="c1"># save</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; entering save model state"</span><span class="p">)</span>

            <span class="n">save_policy</span> <span class="o">=</span> <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span> <span class="n">save_policy</span>
            <span class="p">):</span>
                <span class="n">cpu_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="c1">#print(f"saving process: rank {rank}  done w state_dict")</span>


            <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; saving model ..."</span><span class="p">)</span>
                <span class="n">currEpoch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">"-"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">curr_val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">))</span> <span class="o">+</span> <span class="s2">".pt"</span>
                <span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; attempting to save model prefix </span><span class="si">{</span><span class="n">currEpoch</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                <span class="n">save_name</span> <span class="o">=</span> <span class="n">file_save_name</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">+</span> <span class="n">time_of_run</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">+</span> <span class="n">currEpoch</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt; saving as model name </span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">cpu_state</span><span class="p">,</span> <span class="n">save_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">curr_val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>

            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">curr_val_loss</span>
            <span class="k">if</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--&gt;&gt;&gt;&gt; New Val Loss Record: </span><span class="si">{</span><span class="n">best_val_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
    <span class="n">cleanup</span><span class="p">()</span>
</pre></div>
</div>
<p>2.5 Parse the arguments and set the main function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="c1"># Training settings</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">'PyTorch T5 FSDP Example'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'N'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'input batch size for training (default: 64)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--test-batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'N'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'input batch size for testing (default: 1000)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--epochs'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'N'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'number of epochs to train (default: 3)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--lr'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.002</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'LR'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'learning rate (default: .002)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--gamma'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'M'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'Learning rate step gamma (default: 0.7)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--no-cuda'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">'store_true'</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'disables CUDA training'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--seed'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">'S'</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'random seed (default: 1)'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--track_memory'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">'store_false'</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'track the gpu memory'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--run_validation'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">'store_false'</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'running the validation'</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">'--save-model'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">'store_false'</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">'For Saving the current Model'</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">fsdp_main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>To run the the training using torchrun:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">4</span><span class="w">  </span>T5_training.py
</pre></div>
</div>
</section>
<section id="transformer-wrapping-policy">
<span id="id1"></span><h2>Transformer Wrapping Policy<a class="headerlink" href="#transformer-wrapping-policy" title="Link to this heading">#</a></h2>
<p>As discussed in the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">previous tutorial</a>,
auto_wrap_policy is one of the FSDP features that make it easy to automatically
shard a given model and put the model, optimizer and gradient shards into
distinct FSDP units.</p>
<p>For some architectures such as Transformer encoder-decoders, some parts of the
model such as embedding table is being shared with both encoder and decoder.  In
this case, we need to place the embedding table in the outer FSDP unit so that
it could be accessed from both encoder and decoder.  In addition, by registering
the layer class for a transformer, the sharding plan can be made much more
communication efficient.  In PyTorch 1.12, FSDP added this support and now we
have a wrapping policy for transfomers.</p>
<p>It can be created as follows, where the T5Block represents the T5 transformer
layer class (holding MHSA and FFN).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t5_auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span>
            <span class="n">T5Block</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
    <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">)</span>
</pre></div>
</div>
<p>To see the wrapped model, you can easily print the model and visually inspect
the sharding and FSDP units as well.</p>
</section>
<section id="mixed-precision">
<h2>Mixed Precision<a class="headerlink" href="#mixed-precision" title="Link to this heading">#</a></h2>
<p>FSDP supports flexible mixed precision training allowing for arbitrary reduced
precision types (such as fp16 or bfloat16). Currently BFloat16 is only available
on Ampere GPUs, so you need to confirm native support before you use it. On
V100s for example, BFloat16 can still be run but because it runs non-natively,
it can result in significant slowdowns.</p>
<p>To check if BFloat16 is natively supported, you can use the following :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bf16_ready</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span>
    <span class="ow">and</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="s2">"11.0"</span>
    <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_nccl_available</span><span class="p">()</span>
    <span class="ow">and</span> <span class="n">nccl</span><span class="o">.</span><span class="n">version</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>One of the advantages of mixed precision in FSDP is providing granular control
over different precision levels for parameters, gradients, and buffers as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fpSixteen</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
    <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="c1"># Gradient communication precision.</span>
    <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="c1"># Buffer precision.</span>
    <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">bfSixteen</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
    <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="c1"># Gradient communication precision.</span>
    <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="c1"># Buffer precision.</span>
    <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">fp32_policy</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
    <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="c1"># Gradient communication precision.</span>
    <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="c1"># Buffer precision.</span>
    <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Note that if a certain type (parameter, reduce, buffer) is not specified, they
will not be casted at all.</p>
<p>This flexibility allows users fine grained control, such as only setting
gradient communication to happen in reduced precision, and all parameters /
buffer computation to be done in full precision. This is potentially useful in
cases where intra-node communication is the main bottleneck and parameters /
buffers must be in full precision to avoid accuracy issues. This can be done
with the following policy:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">grad_bf16</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MixedPrecision<span class="o">(</span><span class="nv">reduce_dtype</span><span class="o">=</span>torch.bfloat16<span class="o">)</span>
</pre></div>
</div>
<p>In 2.4 we just add the relevant mixed precision policy to the FSDP wrapper:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
       <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">,</span>
       <span class="n">mixed_precision</span><span class="o">=</span><span class="n">bfSixteen</span><span class="p">)</span>
</pre></div>
</div>
<p>In our experiments, we have observed up to 4x speed up by using BFloat16 for
training and memory reduction of approximately 30% in some experiments that can
be used for batch size increases.</p>
</section>
<section id="intializing-fsdp-model-on-device">
<h2>Intializing FSDP Model on Device<a class="headerlink" href="#intializing-fsdp-model-on-device" title="Link to this heading">#</a></h2>
<p>In 1.12, FSDP supports a <cite>device_id</cite> argument meant to initialize input CPU
module on the device given by <cite>device_id</cite>. This is useful when the entire model
does not fit on a single GPU, but fits in a host’s CPU memory. When <cite>device_id</cite>
is specified, FSDP will move the model to the specified device on a per-FSDP
unit basis, avoiding GPU OOM issues while initializing several times faster than
CPU-based initialization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

 <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">bfSixteen</span><span class="p">,</span>
        <span class="n">device_id</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="sharding-strategy">
<h2>Sharding Strategy<a class="headerlink" href="#sharding-strategy" title="Link to this heading">#</a></h2>
<p>FSDP sharding strategy by default is set to fully shard the model parameters,
gradients and optimizer states get sharded across all ranks. (also termed Zero3
sharding). In case you are interested to have the Zero2 sharding strategy, where
only optimizer states and gradients are sharded, FSDP support this feature by
passing the Sharding strategy by using  “ShardingStrategy.SHARD_GRAD_OP”,
instead of “ShardingStrategy.FULL_SHARD” to the FSDP initialization  as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

 <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">bfSixteen</span><span class="p">,</span>
        <span class="n">device_id</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">(),</span>
        <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">SHARD_GRAD_OP</span> <span class="c1"># ZERO2)</span>
</pre></div>
</div>
<p>This will reduce the communication overhead in FSDP, in this case, it holds full
parameters after forward and through the backwards pass.</p>
<p>This saves an all_gather during backwards so there is less communication at the
cost of a higher memory footprint. Note that full model params are freed at the
end of backwards and all_gather will happen on the next forward pass.</p>
</section>
<section id="backward-prefetch">
<h2>Backward Prefetch<a class="headerlink" href="#backward-prefetch" title="Link to this heading">#</a></h2>
<p>The backward prefetch setting controls the timing of when the next FSDP unit’s
parameters should be requested.  By setting it to <cite>BACKWARD_PRE</cite>, the next
FSDP’s unit params can begin to be requested and arrive sooner before the
computation of the current unit starts. This overlaps the <cite>all_gather</cite>
communication and gradient computation which can increase the training speed in
exchange for slightly higher memory consumption. It can be utilized in the FSDP
wrapper in 2.4 as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

 <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">t5_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">bfSixteen</span><span class="p">,</span>
        <span class="n">device_id</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">(),</span>
        <span class="n">backward_prefetch</span> <span class="o">=</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">)</span>
</pre></div>
</div>
<p><cite>backward_prefetch</cite> has two modes, <cite>BACKWARD_PRE</cite> and <cite>BACKWARD_POST</cite>.
<cite>BACKWARD_POST</cite> means that the next FSDP unit’s params will not be requested
until the current FSDP unit processing is complete, thus minimizing memory
overhead.  In some cases, using <cite>BACKWARD_PRE</cite> can increase model training speed
up to 2-10%, with even higher speed improvements noted for larger models.</p>
</section>
<section id="model-checkpoint-saving-by-streaming-to-the-rank0-cpu">
<h2>Model Checkpoint Saving, by streaming to the Rank0 CPU<a class="headerlink" href="#model-checkpoint-saving-by-streaming-to-the-rank0-cpu" title="Link to this heading">#</a></h2>
<p>To save model checkpoints using FULL_STATE_DICT saving which saves model in the
same fashion as a local model, PyTorch 1.12 offers a few utilities to support
the saving of larger models.</p>
<p>First, a FullStateDictConfig can be specified, allowing the state_dict to be
populated on rank 0 only and offloaded to the CPU.</p>
<p>When using this configuration, FSDP will allgather model parameters, offloading
them to the CPU one by one, only on rank 0. When the state_dict is finally
saved, it will only be populated on rank 0 and contain CPU tensors. This avoids
potential OOM for models that are larger than a single GPU memory and allows
users to checkpoint models whose size is roughly the available CPU RAM on the
user’s machine.</p>
<p>This feature can be run as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_policy</span> <span class="o">=</span> <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span> <span class="n">save_policy</span>
        <span class="p">):</span>
            <span class="n">cpu_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
 <span class="n">save_name</span> <span class="o">=</span> <span class="n">file_save_name</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">+</span> <span class="n">time_of_run</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">+</span> <span class="n">currEpoch</span>
 <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">cpu_state</span><span class="p">,</span> <span class="n">save_name</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this tutorial, we have introduced many new features for FSDP available in
Pytorch 1.12 and used HF T5 as the running example.  Using the proper wrapping
policy especially for transformer models, along with mixed precision and
backward prefetch should speed up your training runs. Also, features such as
initializing the model on device, and checkpoint saving via streaming to CPU
should help to avoid OOM error in dealing with large models.</p>
<p>We are actively working to add new features to FSDP for the next release. If
you have feedback, feature requests, questions or are encountering issues
using FSDP, please feel free to contact us by opening an issue in the
<a class="reference external" href="https://github.com/pytorch/pytorch">PyTorch Github repository</a>.</p>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fsdp-features-in-this-tutorial">FSDP Features in This Tutorial</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-on-how-fsdp-works">Recap on How FSDP Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-hf-t5">Fine-tuning HF T5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-wrapping-policy">Transformer Wrapping Policy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision">Mixed Precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intializing-fsdp-model-on-device">Intializing FSDP Model on Device</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sharding-strategy">Sharding Strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-prefetch">Backward Prefetch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checkpoint-saving-by-streaming-to-the-rank0-cpu">Model Checkpoint Saving, by streaming to the Rank0 CPU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Advanced Model Training with Fully Sharded Data Parallel (FSDP)",
       "headline": "Advanced Model Training with Fully Sharded Data Parallel (FSDP)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/FSDP_advanced_tutorial.html",
       "articleBody": "Advanced Model Training with Fully Sharded Data Parallel (FSDP)# Author: Hamid Shojanazeri, Less Wright, Rohan Varma, Yanli Zhao What you will learn PyTorch\u2019s Fully Sharded Data Parallel Module: A wrapper for sharding module parameters across data parallel workers. Prerequisites PyTorch 1.12 or later Read about the FSDP API. This tutorial introduces more advanced features of Fully Sharded Data Parallel (FSDP) as part of the PyTorch 1.12 release. To get familiar with FSDP, please refer to the FSDP getting started tutorial. In this tutorial, we fine-tune a HuggingFace (HF) T5 model with FSDP for text summarization as a working example. The example uses Wikihow and for simplicity, we will showcase the training on a single node, P4dn instance with 8 A100 GPUs. We now have several blog posts ( (link1), (link2)) and a paper on large scale FSDP training on a multi-node cluster. FSDP is a production ready package with focus on ease of use, performance, and long-term support. One of the main benefits of FSDP is reducing the memory footprint on each GPU. This enables training of larger models with lower total memory vs DDP, and leverages the overlap of computation and communication to train models efficiently. This reduced memory pressure can be leveraged to either train larger models or increase batch size, potentially helping overall training throughput. You can read more about PyTorch FSDP here. FSDP Features in This Tutorial# Transformer Auto Wrap Policy Mixed Precision Initializing FSDP Model on Device Sharding Strategy Backward Prefetch Model Checkpoint Saving via Streaming to CPU Recap on How FSDP Works# At a high level FDSP works as follow: In the constructor Shard model parameters and each rank only keeps its own shard In the forward pass Run all_gather to collect all shards from all ranks to recover the full parameter for this FSDP unit and run the forward computation Discard the non-owned parameter shards it has just collected to free memory In the backward pass Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit and run backward computation Discard non-owned parameters to free memory. Run reduce_scatter to sync gradients Fine-tuning HF T5# HF T5 pre-trained models are available in four different sizes, ranging from small with 60 Million parameters to XXL with 11 Billion parameters. In this tutorial, we demonstrate the fine-tuning of a T5 3B with FSDP for text summarization using WikiHow dataset. The main focus of this tutorial is to highlight different available features in FSDP that are helpful for training large scale model above 3B parameters. Also, we cover specific features for Transformer based models. The code for this tutorial is available in Pytorch examples. Setup 1.1 Install the latest PyTorch pip3 install torch torchvision torchaudio 1.2 Dataset Setup Please create a data folder, download the WikiHow dataset from wikihowAll.csv and wikihowSep.cs, and place them in the data folder. We will use the wikihow dataset from summarization_dataset. Next, we add the following code snippets to a Python script \u201cT5_training.py\u201d. Note The full source code for this tutorial is available in PyTorch examples. 1.3 Import necessary packages: import os import argparse import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from transformers import AutoTokenizer, GPT2TokenizerFast from transformers import T5Tokenizer, T5ForConditionalGeneration import functools from torch.optim.lr_scheduler import StepLR import torch.nn.functional as F import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data.distributed import DistributedSampler from transformers.models.t5.modeling_t5 import T5Block from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import ( checkpoint_wrapper, CheckpointImpl, apply_activation_checkpointing_wrapper) from torch.distributed.fsdp import ( FullyShardedDataParallel as FSDP, MixedPrecision, BackwardPrefetch, ShardingStrategy, FullStateDictConfig, StateDictType, ) from torch.distributed.fsdp.wrap import ( transformer_auto_wrap_policy, enable_wrap, wrap, ) from functools import partial from torch.utils.data import DataLoader from pathlib import Path from summarization_dataset import * from transformers.models.t5.modeling_t5 import T5Block from typing import Type import time import tqdm from datetime import datetime 1.4 Distributed training setup. Here we use two helper functions to initialize the processes for distributed training, and then to clean up after training completion. In this tutorial, we are going to use torch elastic, using torchrun , which will set the worker RANK and WORLD_SIZE automatically. def setup(): # initialize the process group dist.init_process_group(\"nccl\") def cleanup(): dist.destroy_process_group() 2.1 Set up the HuggingFace T5 model: def setup_model(model_name): model = T5ForConditionalGeneration.from_pretrained(model_name) tokenizer = T5Tokenizer.from_pretrained(model_name) return model, tokenizer We also, add couple of helper functions here for date and formatting memory metrics. def get_date_of_run(): \"\"\"create date and time for file save uniqueness example: 2022-05-07-08:31:12_PM\u0027 \"\"\" date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\") print(f\"--\u003e current date and time of run = {date_of_run}\") return date_of_run def format_metrics_to_gb(item): \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\" metric_num = item / g_gigabyte metric_num = round(metric_num, ndigits=4) return metric_num 2.2 Define a train function: def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None): model.train() local_rank = int(os.environ[\u0027LOCAL_RANK\u0027]) fsdp_loss = torch.zeros(2).to(local_rank) if sampler: sampler.set_epoch(epoch) if rank==0: inner_pbar = tqdm.tqdm( range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\" ) for batch in train_loader: for key in batch.keys(): batch[key] = batch[key].to(local_rank) optimizer.zero_grad() output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] ) loss = output[\"loss\"] loss.backward() optimizer.step() fsdp_loss[0] += loss.item() fsdp_loss[1] += len(batch) if rank==0: inner_pbar.update(1) dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM) train_accuracy = fsdp_loss[0] / fsdp_loss[1] if rank == 0: inner_pbar.close() print( f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\" ) return train_accuracy 2.3 Define a validation function: def validation(model, rank, world_size, val_loader): model.eval() correct = 0 local_rank = int(os.environ[\u0027LOCAL_RANK\u0027]) fsdp_loss = torch.zeros(3).to(local_rank) if rank == 0: inner_pbar = tqdm.tqdm( range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\" ) with torch.no_grad(): for batch in val_loader: for key in batch.keys(): batch[key] = batch[key].to(local_rank) output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"]) fsdp_loss[0] += output[\"loss\"].item() # sum up batch loss fsdp_loss[1] += len(batch) if rank==0: inner_pbar.update(1) dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM) val_loss = fsdp_loss[0] / fsdp_loss[1] if rank == 0: inner_pbar.close() print(f\"Validation Loss: {val_loss:.4f}\") return val_loss 2.4 Define a distributed train function that wraps the model in FSDP: def fsdp_main(args): model, tokenizer = setup_model(\"t5-base\") local_rank = int(os.environ[\u0027LOCAL_RANK\u0027]) rank = int(os.environ[\u0027RANK\u0027]) world_size = int(os.environ[\u0027WORLD_SIZE\u0027]) dataset = load_dataset(\u0027wikihow\u0027, \u0027all\u0027, data_dir=\u0027data/\u0027) print(dataset.keys()) print(\"Size of train dataset: \", dataset[\u0027train\u0027].shape) print(\"Size of Validation dataset: \", dataset[\u0027validation\u0027].shape) #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False) train_dataset = wikihow(tokenizer, \u0027train\u0027, 1500, 512, 150, False) val_dataset = wikihow(tokenizer, \u0027validation\u0027, 300, 512, 150, False) sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True) sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size) setup() train_kwargs = {\u0027batch_size\u0027: args.batch_size, \u0027sampler\u0027: sampler1} test_kwargs = {\u0027batch_size\u0027: args.test_batch_size, \u0027sampler\u0027: sampler2} cuda_kwargs = {\u0027num_workers\u0027: 2, \u0027pin_memory\u0027: True, \u0027shuffle\u0027: False} train_kwargs.update(cuda_kwargs) test_kwargs.update(cuda_kwargs) train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs) val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs) t5_auto_wrap_policy = functools.partial( transformer_auto_wrap_policy, transformer_layer_cls={ T5Block, }, ) sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3 torch.cuda.set_device(local_rank) #init_start_event = torch.cuda.Event(enable_timing=True) #init_end_event = torch.cuda.Event(enable_timing=True) #init_start_event.record() bf16_ready = ( torch.version.cuda and torch.cuda.is_bf16_supported() and LooseVersion(torch.version.cuda) \u003e= \"11.0\" and dist.is_nccl_available() and nccl.version() \u003e= (2, 10) ) if bf16_ready: mp_policy = bfSixteen else: mp_policy = None # defaults to fp32 # model is on CPU before input to FSDP model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy, mixed_precision=mp_policy, #sharding_strategy=sharding_strategy, device_id=torch.cuda.current_device()) optimizer = optim.AdamW(model.parameters(), lr=args.lr) scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma) best_val_loss = float(\"inf\") curr_val_loss = float(\"inf\") file_save_name = \"T5-model-\" if rank == 0: time_of_run = get_date_of_run() dur = [] train_acc_tracking = [] val_acc_tracking = [] training_start_time = time.time() if rank == 0 and args.track_memory: mem_alloc_tracker = [] mem_reserved_tracker = [] for epoch in range(1, args.epochs + 1): t0 = time.time() train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1) if args.run_validation: curr_val_loss = validation(model, rank, world_size, val_loader) scheduler.step() if rank == 0: print(f\"--\u003e epoch {epoch} completed...entering save and stats zone\") dur.append(time.time() - t0) train_acc_tracking.append(train_accuracy.item()) if args.run_validation: val_acc_tracking.append(curr_val_loss.item()) if args.track_memory: mem_alloc_tracker.append( format_metrics_to_gb(torch.cuda.memory_allocated()) ) mem_reserved_tracker.append( format_metrics_to_gb(torch.cuda.memory_reserved()) ) print(f\"completed save and stats zone...\") if args.save_model and curr_val_loss \u003c best_val_loss: # save if rank == 0: print(f\"--\u003e entering save model state\") save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True) with FSDP.state_dict_type( model, StateDictType.FULL_STATE_DICT, save_policy ): cpu_state = model.state_dict() #print(f\"saving process: rank {rank} done w state_dict\") if rank == 0: print(f\"--\u003e saving model ...\") currEpoch = ( \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\" ) print(f\"--\u003e attempting to save model prefix {currEpoch}\") save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch print(f\"--\u003e saving as model name {save_name}\") torch.save(cpu_state, save_name) if curr_val_loss \u003c best_val_loss: best_val_loss = curr_val_loss if rank==0: print(f\"--\u003e\u003e\u003e\u003e New Val Loss Record: {best_val_loss}\") dist.barrier() cleanup() 2.5 Parse the arguments and set the main function: if __name__ == \u0027__main__\u0027: # Training settings parser = argparse.ArgumentParser(description=\u0027PyTorch T5 FSDP Example\u0027) parser.add_argument(\u0027--batch-size\u0027, type=int, default=4, metavar=\u0027N\u0027, help=\u0027input batch size for training (default: 64)\u0027) parser.add_argument(\u0027--test-batch-size\u0027, type=int, default=4, metavar=\u0027N\u0027, help=\u0027input batch size for testing (default: 1000)\u0027) parser.add_argument(\u0027--epochs\u0027, type=int, default=2, metavar=\u0027N\u0027, help=\u0027number of epochs to train (default: 3)\u0027) parser.add_argument(\u0027--lr\u0027, type=float, default=.002, metavar=\u0027LR\u0027, help=\u0027learning rate (default: .002)\u0027) parser.add_argument(\u0027--gamma\u0027, type=float, default=0.7, metavar=\u0027M\u0027, help=\u0027Learning rate step gamma (default: 0.7)\u0027) parser.add_argument(\u0027--no-cuda\u0027, action=\u0027store_true\u0027, default=False, help=\u0027disables CUDA training\u0027) parser.add_argument(\u0027--seed\u0027, type=int, default=1, metavar=\u0027S\u0027, help=\u0027random seed (default: 1)\u0027) parser.add_argument(\u0027--track_memory\u0027, action=\u0027store_false\u0027, default=True, help=\u0027track the gpu memory\u0027) parser.add_argument(\u0027--run_validation\u0027, action=\u0027store_false\u0027, default=True, help=\u0027running the validation\u0027) parser.add_argument(\u0027--save-model\u0027, action=\u0027store_false\u0027, default=True, help=\u0027For Saving the current Model\u0027) args = parser.parse_args() torch.manual_seed(args.seed) fsdp_main(args) To run the the training using torchrun: torchrun --nnodes 1 --nproc_per_node 4 T5_training.py Transformer Wrapping Policy# As discussed in the previous tutorial, auto_wrap_policy is one of the FSDP features that make it easy to automatically shard a given model and put the model, optimizer and gradient shards into distinct FSDP units. For some architectures such as Transformer encoder-decoders, some parts of the model such as embedding table is being shared with both encoder and decoder. In this case, we need to place the embedding table in the outer FSDP unit so that it could be accessed from both encoder and decoder. In addition, by registering the layer class for a transformer, the sharding plan can be made much more communication efficient. In PyTorch 1.12, FSDP added this support and now we have a wrapping policy for transfomers. It can be created as follows, where the T5Block represents the T5 transformer layer class (holding MHSA and FFN). t5_auto_wrap_policy = functools.partial( transformer_auto_wrap_policy, transformer_layer_cls={ T5Block, }, ) torch.cuda.set_device(local_rank) model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy) To see the wrapped model, you can easily print the model and visually inspect the sharding and FSDP units as well. Mixed Precision# FSDP supports flexible mixed precision training allowing for arbitrary reduced precision types (such as fp16 or bfloat16). Currently BFloat16 is only available on Ampere GPUs, so you need to confirm native support before you use it. On V100s for example, BFloat16 can still be run but because it runs non-natively, it can result in significant slowdowns. To check if BFloat16 is natively supported, you can use the following : bf16_ready = ( torch.version.cuda and torch.cuda.is_bf16_supported() and LooseVersion(torch.version.cuda) \u003e= \"11.0\" and dist.is_nccl_available() and nccl.version() \u003e= (2, 10) ) One of the advantages of mixed precision in FSDP is providing granular control over different precision levels for parameters, gradients, and buffers as follows: fpSixteen = MixedPrecision( param_dtype=torch.float16, # Gradient communication precision. reduce_dtype=torch.float16, # Buffer precision. buffer_dtype=torch.float16, ) bfSixteen = MixedPrecision( param_dtype=torch.bfloat16, # Gradient communication precision. reduce_dtype=torch.bfloat16, # Buffer precision. buffer_dtype=torch.bfloat16, ) fp32_policy = MixedPrecision( param_dtype=torch.float32, # Gradient communication precision. reduce_dtype=torch.float32, # Buffer precision. buffer_dtype=torch.float32, ) Note that if a certain type (parameter, reduce, buffer) is not specified, they will not be casted at all. This flexibility allows users fine grained control, such as only setting gradient communication to happen in reduced precision, and all parameters / buffer computation to be done in full precision. This is potentially useful in cases where intra-node communication is the main bottleneck and parameters / buffers must be in full precision to avoid accuracy issues. This can be done with the following policy: grad_bf16 = MixedPrecision(reduce_dtype=torch.bfloat16) In 2.4 we just add the relevant mixed precision policy to the FSDP wrapper: model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy, mixed_precision=bfSixteen) In our experiments, we have observed up to 4x speed up by using BFloat16 for training and memory reduction of approximately 30% in some experiments that can be used for batch size increases. Intializing FSDP Model on Device# In 1.12, FSDP supports a device_id argument meant to initialize input CPU module on the device given by device_id. This is useful when the entire model does not fit on a single GPU, but fits in a host\u2019s CPU memory. When device_id is specified, FSDP will move the model to the specified device on a per-FSDP unit basis, avoiding GPU OOM issues while initializing several times faster than CPU-based initialization: torch.cuda.set_device(local_rank) model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy, mixed_precision=bfSixteen, device_id=torch.cuda.current_device()) Sharding Strategy# FSDP sharding strategy by default is set to fully shard the model parameters, gradients and optimizer states get sharded across all ranks. (also termed Zero3 sharding). In case you are interested to have the Zero2 sharding strategy, where only optimizer states and gradients are sharded, FSDP support this feature by passing the Sharding strategy by using \u201cShardingStrategy.SHARD_GRAD_OP\u201d, instead of \u201cShardingStrategy.FULL_SHARD\u201d to the FSDP initialization as follows: torch.cuda.set_device(local_rank) model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy, mixed_precision=bfSixteen, device_id=torch.cuda.current_device(), sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2) This will reduce the communication overhead in FSDP, in this case, it holds full parameters after forward and through the backwards pass. This saves an all_gather during backwards so there is less communication at the cost of a higher memory footprint. Note that full model params are freed at the end of backwards and all_gather will happen on the next forward pass. Backward Prefetch# The backward prefetch setting controls the timing of when the next FSDP unit\u2019s parameters should be requested. By setting it to BACKWARD_PRE, the next FSDP\u2019s unit params can begin to be requested and arrive sooner before the computation of the current unit starts. This overlaps the all_gather communication and gradient computation which can increase the training speed in exchange for slightly higher memory consumption. It can be utilized in the FSDP wrapper in 2.4 as follows: torch.cuda.set_device(local_rank) model = FSDP(model, auto_wrap_policy=t5_auto_wrap_policy, mixed_precision=bfSixteen, device_id=torch.cuda.current_device(), backward_prefetch = BackwardPrefetch.BACKWARD_PRE) backward_prefetch has two modes, BACKWARD_PRE and BACKWARD_POST. BACKWARD_POST means that the next FSDP unit\u2019s params will not be requested until the current FSDP unit processing is complete, thus minimizing memory overhead. In some cases, using BACKWARD_PRE can increase model training speed up to 2-10%, with even higher speed improvements noted for larger models. Model Checkpoint Saving, by streaming to the Rank0 CPU# To save model checkpoints using FULL_STATE_DICT saving which saves model in the same fashion as a local model, PyTorch 1.12 offers a few utilities to support the saving of larger models. First, a FullStateDictConfig can be specified, allowing the state_dict to be populated on rank 0 only and offloaded to the CPU. When using this configuration, FSDP will allgather model parameters, offloading them to the CPU one by one, only on rank 0. When the state_dict is finally saved, it will only be populated on rank 0 and contain CPU tensors. This avoids potential OOM for models that are larger than a single GPU memory and allows users to checkpoint models whose size is roughly the available CPU RAM on the user\u2019s machine. This feature can be run as follows: save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True) with FSDP.state_dict_type( model, StateDictType.FULL_STATE_DICT, save_policy ): cpu_state = model.state_dict() if rank == 0: save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch torch.save(cpu_state, save_name) Summary# In this tutorial, we have introduced many new features for FSDP available in Pytorch 1.12 and used HF T5 as the running example. Using the proper wrapping policy especially for transformer models, along with mixed precision and backward prefetch should speed up your training runs. Also, features such as initializing the model on device, and checkpoint saving via streaming to CPU should help to avoid OOM error in dealing with large models. We are actively working to add new features to FSDP for the next release. If you have feedback, feature requests, questions or are encountering issues using FSDP, please feel free to contact us by opening an issue in the PyTorch Github repository.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/FSDP_advanced_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>