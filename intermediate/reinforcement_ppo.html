
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2023-03-15T22:53:05+00:00" property="article:modified_time"/>
<title>Reinforcement Learning (PPO) with TorchRL Tutorial — PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/reinforcement_ppo';</script>
<link href="https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="mario_rl_tutorial.html" rel="next" title="Train a Mario-playing RL Agent"/>
<link href="reinforcement_q_learning.html" rel="prev" title="Reinforcement Learning (DQN) Tutorial"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Mar 15, 2023" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/intermediate/reinforcement_ppo.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname === 'docs.pytorch.org' ||
        window.location.hostname === 'docs-preview.pytorch.org' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Mar 15, 2023" name="docbuild:last-update"/>
</head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torchrec_intro_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Domains</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable/index.html">See Audio tutorials on the audio website</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/index.html">See ExecuTorch tutorials on the ExecuTorch website</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../domains.html">Domains</a></li>
<li aria-current="page" class="breadcrumb-item active">Reinforcemen...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../domains.html" itemprop="item"/>
<meta content="Domains" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Reinforcement Learning (PPO) with TorchRL Tutorial" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/reinforcement_ppo</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-reinforcement-ppo-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="reinforcement-learning-ppo-with-torchrl-tutorial">
<span id="sphx-glr-intermediate-reinforcement-ppo-py"></span><h1>Reinforcement Learning (PPO) with TorchRL Tutorial<a class="headerlink" href="#reinforcement-learning-ppo-with-torchrl-tutorial" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Mar 15, 2023 | Last Updated: Mar 20, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/vmoens">Vincent Moens</a></p>
<p>This tutorial demonstrates how to use PyTorch and <code class="xref py py-mod docutils literal notranslate"><span class="pre">torchrl</span></code> to train a parametric policy
network to solve the Inverted Pendulum task from the <a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium">OpenAI-Gym/Farama-Gymnasium
control library</a>.</p>
<figure class="align-default" id="id1">
<img alt="Inverted pendulum" src="../_images/invpendulum.gif"/>
<figcaption>
<p><span class="caption-text">Inverted pendulum</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Key learnings:</p>
<ul class="simple">
<li><p>How to create an environment in TorchRL, transform its outputs, and collect data from this environment;</p></li>
<li><p>How to make your classes talk to each other using <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a>;</p></li>
<li><p>The basics of building your training loop with TorchRL:</p>
<ul>
<li><p>How to compute the advantage signal for policy gradient methods;</p></li>
<li><p>How to create a stochastic policy using a probabilistic neural network;</p></li>
<li><p>How to create a dynamic replay buffer and sample from it without repetition.</p></li>
</ul>
</li>
</ul>
<p>We will cover six crucial components of TorchRL:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/envs.html">environments</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/envs.html#transforms">transforms</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/modules.html">models (policy and value function)</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/objectives.html">loss modules</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/collectors.html">data collectors</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/rl/reference/data.html#replay-buffers">replay buffers</a></p></li>
</ul>
<p>If you are running this in Google Colab, make sure you install the following dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!pip3<span class="w"> </span>install<span class="w"> </span>torchrl
!pip3<span class="w"> </span>install<span class="w"> </span>gym<span class="o">[</span>mujoco<span class="o">]</span>
!pip3<span class="w"> </span>install<span class="w"> </span>tqdm
</pre></div>
</div>
<p>Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a
batch of data is being collected and directly consumed to train the policy to maximise
the expected return given some proximality constraints. You can think of it
as a sophisticated version of <a class="reference external" href="https://link.springer.com/content/pdf/10.1007/BF00992696.pdf">REINFORCE</a>,
the foundational policy-optimization algorithm. For more information, see the
<a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a> paper.</p>
<p>PPO is usually regarded as a fast and efficient method for online, on-policy
reinforcement algorithm. TorchRL provides a loss-module that does all the work
for you, so that you can rely on this implementation and focus on solving your
problem rather than re-inventing the wheel every time you want to train a policy.</p>
<p>For completeness, here is a brief overview of what the loss computes, even though
this is taken care of by our <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ClipPPOLoss</span></code></a> module—the algorithm works as follows:
1. we will sample a batch of data by playing the
policy in the environment for a given number of steps.
2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using
a clipped version of the REINFORCE loss.
3. The clipping will put a pessimistic bound on our loss: lower return estimates will
be favored compared to higher ones.
The precise formula of the loss is:</p>
<div class="math">
\[L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
g(\epsilon, A^{\pi_{\theta_k}}(s,a))
\right),\]</div>
<p>There are two components in that loss: in the first part of the minimum operator,
we simply compute an importance-weighted version of the REINFORCE loss (for example, a
REINFORCE loss that we have corrected for the fact that the current policy
configuration lags the one that was used for the data collection).
The second part of that minimum operator is a similar loss where we have clipped
the ratios when they exceeded or were below a given pair of thresholds.</p>
<p>This loss ensures that whether the advantage is positive or negative, policy
updates that would produce significant shifts from the previous configuration
are being discouraged.</p>
<p>This tutorial is structured as follows:</p>
<ol class="arabic simple">
<li><p>First, we will define a set of hyperparameters we will be using for training.</p></li>
<li><p>Next, we will focus on creating our environment, or simulator, using TorchRL’s
wrappers and transforms.</p></li>
<li><p>Next, we will design the policy network and the value model,
which is indispensable to the loss function. These modules will be used
to configure our loss module.</p></li>
<li><p>Next, we will create the replay buffer and data loader.</p></li>
<li><p>Finally, we will run our training loop and analyze the results.</p></li>
</ol>
<p>Throughout this tutorial, we’ll be using the <code class="xref py py-mod docutils literal notranslate"><span class="pre">tensordict</span></code> library.
<a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a> is the lingua franca of TorchRL: it helps us abstract
what a module reads and writes and care less about the specific data
description and more about the algorithm itself.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">multiprocessing</span>


<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="tensordict.nn.TensorDictModule"><span class="n">TensorDictModule</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn.distributions</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-tensordict-nn-distributions-continuous sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.distributions.NormalParamExtractor.html#tensordict.nn.distributions.NormalParamExtractor" title="tensordict.nn.distributions.continuous.NormalParamExtractor"><span class="n">NormalParamExtractor</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-collectors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.SyncDataCollector.html#torchrl.collectors.SyncDataCollector" title="torchrl.collectors.SyncDataCollector"><span class="n">SyncDataCollector</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer" title="torchrl.data.ReplayBuffer"><span class="n">ReplayBuffer</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.samplers</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement" title="torchrl.data.replay_buffers.samplers.SamplerWithoutReplacement"><span class="n">SamplerWithoutReplacement</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.storages</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-data-replay_buffers-storages sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.LazyTensorStorage.html#torchrl.data.replay_buffers.LazyTensorStorage" title="torchrl.data.replay_buffers.storages.LazyTensorStorage"><span class="n">LazyTensorStorage</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span><a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.Compose.html#torchrl.envs.transforms.Compose" title="torchrl.envs.transforms.transforms.Compose"><span class="n">Compose</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat" title="torchrl.envs.transforms.transforms.DoubleToFloat"><span class="n">DoubleToFloat</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="torchrl.envs.transforms.transforms.ObservationNorm"><span class="n">ObservationNorm</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="torchrl.envs.transforms.transforms.StepCounter"><span class="n">StepCounter</span></a><span class="p">,</span>
                          <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">TransformedEnv</span></a><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.libs.gym</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv" title="torchrl.envs.GymEnv"><span class="n">GymEnv</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.utils</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.check_env_specs.html#torchrl.envs.check_env_specs" title="torchrl.envs.check_env_specs"><span class="n">check_env_specs</span></a><span class="p">,</span> <span class="n">ExplorationType</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.set_exploration_type.html#torchrl.envs.set_exploration_type" title="torchrl.envs.set_exploration_type"><span class="n">set_exploration_type</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">ProbabilisticActor</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal" title="torchrl.modules.TanhNormal"><span class="n">TanhNormal</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">ValueOperator</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="torchrl.objectives.ClipPPOLoss"><span class="n">ClipPPOLoss</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives.value</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torchrl-objectives-value sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE" title="torchrl.objectives.value.GAE"><span class="n">GAE</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
<section id="define-hyperparameters">
<h2>Define Hyperparameters<a class="headerlink" href="#define-hyperparameters" title="Link to this heading">#</a></h2>
<p>We set the hyperparameters for our algorithm. Depending on the resources
available, one may choose to execute the policy on GPU or on another
device.
The <code class="docutils literal notranslate"><span class="pre">frame_skip</span></code> will control how for how many frames is a single
action being executed. The rest of the arguments that count frames
must be corrected for this value (since one environment step will
actually return <code class="docutils literal notranslate"><span class="pre">frame_skip</span></code> frames).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">is_fork</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">get_start_method</span><span class="p">()</span> <span class="o">==</span> <span class="s2">"fork"</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_fork</span>
    <span class="k">else</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">num_cells</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># number of cells in each layer i.e. output dim.</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
</pre></div>
</div>
<section id="data-collection-parameters">
<h3>Data collection parameters<a class="headerlink" href="#data-collection-parameters" title="Link to this heading">#</a></h3>
<p>When collecting data, we will be able to choose how big each batch will be
by defining a <code class="docutils literal notranslate"><span class="pre">frames_per_batch</span></code> parameter. We will also define how many
frames (such as the number of interactions with the simulator) we will allow ourselves to
use. In general, the goal of an RL algorithm is to learn to solve the task
as fast as it can in terms of environment interactions: the lower the <code class="docutils literal notranslate"><span class="pre">total_frames</span></code>
the better.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">frames_per_batch</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="c1"># For a complete training, bring the number of frames up to 1M</span>
<span class="n">total_frames</span> <span class="o">=</span> <span class="mi">50_000</span>
</pre></div>
</div>
</section>
<section id="ppo-parameters">
<h3>PPO parameters<a class="headerlink" href="#ppo-parameters" title="Link to this heading">#</a></h3>
<p>At each data collection (or batch collection) we will run the optimization
over a certain number of <em>epochs</em>, each time consuming the entire data we just
acquired in a nested training loop. Here, the <code class="docutils literal notranslate"><span class="pre">sub_batch_size</span></code> is different from the
<code class="docutils literal notranslate"><span class="pre">frames_per_batch</span></code> here above: recall that we are working with a “batch of data”
coming from our collector, which size is defined by <code class="docutils literal notranslate"><span class="pre">frames_per_batch</span></code>, and that
we will further split in smaller sub-batches during the inner training loop.
The size of these sub-batches is controlled by <code class="docutils literal notranslate"><span class="pre">sub_batch_size</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">sub_batch_size</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># cardinality of the sub-samples gathered from the current data in the inner loop</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># optimization steps per batch of data collected</span>
<span class="n">clip_epsilon</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mf">0.2</span>  <span class="c1"># clip value for PPO loss: see the equation in the intro for more context.</span>
<span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">lmbda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">entropy_eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
</pre></div>
</div>
</section>
</section>
<section id="define-an-environment">
<h2>Define an environment<a class="headerlink" href="#define-an-environment" title="Link to this heading">#</a></h2>
<p>In RL, an <em>environment</em> is usually the way we refer to a simulator or a
control system. Various libraries provide simulation environments for reinforcement
learning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and
many others.
As a general library, TorchRL’s goal is to provide an interchangeable interface
to a large panel of RL simulators, allowing you to easily swap one environment
with another. For example, creating a wrapped gym environment can be achieved with few characters:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv" title="torchrl.envs.GymEnv"><span class="n">base_env</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv" title="torchrl.envs.GymEnv"><span class="n">GymEnv</span></a><span class="p">(</span><span class="s2">"InvertedDoublePendulum-v4"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
</pre></div>
</div>
<p>There are a few things to notice in this code: first, we created
the environment by calling the <code class="docutils literal notranslate"><span class="pre">GymEnv</span></code> wrapper. If extra keyword arguments
are passed, they will be transmitted to the <code class="docutils literal notranslate"><span class="pre">gym.make</span></code> method, hence covering
the most common environment construction commands.
Alternatively, one could also directly create a gym environment using <code class="docutils literal notranslate"><span class="pre">gym.make(env_name,</span> <span class="pre">**kwargs)</span></code>
and wrap it in a <cite>GymWrapper</cite> class.</p>
<p>Also the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument: for gym, this only controls the device where
input action and observed states will be stored, but the execution will always
be done on CPU. The reason for this is simply that gym does not support on-device
execution, unless specified otherwise. For other libraries, we have control over
the execution device and, as much as we can, we try to stay consistent in terms of
storing and execution backends.</p>
<section id="transforms">
<h3>Transforms<a class="headerlink" href="#transforms" title="Link to this heading">#</a></h3>
<p>We will append some transforms to our environments to prepare the data for
the policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different
approach, more similar to other pytorch domain libraries, through the use of transforms.
To add transforms to an environment, one should simply wrap it in a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedEnv</span></code></a>
instance and append the sequence of transforms to it. The transformed environment will inherit
the device and meta-data of the wrapped environment, and transform these depending on the sequence
of transforms it contains.</p>
</section>
<section id="normalization">
<h3>Normalization<a class="headerlink" href="#normalization" title="Link to this heading">#</a></h3>
<p>The first to encode is a normalization transform.
As a rule of thumbs, it is preferable to have data that loosely
match a unit Gaussian distribution: to obtain this, we will
run a certain number of random steps in the environment and compute
the summary statistics of these observations.</p>
<p>We’ll append two other transforms: the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DoubleToFloat</span></code></a> transform will
convert double entries to single-precision numbers, ready to be read by the
policy. The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">StepCounter</span></code></a> transform will be used to count the steps before
the environment is terminated. We will use this measure as a supplementary measure
of performance.</p>
<p>As we will see later, many of the TorchRL’s classes rely on <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a>
to communicate. You could think of it as a python dictionary with some extra
tensor features. In practice, this means that many modules we will be working
with need to be told what key to read (<code class="docutils literal notranslate"><span class="pre">in_keys</span></code>) and what key to write
(<code class="docutils literal notranslate"><span class="pre">out_keys</span></code>) in the <code class="docutils literal notranslate"><span class="pre">tensordict</span></code> they will receive. Usually, if <code class="docutils literal notranslate"><span class="pre">out_keys</span></code>
is omitted, it is assumed that the <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> entries will be updated
in-place. For our transforms, the only entry we are interested in is referred
to as <code class="docutils literal notranslate"><span class="pre">"observation"</span></code> and our transform layers will be told to modify this
entry and this entry only:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">env</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">TransformedEnv</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv" title="torchrl.envs.GymEnv"><span class="n">base_env</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.Compose.html#torchrl.envs.transforms.Compose" title="torchrl.envs.transforms.transforms.Compose"><span class="n">Compose</span></a><span class="p">(</span>
        <span class="c1"># normalize observations</span>
        <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="torchrl.envs.transforms.transforms.ObservationNorm"><span class="n">ObservationNorm</span></a><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">"observation"</span><span class="p">]),</span>
        <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat" title="torchrl.envs.transforms.transforms.DoubleToFloat"><span class="n">DoubleToFloat</span></a><span class="p">(),</span>
        <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="torchrl.envs.transforms.transforms.StepCounter"><span class="n">StepCounter</span></a><span class="p">(),</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>As you may have noticed, we have created a normalization layer but we did not
set its normalization parameters. To do this, <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ObservationNorm</span></code></a> can
automatically gather the summary statistics of our environment:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">env</span></a><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">reduce_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cat_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ObservationNorm</span></code></a> transform has now been populated with a
location and a scale that will be used to normalize the data.</p>
<p>Let us do a little sanity check for the shape of our summary stats:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"normalization constant shape:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">env</span></a><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>normalization constant shape: torch.Size([11])
</pre></div>
</div>
<p>An environment is not only defined by its simulator and transforms, but also
by a series of metadata that describe what can be expected during its
execution.
For efficiency purposes, TorchRL is quite stringent when it comes to
environment specs, but you can easily check that your environment specs are
adequate.
In our example, the <code class="xref py py-class docutils literal notranslate"><span class="pre">GymWrapper</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">GymEnv</span></code> that inherits
from it already take care of setting the proper specs for your environment so
you should not have to care about this.</p>
<p>Nevertheless, let’s see a concrete example using our transformed
environment by looking at its specs.
There are three specs to look at: <code class="docutils literal notranslate"><span class="pre">observation_spec</span></code> which defines what
is to be expected when executing an action in the environment,
<code class="docutils literal notranslate"><span class="pre">reward_spec</span></code> which indicates the reward domain and finally the
<code class="docutils literal notranslate"><span class="pre">input_spec</span></code> (which contains the <code class="docutils literal notranslate"><span class="pre">action_spec</span></code>) and which represents
everything an environment requires to execute a single step.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"observation_spec:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.observation_spec" title="torchrl.envs.EnvBase.observation_spec"><span class="n">env</span><span class="o">.</span><span class="n">observation_spec</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"reward_spec:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.reward_spec" title="torchrl.envs.EnvBase.reward_spec"><span class="n">env</span><span class="o">.</span><span class="n">reward_spec</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"input_spec:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs-transforms sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv.input_spec" title="torchrl.envs.transforms.TransformedEnv.input_spec"><span class="n">env</span><span class="o">.</span><span class="n">input_spec</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"action_spec (as defined by input_spec):"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec" title="torchrl.envs.EnvBase.action_spec"><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>observation_spec: Composite(
    observation: UnboundedContinuous(
        shape=torch.Size([11]),
        space=ContinuousBox(
            low=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True),
            high=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True)),
        device=cpu,
        dtype=torch.float32,
        domain=continuous),
    step_count: BoundedDiscrete(
        shape=torch.Size([1]),
        space=ContinuousBox(
            low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),
            high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),
        device=cpu,
        dtype=torch.int64,
        domain=discrete),
    device=cpu,
    shape=torch.Size([]),
    data_cls=None)
reward_spec: UnboundedContinuous(
    shape=torch.Size([1]),
    space=ContinuousBox(
        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),
        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),
    device=cpu,
    dtype=torch.float32,
    domain=continuous)
input_spec: Composite(
    full_state_spec: Composite(
        step_count: BoundedDiscrete(
            shape=torch.Size([1]),
            space=ContinuousBox(
                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),
                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),
            device=cpu,
            dtype=torch.int64,
            domain=discrete),
        device=cpu,
        shape=torch.Size([]),
        data_cls=None),
    full_action_spec: Composite(
        action: BoundedContinuous(
            shape=torch.Size([1]),
            space=ContinuousBox(
                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),
                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),
            device=cpu,
            dtype=torch.float32,
            domain=continuous),
        device=cpu,
        shape=torch.Size([]),
        data_cls=None),
    device=cpu,
    shape=torch.Size([]),
    data_cls=None)
action_spec (as defined by input_spec): BoundedContinuous(
    shape=torch.Size([1]),
    space=ContinuousBox(
        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),
        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),
    device=cpu,
    dtype=torch.float32,
    domain=continuous)
</pre></div>
</div>
<p>the <code class="xref py py-func docutils literal notranslate"><span class="pre">check_env_specs()</span></code> function runs a small rollout and compares its output against the environment
specs. If no error is raised, we can be confident that the specs are properly defined:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.check_env_specs.html#torchrl.envs.check_env_specs" title="torchrl.envs.check_env_specs"><span class="n">check_env_specs</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">env</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>2025-09-09 21:08:46,713 [torchrl][INFO]    check_env_specs succeeded! [END]
</pre></div>
</div>
<p>For fun, let’s see what a simple random rollout looks like. You can
call <cite>env.rollout(n_steps)</cite> and get an overview of what the environment inputs
and outputs look like. Actions will automatically be drawn from the action spec
domain, so you don’t need to care about designing a random sampler.</p>
<p>Typically, at each step, an RL environment receives an
action as input, and outputs an observation, a reward and a done state. The
observation may be composite, meaning that it could be composed of more than one
tensor. This is not a problem for TorchRL, since the whole set of observations
is automatically packed in the output <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a>. After executing a rollout
(for example, a sequence of environment steps and random action generations) over a given
number of steps, we will retrieve a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a> instance with a shape
that matches this trajectory length:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">rollout</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#id2" title="torchrl.envs.EnvBase.rollout"><span class="n">env</span><span class="o">.</span><span class="n">rollout</span></a><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"rollout of three steps:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">rollout</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Shape of the rollout TensorDict:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.batch_size" title="tensordict.TensorDict.batch_size"><span class="n">rollout</span><span class="o">.</span><span class="n">batch_size</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>rollout of three steps: TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        next: TensorDict(
            fields={
                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),
                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),
                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},
            batch_size=torch.Size([3]),
            device=cpu,
            is_shared=False),
        observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),
        step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),
        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},
    batch_size=torch.Size([3]),
    device=cpu,
    is_shared=False)
Shape of the rollout TensorDict: torch.Size([3])
</pre></div>
</div>
<p>Our rollout data has a shape of <code class="docutils literal notranslate"><span class="pre">torch.Size([3])</span></code>, which matches the number of steps
we ran it for. The <code class="docutils literal notranslate"><span class="pre">"next"</span></code> entry points to the data coming after the current step.
In most cases, the <code class="docutils literal notranslate"><span class="pre">"next"</span></code> data at time <cite>t</cite> matches the data at <code class="docutils literal notranslate"><span class="pre">t+1</span></code>, but this
may not be the case if we are using some specific transformations (for example, multi-step).</p>
</section>
</section>
<section id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Link to this heading">#</a></h2>
<p>PPO utilizes a stochastic policy to handle exploration. This means that our
neural network will have to output the parameters of a distribution, rather
than a single value corresponding to the action taken.</p>
<p>As the data is continuous, we use a Tanh-Normal distribution to respect the
action space boundaries. TorchRL provides such distribution, and the only
thing we need to care about is to build a neural network that outputs the
right number of parameters for the policy to work with (a location, or mean,
and a scale):</p>
<div class="math">
\[f_{\theta}(\text{observation}) = \mu_{\theta}(\text{observation}), \sigma^{+}_{\theta}(\text{observation})\]</div>
<p>The only extra-difficulty that is brought up here is to split our output in two
equal parts and map the second to a strictly positive space.</p>
<p>We design the policy in three steps:</p>
<ol class="arabic simple">
<li><p>Define a neural network <code class="docutils literal notranslate"><span class="pre">D_obs</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">D_action</span></code>. Indeed, our <code class="docutils literal notranslate"><span class="pre">loc</span></code> (mu) and <code class="docutils literal notranslate"><span class="pre">scale</span></code> (sigma) both have dimension <code class="docutils literal notranslate"><span class="pre">D_action</span></code>.</p></li>
<li><p>Append a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.distributions.NormalParamExtractor.html#tensordict.nn.distributions.NormalParamExtractor" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NormalParamExtractor</span></code></a> to extract a location and a scale (for example, splits the input in two equal parts and applies a positive transformation to the scale parameter).</p></li>
<li><p>Create a probabilistic <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code></a> that can generate this distribution and sample from it.</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">actor_net</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec" title="torchrl.envs.EnvBase.action_spec"><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-tensordict-nn-distributions-continuous sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.distributions.NormalParamExtractor.html#tensordict.nn.distributions.NormalParamExtractor" title="tensordict.nn.distributions.continuous.NormalParamExtractor"><span class="n">NormalParamExtractor</span></a><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To enable the policy to “talk” with the environment through the <code class="docutils literal notranslate"><span class="pre">tensordict</span></code>
data carrier, we wrap the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> in a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code></a>. This
class will simply ready the <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> it is provided with and write the
outputs in-place at the registered <code class="docutils literal notranslate"><span class="pre">out_keys</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="tensordict.nn.TensorDictModule"><span class="n">TensorDictModule</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">actor_net</span></a><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">"observation"</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">"loc"</span><span class="p">,</span> <span class="s2">"scale"</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We now need to build a distribution out of the location and scale of our
normal distribution. To do so, we instruct the
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProbabilisticActor</span></code></a>
class to build a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TanhNormal</span></code></a> out of the location and scale
parameters. We also provide the minimum and maximum values of this
distribution, which we gather from the environment specs.</p>
<p>The name of the <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> (and hence the name of the <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> from
the <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code></a> above) cannot be set to any value one may
like, as the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TanhNormal</span></code></a> distribution constructor will expect the
<code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> keyword arguments. That being said,
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProbabilisticActor</span></code></a> also accepts
<code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code> typed <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> where the key-value pair indicates
what <code class="docutils literal notranslate"><span class="pre">in_key</span></code> string should be used for every keyword argument that is to be used.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">ProbabilisticActor</span></a><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a><span class="p">,</span>
    <span class="n">spec</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec" title="torchrl.envs.EnvBase.action_spec"><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span></a><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">"loc"</span><span class="p">,</span> <span class="s2">"scale"</span><span class="p">],</span>
    <span class="n">distribution_class</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal" title="torchrl.modules.TanhNormal"><span class="n">TanhNormal</span></a><span class="p">,</span>
    <span class="n">distribution_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"low"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec" title="torchrl.envs.EnvBase.action_spec"><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">low</span></a><span class="p">,</span>
        <span class="s2">"high"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec" title="torchrl.envs.EnvBase.action_spec"><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">high</span></a><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">return_log_prob</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># we'll need the log-prob for the numerator of the importance weights</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="value-network">
<h2>Value network<a class="headerlink" href="#value-network" title="Link to this heading">#</a></h2>
<p>The value network is a crucial component of the PPO algorithm, even though it
won’t be used at inference time. This module will read the observations and
return an estimation of the discounted return for the following trajectory.
This allows us to amortize learning by relying on the some utility estimation
that is learned on-the-fly during training. Our value network share the same
structure as the policy, but for simplicity we assign it its own set of
parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">value_net</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="n">num_cells</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">(),</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">),</span>
<span class="p">)</span>

<a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">value_module</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">ValueOperator</span></a><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">value_net</span></a><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">"observation"</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>let’s try our policy and value modules. As we said earlier, the usage of
<a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code></a> makes it possible to directly read the output
of the environment to run these modules, as they know what information to read
and where to write it:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Running policy:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#id1" title="torchrl.envs.EnvBase.reset"><span class="n">env</span><span class="o">.</span><span class="n">reset</span></a><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Running value:"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">value_module</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#id1" title="torchrl.envs.EnvBase.reset"><span class="n">env</span><span class="o">.</span><span class="n">reset</span></a><span class="p">()))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Running policy: TensorDict(
    fields={
        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
        action_log_prob: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
        loc: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),
        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},
    batch_size=torch.Size([]),
    device=cpu,
    is_shared=False)
Running value: TensorDict(
    fields={
        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),
        state_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),
        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},
    batch_size=torch.Size([]),
    device=cpu,
    is_shared=False)
</pre></div>
</div>
</section>
<section id="data-collector">
<h2>Data collector<a class="headerlink" href="#data-collector" title="Link to this heading">#</a></h2>
<p>TorchRL provides a set of <a class="reference external" href="https://pytorch.org/rl/reference/collectors.html">DataCollector classes</a>.
Briefly, these classes execute three operations: reset an environment,
compute an action given the latest observation, execute a step in the environment,
and repeat the last two steps until the environment signals a stop (or reaches
a done state).</p>
<p>They allow you to control how many frames to collect at each iteration
(through the <code class="docutils literal notranslate"><span class="pre">frames_per_batch</span></code> parameter),
when to reset the environment (through the <code class="docutils literal notranslate"><span class="pre">max_frames_per_traj</span></code> argument),
on which <code class="docutils literal notranslate"><span class="pre">device</span></code> the policy should be executed, etc. They are also
designed to work efficiently with batched and multiprocessed environments.</p>
<p>The simplest data collector is the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.SyncDataCollector.html#torchrl.collectors.SyncDataCollector" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SyncDataCollector</span></code></a>:
it is an iterator that you can use to get batches of data of a given length, and
that will stop once a total number of frames (<code class="docutils literal notranslate"><span class="pre">total_frames</span></code>) have been
collected.
Other data collectors (<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.MultiSyncDataCollector.html#torchrl.collectors.MultiSyncDataCollector" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiSyncDataCollector</span></code></a> and
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.MultiaSyncDataCollector.html#torchrl.collectors.MultiaSyncDataCollector" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiaSyncDataCollector</span></code></a>) will execute
the same operations in synchronous and asynchronous manner over a
set of multiprocessed workers.</p>
<p>As for the policy and environment before, the data collector will return
<a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a> instances with a total number of elements that will
match <code class="docutils literal notranslate"><span class="pre">frames_per_batch</span></code>. Using <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDict</span></code></a> to pass data to the
training loop allows you to write data loading pipelines
that are 100% oblivious to the actual specificities of the rollout content.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.SyncDataCollector.html#torchrl.collectors.SyncDataCollector" title="torchrl.collectors.SyncDataCollector"><span class="n">collector</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-collectors sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.SyncDataCollector.html#torchrl.collectors.SyncDataCollector" title="torchrl.collectors.SyncDataCollector"><span class="n">SyncDataCollector</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv"><span class="n">env</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a><span class="p">,</span>
    <span class="n">frames_per_batch</span><span class="o">=</span><span class="n">frames_per_batch</span><span class="p">,</span>
    <span class="n">total_frames</span><span class="o">=</span><span class="n">total_frames</span><span class="p">,</span>
    <span class="n">split_trajs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="replay-buffer">
<h2>Replay buffer<a class="headerlink" href="#replay-buffer" title="Link to this heading">#</a></h2>
<p>Replay buffers are a common building piece of off-policy RL algorithms.
In on-policy contexts, a replay buffer is refilled every time a batch of
data is collected, and its data is repeatedly consumed for a certain number
of epochs.</p>
<p>TorchRL’s replay buffers are built using a common container
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReplayBuffer</span></code></a> which takes as argument the components
of the buffer: a storage, a writer, a sampler and possibly some transforms.
Only the storage (which indicates the replay buffer capacity) is mandatory.
We also specify a sampler without repetition to avoid sampling multiple times
the same item in one epoch.
Using a replay buffer for PPO is not mandatory and we could simply
sample the sub-batches from the collected batch, but using these classes
make it easy for us to build the inner training loop in a reproducible way.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer" title="torchrl.data.ReplayBuffer"><span class="n">replay_buffer</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer" title="torchrl.data.ReplayBuffer"><span class="n">ReplayBuffer</span></a><span class="p">(</span>
    <span class="n">storage</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-data-replay_buffers-storages sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.LazyTensorStorage.html#torchrl.data.replay_buffers.LazyTensorStorage" title="torchrl.data.replay_buffers.storages.LazyTensorStorage"><span class="n">LazyTensorStorage</span></a><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">frames_per_batch</span><span class="p">),</span>
    <span class="n">sampler</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement" title="torchrl.data.replay_buffers.samplers.SamplerWithoutReplacement"><span class="n">SamplerWithoutReplacement</span></a><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loss-function">
<h2>Loss function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>The PPO loss can be directly imported from TorchRL for convenience using the
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ClipPPOLoss</span></code></a> class. This is the easiest way of utilizing PPO:
it hides away the mathematical operations of PPO and the control flow that
goes with it.</p>
<p>PPO requires some “advantage estimation” to be computed. In short, an advantage
is a value that reflects an expectancy over the return value while dealing with
the bias / variance tradeoff.
To compute the advantage, one just needs to (1) build the advantage module, which
utilizes our value operator, and (2) pass each batch of data through it before each
epoch.
The GAE module will update the input <code class="docutils literal notranslate"><span class="pre">tensordict</span></code> with new <code class="docutils literal notranslate"><span class="pre">"advantage"</span></code> and
<code class="docutils literal notranslate"><span class="pre">"value_target"</span></code> entries.
The <code class="docutils literal notranslate"><span class="pre">"value_target"</span></code> is a gradient-free tensor that represents the empirical
value that the value network should represent with the input observation.
Both of these will be used by <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ClipPPOLoss</span></code></a> to
return the policy and value losses.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torchrl-objectives-value sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE" title="torchrl.objectives.value.GAE"><span class="n">advantage_module</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-objectives-value sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE" title="torchrl.objectives.value.GAE"><span class="n">GAE</span></a><span class="p">(</span>
    <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">lmbda</span><span class="o">=</span><span class="n">lmbda</span><span class="p">,</span> <span class="n">value_network</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">value_module</span></a><span class="p">,</span> <span class="n">average_gae</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">,</span>
<span class="p">)</span>

<a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="torchrl.objectives.ClipPPOLoss"><span class="n">loss_module</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="torchrl.objectives.ClipPPOLoss"><span class="n">ClipPPOLoss</span></a><span class="p">(</span>
    <span class="n">actor_network</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a><span class="p">,</span>
    <span class="n">critic_network</span><span class="o">=</span><a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator"><span class="n">value_module</span></a><span class="p">,</span>
    <span class="n">clip_epsilon</span><span class="o">=</span><span class="n">clip_epsilon</span><span class="p">,</span>
    <span class="n">entropy_bonus</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">entropy_eps</span><span class="p">),</span>
    <span class="n">entropy_coef</span><span class="o">=</span><span class="n">entropy_eps</span><span class="p">,</span>
    <span class="c1"># these keys match by default but we set this for completeness</span>
    <span class="n">critic_coef</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">loss_critic_type</span><span class="o">=</span><span class="s2">"smooth_l1"</span><span class="p">,</span>
<span class="p">)</span>

<a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.LossModule.html#torchrl.objectives.LossModule.parameters" title="torchrl.objectives.LossModule.parameters"><span class="n">loss_module</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR"><span class="n">scheduler</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span></a><span class="p">,</span> <span class="n">total_frames</span> <span class="o">//</span> <span class="n">frames_per_batch</span><span class="p">,</span> <span class="mf">0.0</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/torchrl/objectives/ppo.py:384: DeprecationWarning:

'critic_coef' is deprecated and will be removed in torchrl v0.11. Please use 'critic_coeff' instead.

/usr/local/lib/python3.10/dist-packages/torchrl/objectives/ppo.py:450: DeprecationWarning:

'entropy_coef' is deprecated and will be removed in torchrl v0.11. Please use 'entropy_coeff' instead.
</pre></div>
</div>
</section>
<section id="training-loop">
<h2>Training loop<a class="headerlink" href="#training-loop" title="Link to this heading">#</a></h2>
<p>We now have all the pieces needed to code our training loop.
The steps include:</p>
<ul class="simple">
<li><p>Collect data</p>
<ul>
<li><p>Compute advantage</p>
<ul>
<li><p>Loop over the collected to compute loss values</p></li>
<li><p>Back propagate</p></li>
<li><p>Optimize</p></li>
<li><p>Repeat</p></li>
</ul>
</li>
<li><p>Repeat</p></li>
</ul>
</li>
<li><p>Repeat</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">logs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_frames</span><span class="p">)</span>
<span class="n">eval_str</span> <span class="o">=</span> <span class="s2">""</span>

<span class="c1"># We iterate over the collector until it reaches the total number of frames it was</span>
<span class="c1"># designed to collect:</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">tensordict_data</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.collectors.SyncDataCollector.html#torchrl.collectors.SyncDataCollector" title="torchrl.collectors.SyncDataCollector"><span class="n">collector</span></a><span class="p">):</span>
    <span class="c1"># we now have a batch of data to work with. Let's learn something from it.</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># We'll need an "advantage" signal to make PPO work.</span>
        <span class="c1"># We re-compute it at each epoch as its value depends on the value</span>
        <span class="c1"># network which is updated in the inner loop.</span>
        <a class="sphx-glr-backref-module-torchrl-objectives-value sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE" title="torchrl.objectives.value.GAE"><span class="n">advantage_module</span></a><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">tensordict_data</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">data_view</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.reshape" title="tensordict.TensorDict.reshape"><span class="n">tensordict_data</span><span class="o">.</span><span class="n">reshape</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <a class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.extend" title="torchrl.data.ReplayBuffer.extend"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">extend</span></a><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.cpu" title="tensordict.TensorDict.cpu"><span class="n">data_view</span><span class="o">.</span><span class="n">cpu</span></a><span class="p">())</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">frames_per_batch</span> <span class="o">//</span> <span class="n">sub_batch_size</span><span class="p">):</span>
            <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">subdata</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.sample" title="torchrl.data.ReplayBuffer.sample"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span></a><span class="p">(</span><span class="n">sub_batch_size</span><span class="p">)</span>
            <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">loss_vals</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss" title="torchrl.objectives.ClipPPOLoss"><span class="n">loss_module</span></a><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.to" title="tensordict.TensorDict.to"><span class="n">subdata</span><span class="o">.</span><span class="n">to</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">device</span></a><span class="p">))</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">loss_value</span></a> <span class="o">=</span> <span class="p">(</span>
                <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">loss_vals</span></a><span class="p">[</span><span class="s2">"loss_objective"</span><span class="p">]</span>
                <span class="o">+</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">loss_vals</span></a><span class="p">[</span><span class="s2">"loss_critic"</span><span class="p">]</span>
                <span class="o">+</span> <a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">loss_vals</span></a><span class="p">[</span><span class="s2">"loss_entropy"</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="c1"># Optimization: backward, grad clipping and optimization step</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"><span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span></a><span class="p">()</span>
            <span class="c1"># this is not strictly mandatory but it's good practice to keep</span>
            <span class="c1"># your gradient norm bounded</span>
            <a class="sphx-glr-backref-module-torch-nn-utils sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.LossModule.html#torchrl.objectives.LossModule.parameters" title="torchrl.objectives.LossModule.parameters"><span class="n">loss_module</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>
            <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad"><span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

    <span class="n">logs</span><span class="p">[</span><span class="s2">"reward"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">tensordict_data</span></a><span class="p">[</span><span class="s2">"next"</span><span class="p">,</span> <span class="s2">"reward"</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.numel" title="tensordict.TensorDict.numel"><span class="n">tensordict_data</span><span class="o">.</span><span class="n">numel</span></a><span class="p">())</span>
    <span class="n">cum_reward_str</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"average reward=</span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'reward'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2"> (init=</span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'reward'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">)"</span>
    <span class="p">)</span>
    <span class="n">logs</span><span class="p">[</span><span class="s2">"step_count"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict"><span class="n">tensordict_data</span></a><span class="p">[</span><span class="s2">"step_count"</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">stepcount_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"step count (max): </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'step_count'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">logs</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">optim</span></a><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"lr"</span><span class="p">])</span>
    <span class="n">lr_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"lr policy: </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">"</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># We evaluate the policy once every 10 batches of data.</span>
        <span class="c1"># Evaluation is rather simple: execute the policy without exploration</span>
        <span class="c1"># (take the expected value of the action distribution) for a given</span>
        <span class="c1"># number of steps (1000, which is our ``env`` horizon).</span>
        <span class="c1"># The ``rollout`` method of the ``env`` can take a policy as argument:</span>
        <span class="c1"># it will then execute this policy at each step.</span>
        <span class="k">with</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.set_exploration_type.html#torchrl.envs.set_exploration_type" title="torchrl.envs.set_exploration_type"><span class="n">set_exploration_type</span></a><span class="p">(</span><span class="n">ExplorationType</span><span class="o">.</span><span class="n">DETERMINISTIC</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
            <span class="c1"># execute a rollout with the trained policy</span>
            <span class="n">eval_rollout</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#id2" title="torchrl.envs.EnvBase.rollout"><span class="n">env</span><span class="o">.</span><span class="n">rollout</span></a><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <a class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor"><span class="n">policy_module</span></a><span class="p">)</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">"eval reward"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_rollout</span><span class="p">[</span><span class="s2">"next"</span><span class="p">,</span> <span class="s2">"reward"</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">"eval reward (sum)"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">eval_rollout</span><span class="p">[</span><span class="s2">"next"</span><span class="p">,</span> <span class="s2">"reward"</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">"eval step_count"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_rollout</span><span class="p">[</span><span class="s2">"step_count"</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">eval_str</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">"eval cumulative reward: </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'eval reward (sum)'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2"> "</span>
                <span class="sa">f</span><span class="s2">"(init: </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'eval reward (sum)'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">), "</span>
                <span class="sa">f</span><span class="s2">"eval step-count: </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">'eval step_count'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
            <span class="p">)</span>
            <span class="k">del</span> <span class="n">eval_rollout</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">eval_str</span><span class="p">,</span> <span class="n">cum_reward_str</span><span class="p">,</span> <span class="n">stepcount_str</span><span class="p">,</span> <span class="n">lr_str</span><span class="p">]))</span>

    <span class="c1"># We're also using a learning rate scheduler. Like the gradient clipping,</span>
    <span class="c1"># this is a nice-to-have but nothing necessary for PPO to work.</span>
    <a class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.step" title="torch.optim.lr_scheduler.CosineAnnealingLR.step"><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span></a><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/50000 [00:00&lt;?, ?it/s]
  2%|▏         | 1000/50000 [00:03&lt;02:45, 296.82it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.0765 (init= 9.0765), step count (max): 12, lr policy:  0.0003:   2%|▏         | 1000/50000 [00:03&lt;02:45, 296.82it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.0765 (init= 9.0765), step count (max): 12, lr policy:  0.0003:   4%|▍         | 2000/50000 [00:06&lt;02:33, 311.93it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1294 (init= 9.0765), step count (max): 13, lr policy:  0.0003:   4%|▍         | 2000/50000 [00:06&lt;02:33, 311.93it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1294 (init= 9.0765), step count (max): 13, lr policy:  0.0003:   6%|▌         | 3000/50000 [00:09&lt;02:26, 321.54it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1462 (init= 9.0765), step count (max): 16, lr policy:  0.0003:   6%|▌         | 3000/50000 [00:09&lt;02:26, 321.54it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1462 (init= 9.0765), step count (max): 16, lr policy:  0.0003:   8%|▊         | 4000/50000 [00:12&lt;02:20, 327.09it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1730 (init= 9.0765), step count (max): 24, lr policy:  0.0003:   8%|▊         | 4000/50000 [00:12&lt;02:20, 327.09it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.1730 (init= 9.0765), step count (max): 24, lr policy:  0.0003:  10%|█         | 5000/50000 [00:15&lt;02:16, 329.82it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2085 (init= 9.0765), step count (max): 37, lr policy:  0.0003:  10%|█         | 5000/50000 [00:15&lt;02:16, 329.82it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2085 (init= 9.0765), step count (max): 37, lr policy:  0.0003:  12%|█▏        | 6000/50000 [00:18&lt;02:12, 332.52it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2212 (init= 9.0765), step count (max): 26, lr policy:  0.0003:  12%|█▏        | 6000/50000 [00:18&lt;02:12, 332.52it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2212 (init= 9.0765), step count (max): 26, lr policy:  0.0003:  14%|█▍        | 7000/50000 [00:21&lt;02:10, 329.76it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2448 (init= 9.0765), step count (max): 38, lr policy:  0.0003:  14%|█▍        | 7000/50000 [00:21&lt;02:10, 329.76it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2448 (init= 9.0765), step count (max): 38, lr policy:  0.0003:  16%|█▌        | 8000/50000 [00:24&lt;02:05, 333.52it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2331 (init= 9.0765), step count (max): 59, lr policy:  0.0003:  16%|█▌        | 8000/50000 [00:24&lt;02:05, 333.52it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2331 (init= 9.0765), step count (max): 59, lr policy:  0.0003:  18%|█▊        | 9000/50000 [00:27&lt;02:01, 336.21it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2632 (init= 9.0765), step count (max): 42, lr policy:  0.0003:  18%|█▊        | 9000/50000 [00:27&lt;02:01, 336.21it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2632 (init= 9.0765), step count (max): 42, lr policy:  0.0003:  20%|██        | 10000/50000 [00:30&lt;01:58, 338.21it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2542 (init= 9.0765), step count (max): 58, lr policy:  0.0003:  20%|██        | 10000/50000 [00:30&lt;01:58, 338.21it/s]
eval cumulative reward:  139.0010 (init:  139.0010), eval step-count: 14, average reward= 9.2542 (init= 9.0765), step count (max): 58, lr policy:  0.0003:  22%|██▏       | 11000/50000 [00:33&lt;01:54, 339.60it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2481 (init= 9.0765), step count (max): 47, lr policy:  0.0003:  22%|██▏       | 11000/50000 [00:33&lt;01:54, 339.60it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2481 (init= 9.0765), step count (max): 47, lr policy:  0.0003:  24%|██▍       | 12000/50000 [00:36&lt;01:51, 339.71it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2591 (init= 9.0765), step count (max): 59, lr policy:  0.0003:  24%|██▍       | 12000/50000 [00:36&lt;01:51, 339.71it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2591 (init= 9.0765), step count (max): 59, lr policy:  0.0003:  26%|██▌       | 13000/50000 [00:39&lt;01:48, 340.66it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2543 (init= 9.0765), step count (max): 53, lr policy:  0.0003:  26%|██▌       | 13000/50000 [00:39&lt;01:48, 340.66it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2543 (init= 9.0765), step count (max): 53, lr policy:  0.0003:  28%|██▊       | 14000/50000 [00:42&lt;01:46, 336.53it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2668 (init= 9.0765), step count (max): 49, lr policy:  0.0003:  28%|██▊       | 14000/50000 [00:42&lt;01:46, 336.53it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2668 (init= 9.0765), step count (max): 49, lr policy:  0.0003:  30%|███       | 15000/50000 [00:44&lt;01:43, 338.30it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2620 (init= 9.0765), step count (max): 65, lr policy:  0.0002:  30%|███       | 15000/50000 [00:44&lt;01:43, 338.30it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2620 (init= 9.0765), step count (max): 65, lr policy:  0.0002:  32%|███▏      | 16000/50000 [00:47&lt;01:40, 339.85it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2642 (init= 9.0765), step count (max): 66, lr policy:  0.0002:  32%|███▏      | 16000/50000 [00:47&lt;01:40, 339.85it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2642 (init= 9.0765), step count (max): 66, lr policy:  0.0002:  34%|███▍      | 17000/50000 [00:50&lt;01:37, 339.62it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2748 (init= 9.0765), step count (max): 56, lr policy:  0.0002:  34%|███▍      | 17000/50000 [00:50&lt;01:37, 339.62it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2748 (init= 9.0765), step count (max): 56, lr policy:  0.0002:  36%|███▌      | 18000/50000 [00:53&lt;01:33, 340.78it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2773 (init= 9.0765), step count (max): 80, lr policy:  0.0002:  36%|███▌      | 18000/50000 [00:53&lt;01:33, 340.78it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2773 (init= 9.0765), step count (max): 80, lr policy:  0.0002:  38%|███▊      | 19000/50000 [00:56&lt;01:30, 341.46it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2838 (init= 9.0765), step count (max): 76, lr policy:  0.0002:  38%|███▊      | 19000/50000 [00:56&lt;01:30, 341.46it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2838 (init= 9.0765), step count (max): 76, lr policy:  0.0002:  40%|████      | 20000/50000 [00:59&lt;01:28, 337.52it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2824 (init= 9.0765), step count (max): 74, lr policy:  0.0002:  40%|████      | 20000/50000 [00:59&lt;01:28, 337.52it/s]
eval cumulative reward:  185.2007 (init:  139.0010), eval step-count: 19, average reward= 9.2824 (init= 9.0765), step count (max): 74, lr policy:  0.0002:  42%|████▏     | 21000/50000 [01:02&lt;01:25, 339.36it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2797 (init= 9.0765), step count (max): 58, lr policy:  0.0002:  42%|████▏     | 21000/50000 [01:02&lt;01:25, 339.36it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2797 (init= 9.0765), step count (max): 58, lr policy:  0.0002:  44%|████▍     | 22000/50000 [01:05&lt;01:23, 336.56it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2753 (init= 9.0765), step count (max): 60, lr policy:  0.0002:  44%|████▍     | 22000/50000 [01:05&lt;01:23, 336.56it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2753 (init= 9.0765), step count (max): 60, lr policy:  0.0002:  46%|████▌     | 23000/50000 [01:08&lt;01:20, 337.30it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2819 (init= 9.0765), step count (max): 82, lr policy:  0.0002:  46%|████▌     | 23000/50000 [01:08&lt;01:20, 337.30it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2819 (init= 9.0765), step count (max): 82, lr policy:  0.0002:  48%|████▊     | 24000/50000 [01:11&lt;01:16, 339.06it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2949 (init= 9.0765), step count (max): 99, lr policy:  0.0002:  48%|████▊     | 24000/50000 [01:11&lt;01:16, 339.06it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2949 (init= 9.0765), step count (max): 99, lr policy:  0.0002:  50%|█████     | 25000/50000 [01:14&lt;01:13, 339.52it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2834 (init= 9.0765), step count (max): 65, lr policy:  0.0002:  50%|█████     | 25000/50000 [01:14&lt;01:13, 339.52it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2834 (init= 9.0765), step count (max): 65, lr policy:  0.0002:  52%|█████▏    | 26000/50000 [01:17&lt;01:10, 340.38it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2981 (init= 9.0765), step count (max): 90, lr policy:  0.0001:  52%|█████▏    | 26000/50000 [01:17&lt;01:10, 340.38it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2981 (init= 9.0765), step count (max): 90, lr policy:  0.0001:  54%|█████▍    | 27000/50000 [01:20&lt;01:08, 336.71it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2999 (init= 9.0765), step count (max): 77, lr policy:  0.0001:  54%|█████▍    | 27000/50000 [01:20&lt;01:08, 336.71it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2999 (init= 9.0765), step count (max): 77, lr policy:  0.0001:  56%|█████▌    | 28000/50000 [01:23&lt;01:04, 338.64it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2983 (init= 9.0765), step count (max): 75, lr policy:  0.0001:  56%|█████▌    | 28000/50000 [01:23&lt;01:04, 338.64it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.2983 (init= 9.0765), step count (max): 75, lr policy:  0.0001:  58%|█████▊    | 29000/50000 [01:26&lt;01:01, 340.53it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.3038 (init= 9.0765), step count (max): 126, lr policy:  0.0001:  58%|█████▊    | 29000/50000 [01:26&lt;01:01, 340.53it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.3038 (init= 9.0765), step count (max): 126, lr policy:  0.0001:  60%|██████    | 30000/50000 [01:29&lt;00:58, 341.52it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.3015 (init= 9.0765), step count (max): 122, lr policy:  0.0001:  60%|██████    | 30000/50000 [01:29&lt;00:58, 341.52it/s]
eval cumulative reward:  390.5743 (init:  139.0010), eval step-count: 41, average reward= 9.3015 (init= 9.0765), step count (max): 122, lr policy:  0.0001:  62%|██████▏   | 31000/50000 [01:32&lt;00:55, 342.12it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3018 (init= 9.0765), step count (max): 103, lr policy:  0.0001:  62%|██████▏   | 31000/50000 [01:32&lt;00:55, 342.12it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3018 (init= 9.0765), step count (max): 103, lr policy:  0.0001:  64%|██████▍   | 32000/50000 [01:35&lt;00:53, 339.61it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3080 (init= 9.0765), step count (max): 127, lr policy:  0.0001:  64%|██████▍   | 32000/50000 [01:35&lt;00:53, 339.61it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3080 (init= 9.0765), step count (max): 127, lr policy:  0.0001:  66%|██████▌   | 33000/50000 [01:38&lt;00:50, 336.50it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3236 (init= 9.0765), step count (max): 165, lr policy:  0.0001:  66%|██████▌   | 33000/50000 [01:38&lt;00:50, 336.50it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3236 (init= 9.0765), step count (max): 165, lr policy:  0.0001:  68%|██████▊   | 34000/50000 [01:40&lt;00:47, 338.07it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3255 (init= 9.0765), step count (max): 224, lr policy:  0.0001:  68%|██████▊   | 34000/50000 [01:40&lt;00:47, 338.07it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3255 (init= 9.0765), step count (max): 224, lr policy:  0.0001:  70%|███████   | 35000/50000 [01:43&lt;00:44, 338.36it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3039 (init= 9.0765), step count (max): 272, lr policy:  0.0001:  70%|███████   | 35000/50000 [01:43&lt;00:44, 338.36it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3039 (init= 9.0765), step count (max): 272, lr policy:  0.0001:  72%|███████▏  | 36000/50000 [01:46&lt;00:41, 337.91it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3138 (init= 9.0765), step count (max): 137, lr policy:  0.0001:  72%|███████▏  | 36000/50000 [01:46&lt;00:41, 337.91it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3138 (init= 9.0765), step count (max): 137, lr policy:  0.0001:  74%|███████▍  | 37000/50000 [01:49&lt;00:38, 340.19it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3243 (init= 9.0765), step count (max): 244, lr policy:  0.0001:  74%|███████▍  | 37000/50000 [01:49&lt;00:38, 340.19it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3243 (init= 9.0765), step count (max): 244, lr policy:  0.0001:  76%|███████▌  | 38000/50000 [01:52&lt;00:35, 341.76it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3262 (init= 9.0765), step count (max): 356, lr policy:  0.0000:  76%|███████▌  | 38000/50000 [01:52&lt;00:35, 341.76it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3262 (init= 9.0765), step count (max): 356, lr policy:  0.0000:  78%|███████▊  | 39000/50000 [01:55&lt;00:32, 337.87it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3171 (init= 9.0765), step count (max): 252, lr policy:  0.0000:  78%|███████▊  | 39000/50000 [01:55&lt;00:32, 337.87it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3171 (init= 9.0765), step count (max): 252, lr policy:  0.0000:  80%|████████  | 40000/50000 [01:58&lt;00:29, 339.72it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3164 (init= 9.0765), step count (max): 130, lr policy:  0.0000:  80%|████████  | 40000/50000 [01:58&lt;00:29, 339.72it/s]
eval cumulative reward:  465.8858 (init:  139.0010), eval step-count: 49, average reward= 9.3164 (init= 9.0765), step count (max): 130, lr policy:  0.0000:  82%|████████▏ | 41000/50000 [02:01&lt;00:26, 341.15it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3227 (init= 9.0765), step count (max): 152, lr policy:  0.0000:  82%|████████▏ | 41000/50000 [02:01&lt;00:26, 341.15it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3227 (init= 9.0765), step count (max): 152, lr policy:  0.0000:  84%|████████▍ | 42000/50000 [02:04&lt;00:23, 337.07it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3220 (init= 9.0765), step count (max): 131, lr policy:  0.0000:  84%|████████▍ | 42000/50000 [02:04&lt;00:23, 337.07it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3220 (init= 9.0765), step count (max): 131, lr policy:  0.0000:  86%|████████▌ | 43000/50000 [02:07&lt;00:20, 339.93it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3256 (init= 9.0765), step count (max): 188, lr policy:  0.0000:  86%|████████▌ | 43000/50000 [02:07&lt;00:20, 339.93it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3256 (init= 9.0765), step count (max): 188, lr policy:  0.0000:  88%|████████▊ | 44000/50000 [02:10&lt;00:17, 342.05it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3346 (init= 9.0765), step count (max): 229, lr policy:  0.0000:  88%|████████▊ | 44000/50000 [02:10&lt;00:17, 342.05it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3346 (init= 9.0765), step count (max): 229, lr policy:  0.0000:  90%|█████████ | 45000/50000 [02:13&lt;00:14, 342.99it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3174 (init= 9.0765), step count (max): 143, lr policy:  0.0000:  90%|█████████ | 45000/50000 [02:13&lt;00:14, 342.99it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3174 (init= 9.0765), step count (max): 143, lr policy:  0.0000:  92%|█████████▏| 46000/50000 [02:16&lt;00:11, 339.17it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3239 (init= 9.0765), step count (max): 167, lr policy:  0.0000:  92%|█████████▏| 46000/50000 [02:16&lt;00:11, 339.17it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3239 (init= 9.0765), step count (max): 167, lr policy:  0.0000:  94%|█████████▍| 47000/50000 [02:19&lt;00:08, 340.87it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3238 (init= 9.0765), step count (max): 245, lr policy:  0.0000:  94%|█████████▍| 47000/50000 [02:19&lt;00:08, 340.87it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3238 (init= 9.0765), step count (max): 245, lr policy:  0.0000:  96%|█████████▌| 48000/50000 [02:22&lt;00:05, 342.62it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3288 (init= 9.0765), step count (max): 184, lr policy:  0.0000:  96%|█████████▌| 48000/50000 [02:22&lt;00:05, 342.62it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3288 (init= 9.0765), step count (max): 184, lr policy:  0.0000:  98%|█████████▊| 49000/50000 [02:24&lt;00:02, 343.52it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3298 (init= 9.0765), step count (max): 175, lr policy:  0.0000:  98%|█████████▊| 49000/50000 [02:24&lt;00:02, 343.52it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3298 (init= 9.0765), step count (max): 175, lr policy:  0.0000: 100%|██████████| 50000/50000 [02:27&lt;00:00, 344.23it/s]
eval cumulative reward:  933.0149 (init:  139.0010), eval step-count: 99, average reward= 9.3236 (init= 9.0765), step count (max): 236, lr policy:  0.0000: 100%|██████████| 50000/50000 [02:27&lt;00:00, 344.23it/s]
</pre></div>
</div>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h2>
<p>Before the 1M step cap is reached, the algorithm should have reached a max
step count of 1000 steps, which is the maximum number of steps before the
trajectory is truncated.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">"reward"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"training rewards (average)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">"step_count"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Max step count (training)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">"eval reward (sum)"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Return (test)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">"eval step_count"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Max step count (test)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="training rewards (average), Max step count (training), Return (test), Max step count (test)" class="sphx-glr-single-img" src="../_images/sphx_glr_reinforcement_ppo_001.png" srcset="../_images/sphx_glr_reinforcement_ppo_001.png"/></section>
<section id="conclusion-and-next-steps">
<h2>Conclusion and next steps<a class="headerlink" href="#conclusion-and-next-steps" title="Link to this heading">#</a></h2>
<p>In this tutorial, we have learned:</p>
<ol class="arabic simple">
<li><p>How to create and customize an environment with <code class="xref py py-mod docutils literal notranslate"><span class="pre">torchrl</span></code>;</p></li>
<li><p>How to write a model and a loss function;</p></li>
<li><p>How to set up a typical training loop.</p></li>
</ol>
<p>If you want to experiment with this tutorial a bit more, you can apply the following modifications:</p>
<ul class="simple">
<li><p>From an efficiency perspective,
we could run several simulations in parallel to speed up data collection.
Check <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv" title="(in torchrl v0.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParallelEnv</span></code></a> for further information.</p></li>
<li><p>From a logging perspective, one could add a <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.record.VideoRecorder</span></code> transform to
the environment after asking for rendering to get a visual rendering of the
inverted pendulum in action. Check <code class="xref py py-mod docutils literal notranslate"><span class="pre">torchrl.record</span></code> to
know more.</p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (2 minutes 29.759 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-reinforcement-ppo-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4065a985b933a4377d3c7d93557e2282/reinforcement_ppo.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">reinforcement_ppo.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7ed508ed54ec36ee5c1d3fa1e8ceede0/reinforcement_ppo.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">reinforcement_ppo.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/67b7ac26f2ad3d834e0413c0bff24803/reinforcement_ppo.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">reinforcement_ppo.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="reinforcement_q_learning.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reinforcement Learning (DQN) Tutorial</p>
</div>
</a>
<a class="right-next" href="mario_rl_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Train a Mario-playing RL Agent</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="reinforcement_q_learning.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reinforcement Learning (DQN) Tutorial</p>
</div>
</a>
<a class="right-next" href="mario_rl_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Train a Mario-playing RL Agent</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-hyperparameters">Define Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection-parameters">Data collection parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-parameters">PPO parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-an-environment">Define an environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transforms">Transforms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-network">Value network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collector">Data collector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replay-buffer">Replay buffer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training loop</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-and-next-steps">Conclusion and next steps</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "name": "Reinforcement Learning (PPO) with TorchRL Tutorial",
         "headline": "Reinforcement Learning (PPO) with TorchRL Tutorial",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/intermediate/reinforcement_ppo.html",
         "articleBody": "Note Go to the end to download the full example code. Reinforcement Learning (PPO) with TorchRL Tutorial# Author: Vincent Moens This tutorial demonstrates how to use PyTorch and torchrl to train a parametric policy network to solve the Inverted Pendulum task from the OpenAI-Gym/Farama-Gymnasium control library. Inverted pendulum# Key learnings: How to create an environment in TorchRL, transform its outputs, and collect data from this environment; How to make your classes talk to each other using TensorDict; The basics of building your training loop with TorchRL: How to compute the advantage signal for policy gradient methods; How to create a stochastic policy using a probabilistic neural network; How to create a dynamic replay buffer and sample from it without repetition. We will cover six crucial components of TorchRL: environments transforms models (policy and value function) loss modules data collectors replay buffers If you are running this in Google Colab, make sure you install the following dependencies: !pip3 install torchrl !pip3 install gym[mujoco] !pip3 install tqdm Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a batch of data is being collected and directly consumed to train the policy to maximise the expected return given some proximality constraints. You can think of it as a sophisticated version of REINFORCE, the foundational policy-optimization algorithm. For more information, see the Proximal Policy Optimization Algorithms paper. PPO is usually regarded as a fast and efficient method for online, on-policy reinforcement algorithm. TorchRL provides a loss-module that does all the work for you, so that you can rely on this implementation and focus on solving your problem rather than re-inventing the wheel every time you want to train a policy. For completeness, here is a brief overview of what the loss computes, even though this is taken care of by our ClipPPOLoss module\u2014the algorithm works as follows: 1. we will sample a batch of data by playing the policy in the environment for a given number of steps. 2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using a clipped version of the REINFORCE loss. 3. The clipping will put a pessimistic bound on our loss: lower return estimates will be favored compared to higher ones. The precise formula of the loss is: \\[L(s,a,\\theta_k,\\theta) = \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a), \\;\\; g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a)) \\right),\\] There are two components in that loss: in the first part of the minimum operator, we simply compute an importance-weighted version of the REINFORCE loss (for example, a REINFORCE loss that we have corrected for the fact that the current policy configuration lags the one that was used for the data collection). The second part of that minimum operator is a similar loss where we have clipped the ratios when they exceeded or were below a given pair of thresholds. This loss ensures that whether the advantage is positive or negative, policy updates that would produce significant shifts from the previous configuration are being discouraged. This tutorial is structured as follows: First, we will define a set of hyperparameters we will be using for training. Next, we will focus on creating our environment, or simulator, using TorchRL\u2019s wrappers and transforms. Next, we will design the policy network and the value model, which is indispensable to the loss function. These modules will be used to configure our loss module. Next, we will create the replay buffer and data loader. Finally, we will run our training loop and analyze the results. Throughout this tutorial, we\u2019ll be using the tensordict library. TensorDict is the lingua franca of TorchRL: it helps us abstract what a module reads and writes and care less about the specific data description and more about the algorithm itself. import warnings warnings.filterwarnings(\"ignore\") from torch import multiprocessing from collections import defaultdict import matplotlib.pyplot as plt import torch from tensordict.nn import TensorDictModule from tensordict.nn.distributions import NormalParamExtractor from torch import nn from torchrl.collectors import SyncDataCollector from torchrl.data.replay_buffers import ReplayBuffer from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement from torchrl.data.replay_buffers.storages import LazyTensorStorage from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter, TransformedEnv) from torchrl.envs.libs.gym import GymEnv from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator from torchrl.objectives import ClipPPOLoss from torchrl.objectives.value import GAE from tqdm import tqdm Define Hyperparameters# We set the hyperparameters for our algorithm. Depending on the resources available, one may choose to execute the policy on GPU or on another device. The frame_skip will control how for how many frames is a single action being executed. The rest of the arguments that count frames must be corrected for this value (since one environment step will actually return frame_skip frames). is_fork = multiprocessing.get_start_method() == \"fork\" device = ( torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\") ) num_cells = 256 # number of cells in each layer i.e. output dim. lr = 3e-4 max_grad_norm = 1.0 Data collection parameters# When collecting data, we will be able to choose how big each batch will be by defining a frames_per_batch parameter. We will also define how many frames (such as the number of interactions with the simulator) we will allow ourselves to use. In general, the goal of an RL algorithm is to learn to solve the task as fast as it can in terms of environment interactions: the lower the total_frames the better. frames_per_batch = 1000 # For a complete training, bring the number of frames up to 1M total_frames = 50_000 PPO parameters# At each data collection (or batch collection) we will run the optimization over a certain number of epochs, each time consuming the entire data we just acquired in a nested training loop. Here, the sub_batch_size is different from the frames_per_batch here above: recall that we are working with a \u201cbatch of data\u201d coming from our collector, which size is defined by frames_per_batch, and that we will further split in smaller sub-batches during the inner training loop. The size of these sub-batches is controlled by sub_batch_size. sub_batch_size = 64 # cardinality of the sub-samples gathered from the current data in the inner loop num_epochs = 10 # optimization steps per batch of data collected clip_epsilon = ( 0.2 # clip value for PPO loss: see the equation in the intro for more context. ) gamma = 0.99 lmbda = 0.95 entropy_eps = 1e-4 Define an environment# In RL, an environment is usually the way we refer to a simulator or a control system. Various libraries provide simulation environments for reinforcement learning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and many others. As a general library, TorchRL\u2019s goal is to provide an interchangeable interface to a large panel of RL simulators, allowing you to easily swap one environment with another. For example, creating a wrapped gym environment can be achieved with few characters: base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device) Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality. Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade. Users of this version of Gym should be able to simply replace \u0027import gym\u0027 with \u0027import gymnasium as gym\u0027 in the vast majority of cases. See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information. There are a few things to notice in this code: first, we created the environment by calling the GymEnv wrapper. If extra keyword arguments are passed, they will be transmitted to the gym.make method, hence covering the most common environment construction commands. Alternatively, one could also directly create a gym environment using gym.make(env_name, **kwargs) and wrap it in a GymWrapper class. Also the device argument: for gym, this only controls the device where input action and observed states will be stored, but the execution will always be done on CPU. The reason for this is simply that gym does not support on-device execution, unless specified otherwise. For other libraries, we have control over the execution device and, as much as we can, we try to stay consistent in terms of storing and execution backends. Transforms# We will append some transforms to our environments to prepare the data for the policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different approach, more similar to other pytorch domain libraries, through the use of transforms. To add transforms to an environment, one should simply wrap it in a TransformedEnv instance and append the sequence of transforms to it. The transformed environment will inherit the device and meta-data of the wrapped environment, and transform these depending on the sequence of transforms it contains. Normalization# The first to encode is a normalization transform. As a rule of thumbs, it is preferable to have data that loosely match a unit Gaussian distribution: to obtain this, we will run a certain number of random steps in the environment and compute the summary statistics of these observations. We\u2019ll append two other transforms: the DoubleToFloat transform will convert double entries to single-precision numbers, ready to be read by the policy. The StepCounter transform will be used to count the steps before the environment is terminated. We will use this measure as a supplementary measure of performance. As we will see later, many of the TorchRL\u2019s classes rely on TensorDict to communicate. You could think of it as a python dictionary with some extra tensor features. In practice, this means that many modules we will be working with need to be told what key to read (in_keys) and what key to write (out_keys) in the tensordict they will receive. Usually, if out_keys is omitted, it is assumed that the in_keys entries will be updated in-place. For our transforms, the only entry we are interested in is referred to as \"observation\" and our transform layers will be told to modify this entry and this entry only: env = TransformedEnv( base_env, Compose( # normalize observations ObservationNorm(in_keys=[\"observation\"]), DoubleToFloat(), StepCounter(), ), ) As you may have noticed, we have created a normalization layer but we did not set its normalization parameters. To do this, ObservationNorm can automatically gather the summary statistics of our environment: env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0) The ObservationNorm transform has now been populated with a location and a scale that will be used to normalize the data. Let us do a little sanity check for the shape of our summary stats: print(\"normalization constant shape:\", env.transform[0].loc.shape) normalization constant shape: torch.Size([11]) An environment is not only defined by its simulator and transforms, but also by a series of metadata that describe what can be expected during its execution. For efficiency purposes, TorchRL is quite stringent when it comes to environment specs, but you can easily check that your environment specs are adequate. In our example, the GymWrapper and GymEnv that inherits from it already take care of setting the proper specs for your environment so you should not have to care about this. Nevertheless, let\u2019s see a concrete example using our transformed environment by looking at its specs. There are three specs to look at: observation_spec which defines what is to be expected when executing an action in the environment, reward_spec which indicates the reward domain and finally the input_spec (which contains the action_spec) and which represents everything an environment requires to execute a single step. print(\"observation_spec:\", env.observation_spec) print(\"reward_spec:\", env.reward_spec) print(\"input_spec:\", env.input_spec) print(\"action_spec (as defined by input_spec):\", env.action_spec) observation_spec: Composite( observation: UnboundedContinuous( shape=torch.Size([11]), space=ContinuousBox( low=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), step_count: BoundedDiscrete( shape=torch.Size([1]), space=ContinuousBox( low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True), high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int64, domain=discrete), device=cpu, shape=torch.Size([]), data_cls=None) reward_spec: UnboundedContinuous( shape=torch.Size([1]), space=ContinuousBox( low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous) input_spec: Composite( full_state_spec: Composite( step_count: BoundedDiscrete( shape=torch.Size([1]), space=ContinuousBox( low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True), high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int64, domain=discrete), device=cpu, shape=torch.Size([]), data_cls=None), full_action_spec: Composite( action: BoundedContinuous( shape=torch.Size([1]), space=ContinuousBox( low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([]), data_cls=None), device=cpu, shape=torch.Size([]), data_cls=None) action_spec (as defined by input_spec): BoundedContinuous( shape=torch.Size([1]), space=ContinuousBox( low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous) the check_env_specs() function runs a small rollout and compares its output against the environment specs. If no error is raised, we can be confident that the specs are properly defined: check_env_specs(env) 2025-09-09 21:08:46,713 [torchrl][INFO] check_env_specs succeeded! [END] For fun, let\u2019s see what a simple random rollout looks like. You can call env.rollout(n_steps) and get an overview of what the environment inputs and outputs look like. Actions will automatically be drawn from the action spec domain, so you don\u2019t need to care about designing a random sampler. Typically, at each step, an RL environment receives an action as input, and outputs an observation, a reward and a done state. The observation may be composite, meaning that it could be composed of more than one tensor. This is not a problem for TorchRL, since the whole set of observations is automatically packed in the output TensorDict. After executing a rollout (for example, a sequence of environment steps and random action generations) over a given number of steps, we will retrieve a TensorDict instance with a shape that matches this trajectory length: rollout = env.rollout(3) print(\"rollout of three steps:\", rollout) print(\"Shape of the rollout TensorDict:\", rollout.batch_size) rollout of three steps: TensorDict( fields={ action: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False), done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False), next: TensorDict( fields={ done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False), observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False), reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False), step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False), terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False), truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)}, batch_size=torch.Size([3]), device=cpu, is_shared=False), observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False), step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False), terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False), truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)}, batch_size=torch.Size([3]), device=cpu, is_shared=False) Shape of the rollout TensorDict: torch.Size([3]) Our rollout data has a shape of torch.Size([3]), which matches the number of steps we ran it for. The \"next\" entry points to the data coming after the current step. In most cases, the \"next\" data at time t matches the data at t+1, but this may not be the case if we are using some specific transformations (for example, multi-step). Policy# PPO utilizes a stochastic policy to handle exploration. This means that our neural network will have to output the parameters of a distribution, rather than a single value corresponding to the action taken. As the data is continuous, we use a Tanh-Normal distribution to respect the action space boundaries. TorchRL provides such distribution, and the only thing we need to care about is to build a neural network that outputs the right number of parameters for the policy to work with (a location, or mean, and a scale): \\[f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})\\] The only extra-difficulty that is brought up here is to split our output in two equal parts and map the second to a strictly positive space. We design the policy in three steps: Define a neural network D_obs -\u003e 2 * D_action. Indeed, our loc (mu) and scale (sigma) both have dimension D_action. Append a NormalParamExtractor to extract a location and a scale (for example, splits the input in two equal parts and applies a positive transformation to the scale parameter). Create a probabilistic TensorDictModule that can generate this distribution and sample from it. actor_net = nn.Sequential( nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(2 * env.action_spec.shape[-1], device=device), NormalParamExtractor(), ) To enable the policy to \u201ctalk\u201d with the environment through the tensordict data carrier, we wrap the nn.Module in a TensorDictModule. This class will simply ready the in_keys it is provided with and write the outputs in-place at the registered out_keys. policy_module = TensorDictModule( actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"] ) We now need to build a distribution out of the location and scale of our normal distribution. To do so, we instruct the ProbabilisticActor class to build a TanhNormal out of the location and scale parameters. We also provide the minimum and maximum values of this distribution, which we gather from the environment specs. The name of the in_keys (and hence the name of the out_keys from the TensorDictModule above) cannot be set to any value one may like, as the TanhNormal distribution constructor will expect the loc and scale keyword arguments. That being said, ProbabilisticActor also accepts Dict[str, str] typed in_keys where the key-value pair indicates what in_key string should be used for every keyword argument that is to be used. policy_module = ProbabilisticActor( module=policy_module, spec=env.action_spec, in_keys=[\"loc\", \"scale\"], distribution_class=TanhNormal, distribution_kwargs={ \"low\": env.action_spec.space.low, \"high\": env.action_spec.space.high, }, return_log_prob=True, # we\u0027ll need the log-prob for the numerator of the importance weights ) Value network# The value network is a crucial component of the PPO algorithm, even though it won\u2019t be used at inference time. This module will read the observations and return an estimation of the discounted return for the following trajectory. This allows us to amortize learning by relying on the some utility estimation that is learned on-the-fly during training. Our value network share the same structure as the policy, but for simplicity we assign it its own set of parameters. value_net = nn.Sequential( nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear(1, device=device), ) value_module = ValueOperator( module=value_net, in_keys=[\"observation\"], ) let\u2019s try our policy and value modules. As we said earlier, the usage of TensorDictModule makes it possible to directly read the output of the environment to run these modules, as they know what information to read and where to write it: print(\"Running policy:\", policy_module(env.reset())) print(\"Running value:\", value_module(env.reset())) Running policy: TensorDict( fields={ action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False), action_log_prob: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False), done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False), loc: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False), observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False), scale: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False), step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False), terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False), truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)}, batch_size=torch.Size([]), device=cpu, is_shared=False) Running value: TensorDict( fields={ done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False), observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False), state_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False), step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False), terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False), truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)}, batch_size=torch.Size([]), device=cpu, is_shared=False) Data collector# TorchRL provides a set of DataCollector classes. Briefly, these classes execute three operations: reset an environment, compute an action given the latest observation, execute a step in the environment, and repeat the last two steps until the environment signals a stop (or reaches a done state). They allow you to control how many frames to collect at each iteration (through the frames_per_batch parameter), when to reset the environment (through the max_frames_per_traj argument), on which device the policy should be executed, etc. They are also designed to work efficiently with batched and multiprocessed environments. The simplest data collector is the SyncDataCollector: it is an iterator that you can use to get batches of data of a given length, and that will stop once a total number of frames (total_frames) have been collected. Other data collectors (MultiSyncDataCollector and MultiaSyncDataCollector) will execute the same operations in synchronous and asynchronous manner over a set of multiprocessed workers. As for the policy and environment before, the data collector will return TensorDict instances with a total number of elements that will match frames_per_batch. Using TensorDict to pass data to the training loop allows you to write data loading pipelines that are 100% oblivious to the actual specificities of the rollout content. collector = SyncDataCollector( env, policy_module, frames_per_batch=frames_per_batch, total_frames=total_frames, split_trajs=False, device=device, ) Replay buffer# Replay buffers are a common building piece of off-policy RL algorithms. In on-policy contexts, a replay buffer is refilled every time a batch of data is collected, and its data is repeatedly consumed for a certain number of epochs. TorchRL\u2019s replay buffers are built using a common container ReplayBuffer which takes as argument the components of the buffer: a storage, a writer, a sampler and possibly some transforms. Only the storage (which indicates the replay buffer capacity) is mandatory. We also specify a sampler without repetition to avoid sampling multiple times the same item in one epoch. Using a replay buffer for PPO is not mandatory and we could simply sample the sub-batches from the collected batch, but using these classes make it easy for us to build the inner training loop in a reproducible way. replay_buffer = ReplayBuffer( storage=LazyTensorStorage(max_size=frames_per_batch), sampler=SamplerWithoutReplacement(), ) Loss function# The PPO loss can be directly imported from TorchRL for convenience using the ClipPPOLoss class. This is the easiest way of utilizing PPO: it hides away the mathematical operations of PPO and the control flow that goes with it. PPO requires some \u201cadvantage estimation\u201d to be computed. In short, an advantage is a value that reflects an expectancy over the return value while dealing with the bias / variance tradeoff. To compute the advantage, one just needs to (1) build the advantage module, which utilizes our value operator, and (2) pass each batch of data through it before each epoch. The GAE module will update the input tensordict with new \"advantage\" and \"value_target\" entries. The \"value_target\" is a gradient-free tensor that represents the empirical value that the value network should represent with the input observation. Both of these will be used by ClipPPOLoss to return the policy and value losses. advantage_module = GAE( gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True, device=device, ) loss_module = ClipPPOLoss( actor_network=policy_module, critic_network=value_module, clip_epsilon=clip_epsilon, entropy_bonus=bool(entropy_eps), entropy_coef=entropy_eps, # these keys match by default but we set this for completeness critic_coef=1.0, loss_critic_type=\"smooth_l1\", ) optim = torch.optim.Adam(loss_module.parameters(), lr) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( optim, total_frames // frames_per_batch, 0.0 ) /usr/local/lib/python3.10/dist-packages/torchrl/objectives/ppo.py:384: DeprecationWarning: \u0027critic_coef\u0027 is deprecated and will be removed in torchrl v0.11. Please use \u0027critic_coeff\u0027 instead. /usr/local/lib/python3.10/dist-packages/torchrl/objectives/ppo.py:450: DeprecationWarning: \u0027entropy_coef\u0027 is deprecated and will be removed in torchrl v0.11. Please use \u0027entropy_coeff\u0027 instead. Training loop# We now have all the pieces needed to code our training loop. The steps include: Collect data Compute advantage Loop over the collected to compute loss values Back propagate Optimize Repeat Repeat Repeat logs = defaultdict(list) pbar = tqdm(total=total_frames) eval_str = \"\" # We iterate over the collector until it reaches the total number of frames it was # designed to collect: for i, tensordict_data in enumerate(collector): # we now have a batch of data to work with. Let\u0027s learn something from it. for _ in range(num_epochs): # We\u0027ll need an \"advantage\" signal to make PPO work. # We re-compute it at each epoch as its value depends on the value # network which is updated in the inner loop. advantage_module(tensordict_data) data_view = tensordict_data.reshape(-1) replay_buffer.extend(data_view.cpu()) for _ in range(frames_per_batch // sub_batch_size): subdata = replay_buffer.sample(sub_batch_size) loss_vals = loss_module(subdata.to(device)) loss_value = ( loss_vals[\"loss_objective\"] + loss_vals[\"loss_critic\"] + loss_vals[\"loss_entropy\"] ) # Optimization: backward, grad clipping and optimization step loss_value.backward() # this is not strictly mandatory but it\u0027s good practice to keep # your gradient norm bounded torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm) optim.step() optim.zero_grad() logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item()) pbar.update(tensordict_data.numel()) cum_reward_str = ( f\"average reward={logs[\u0027reward\u0027][-1]: 4.4f} (init={logs[\u0027reward\u0027][0]: 4.4f})\" ) logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item()) stepcount_str = f\"step count (max): {logs[\u0027step_count\u0027][-1]}\" logs[\"lr\"].append(optim.param_groups[0][\"lr\"]) lr_str = f\"lr policy: {logs[\u0027lr\u0027][-1]: 4.4f}\" if i % 10 == 0: # We evaluate the policy once every 10 batches of data. # Evaluation is rather simple: execute the policy without exploration # (take the expected value of the action distribution) for a given # number of steps (1000, which is our ``env`` horizon). # The ``rollout`` method of the ``env`` can take a policy as argument: # it will then execute this policy at each step. with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad(): # execute a rollout with the trained policy eval_rollout = env.rollout(1000, policy_module) logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item()) logs[\"eval reward (sum)\"].append( eval_rollout[\"next\", \"reward\"].sum().item() ) logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item()) eval_str = ( f\"eval cumulative reward: {logs[\u0027eval reward (sum)\u0027][-1]: 4.4f} \" f\"(init: {logs[\u0027eval reward (sum)\u0027][0]: 4.4f}), \" f\"eval step-count: {logs[\u0027eval step_count\u0027][-1]}\" ) del eval_rollout pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str])) # We\u0027re also using a learning rate scheduler. Like the gradient clipping, # this is a nice-to-have but nothing necessary for PPO to work. scheduler.step() 0%| | 0/50000 [00:00\u003c?, ?it/s] 2%|\u258f | 1000/50000 [00:03\u003c02:45, 296.82it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.0765 (init= 9.0765), step count (max): 12, lr policy: 0.0003: 2%|\u258f | 1000/50000 [00:03\u003c02:45, 296.82it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.0765 (init= 9.0765), step count (max): 12, lr policy: 0.0003: 4%|\u258d | 2000/50000 [00:06\u003c02:33, 311.93it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1294 (init= 9.0765), step count (max): 13, lr policy: 0.0003: 4%|\u258d | 2000/50000 [00:06\u003c02:33, 311.93it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1294 (init= 9.0765), step count (max): 13, lr policy: 0.0003: 6%|\u258c | 3000/50000 [00:09\u003c02:26, 321.54it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1462 (init= 9.0765), step count (max): 16, lr policy: 0.0003: 6%|\u258c | 3000/50000 [00:09\u003c02:26, 321.54it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1462 (init= 9.0765), step count (max): 16, lr policy: 0.0003: 8%|\u258a | 4000/50000 [00:12\u003c02:20, 327.09it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1730 (init= 9.0765), step count (max): 24, lr policy: 0.0003: 8%|\u258a | 4000/50000 [00:12\u003c02:20, 327.09it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.1730 (init= 9.0765), step count (max): 24, lr policy: 0.0003: 10%|\u2588 | 5000/50000 [00:15\u003c02:16, 329.82it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2085 (init= 9.0765), step count (max): 37, lr policy: 0.0003: 10%|\u2588 | 5000/50000 [00:15\u003c02:16, 329.82it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2085 (init= 9.0765), step count (max): 37, lr policy: 0.0003: 12%|\u2588\u258f | 6000/50000 [00:18\u003c02:12, 332.52it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2212 (init= 9.0765), step count (max): 26, lr policy: 0.0003: 12%|\u2588\u258f | 6000/50000 [00:18\u003c02:12, 332.52it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2212 (init= 9.0765), step count (max): 26, lr policy: 0.0003: 14%|\u2588\u258d | 7000/50000 [00:21\u003c02:10, 329.76it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2448 (init= 9.0765), step count (max): 38, lr policy: 0.0003: 14%|\u2588\u258d | 7000/50000 [00:21\u003c02:10, 329.76it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2448 (init= 9.0765), step count (max): 38, lr policy: 0.0003: 16%|\u2588\u258c | 8000/50000 [00:24\u003c02:05, 333.52it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2331 (init= 9.0765), step count (max): 59, lr policy: 0.0003: 16%|\u2588\u258c | 8000/50000 [00:24\u003c02:05, 333.52it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2331 (init= 9.0765), step count (max): 59, lr policy: 0.0003: 18%|\u2588\u258a | 9000/50000 [00:27\u003c02:01, 336.21it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2632 (init= 9.0765), step count (max): 42, lr policy: 0.0003: 18%|\u2588\u258a | 9000/50000 [00:27\u003c02:01, 336.21it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2632 (init= 9.0765), step count (max): 42, lr policy: 0.0003: 20%|\u2588\u2588 | 10000/50000 [00:30\u003c01:58, 338.21it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2542 (init= 9.0765), step count (max): 58, lr policy: 0.0003: 20%|\u2588\u2588 | 10000/50000 [00:30\u003c01:58, 338.21it/s] eval cumulative reward: 139.0010 (init: 139.0010), eval step-count: 14, average reward= 9.2542 (init= 9.0765), step count (max): 58, lr policy: 0.0003: 22%|\u2588\u2588\u258f | 11000/50000 [00:33\u003c01:54, 339.60it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2481 (init= 9.0765), step count (max): 47, lr policy: 0.0003: 22%|\u2588\u2588\u258f | 11000/50000 [00:33\u003c01:54, 339.60it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2481 (init= 9.0765), step count (max): 47, lr policy: 0.0003: 24%|\u2588\u2588\u258d | 12000/50000 [00:36\u003c01:51, 339.71it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2591 (init= 9.0765), step count (max): 59, lr policy: 0.0003: 24%|\u2588\u2588\u258d | 12000/50000 [00:36\u003c01:51, 339.71it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2591 (init= 9.0765), step count (max): 59, lr policy: 0.0003: 26%|\u2588\u2588\u258c | 13000/50000 [00:39\u003c01:48, 340.66it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2543 (init= 9.0765), step count (max): 53, lr policy: 0.0003: 26%|\u2588\u2588\u258c | 13000/50000 [00:39\u003c01:48, 340.66it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2543 (init= 9.0765), step count (max): 53, lr policy: 0.0003: 28%|\u2588\u2588\u258a | 14000/50000 [00:42\u003c01:46, 336.53it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2668 (init= 9.0765), step count (max): 49, lr policy: 0.0003: 28%|\u2588\u2588\u258a | 14000/50000 [00:42\u003c01:46, 336.53it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2668 (init= 9.0765), step count (max): 49, lr policy: 0.0003: 30%|\u2588\u2588\u2588 | 15000/50000 [00:44\u003c01:43, 338.30it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2620 (init= 9.0765), step count (max): 65, lr policy: 0.0002: 30%|\u2588\u2588\u2588 | 15000/50000 [00:44\u003c01:43, 338.30it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2620 (init= 9.0765), step count (max): 65, lr policy: 0.0002: 32%|\u2588\u2588\u2588\u258f | 16000/50000 [00:47\u003c01:40, 339.85it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2642 (init= 9.0765), step count (max): 66, lr policy: 0.0002: 32%|\u2588\u2588\u2588\u258f | 16000/50000 [00:47\u003c01:40, 339.85it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2642 (init= 9.0765), step count (max): 66, lr policy: 0.0002: 34%|\u2588\u2588\u2588\u258d | 17000/50000 [00:50\u003c01:37, 339.62it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2748 (init= 9.0765), step count (max): 56, lr policy: 0.0002: 34%|\u2588\u2588\u2588\u258d | 17000/50000 [00:50\u003c01:37, 339.62it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2748 (init= 9.0765), step count (max): 56, lr policy: 0.0002: 36%|\u2588\u2588\u2588\u258c | 18000/50000 [00:53\u003c01:33, 340.78it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2773 (init= 9.0765), step count (max): 80, lr policy: 0.0002: 36%|\u2588\u2588\u2588\u258c | 18000/50000 [00:53\u003c01:33, 340.78it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2773 (init= 9.0765), step count (max): 80, lr policy: 0.0002: 38%|\u2588\u2588\u2588\u258a | 19000/50000 [00:56\u003c01:30, 341.46it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2838 (init= 9.0765), step count (max): 76, lr policy: 0.0002: 38%|\u2588\u2588\u2588\u258a | 19000/50000 [00:56\u003c01:30, 341.46it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2838 (init= 9.0765), step count (max): 76, lr policy: 0.0002: 40%|\u2588\u2588\u2588\u2588 | 20000/50000 [00:59\u003c01:28, 337.52it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2824 (init= 9.0765), step count (max): 74, lr policy: 0.0002: 40%|\u2588\u2588\u2588\u2588 | 20000/50000 [00:59\u003c01:28, 337.52it/s] eval cumulative reward: 185.2007 (init: 139.0010), eval step-count: 19, average reward= 9.2824 (init= 9.0765), step count (max): 74, lr policy: 0.0002: 42%|\u2588\u2588\u2588\u2588\u258f | 21000/50000 [01:02\u003c01:25, 339.36it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2797 (init= 9.0765), step count (max): 58, lr policy: 0.0002: 42%|\u2588\u2588\u2588\u2588\u258f | 21000/50000 [01:02\u003c01:25, 339.36it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2797 (init= 9.0765), step count (max): 58, lr policy: 0.0002: 44%|\u2588\u2588\u2588\u2588\u258d | 22000/50000 [01:05\u003c01:23, 336.56it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2753 (init= 9.0765), step count (max): 60, lr policy: 0.0002: 44%|\u2588\u2588\u2588\u2588\u258d | 22000/50000 [01:05\u003c01:23, 336.56it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2753 (init= 9.0765), step count (max): 60, lr policy: 0.0002: 46%|\u2588\u2588\u2588\u2588\u258c | 23000/50000 [01:08\u003c01:20, 337.30it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2819 (init= 9.0765), step count (max): 82, lr policy: 0.0002: 46%|\u2588\u2588\u2588\u2588\u258c | 23000/50000 [01:08\u003c01:20, 337.30it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2819 (init= 9.0765), step count (max): 82, lr policy: 0.0002: 48%|\u2588\u2588\u2588\u2588\u258a | 24000/50000 [01:11\u003c01:16, 339.06it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2949 (init= 9.0765), step count (max): 99, lr policy: 0.0002: 48%|\u2588\u2588\u2588\u2588\u258a | 24000/50000 [01:11\u003c01:16, 339.06it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2949 (init= 9.0765), step count (max): 99, lr policy: 0.0002: 50%|\u2588\u2588\u2588\u2588\u2588 | 25000/50000 [01:14\u003c01:13, 339.52it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2834 (init= 9.0765), step count (max): 65, lr policy: 0.0002: 50%|\u2588\u2588\u2588\u2588\u2588 | 25000/50000 [01:14\u003c01:13, 339.52it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2834 (init= 9.0765), step count (max): 65, lr policy: 0.0002: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 26000/50000 [01:17\u003c01:10, 340.38it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2981 (init= 9.0765), step count (max): 90, lr policy: 0.0001: 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 26000/50000 [01:17\u003c01:10, 340.38it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2981 (init= 9.0765), step count (max): 90, lr policy: 0.0001: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 27000/50000 [01:20\u003c01:08, 336.71it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2999 (init= 9.0765), step count (max): 77, lr policy: 0.0001: 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 27000/50000 [01:20\u003c01:08, 336.71it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2999 (init= 9.0765), step count (max): 77, lr policy: 0.0001: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 28000/50000 [01:23\u003c01:04, 338.64it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2983 (init= 9.0765), step count (max): 75, lr policy: 0.0001: 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 28000/50000 [01:23\u003c01:04, 338.64it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.2983 (init= 9.0765), step count (max): 75, lr policy: 0.0001: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 29000/50000 [01:26\u003c01:01, 340.53it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.3038 (init= 9.0765), step count (max): 126, lr policy: 0.0001: 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 29000/50000 [01:26\u003c01:01, 340.53it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.3038 (init= 9.0765), step count (max): 126, lr policy: 0.0001: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 30000/50000 [01:29\u003c00:58, 341.52it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.3015 (init= 9.0765), step count (max): 122, lr policy: 0.0001: 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 30000/50000 [01:29\u003c00:58, 341.52it/s] eval cumulative reward: 390.5743 (init: 139.0010), eval step-count: 41, average reward= 9.3015 (init= 9.0765), step count (max): 122, lr policy: 0.0001: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 31000/50000 [01:32\u003c00:55, 342.12it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3018 (init= 9.0765), step count (max): 103, lr policy: 0.0001: 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 31000/50000 [01:32\u003c00:55, 342.12it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3018 (init= 9.0765), step count (max): 103, lr policy: 0.0001: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 32000/50000 [01:35\u003c00:53, 339.61it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3080 (init= 9.0765), step count (max): 127, lr policy: 0.0001: 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 32000/50000 [01:35\u003c00:53, 339.61it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3080 (init= 9.0765), step count (max): 127, lr policy: 0.0001: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 33000/50000 [01:38\u003c00:50, 336.50it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3236 (init= 9.0765), step count (max): 165, lr policy: 0.0001: 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 33000/50000 [01:38\u003c00:50, 336.50it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3236 (init= 9.0765), step count (max): 165, lr policy: 0.0001: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 34000/50000 [01:40\u003c00:47, 338.07it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3255 (init= 9.0765), step count (max): 224, lr policy: 0.0001: 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 34000/50000 [01:40\u003c00:47, 338.07it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3255 (init= 9.0765), step count (max): 224, lr policy: 0.0001: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 35000/50000 [01:43\u003c00:44, 338.36it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3039 (init= 9.0765), step count (max): 272, lr policy: 0.0001: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 35000/50000 [01:43\u003c00:44, 338.36it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3039 (init= 9.0765), step count (max): 272, lr policy: 0.0001: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 36000/50000 [01:46\u003c00:41, 337.91it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3138 (init= 9.0765), step count (max): 137, lr policy: 0.0001: 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 36000/50000 [01:46\u003c00:41, 337.91it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3138 (init= 9.0765), step count (max): 137, lr policy: 0.0001: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 37000/50000 [01:49\u003c00:38, 340.19it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3243 (init= 9.0765), step count (max): 244, lr policy: 0.0001: 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 37000/50000 [01:49\u003c00:38, 340.19it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3243 (init= 9.0765), step count (max): 244, lr policy: 0.0001: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 38000/50000 [01:52\u003c00:35, 341.76it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3262 (init= 9.0765), step count (max): 356, lr policy: 0.0000: 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 38000/50000 [01:52\u003c00:35, 341.76it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3262 (init= 9.0765), step count (max): 356, lr policy: 0.0000: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 39000/50000 [01:55\u003c00:32, 337.87it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3171 (init= 9.0765), step count (max): 252, lr policy: 0.0000: 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 39000/50000 [01:55\u003c00:32, 337.87it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3171 (init= 9.0765), step count (max): 252, lr policy: 0.0000: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 40000/50000 [01:58\u003c00:29, 339.72it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3164 (init= 9.0765), step count (max): 130, lr policy: 0.0000: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 40000/50000 [01:58\u003c00:29, 339.72it/s] eval cumulative reward: 465.8858 (init: 139.0010), eval step-count: 49, average reward= 9.3164 (init= 9.0765), step count (max): 130, lr policy: 0.0000: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41000/50000 [02:01\u003c00:26, 341.15it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3227 (init= 9.0765), step count (max): 152, lr policy: 0.0000: 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41000/50000 [02:01\u003c00:26, 341.15it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3227 (init= 9.0765), step count (max): 152, lr policy: 0.0000: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 42000/50000 [02:04\u003c00:23, 337.07it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3220 (init= 9.0765), step count (max): 131, lr policy: 0.0000: 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 42000/50000 [02:04\u003c00:23, 337.07it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3220 (init= 9.0765), step count (max): 131, lr policy: 0.0000: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 43000/50000 [02:07\u003c00:20, 339.93it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3256 (init= 9.0765), step count (max): 188, lr policy: 0.0000: 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 43000/50000 [02:07\u003c00:20, 339.93it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3256 (init= 9.0765), step count (max): 188, lr policy: 0.0000: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44000/50000 [02:10\u003c00:17, 342.05it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3346 (init= 9.0765), step count (max): 229, lr policy: 0.0000: 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44000/50000 [02:10\u003c00:17, 342.05it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3346 (init= 9.0765), step count (max): 229, lr policy: 0.0000: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 45000/50000 [02:13\u003c00:14, 342.99it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3174 (init= 9.0765), step count (max): 143, lr policy: 0.0000: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 45000/50000 [02:13\u003c00:14, 342.99it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3174 (init= 9.0765), step count (max): 143, lr policy: 0.0000: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 46000/50000 [02:16\u003c00:11, 339.17it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3239 (init= 9.0765), step count (max): 167, lr policy: 0.0000: 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 46000/50000 [02:16\u003c00:11, 339.17it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3239 (init= 9.0765), step count (max): 167, lr policy: 0.0000: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 47000/50000 [02:19\u003c00:08, 340.87it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3238 (init= 9.0765), step count (max): 245, lr policy: 0.0000: 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 47000/50000 [02:19\u003c00:08, 340.87it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3238 (init= 9.0765), step count (max): 245, lr policy: 0.0000: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 48000/50000 [02:22\u003c00:05, 342.62it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3288 (init= 9.0765), step count (max): 184, lr policy: 0.0000: 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 48000/50000 [02:22\u003c00:05, 342.62it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3288 (init= 9.0765), step count (max): 184, lr policy: 0.0000: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 49000/50000 [02:24\u003c00:02, 343.52it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3298 (init= 9.0765), step count (max): 175, lr policy: 0.0000: 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 49000/50000 [02:24\u003c00:02, 343.52it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3298 (init= 9.0765), step count (max): 175, lr policy: 0.0000: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [02:27\u003c00:00, 344.23it/s] eval cumulative reward: 933.0149 (init: 139.0010), eval step-count: 99, average reward= 9.3236 (init= 9.0765), step count (max): 236, lr policy: 0.0000: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [02:27\u003c00:00, 344.23it/s] Results# Before the 1M step cap is reached, the algorithm should have reached a max step count of 1000 steps, which is the maximum number of steps before the trajectory is truncated. plt.figure(figsize=(10, 10)) plt.subplot(2, 2, 1) plt.plot(logs[\"reward\"]) plt.title(\"training rewards (average)\") plt.subplot(2, 2, 2) plt.plot(logs[\"step_count\"]) plt.title(\"Max step count (training)\") plt.subplot(2, 2, 3) plt.plot(logs[\"eval reward (sum)\"]) plt.title(\"Return (test)\") plt.subplot(2, 2, 4) plt.plot(logs[\"eval step_count\"]) plt.title(\"Max step count (test)\") plt.show() Conclusion and next steps# In this tutorial, we have learned: How to create and customize an environment with torchrl; How to write a model and a loss function; How to set up a typical training loop. If you want to experiment with this tutorial a bit more, you can apply the following modifications: From an efficiency perspective, we could run several simulations in parallel to speed up data collection. Check ParallelEnv for further information. From a logging perspective, one could add a torchrl.record.VideoRecorder transform to the environment after asking for rendering to get a visual rendering of the inverted pendulum in action. Check torchrl.record to know more. Total running time of the script: (2 minutes 29.759 seconds) Download Jupyter notebook: reinforcement_ppo.ipynb Download Python source code: reinforcement_ppo.py Download zipped: reinforcement_ppo.zip",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/intermediate/reinforcement_ppo.html"
         },
         "datePublished": "2023-01-01T00:00:00Z",
         "dateModified": "2023-01-01T00:00:00Z"
       }
   </script>
</body>
</body></html>