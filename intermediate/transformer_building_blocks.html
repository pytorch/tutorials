
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Learn how to optimize transformer models by replacing nn.Transformer with Nested Tensors and torch.compile() for significant performance gains in PyTorch." name="description"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile() — PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/transformer_building_blocks';</script>
<link href="https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/intermediate/transformer_building_blocks.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects.
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Accelerating...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Accelerating PyTorch Transformers by replacing &lt;code class=" docutils="" itemprop="name" literal="" notranslate"=""/><span class="pre">nn.Transformer</span> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>"&gt;
        <meta content="1" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/transformer_building_blocks</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-transformer-building-blocks-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="accelerating-pytorch-transformers-by-replacing-nn-transformer-with-nested-tensors-and-torch-compile">
<span id="sphx-glr-intermediate-transformer-building-blocks-py"></span><h1>Accelerating PyTorch Transformers by replacing <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code><a class="headerlink" href="#accelerating-pytorch-transformers-by-replacing-nn-transformer-with-nested-tensors-and-torch-compile" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/mikaylagawarecki">Mikayla Gawarecki</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">Learn about the low-level building blocks PyTorch provides to build custom transformer layers (
nested tensors, <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, and <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code>)</p></li>
<li><p class="sd-card-text">Discover how the above improve memory usage and performance using MultiHeadAttention as an example</p></li>
<li><p class="sd-card-text">Explore advanced customizations using the aforementioned building blocks</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v.2.6.0 or later</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Over the past few years, the PyTorch team has developed various lower level
features that, when composed, can create a variety of transformer variants. These
include:</p>
<ul class="simple">
<li><p>Nested Tensors with the <code class="docutils literal notranslate"><span class="pre">torch.jagged</span></code> layout (AKA NJTs)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code></p></li>
</ul>
<p>This tutorial will give a brief overview of the above technologies and
demonstrate how they can be composed to yield flexible and performant transformer layers with improved user experience.</p>
<p>One may observe that the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module currently provides various <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>-related layers.
In particular, it includes <code class="docutils literal notranslate"><span class="pre">TransformerEncoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerDecoderLayer</span></code>,
<code class="docutils literal notranslate"><span class="pre">TransformerDecoder</span></code>, <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> and <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code>. This family
of layers was initially implemented following the <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is All
You Need</a> paper. The components discussed in
this tutorial provide improved user experience, flexibility and performance over
the existing <code class="docutils literal notranslate"><span class="pre">nn</span></code> layers.</p>
</section>
<section id="is-this-tutorial-for-me">
<h1>Is this tutorial for me?<a class="headerlink" href="#is-this-tutorial-for-me" title="Link to this heading">#</a></h1>
<p>If you are wondering about what building blocks the <code class="docutils literal notranslate"><span class="pre">torch</span></code> library provides
for writing your own transformer layers and best practices, you are in the
right place. Please keep reading!</p>
<p>If you are looking for an out-of-the-box implementation of a popular transformer
architecture, note that there are many open-source libraries that provide them,
including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace transformers</a></p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/xformers">xformers</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a></p></li>
</ul>
<p>If you are only interested in performant attention score modifications, please
check out the <a class="reference external" href="https://pytorch.org/blog/flexattention/">FlexAttention blog</a> that
contains a <a class="reference external" href="https://github.com/meta-pytorch/attention-gym">gym of masks</a>.</p>
</section>
<section id="introducing-the-building-blocks">
<h1>Introducing the Building Blocks<a class="headerlink" href="#introducing-the-building-blocks" title="Link to this heading">#</a></h1>
<p>First, we will briefly introduce the four technologies mentioned in the introduction</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/unstable/nestedtensor.html">torch.nested</a></p></li>
</ul>
<p>Nested tensors generalize the shape of regular dense tensors, allowing for
representation of ragged-sized data with the same tensor UX. In the context of
transformers, we can think of nested tensors as a tool for representing variable
sequence lengths. They eliminate the need for the bug-prone practices of explicit
padding and masking (think <code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> in <code class="docutils literal notranslate"><span class="pre">nn.MultiHeadAttention</span></code>).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">scaled_dot_product_attention</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> is a primitive for
<span class="math">\(\text{softmax}(\frac{QK^T}{\sqrt{E}} + B)V\)</span> that dispatches into either fused
implementations of the operator or a fallback implementation. It works out of
the box in eager mode (i.e. the default mode of using PyTorch where operations
are executed on the fly as they are encountered) and also integrates seamlessly
with <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>. As of 2.6, it will also offer grouped query attention
natively.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile()</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> is a compiler introduced in version 2.0 that is able to
capture a graph of PyTorch code and perform various optimizations on it, such as
fusing together sequences of ops. Nested tensors with the <code class="docutils literal notranslate"><span class="pre">torch.jagged</span></code> layout
and <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> work seamlessly with compile. In the
context of transformers, the value add of using compile with nested tensor
and SDPA is that compile can remove framework overhead ones sees in eager mode
and fuse sequences of ops in transformers together, such as projection and
activation.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/blog/flexattention/">FlexAttention</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code> is a primitive that allows users to modify attention scores
prior to the softmax operation. It generalizes the additive <code class="docutils literal notranslate"><span class="pre">B</span></code> term above
for <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code>, allowing for arbitrary calculation. It
requires compile to achieve good performance.</p>
</section>
<section id="the-above-building-blocks-are-all-you-need-as-of-october-2024">
<h1>The above building blocks are “All You Need” (as of October 2024)<a class="headerlink" href="#the-above-building-blocks-are-all-you-need-as-of-october-2024" title="Link to this heading">#</a></h1>
<p>The main premise in this section is that most transformer variations are
GPT-style, consisting of layers like Embedding, Positional Encoding, Attention
Blocks and Feed Forward networks. If we were to try to classify the differences
in this space, we might land on something like:</p>
<ol class="arabic simple">
<li><p>Layer type (activation functions such as <code class="docutils literal notranslate"><span class="pre">SwiGLU</span></code> and others, normalization functions
such as <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> and others, positional encodings, such as Sinusoidal, Rotary.)</p></li>
<li><p>Layer ordering, such as where to apply norms and positional encoding.</p></li>
<li><p>Modifications to attention score, such as <code class="docutils literal notranslate"><span class="pre">ALiBi</span></code>, Relative Positional Bias and so on.</p></li>
</ol>
<p>In a pre-compiler environment, you might write a custom transformer and notice
that it functions correctly but is slow. To address this, you might develop a
custom fused kernel for the specific series of operations. In a compiler environment,
you can simply perform the initial step and then compile and benefit from improved performance.</p>
<section id="multiheadattention">
<h2>MultiheadAttention<a class="headerlink" href="#multiheadattention" title="Link to this heading">#</a></h2>
<p>Remember that MultiheadAttention takes in a query, key, and value, and consists
of an input projection, a <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> operator and an
output projection. The main takeaway we want to demonstrate here is the
improvement yielded when we replaced padded/masked inputs with nested tensors.
The improvements are threefold:</p>
<ul class="simple">
<li><p><strong>User Experience</strong>
Remember that <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> requires <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and
<code class="docutils literal notranslate"><span class="pre">value</span></code> to be dense <code class="docutils literal notranslate"><span class="pre">torch.Tensors</span></code>. It also provides a
<code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> that is used to mask out padding tokens in the <code class="docutils literal notranslate"><span class="pre">key</span></code>
that arise due to different sequence lengths within a batch. Since there is
no <code class="docutils literal notranslate"><span class="pre">query_padding_mask</span></code> in <code class="docutils literal notranslate"><span class="pre">nn.MHA</span></code>, users have to take care to mask/slice
the outputs appropriately to account for query sequence lengths. <code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code>
cleanly removes the need for this sort of error-prone padding masks.</p></li>
<li><p><strong>Memory</strong>
Instead of materializing a dense <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S,</span> <span class="pre">D]</span></code> tensor with a <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S]</span></code>
padding mask (where <code class="docutils literal notranslate"><span class="pre">B</span></code> is batch size, <code class="docutils literal notranslate"><span class="pre">S</span></code> is max sequence length in the
batch and <code class="docutils literal notranslate"><span class="pre">D</span></code> is embedding size), nested tensors allow you to cleanly
represent the batch of varying sequence lengths. As a result, the input and
intermediate activations will use less memory.</p></li>
<li><p><strong>Performance</strong>
Since padding is not materialized and unnecessary computation on padding is
skipped, performance and memory usage improve.</p></li>
</ul>
<p>We’ll demonstrate the above by building upon the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer in the
<a class="reference external" href="https://pytorch.org/tutorials/unstable/nestedtensor.html">Nested Tensor tutorial</a>
and comparing it to the <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> layer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Computes multi-head attention. Supports nested or padded tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        E_q (int): Size of embedding dim for query</span>
<span class="sd">        E_k (int): Size of embedding dim for key</span>
<span class="sd">        E_v (int): Size of embedding dim for value</span>
<span class="sd">        E_total (int): Total embedding dim of combined heads post input projection. Each head</span>
<span class="sd">            has dim E_total // nheads</span>
<span class="sd">        nheads (int): Number of heads</span>
<span class="sd">        dropout (float, optional): Dropout probability. Default: 0.0</span>
<span class="sd">        bias (bool, optional): Whether to add bias to input projection. Default: True</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">E_q</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_v</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_total</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">nheads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">"dtype"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nheads</span> <span class="o">=</span> <span class="n">nheads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span> <span class="o">=</span> <span class="n">E_q</span> <span class="o">==</span> <span class="n">E_k</span> <span class="ow">and</span> <span class="n">E_q</span> <span class="o">==</span> <span class="n">E_v</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_k</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="n">E_out</span> <span class="o">=</span> <span class="n">E_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_total</span><span class="p">,</span> <span class="n">E_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">E_total</span> <span class="o">%</span> <span class="n">nheads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"Embedding dim is not divisible by nheads"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span> <span class="o">=</span> <span class="n">E_total</span> <span class="o">//</span> <span class="n">nheads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Forward pass; runs the following process:</span>
<span class="sd">            1. Apply input projection</span>
<span class="sd">            2. Split heads and prepare for SDPA</span>
<span class="sd">            3. Run SDPA</span>
<span class="sd">            4. Apply output projection</span>

<span class="sd">        Args:</span>
<span class="sd">            query (torch.Tensor): query of shape (``N``, ``L_q``, ``E_qk``)</span>
<span class="sd">            key (torch.Tensor): key of shape (``N``, ``L_kv``, ``E_qk``)</span>
<span class="sd">            value (torch.Tensor): value of shape (``N``, ``L_kv``, ``E_v``)</span>
<span class="sd">            attn_mask (torch.Tensor, optional): attention mask of shape (``N``, ``L_q``, ``L_kv``) to pass to SDPA. Default: None</span>
<span class="sd">            is_causal (bool, optional): Whether to apply causal mask. Default: False</span>

<span class="sd">        Returns:</span>
<span class="sd">            attn_output (torch.Tensor): output of shape (N, L_t, E_q)</span>
<span class="sd">        """</span>
        <span class="c1"># Step 1. Apply input projection</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">query</span> <span class="ow">is</span> <span class="n">key</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">is</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="torch.chunk"><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span></a><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">q_weight</span><span class="p">,</span> <span class="n">k_weight</span><span class="p">,</span> <span class="n">v_weight</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="torch.chunk"><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span></a><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                    <span class="n">q_bias</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">,</span> <span class="n">v_bias</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="torch.chunk"><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span></a><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">q_bias</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">,</span> <span class="n">v_bias</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">(</span>
                    <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear" title="torch.nn.functional.linear"><span class="n">F</span><span class="o">.</span><span class="n">linear</span></a><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">q_weight</span><span class="p">,</span> <span class="n">q_bias</span><span class="p">),</span>
                    <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear" title="torch.nn.functional.linear"><span class="n">F</span><span class="o">.</span><span class="n">linear</span></a><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">k_weight</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">),</span>
                    <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear" title="torch.nn.functional.linear"><span class="n">F</span><span class="o">.</span><span class="n">linear</span></a><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">v_weight</span><span class="p">,</span> <span class="n">v_bias</span><span class="p">),</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Step 2. Split heads and prepare for SDPA</span>
        <span class="c1"># reshape query, key, value to separate by head</span>
        <span class="c1"># (N, L_t, E_total) -&gt; (N, L_t, nheads, E_head) -&gt; (N, nheads, L_t, E_head)</span>
        <span class="n">query</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">query</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># (N, L_s, E_total) -&gt; (N, L_s, nheads, E_head) -&gt; (N, nheads, L_s, E_head)</span>
        <span class="n">key</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">key</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># (N, L_s, E_total) -&gt; (N, L_s, nheads, E_head) -&gt; (N, nheads, L_s, E_head)</span>
        <span class="n">value</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">value</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 3. Run SDPA</span>
        <span class="c1"># (N, nheads, L_t, E_head)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention" title="torch.nn.functional.scaled_dot_product_attention"><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span></a><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span>
        <span class="p">)</span>
        <span class="c1"># (N, nheads, L_t, E_head) -&gt; (N, L_t, nheads, E_head) -&gt; (N, L_t, E_total)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 4. Apply output projection</span>
        <span class="c1"># (N, L_t, E_total) -&gt; (N, L_t, E_out)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attn_output</span>
</pre></div>
</div>
<section id="utilities">
<h3>Utilities<a class="headerlink" href="#utilities" title="Link to this heading">#</a></h3>
<p>In this section, we include a utility to generate semi-realistic data using
<code class="docutils literal notranslate"><span class="pre">Zipf</span></code> distribution for sentence lengths. This is used to generate the nested
query, key, and value tensors. We also include a benchmark utility.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="k">def</span><span class="w"> </span><span class="nf">zipf_sentence_lengths</span><span class="p">(</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">:</span>
    <span class="c1"># generate fake corpus by unigram Zipf distribution</span>
    <span class="c1"># from wikitext-2 corpus, we get rank "." = 3, "!" = 386, "?" = 858</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">zipf</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">while</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">386</span> <span class="ow">and</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">858</span><span class="p">:</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">zipf</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">)</span>


<span class="c1"># Generate a batch of semi-realistic data using Zipf distribution for sentence lengths</span>
<span class="c1"># in the form of nested tensors with the jagged layout.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">float32</span></a><span class="p">,</span> <span class="n">query_seq_len_1</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># generate semi-realistic data using Zipf distribution for sentence lengths</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a> <span class="o">=</span> <span class="n">zipf_sentence_lengths</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Note: the torch.jagged layout is a nested tensor layout that supports a single ragged</span>
    <span class="c1"># dimension and works with torch.compile. The batch items each have shape (B, S*, D)</span>
    <span class="c1"># where B = batch size, S* = ragged sequence length, and D = embedding dimension.</span>
    <span class="k">if</span> <span class="n">query_seq_len_1</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nested sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor" title="torch.nested.nested_tensor"><span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span></a><span class="p">(</span>
            <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">],</span>
            <span class="n">layout</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="torch.layout"><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span></a><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nested sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor" title="torch.nested.nested_tensor"><span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span></a><span class="p">(</span>
            <span class="p">[</span>
                <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_q</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a>
            <span class="p">],</span>
            <span class="n">layout</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="torch.layout"><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span></a><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">key</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nested sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor" title="torch.nested.nested_tensor"><span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span></a><span class="p">(</span>
        <span class="p">[</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_k</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a>
        <span class="p">],</span>
        <span class="n">layout</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="torch.layout"><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span></a><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">value</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nested sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor" title="torch.nested.nested_tensor"><span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span></a><span class="p">(</span>
        <span class="p">[</span>
            <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_v</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a> <span class="ow">in</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a>
        <span class="p">],</span>
        <span class="n">layout</span><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="torch.layout"><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span></a><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a>


<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">timeit</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span></a><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span></a><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">begin</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
</pre></div>
</div>
<p>We will now demonstrate the performance improvements of using nested tensors
in the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer + compile for self attention. We compare this against
the traditional <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> + compile with padding and masking.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span>
<span class="n">E_out</span> <span class="o">=</span> <span class="n">E_q</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">E_q</span>
<span class="n">nheads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">bias</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Total sequence length in nested query </span><span class="si">{</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><span class="n">S</span><span class="si">}</span><span class="s2">"</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_key</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_value</span></a> <span class="o">=</span> <span class="p">(</span>
    <span class="n">t</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="p">)</span>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">mha_layer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MultiHeadAttention</span></a><span class="p">(</span>
    <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">vanilla_mha_layer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention" title="torch.nn.MultiheadAttention"><span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span></a><span class="p">(</span>
    <span class="n">E_q</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span>
<span class="p">)</span>

<span class="c1"># ``nn.MultiheadAttention`` uses a non conventional initialization for layers, so do this for exact parity :(</span>
<a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span></a><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span></a><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span></a><span class="p">(</span><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span></a><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">new_mha_layer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">mha_layer</span><span class="p">)</span>
<span class="c1"># warmup compile</span>
<span class="n">nested_result_warmup</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="n">nested_result</span><span class="p">,</span> <span class="n">nested_time</span><span class="p">,</span> <span class="n">nested_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">new_mha_layer</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_nested_result</span></a> <span class="o">=</span> <span class="n">nested_result</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># For the vanilla ``nn.MultiheadAttention``, we need to construct the ``key_padding_mask``</span>
<span class="c1"># Further, ``nn.MultiheadAttention`` forces one to materialize the ``attn_mask`` even if using ``is_causal``</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">src_key_padding_mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.where.html#torch.where" title="torch.where"><span class="n">torch</span><span class="o">.</span><span class="n">where</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a> <span class="o">==</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.empty.html#torch.empty" title="torch.empty"><span class="n">torch</span><span class="o">.</span><span class="n">empty</span></a><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">"-inf"</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">):</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a><span class="p">,</span> <span class="p">:</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a><span class="p">]</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-Transformer sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask" title="torch.nn.Transformer.generate_square_subsequent_mask"><span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="o">.</span><span class="n">generate_square_subsequent_mask</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">s</span></a><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

<span class="n">vanilla_mha_layer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">vanilla_mha_layer</span><span class="p">)</span>
<span class="c1"># warmup compile</span>
<span class="n">warmup_vanilla_result</span> <span class="o">=</span> <span class="n">vanilla_mha_layer</span><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">src_key_padding_mask</span></a><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">padded_time</span><span class="p">,</span> <span class="n">padded_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">src_key_padding_mask</span></a><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">attn_mask</span></a><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">padded_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, padded_peak_memory=</span><span class="si">{</span><span class="n">padded_peak_memory</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">nested_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, nested_peak_memory=</span><span class="si">{</span><span class="n">nested_peak_memory</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Max difference between vanilla and nested result"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_nested_result</span></a><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Nested speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_time</span><span class="o">/</span><span class="n">nested_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Nested peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_peak_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_peak_memory</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Total sequence length in nested query 10609, max sequence length 110
/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:282: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.

padded_time=0.01400, padded_peak_memory=3.11 GB
nested_time=0.00244, nested_peak_memory=0.65 GB
Max difference between vanilla and nested result 0.0
Nested speedup: 5.74
Nested peak memory reduction 2.45 GB
</pre></div>
</div>
<p>For reference, here are some sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">padded_time</span><span class="o">=</span><span class="mf">0.03454</span><span class="p">,</span> <span class="n">padded_peak_memory</span><span class="o">=</span><span class="mf">4.14</span> <span class="n">GB</span>
<span class="n">nested_time</span><span class="o">=</span><span class="mf">0.00612</span><span class="p">,</span> <span class="n">nested_peak_memory</span><span class="o">=</span><span class="mf">0.76</span> <span class="n">GB</span>
<span class="n">Max</span> <span class="n">difference</span> <span class="n">between</span> <span class="n">vanilla</span> <span class="ow">and</span> <span class="n">nested</span> <span class="n">result</span> <span class="mf">0.0</span>
<span class="n">Nested</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">5.65</span>
<span class="n">Nested</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">3.39</span> <span class="n">GB</span>
</pre></div>
</div>
<p>We can also see the same for backward pass</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">entry_length</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a><span class="p">):</span>
    <span class="c1"># padding-specific step: remove output projection bias from padded entries for fair comparison</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a><span class="p">[</span><span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">entry_length</span></a><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="n">_</span><span class="p">,</span> <span class="n">padded_bw_time</span><span class="p">,</span> <span class="n">padded_bw_peak_mem</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="k">lambda</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">nested_bw_time</span><span class="p">,</span> <span class="n">nested_bw_peak_mem</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="k">lambda</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_nested_result</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">padded_bw_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, padded_bw_peak_mem=</span><span class="si">{</span><span class="n">padded_bw_peak_mem</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">nested_bw_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, nested_bw_peak_mem=</span><span class="si">{</span><span class="n">nested_bw_peak_mem</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Nested backward speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_bw_time</span><span class="o">/</span><span class="n">nested_bw_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Nested backward peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_bw_peak_mem</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_bw_peak_mem</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Difference in out_proj.weight.grad"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span></a><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Difference in packed_proj.weight.grad"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">grad</span></a><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Difference in out_proj.bias.grad"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span></a><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Difference in packed_proj.bias.grad"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">grad</span></a><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>padded_bw_time=1.50520, padded_bw_peak_mem=3.83 GB
nested_bw_time=0.06649, nested_bw_peak_mem=2.47 GB
Nested backward speedup: 22.64
Nested backward peak memory reduction 1.36 GB
Difference in out_proj.weight.grad 0.0003662109375
Difference in packed_proj.weight.grad 0.001708984375
Difference in out_proj.bias.grad 0.0
Difference in packed_proj.bias.grad 0.001953125
</pre></div>
</div>
<p>Sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">padded_bw_time</span><span class="o">=</span><span class="mf">2.09337</span><span class="p">,</span> <span class="n">padded_bw_peak_mem</span><span class="o">=</span><span class="mf">5.10</span> <span class="n">GB</span>
<span class="n">nested_bw_time</span><span class="o">=</span><span class="mf">0.01452</span><span class="p">,</span> <span class="n">nested_bw_peak_mem</span><span class="o">=</span><span class="mf">3.24</span> <span class="n">GB</span>
<span class="n">Nested</span> <span class="n">backward</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">144.13</span>
<span class="n">Nested</span> <span class="n">backward</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">1.86</span> <span class="n">GB</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.000244140625</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.001556396484375</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.0</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.001953125</span>
</pre></div>
</div>
</section>
</section>
<section id="gpt-style-layer">
<h2>GPT-style layer<a class="headerlink" href="#gpt-style-layer" title="Link to this heading">#</a></h2>
<p>A basic GPT-style transformer layer consists of a causal self-attention layer
followed by a feed-forward network (FFN) with skip connections. Implementing
this is fairly straightforward using the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer above and
gives equivalent results to an <code class="docutils literal notranslate"><span class="pre">nn.TransformerEncoderLayer</span></code> with
<code class="docutils literal notranslate"><span class="pre">is_causal=True</span></code>.</p>
<p>We  demonstrate examples of implementing the rest of the <code class="docutils literal notranslate"><span class="pre">nn</span></code> layers
<a class="reference external" href="https://github.com/mikaylagawarecki/transformer_tutorial_accompaniment">here</a>
but omit that from this tutorial for brevity.</p>
</section>
<section id="going-one-step-further">
<h2>Going one step further<a class="headerlink" href="#going-one-step-further" title="Link to this heading">#</a></h2>
<p>So far, we have demonstrated how to implement a performant <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code>
layer that follows the traditional <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code>. Going back to our
classification of modifications to the transformer architecture, remember that we
classified the modifications into layer type, layer ordering, and modifications
to the attention score. We trust that changing layer type and layer ordering
(such as swapping <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> for <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code>) is fairly straightforward.</p>
<p>In this section, we will discuss various functionalities using the
aforementioned building blocks, including the following:</p>
<ul class="simple">
<li><p>Cross Attention</p></li>
<li><p>Fully masked rows no longer cause NaNs</p></li>
<li><p>Modifying attention score: ALiBi with FlexAttention and NJT</p></li>
<li><p>Packed Projection</p></li>
</ul>
</section>
<section id="cross-attention">
<h2>Cross Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h2>
<p>Cross attention is a form of attention where the query and key/value tensors
are from different sequences.</p>
<p>One example of this is in <code class="docutils literal notranslate"><span class="pre">nn.TransformerDecoderLayer</span></code> where the query comes
from the decoder and the key/value come from the encoder.</p>
<p>The above MultiheadAttention layer nicely generalizes to this case with nested
tensors for both query and key/value.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">q_len</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">kv_len</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Total sequence length in nested query </span><span class="si">{</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">q_len</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">q_len</span></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Total sequence length in nested key/value </span><span class="si">{</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">kv_len</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">kv_len</span></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
<span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Total sequence length in nested query 10317, max sequence length 124
Total sequence length in nested key/value 10941, max sequence length 130
</pre></div>
</div>
<p>As above, we can compare this against the vanilla compiled <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">query</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">q_len</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">kv_len</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_key</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_value</span></a> <span class="o">=</span> <span class="p">(</span>
    <span class="n">t</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="p">)</span>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.where.html#torch.where" title="torch.where"><span class="n">torch</span><span class="o">.</span><span class="n">where</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_key</span></a> <span class="o">==</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># warmup compile</span>
<span class="n">warmup_nested_result</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">warmup_vanilla_result</span> <span class="o">=</span> <span class="n">vanilla_mha_layer</span><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_key</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_value</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">nested_result</span><span class="p">,</span> <span class="n">nested_time</span><span class="p">,</span> <span class="n">nested_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">new_mha_layer</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">padded_time</span><span class="p">,</span> <span class="n">padded_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_query</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_key</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_value</span></a><span class="p">,</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">key_padding_mask</span></a><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_nested_result</span></a> <span class="o">=</span> <span class="n">nested_result</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">entry_length</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">q_len</span></a><span class="p">):</span>
    <span class="c1"># padding-specific step: remove output projection bias from padded entries for fair comparison</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a><span class="p">[</span><span class="n">i</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">entry_length</span></a><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"Max difference between vanilla and nested result"</span><span class="p">,</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_result</span></a> <span class="o">-</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">padded_nested_result</span></a><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Nested speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_time</span><span class="o">/</span><span class="n">nested_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"Nested peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_peak_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_peak_memory</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Max difference between vanilla and nested result 0.0
Nested speedup: 5.06
Nested peak memory reduction 1.26 GB
</pre></div>
</div>
<p>Sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">difference</span> <span class="n">between</span> <span class="n">vanilla</span> <span class="ow">and</span> <span class="n">nested</span> <span class="n">result</span> <span class="mf">0.0</span>
<span class="n">Nested</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">4.01</span>
<span class="n">Nested</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">1.40</span> <span class="n">GB</span>
</pre></div>
</div>
</section>
<section id="fully-masked-rows-no-longer-cause-nans">
<h2>Fully masked rows no longer cause NaNs<a class="headerlink" href="#fully-masked-rows-no-longer-cause-nans" title="Link to this heading">#</a></h2>
<p>There has been a long standing issue with <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> and
<code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> where if a row was fully masked out, the output
of the attention layer would be NaN. See <a class="reference external" href="https://github.com/pytorch/pytorch/issues/41508">issue</a>.
This is because the softmax over an empty set is undefined.</p>
<p>Thanks to <a class="reference external" href="https://github.com/pytorch/pytorch/pull/133882">this PR</a>
this is no longer the case. Instead, the output corresponding to fully masked rows
in <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> will be 0. For cases where <code class="docutils literal notranslate"><span class="pre">nn.MHA</span></code> does
not employ the “fast-path”, this will also apply.</p>
<p>Using a custom MHA layer with NJTs is strongly recommended over the
existing “fast-path” in <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> as NJT’s ability to model raggedness
appropriately makes it possible to properly express empty sequences.</p>
</section>
<section id="flexattention-njt">
<h2>FlexAttention + NJT<a class="headerlink" href="#flexattention-njt" title="Link to this heading">#</a></h2>
<p>NJT also composes with the <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code> module. This is a generalization
of the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer that allows for arbitrary modifications
to the attention score. The example below takes the <code class="docutils literal notranslate"><span class="pre">alibi_mod</span></code>
that implements <a class="reference external" href="https://arxiv.org/abs/2108.12409">ALiBi</a> from
<a class="reference external" href="https://github.com/meta-pytorch/attention-gym">attention gym</a> and uses it
with nested input tensors.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention.flex_attention</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.flex_attention" title="torch.nn.attention.flex_attention.flex_attention"><span class="n">flex_attention</span></a>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_alibi_bias</span><span class="p">(</span><span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Returns an alibi bias score_mod given the number of heads H</span>
<span class="sd">    Args:</span>
<span class="sd">        H: number of heads</span>
<span class="sd">    Returns:</span>
<span class="sd">        alibi_bias: alibi bias score_mod</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">alibi_mod</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2" title="torch.exp2"><span class="n">torch</span><span class="o">.</span><span class="n">exp2</span></a><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">8.0</span> <span class="o">/</span> <span class="n">H</span><span class="p">))</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_idx</span> <span class="o">-</span> <span class="n">kv_idx</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="n">bias</span>

    <span class="k">return</span> <span class="n">alibi_mod</span>


<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">E_q</span> <span class="o">//</span> <span class="mi">8</span>
<span class="n">alibi_score_mod</span> <span class="o">=</span> <span class="n">generate_alibi_bias</span><span class="p">(</span><span class="n">n_heads</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">query</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">key</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">key</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">value</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">value</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">out_flex2</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.flex_attention" title="torch.nn.attention.flex_attention.flex_attention"><span class="n">flex_attention</span></a><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">score_mod</span><span class="o">=</span><span class="n">alibi_score_mod</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition, one can also use the <code class="docutils literal notranslate"><span class="pre">block_mask</span></code> utility of <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code>
with NJTs via the <code class="docutils literal notranslate"><span class="pre">create_nested_block_mask</span></code> function. This is useful for
taking advantage of the sparsity of the mask to speed up the attention computation.
In particular, the function creates a sparse block mask for a “stacked sequence” of all
the variable length sequences in the NJT combined into one, while properly masking out
inter-sequence attention. In the following example, we show how to create a
causal block mask using this utility.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention.flex_attention</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_nested_block_mask" title="torch.nn.attention.flex_attention.create_nested_block_mask"><span class="n">create_nested_block_mask</span></a>


<span class="k">def</span><span class="w"> </span><span class="nf">causal_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">q_idx</span> <span class="o">&gt;=</span> <span class="n">kv_idx</span>


<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask" title="torch.nn.attention.flex_attention.BlockMask"><span class="n">block_mask</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_nested_block_mask" title="torch.nn.attention.flex_attention.create_nested_block_mask"><span class="n">create_nested_block_mask</span></a><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">query</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">key</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">key</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">value</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><span class="n">value</span><span class="o">.</span><span class="n">unflatten</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">out_flex</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.flex_attention" title="torch.nn.attention.flex_attention.flex_attention"><span class="n">flex_attention</span></a><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask" title="torch.nn.attention.flex_attention.BlockMask"><span class="n">block_mask</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch-nn-attention-flex_attention sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask" title="torch.nn.attention.flex_attention.BlockMask"><span class="n">block_mask</span></a><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="packed-projection">
<h2>Packed Projection<a class="headerlink" href="#packed-projection" title="Link to this heading">#</a></h2>
<p>Packed projection is a technique that makes use of the fact that when the input
for projection (matrix multiplications) are the same (self-attention), we can pack the projection
weights and biases into single tensors. It is especially useful when the individual
projections are memory bound rather than compute bound. There are
two examples that we will demonstrate here:</p>
<ul class="simple">
<li><p>Input projection for MultiheadAttention</p></li>
<li><p>SwiGLU activation in feed-forward network of Transformer Layer</p></li>
</ul>
<section id="input-projection-for-multiheadattention">
<h3>Input projection for MultiheadAttention<a class="headerlink" href="#input-projection-for-multiheadattention" title="Link to this heading">#</a></h3>
<p>When doing self-attention, the <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and <code class="docutils literal notranslate"><span class="pre">value</span></code>
are the same tensor. Each of these tensors is projected with a
<code class="docutils literal notranslate"><span class="pre">Linear(E_q,</span> <span class="pre">E_total)</span></code> layer. Instead, we can pack this into one layer,
which is what we do in the MultiheadAttention layer above.</p>
<p>Let us compare the performance of the packed projection against the usual method:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">InputProjection</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">"dtype"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PackedInputProjection</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">"dtype"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="torch.chunk"><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="p">(</span><span class="n">query</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">8192</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision" title="torch.set_float32_matmul_precision"><span class="n">torch</span><span class="o">.</span><span class="n">set_float32_matmul_precision</span></a><span class="p">(</span><span class="s2">"high"</span><span class="p">)</span>
<span class="n">in_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">InputProjection</span></a><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">))</span>
<span class="n">packed_in_proj</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">PackedInputProjection</span></a><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">)</span>
<span class="p">)</span>

<span class="n">q</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sequence_lengths</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">)</span>

<span class="c1"># warmup</span>
<span class="n">in_proj</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">packed_in_proj</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="p">(</span><span class="n">q_out</span><span class="p">,</span> <span class="n">k_out</span><span class="p">,</span> <span class="n">v_out</span><span class="p">),</span> <span class="n">time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">in_proj</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="p">(</span><span class="n">q_out</span><span class="p">,</span> <span class="n">k_out</span><span class="p">,</span> <span class="n">v_out</span><span class="p">),</span> <span class="n">time_packed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">packed_in_proj</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="c1"># On my A100 prints 1.05x speedup</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"InputProjection: </span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="s2">5f</span><span class="si">}</span><span class="s2"> s, PackedInputProjection: </span><span class="si">{</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">5f</span><span class="si">}</span><span class="s2"> s, speedup: </span><span class="si">{</span><span class="n">time</span><span class="o">/</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>InputProjection: 0.034053 s, PackedInputProjection: 0.033968 s, speedup: 1.00x
</pre></div>
</div>
</section>
<section id="swiglu-feed-forward-network-of-transformer-layer">
<h3>SwiGLU feed forward network of Transformer Layer<a class="headerlink" href="#swiglu-feed-forward-network-of-transformer-layer" title="Link to this heading">#</a></h3>
<p>Swish-Gated Linear Unit (SwiGLU) is a non-linear activation function that is increasingly popular in the feed-forward
network of the transformer layer (e.g. Llama). A feed-forward network with SwiGLU activation is defined as:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SwiGLUFFN</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">multiple_of</span><span class="p">,</span>
        <span class="n">ffn_dim_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">"dtype"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># custom dim factor multiplier</span>
        <span class="k">if</span> <span class="n">ffn_dim_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ffn_dim_multiplier</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.silu.html#torch.nn.functional.silu" title="torch.nn.functional.silu"><span class="n">F</span><span class="o">.</span><span class="n">silu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>An alternative way of implementing this that uses packed projection is</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PackedSwiGLUFFN</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">multiple_of</span><span class="p">,</span>
        <span class="n">ffn_dim_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"device"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">"dtype"</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># custom dim factor multiplier</span>
        <span class="k">if</span> <span class="n">ffn_dim_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ffn_dim_multiplier</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w13</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="torch.chunk"><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w13</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.silu.html#torch.nn.functional.silu" title="torch.nn.functional.silu"><span class="n">F</span><span class="o">.</span><span class="n">silu</span></a><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x3</span><span class="p">)</span>
</pre></div>
</div>
<p>We can compare the performance of the two implementations as follows
Depending on your hardware, you might see different results. On an A100 I see
1.12x speedup for D=128.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">swigluffn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">SwiGLUFFN</span></a><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">))</span>
<span class="n">packed_swigluffn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">PackedSwiGLUFFN</span></a><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">)</span>
<span class="p">)</span>

<span class="n">q</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">sentence_lengths</span></a> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">dtype</span></a><span class="o">=</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></a><span class="p">)</span>

<span class="c1"># warmup</span>
<span class="n">swigluffn</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">packed_swigluffn</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">swigluffn</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time_packed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">packed_swigluffn</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="c1"># On my A100 prints 1.08x speedup</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"SwiGLUFFN: </span><span class="si">{</span><span class="n">time</span><span class="si">}</span><span class="s2"> s, PackedSwiGLUFFN: </span><span class="si">{</span><span class="n">time_packed</span><span class="si">}</span><span class="s2"> s, speedup: </span><span class="si">{</span><span class="n">time</span><span class="o">/</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SwiGLUFFN: 0.0009492869999121467 s, PackedSwiGLUFFN: 0.0008643020000818069 s, speedup: 1.10x
</pre></div>
</div>
</section>
</section>
<section id="extended-examples">
<h2>Extended examples<a class="headerlink" href="#extended-examples" title="Link to this heading">#</a></h2>
<p>We intend to update this tutorial to demonstrate more examples of how to use
the various performant building blocks such as KV-Caching, Grouped Query Attention
etc. Further, there are several good examples of using various performant building blocks to
implement various transformer architectures. Some examples include</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/meta-pytorch/gpt-fast">gpt-fast</a></p></li>
<li><p><a class="reference external" href="https://github.com/meta-pytorch/segment-anything-fast">segment-anything-fast</a></p></li>
<li><p><a class="reference external" href="https://github.com/lucidrains/vit-pytorch/blob/73199ab486e0fad9eced2e3350a11681db08b61b/vit_pytorch/na_vit_nested_tensor.py">lucidrains implementation of NaViT with nested tensors</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtune/blob/a8a64ec6a99a6ea2be4fdaf0cd5797b03a2567cf/torchtune/modules/vision_transformer.py#L16">torchtune’s implementation of VisionTransformer</a></p></li>
</ul>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this tutorial, we have introduced the low level building blocks PyTorch
provides for writing transformer layers and demonstrated examples how to compose
them. It is our hope that this tutorial has educated the reader on the ease with
which flexible and performant transformer layers can be implemented by users of PyTorch.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 52.674 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-transformer-building-blocks-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/e201125c39959609ca168c306995205c/transformer_building_blocks.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">transformer_building_blocks.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/57114670a041b4c96ed6eb9fc17a6b3f/transformer_building_blocks.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">transformer_building_blocks.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/9e1bf792ffacdc6aa490d8ac2246cba7/transformer_building_blocks.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">transformer_building_blocks.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Accelerating PyTorch Transformers by replacing <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#is-this-tutorial-for-me">Is this tutorial for me?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-the-building-blocks">Introducing the Building Blocks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-above-building-blocks-are-all-you-need-as-of-october-2024">The above building blocks are “All You Need” (as of October 2024)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention">MultiheadAttention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilities">Utilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-style-layer">GPT-style layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#going-one-step-further">Going one step further</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-masked-rows-no-longer-cause-nans">Fully masked rows no longer cause NaNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flexattention-njt">FlexAttention + NJT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packed-projection">Packed Projection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-projection-for-multiheadattention">Input projection for MultiheadAttention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#swiglu-feed-forward-network-of-transformer-layer">SwiGLU feed forward network of Transformer Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-examples">Extended examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "name": "Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()",
         "headline": "Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/intermediate/transformer_building_blocks.html",
         "articleBody": "Note Go to the end to download the full example code. Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()# Author: Mikayla Gawarecki What you will learn Learn about the low-level building blocks PyTorch provides to build custom transformer layers ( nested tensors, scaled_dot_product_attention, torch.compile(), and FlexAttention) Discover how the above improve memory usage and performance using MultiHeadAttention as an example Explore advanced customizations using the aforementioned building blocks Prerequisites PyTorch v.2.6.0 or later Over the past few years, the PyTorch team has developed various lower level features that, when composed, can create a variety of transformer variants. These include: Nested Tensors with the torch.jagged layout (AKA NJTs) scaled_dot_product_attention torch.compile() FlexAttention This tutorial will give a brief overview of the above technologies and demonstrate how they can be composed to yield flexible and performant transformer layers with improved user experience. One may observe that the torch.nn module currently provides various Transformer-related layers. In particular, it includes TransformerEncoderLayer, TransformerEncoder, TransformerDecoderLayer, TransformerDecoder, Transformer and MultiheadAttention. This family of layers was initially implemented following the Attention is All You Need paper. The components discussed in this tutorial provide improved user experience, flexibility and performance over the existing nn layers. Is this tutorial for me?# If you are wondering about what building blocks the torch library provides for writing your own transformer layers and best practices, you are in the right place. Please keep reading! If you are looking for an out-of-the-box implementation of a popular transformer architecture, note that there are many open-source libraries that provide them, including: HuggingFace transformers xformers torchtune If you are only interested in performant attention score modifications, please check out the FlexAttention blog that contains a gym of masks. Introducing the Building Blocks# First, we will briefly introduce the four technologies mentioned in the introduction torch.nested Nested tensors generalize the shape of regular dense tensors, allowing for representation of ragged-sized data with the same tensor UX. In the context of transformers, we can think of nested tensors as a tool for representing variable sequence lengths. They eliminate the need for the bug-prone practices of explicit padding and masking (think key_padding_mask in nn.MultiHeadAttention). scaled_dot_product_attention scaled_dot_product_attention is a primitive for \\(\\text{softmax}(\\frac{QK^T}{\\sqrt{E}} + B)V\\) that dispatches into either fused implementations of the operator or a fallback implementation. It works out of the box in eager mode (i.e. the default mode of using PyTorch where operations are executed on the fly as they are encountered) and also integrates seamlessly with torch.compile(). As of 2.6, it will also offer grouped query attention natively. torch.compile() torch.compile() is a compiler introduced in version 2.0 that is able to capture a graph of PyTorch code and perform various optimizations on it, such as fusing together sequences of ops. Nested tensors with the torch.jagged layout and scaled_dot_product_attention work seamlessly with compile. In the context of transformers, the value add of using compile with nested tensor and SDPA is that compile can remove framework overhead ones sees in eager mode and fuse sequences of ops in transformers together, such as projection and activation. FlexAttention FlexAttention is a primitive that allows users to modify attention scores prior to the softmax operation. It generalizes the additive B term above for scaled_dot_product_attention, allowing for arbitrary calculation. It requires compile to achieve good performance. The above building blocks are \u201cAll You Need\u201d (as of October 2024)# The main premise in this section is that most transformer variations are GPT-style, consisting of layers like Embedding, Positional Encoding, Attention Blocks and Feed Forward networks. If we were to try to classify the differences in this space, we might land on something like: Layer type (activation functions such as SwiGLU and others, normalization functions such as RMSNorm and others, positional encodings, such as Sinusoidal, Rotary.) Layer ordering, such as where to apply norms and positional encoding. Modifications to attention score, such as ALiBi, Relative Positional Bias and so on. In a pre-compiler environment, you might write a custom transformer and notice that it functions correctly but is slow. To address this, you might develop a custom fused kernel for the specific series of operations. In a compiler environment, you can simply perform the initial step and then compile and benefit from improved performance. MultiheadAttention# Remember that MultiheadAttention takes in a query, key, and value, and consists of an input projection, a scaled_dot_product_attention operator and an output projection. The main takeaway we want to demonstrate here is the improvement yielded when we replaced padded/masked inputs with nested tensors. The improvements are threefold: User Experience Remember that nn.MultiheadAttention requires query, key, and value to be dense torch.Tensors. It also provides a key_padding_mask that is used to mask out padding tokens in the key that arise due to different sequence lengths within a batch. Since there is no query_padding_mask in nn.MHA, users have to take care to mask/slice the outputs appropriately to account for query sequence lengths. NestedTensor cleanly removes the need for this sort of error-prone padding masks. Memory Instead of materializing a dense [B, S, D] tensor with a [B, S] padding mask (where B is batch size, S is max sequence length in the batch and D is embedding size), nested tensors allow you to cleanly represent the batch of varying sequence lengths. As a result, the input and intermediate activations will use less memory. Performance Since padding is not materialized and unnecessary computation on padding is skipped, performance and memory usage improve. We\u2019ll demonstrate the above by building upon the MultiheadAttention layer in the Nested Tensor tutorial and comparing it to the nn.MultiheadAttention layer. import torch import torch.nn as nn import torch.nn.functional as F class MultiHeadAttention(nn.Module): \"\"\" Computes multi-head attention. Supports nested or padded tensors. Args: E_q (int): Size of embedding dim for query E_k (int): Size of embedding dim for key E_v (int): Size of embedding dim for value E_total (int): Total embedding dim of combined heads post input projection. Each head has dim E_total // nheads nheads (int): Number of heads dropout (float, optional): Dropout probability. Default: 0.0 bias (bool, optional): Whether to add bias to input projection. Default: True \"\"\" def __init__( self, E_q: int, E_k: int, E_v: int, E_total: int, nheads: int, dropout: float = 0.0, bias=True, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.nheads = nheads self.dropout = dropout self._qkv_same_embed_dim = E_q == E_k and E_q == E_v if self._qkv_same_embed_dim: self.packed_proj = nn.Linear(E_q, E_total * 3, bias=bias, **factory_kwargs) else: self.q_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.k_proj = nn.Linear(E_k, E_total, bias=bias, **factory_kwargs) self.v_proj = nn.Linear(E_v, E_total, bias=bias, **factory_kwargs) E_out = E_q self.out_proj = nn.Linear(E_total, E_out, bias=bias, **factory_kwargs) assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\" self.E_head = E_total // nheads self.bias = bias def forward( self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask=None, is_causal=False, ) -\u003e torch.Tensor: \"\"\" Forward pass; runs the following process: 1. Apply input projection 2. Split heads and prepare for SDPA 3. Run SDPA 4. Apply output projection Args: query (torch.Tensor): query of shape (``N``, ``L_q``, ``E_qk``) key (torch.Tensor): key of shape (``N``, ``L_kv``, ``E_qk``) value (torch.Tensor): value of shape (``N``, ``L_kv``, ``E_v``) attn_mask (torch.Tensor, optional): attention mask of shape (``N``, ``L_q``, ``L_kv``) to pass to SDPA. Default: None is_causal (bool, optional): Whether to apply causal mask. Default: False Returns: attn_output (torch.Tensor): output of shape (N, L_t, E_q) \"\"\" # Step 1. Apply input projection if self._qkv_same_embed_dim: if query is key and key is value: result = self.packed_proj(query) query, key, value = torch.chunk(result, 3, dim=-1) else: q_weight, k_weight, v_weight = torch.chunk( self.packed_proj.weight, 3, dim=0 ) if self.bias: q_bias, k_bias, v_bias = torch.chunk( self.packed_proj.bias, 3, dim=0 ) else: q_bias, k_bias, v_bias = None, None, None query, key, value = ( F.linear(query, q_weight, q_bias), F.linear(key, k_weight, k_bias), F.linear(value, v_weight, v_bias), ) else: query = self.q_proj(query) key = self.k_proj(key) value = self.v_proj(value) # Step 2. Split heads and prepare for SDPA # reshape query, key, value to separate by head # (N, L_t, E_total) -\u003e (N, L_t, nheads, E_head) -\u003e (N, nheads, L_t, E_head) query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # (N, L_s, E_total) -\u003e (N, L_s, nheads, E_head) -\u003e (N, nheads, L_s, E_head) key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # (N, L_s, E_total) -\u003e (N, L_s, nheads, E_head) -\u003e (N, nheads, L_s, E_head) value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # Step 3. Run SDPA # (N, nheads, L_t, E_head) attn_output = F.scaled_dot_product_attention( query, key, value, dropout_p=self.dropout, is_causal=is_causal ) # (N, nheads, L_t, E_head) -\u003e (N, L_t, nheads, E_head) -\u003e (N, L_t, E_total) attn_output = attn_output.transpose(1, 2).flatten(-2) # Step 4. Apply output projection # (N, L_t, E_total) -\u003e (N, L_t, E_out) attn_output = self.out_proj(attn_output) return attn_output Utilities# In this section, we include a utility to generate semi-realistic data using Zipf distribution for sentence lengths. This is used to generate the nested query, key, and value tensors. We also include a benchmark utility. import numpy as np def zipf_sentence_lengths(alpha: float, batch_size: int) -\u003e torch.Tensor: # generate fake corpus by unigram Zipf distribution # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858 sentence_lengths = np.empty(batch_size, dtype=int) for ibatch in range(batch_size): sentence_lengths[ibatch] = 1 word = np.random.zipf(alpha) while word != 3 and word != 386 and word != 858: sentence_lengths[ibatch] += 1 word = np.random.zipf(alpha) return torch.tensor(sentence_lengths) # Generate a batch of semi-realistic data using Zipf distribution for sentence lengths # in the form of nested tensors with the jagged layout. def gen_batch(N, E_q, E_k, E_v, device, dtype=torch.float32, query_seq_len_1=False): # generate semi-realistic data using Zipf distribution for sentence lengths sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N) # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged # dimension and works with torch.compile. The batch items each have shape (B, S*, D) # where B = batch size, S* = ragged sequence length, and D = embedding dimension. if query_seq_len_1: query = torch.nested.nested_tensor( [torch.randn(1, E_q, dtype=dtype, device=device) for l in sentence_lengths], layout=torch.jagged, ) else: query = torch.nested.nested_tensor( [ torch.randn(l.item(), E_q, dtype=dtype, device=device) for l in sentence_lengths ], layout=torch.jagged, ) key = torch.nested.nested_tensor( [ torch.randn(s.item(), E_k, dtype=dtype, device=device) for s in sentence_lengths ], layout=torch.jagged, ) value = torch.nested.nested_tensor( [ torch.randn(s.item(), E_v, dtype=dtype, device=device) for s in sentence_lengths ], layout=torch.jagged, ) return query, key, value, sentence_lengths import math import timeit def benchmark(func, *args, **kwargs): torch.cuda.synchronize() torch.cuda.reset_peak_memory_stats() begin = timeit.default_timer() output = func(*args, **kwargs) torch.cuda.synchronize() end = timeit.default_timer() return output, (end - begin), torch.cuda.max_memory_allocated() We will now demonstrate the performance improvements of using nested tensors in the MultiheadAttention layer + compile for self attention. We compare this against the traditional nn.MultiheadAttention + compile with padding and masking. N, E_q, E_k, E_v, E_total = 512, 512, 512, 512, 512 E_out = E_q d_model = E_q nheads = 8 dropout = 0.0 bias = True device = \"cuda\" torch.manual_seed(6) query, key, value, sentence_lengths = gen_batch(N, E_q, E_k, E_v, device) S = sentence_lengths.max().item() print( f\"Total sequence length in nested query {sentence_lengths.sum().item()}, max sequence length {S}\" ) padded_query, padded_key, padded_value = ( t.to_padded_tensor(0.0) for t in (query, key, value) ) torch.manual_seed(6) mha_layer = MultiHeadAttention( E_q, E_k, E_v, E_total, nheads, dropout=dropout, bias=bias, device=\"cuda\" ) torch.manual_seed(6) vanilla_mha_layer = nn.MultiheadAttention( E_q, nheads, dropout=dropout, batch_first=True, bias=bias, device=\"cuda\" ) # ``nn.MultiheadAttention`` uses a non conventional initialization for layers, so do this for exact parity :( mha_layer.out_proj.weight = nn.Parameter( vanilla_mha_layer.out_proj.weight.clone().detach() ) mha_layer.packed_proj.weight = nn.Parameter( vanilla_mha_layer.in_proj_weight.clone().detach() ) mha_layer.out_proj.bias = nn.Parameter(vanilla_mha_layer.out_proj.bias.clone().detach()) mha_layer.packed_proj.bias = nn.Parameter( vanilla_mha_layer.in_proj_bias.clone().detach() ) new_mha_layer = torch.compile(mha_layer) # warmup compile nested_result_warmup = new_mha_layer(query, query, query, is_causal=True) # benchmark nested_result, nested_time, nested_peak_memory = benchmark( new_mha_layer, query, query, query, is_causal=True ) padded_nested_result = nested_result.to_padded_tensor(0.0) # For the vanilla ``nn.MultiheadAttention``, we need to construct the ``key_padding_mask`` # Further, ``nn.MultiheadAttention`` forces one to materialize the ``attn_mask`` even if using ``is_causal`` src_key_padding_mask = torch.where(padded_query == 0.0, -math.inf, 0)[:, :, 0] attn_mask = torch.empty((N, S, S), device=device).fill_(float(\"-inf\")) for i, s in enumerate(sentence_lengths): attn_mask[i, :s, :s] = nn.Transformer.generate_square_subsequent_mask(s) attn_mask = attn_mask.unsqueeze(1).expand(N, nheads, S, S).reshape(N * nheads, S, S) vanilla_mha_layer = torch.compile(vanilla_mha_layer) # warmup compile warmup_vanilla_result = vanilla_mha_layer( padded_query, padded_query, padded_query, attn_mask=attn_mask, key_padding_mask=src_key_padding_mask, need_weights=False, is_causal=True, ) # benchmark (padded_result, _), padded_time, padded_peak_memory = benchmark( vanilla_mha_layer, padded_query, padded_query, padded_query, key_padding_mask=src_key_padding_mask, need_weights=False, attn_mask=attn_mask, is_causal=True, ) print(f\"{padded_time=:.5f}, padded_peak_memory={padded_peak_memory/1e9:.2f} GB\") print(f\"{nested_time=:.5f}, nested_peak_memory={nested_peak_memory/1e9:.2f} GB\") print( \"Max difference between vanilla and nested result\", (padded_result - padded_nested_result).abs().max().item(), ) print(f\"Nested speedup: {(padded_time/nested_time):.2f}\") print( f\"Nested peak memory reduction {((padded_peak_memory - nested_peak_memory)/1e9):.2f} GB\" ) Total sequence length in nested query 10609, max sequence length 110 /usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(\u0027high\u0027)` for better performance. padded_time=0.01400, padded_peak_memory=3.11 GB nested_time=0.00244, nested_peak_memory=0.65 GB Max difference between vanilla and nested result 0.0 Nested speedup: 5.74 Nested peak memory reduction 2.45 GB For reference, here are some sample outputs on A100: padded_time=0.03454, padded_peak_memory=4.14 GB nested_time=0.00612, nested_peak_memory=0.76 GB Max difference between vanilla and nested result 0.0 Nested speedup: 5.65 Nested peak memory reduction 3.39 GB We can also see the same for backward pass for i, entry_length in enumerate(sentence_lengths): # padding-specific step: remove output projection bias from padded entries for fair comparison padded_result[i, entry_length:, :] = 0.0 _, padded_bw_time, padded_bw_peak_mem = benchmark( lambda: padded_result.sum().backward() ) _, nested_bw_time, nested_bw_peak_mem = benchmark( lambda: padded_nested_result.sum().backward() ) print(f\"{padded_bw_time=:.5f}, padded_bw_peak_mem={padded_bw_peak_mem/1e9:.2f} GB\") print(f\"{nested_bw_time=:.5f}, nested_bw_peak_mem={nested_bw_peak_mem/1e9:.2f} GB\") print(f\"Nested backward speedup: {(padded_bw_time/nested_bw_time):.2f}\") print( f\"Nested backward peak memory reduction {((padded_bw_peak_mem - nested_bw_peak_mem)/1e9):.2f} GB\" ) print( \"Difference in out_proj.weight.grad\", (mha_layer.out_proj.weight.grad - vanilla_mha_layer.out_proj.weight.grad) .abs() .max() .item(), ) print( \"Difference in packed_proj.weight.grad\", (mha_layer.packed_proj.weight.grad - vanilla_mha_layer.in_proj_weight.grad) .abs() .max() .item(), ) print( \"Difference in out_proj.bias.grad\", (mha_layer.out_proj.bias.grad - vanilla_mha_layer.out_proj.bias.grad) .abs() .max() .item(), ) print( \"Difference in packed_proj.bias.grad\", (mha_layer.packed_proj.bias.grad - vanilla_mha_layer.in_proj_bias.grad) .abs() .max() .item(), ) padded_bw_time=1.50520, padded_bw_peak_mem=3.83 GB nested_bw_time=0.06649, nested_bw_peak_mem=2.47 GB Nested backward speedup: 22.64 Nested backward peak memory reduction 1.36 GB Difference in out_proj.weight.grad 0.0003662109375 Difference in packed_proj.weight.grad 0.001708984375 Difference in out_proj.bias.grad 0.0 Difference in packed_proj.bias.grad 0.001953125 Sample outputs on A100: padded_bw_time=2.09337, padded_bw_peak_mem=5.10 GB nested_bw_time=0.01452, nested_bw_peak_mem=3.24 GB Nested backward speedup: 144.13 Nested backward peak memory reduction 1.86 GB Difference in out_proj.weight.grad 0.000244140625 Difference in packed_proj.weight.grad 0.001556396484375 Difference in out_proj.bias.grad 0.0 Difference in packed_proj.bias.grad 0.001953125 GPT-style layer# A basic GPT-style transformer layer consists of a causal self-attention layer followed by a feed-forward network (FFN) with skip connections. Implementing this is fairly straightforward using the MultiheadAttention layer above and gives equivalent results to an nn.TransformerEncoderLayer with is_causal=True. We demonstrate examples of implementing the rest of the nn layers here but omit that from this tutorial for brevity. Going one step further# So far, we have demonstrated how to implement a performant MultiheadAttention layer that follows the traditional nn.MultiheadAttention. Going back to our classification of modifications to the transformer architecture, remember that we classified the modifications into layer type, layer ordering, and modifications to the attention score. We trust that changing layer type and layer ordering (such as swapping LayerNorm for RMSNorm) is fairly straightforward. In this section, we will discuss various functionalities using the aforementioned building blocks, including the following: Cross Attention Fully masked rows no longer cause NaNs Modifying attention score: ALiBi with FlexAttention and NJT Packed Projection Cross Attention# Cross attention is a form of attention where the query and key/value tensors are from different sequences. One example of this is in nn.TransformerDecoderLayer where the query comes from the decoder and the key/value come from the encoder. The above MultiheadAttention layer nicely generalizes to this case with nested tensors for both query and key/value. query, _, _, q_len = gen_batch(N, E_q, E_k, E_v, device) _, key, value, kv_len = gen_batch(N, E_q, E_k, E_v, device) print( f\"Total sequence length in nested query {q_len.sum().item()}, max sequence length {q_len.max().item()}\" ) print( f\"Total sequence length in nested key/value {kv_len.sum().item()}, max sequence length {kv_len.max().item()}\" ) out = new_mha_layer(query, key, value, is_causal=False) Total sequence length in nested query 10317, max sequence length 124 Total sequence length in nested key/value 10941, max sequence length 130 As above, we can compare this against the vanilla compiled nn.MultiheadAttention. torch.manual_seed(6) query, _, _, q_len = gen_batch(N, E_q, E_k, E_v, device) _, key, value, kv_len = gen_batch(N, E_q, E_k, E_v, device) padded_query, padded_key, padded_value = ( t.to_padded_tensor(0.0) for t in (query, key, value) ) key_padding_mask = torch.where(padded_key == 0.0, -math.inf, 0)[:, :, 0] # warmup compile warmup_nested_result = new_mha_layer(query, key, value, is_causal=False) warmup_vanilla_result = vanilla_mha_layer( padded_query, padded_key, padded_value, key_padding_mask=key_padding_mask, need_weights=False, is_causal=False, ) nested_result, nested_time, nested_peak_memory = benchmark( new_mha_layer, query, key, value, is_causal=False ) (padded_result, _), padded_time, padded_peak_memory = benchmark( vanilla_mha_layer, padded_query, padded_key, padded_value, key_padding_mask=key_padding_mask, need_weights=False, is_causal=False, ) padded_nested_result = nested_result.to_padded_tensor(0.0) for i, entry_length in enumerate(q_len): # padding-specific step: remove output projection bias from padded entries for fair comparison padded_result[i, entry_length:, :] = 0.0 print( \"Max difference between vanilla and nested result\", (padded_result - padded_nested_result).abs().max().item(), ) print(f\"Nested speedup: {(padded_time/nested_time):.2f}\") print( f\"Nested peak memory reduction {((padded_peak_memory - nested_peak_memory)/1e9):.2f} GB\" ) Max difference between vanilla and nested result 0.0 Nested speedup: 5.06 Nested peak memory reduction 1.26 GB Sample outputs on A100: Max difference between vanilla and nested result 0.0 Nested speedup: 4.01 Nested peak memory reduction 1.40 GB Fully masked rows no longer cause NaNs# There has been a long standing issue with nn.MultiheadAttention and scaled_dot_product_attention where if a row was fully masked out, the output of the attention layer would be NaN. See issue. This is because the softmax over an empty set is undefined. Thanks to this PR this is no longer the case. Instead, the output corresponding to fully masked rows in scaled_dot_product_attention will be 0. For cases where nn.MHA does not employ the \u201cfast-path\u201d, this will also apply. Using a custom MHA layer with NJTs is strongly recommended over the existing \u201cfast-path\u201d in nn.MultiheadAttention as NJT\u2019s ability to model raggedness appropriately makes it possible to properly express empty sequences. FlexAttention + NJT# NJT also composes with the FlexAttention module. This is a generalization of the MultiheadAttention layer that allows for arbitrary modifications to the attention score. The example below takes the alibi_mod that implements ALiBi from attention gym and uses it with nested input tensors. from torch.nn.attention.flex_attention import flex_attention def generate_alibi_bias(H: int): \"\"\"Returns an alibi bias score_mod given the number of heads H Args: H: number of heads Returns: alibi_bias: alibi bias score_mod \"\"\" def alibi_mod(score, b, h, q_idx, kv_idx): scale = torch.exp2(-((h + 1) * 8.0 / H)) bias = (q_idx - kv_idx) * scale return score + bias return alibi_mod query, key, value, _ = gen_batch(N, E_q, E_k, E_v, device) n_heads, D = 8, E_q // 8 alibi_score_mod = generate_alibi_bias(n_heads) query = query.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() key = key.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() value = value.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() out_flex2 = flex_attention(query, key, value, score_mod=alibi_score_mod) In addition, one can also use the block_mask utility of FlexAttention with NJTs via the create_nested_block_mask function. This is useful for taking advantage of the sparsity of the mask to speed up the attention computation. In particular, the function creates a sparse block mask for a \u201cstacked sequence\u201d of all the variable length sequences in the NJT combined into one, while properly masking out inter-sequence attention. In the following example, we show how to create a causal block mask using this utility. from torch.nn.attention.flex_attention import create_nested_block_mask def causal_mask(b, h, q_idx, kv_idx): return q_idx \u003e= kv_idx query, key, value, _ = gen_batch(N, E_q, E_k, E_v, device) block_mask = create_nested_block_mask(causal_mask, 1, 1, query, _compile=True) query = query.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() key = key.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() value = value.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() out_flex = flex_attention(query, key, value, block_mask=block_mask) Packed Projection# Packed projection is a technique that makes use of the fact that when the input for projection (matrix multiplications) are the same (self-attention), we can pack the projection weights and biases into single tensors. It is especially useful when the individual projections are memory bound rather than compute bound. There are two examples that we will demonstrate here: Input projection for MultiheadAttention SwiGLU activation in feed-forward network of Transformer Layer Input projection for MultiheadAttention# When doing self-attention, the query, key, and value are the same tensor. Each of these tensors is projected with a Linear(E_q, E_total) layer. Instead, we can pack this into one layer, which is what we do in the MultiheadAttention layer above. Let us compare the performance of the packed projection against the usual method: class InputProjection(nn.Module): def __init__(self, E_q, E_total, bias=False, device=None, dtype=None): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.q_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.k_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.v_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) def forward(self, x): return self.q_proj(x), self.k_proj(x), self.v_proj(x) class PackedInputProjection(nn.Module): def __init__(self, E_q, E_total, bias=False, device=None, dtype=None): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.packed_proj = nn.Linear(E_q, E_total * 3, bias=bias, **factory_kwargs) def forward(self, query): return torch.chunk(self.packed_proj(query), 3, dim=-1) B, D, dtype = 256, 8192, torch.bfloat16 torch.set_float32_matmul_precision(\"high\") in_proj = torch.compile(InputProjection(D, D, device=\"cuda\", dtype=torch.bfloat16)) packed_in_proj = torch.compile( PackedInputProjection(D, D, device=\"cuda\", dtype=torch.bfloat16) ) q, _, _, sequence_lengths = gen_batch(B, D, D, D, device=\"cuda\", dtype=torch.bfloat16) # warmup in_proj(q) packed_in_proj(q) # benchmark (q_out, k_out, v_out), time, _ = benchmark(in_proj, q) (q_out, k_out, v_out), time_packed, _ = benchmark(packed_in_proj, q) # On my A100 prints 1.05x speedup print( f\"InputProjection: {time:5f} s, PackedInputProjection: {time_packed:5f} s, speedup: {time/time_packed:.2f}x\" ) InputProjection: 0.034053 s, PackedInputProjection: 0.033968 s, speedup: 1.00x SwiGLU feed forward network of Transformer Layer# Swish-Gated Linear Unit (SwiGLU) is a non-linear activation function that is increasingly popular in the feed-forward network of the transformer layer (e.g. Llama). A feed-forward network with SwiGLU activation is defined as: class SwiGLUFFN(nn.Module): def __init__( self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() hidden_dim = int(2 * hidden_dim / 3) # custom dim factor multiplier if ffn_dim_multiplier is not None: hidden_dim = int(ffn_dim_multiplier * hidden_dim) hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) self.w1 = nn.Linear(dim, hidden_dim, bias=False, **factory_kwargs) self.w2 = nn.Linear(hidden_dim, dim, bias=False, **factory_kwargs) self.w3 = nn.Linear(dim, hidden_dim, bias=False, **factory_kwargs) def forward(self, x): return self.w2(F.silu(self.w1(x)) * self.w3(x)) An alternative way of implementing this that uses packed projection is class PackedSwiGLUFFN(nn.Module): def __init__( self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() hidden_dim = int(2 * hidden_dim / 3) # custom dim factor multiplier if ffn_dim_multiplier is not None: hidden_dim = int(ffn_dim_multiplier * hidden_dim) hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) self.w13 = nn.Linear(dim, 2 * hidden_dim, bias=False, **factory_kwargs) self.w2 = nn.Linear(hidden_dim, dim, bias=False, **factory_kwargs) def forward(self, x): x1, x3 = torch.chunk(self.w13(x), 2, dim=-1) return self.w2(F.silu(x1) * x3) We can compare the performance of the two implementations as follows Depending on your hardware, you might see different results. On an A100 I see 1.12x speedup for D=128. D = 128 swigluffn = torch.compile(SwiGLUFFN(D, D * 4, 256, device=\"cuda\", dtype=torch.bfloat16)) packed_swigluffn = torch.compile( PackedSwiGLUFFN(D, D * 4, 256, device=\"cuda\", dtype=torch.bfloat16) ) q, _, _, sentence_lengths = gen_batch(D, D, D, D, device=\"cuda\", dtype=torch.bfloat16) # warmup swigluffn(q) packed_swigluffn(q) # benchmark _, time, _ = benchmark(swigluffn, q) _, time_packed, _ = benchmark(packed_swigluffn, q) # On my A100 prints 1.08x speedup print( f\"SwiGLUFFN: {time} s, PackedSwiGLUFFN: {time_packed} s, speedup: {time/time_packed:.2f}x\" ) SwiGLUFFN: 0.0009492869999121467 s, PackedSwiGLUFFN: 0.0008643020000818069 s, speedup: 1.10x Extended examples# We intend to update this tutorial to demonstrate more examples of how to use the various performant building blocks such as KV-Caching, Grouped Query Attention etc. Further, there are several good examples of using various performant building blocks to implement various transformer architectures. Some examples include gpt-fast segment-anything-fast lucidrains implementation of NaViT with nested tensors torchtune\u2019s implementation of VisionTransformer Conclusion# In this tutorial, we have introduced the low level building blocks PyTorch provides for writing transformer layers and demonstrated examples how to compose them. It is our hope that this tutorial has educated the reader on the ease with which flexible and performant transformer layers can be implemented by users of PyTorch. Total running time of the script: (0 minutes 52.674 seconds) Download Jupyter notebook: transformer_building_blocks.ipynb Download Python source code: transformer_building_blocks.py Download zipped: transformer_building_blocks.zip",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/intermediate/transformer_building_blocks.html"
         },
         "datePublished": "2023-01-01T00:00:00Z",
         "dateModified": "2023-01-01T00:00:00Z"
       }
   </script>
</body>
</body></html>