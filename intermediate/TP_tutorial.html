
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2024-04-19T20:26:28+00:00" property="article:modified_time"/>
<title>Large Scale Transformer model training with Tensor Parallel (TP) — PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/TP_tutorial';</script>
<link href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="pipelining_tutorial.html" rel="next" title="Introduction to Distributed Pipeline Parallelism"/>
<link href="TCPStore_libuv_backend.html" rel="prev" title="Introduction to Libuv TCPStore Backend"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Apr 19, 2024" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/intermediate/TP_tutorial.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects.
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Apr 19, 2024" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel (FSDP2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="TCPStore_libuv_backend.html">Introduction to Libuv TCPStore Backend</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelining_tutorial.html">Introduction to Distributed Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../distributed.html">Distributed</a></li>
<li aria-current="page" class="breadcrumb-item active">Large Scale...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../distributed.html" itemprop="item"/>
<meta content="Distributed" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Large Scale Transformer model training with Tensor Parallel (TP)" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/TP_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="large-scale-transformer-model-training-with-tensor-parallel-tp">
<h1>Large Scale Transformer model training with Tensor Parallel (TP)<a class="headerlink" href="#large-scale-transformer-model-training-with-tensor-parallel-tp" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Apr 19, 2024 | Last Updated: Jul 18, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/wanchaol">Wanchao Liang</a>, <a class="reference external" href="https://github.com/tianyu-l">Tianyu Liu</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="../_images/pencil-16.png"><img alt="edit" src="../_images/pencil-16.png" style="width: 16px; height: 16px;"/></a> View and edit this tutorial in <a class="reference external" href="https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst">github</a>.</p>
</div>
<p>This tutorial demonstrates how to train a large Transformer-like model across hundreds to thousands of GPUs using Tensor Parallel and Fully Sharded Data Parallel.</p>
<p>Prerequisites:</p>
<ul class="simple">
<li><p>PyTorch 2.3.0 or later installed with CUDA/Linux</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html">Tensor Parallel APIs</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/distributed_device_mesh.html">Getting Started with DeviceMesh</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel</a></p></li>
</ul>
<section id="how-tensor-parallel-works">
<h2>How Tensor Parallel works?<a class="headerlink" href="#how-tensor-parallel-works" title="Link to this heading">#</a></h2>
<p>Tensor Parallel (TP) was originally proposed in the <a class="reference external" href="https://arxiv.org/abs/1909.08053">Megatron-LM</a> paper,
and it is an efficient model parallelism technique to train large scale Transformer models.
<a class="reference external" href="https://arxiv.org/abs/2205.05198">Sequence Parallel</a> (SP) we mention in this tutorial is a variant of Tensor
Parallel that shards on the sequence dimension for <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code> or <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> to further save activation memory
during training. As the model becomes larger, the activation memory becomes the bottleneck, so in Tensor
Parallel training it usually applies Sequence Parallel to <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> or <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> layers.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/megatron_lm.png"><img alt="Megatron-LM TP" src="../_images/megatron_lm.png" style="width: 100%;"/></a>
<figcaption>
<p><span class="caption-text">Figure 1. represents the sharding in Tensor Parallel style on a Transformer model’s MLP and Self-Attention layer, where the matrix multiplications in both attention/MLP happens through sharded computations (<a class="reference external" href="https://arxiv.org/abs/1909.08053">image source</a>)</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>At a high level, PyTorch Tensor Parallel works as follows:</p>
<p><strong>Sharding initialization</strong></p>
<ul class="simple">
<li><p>Determine which <code class="docutils literal notranslate"><span class="pre">ParallelStyle</span></code> to apply to each layer and shard the initialized module by calling <code class="docutils literal notranslate"><span class="pre">parallelize_module</span></code>.</p></li>
<li><p>The parallelized modules would have their model parameters be swapped to DTensors, and DTensor would be responsible to run the parallelized module using sharded computation.</p></li>
</ul>
<p><strong>Runtime foward/backward</strong></p>
<ul class="simple">
<li><p>Depending on the input/outputs DTensor layouts user specified for each <code class="docutils literal notranslate"><span class="pre">ParallelStyle</span></code>, it would run proper communication operation to transform the DTensor layouts for inputs/outputs (such as <code class="docutils literal notranslate"><span class="pre">allreduce</span></code>, <code class="docutils literal notranslate"><span class="pre">allgather</span></code> and <code class="docutils literal notranslate"><span class="pre">reduce_scatter</span></code>).</p></li>
<li><p>Run sharded computation for the parallelized layers to save compute/memory (for example, <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>).</p></li>
</ul>
</section>
<section id="when-and-why-you-should-apply-tensor-parallel">
<h2>When and Why you should apply Tensor Parallel<a class="headerlink" href="#when-and-why-you-should-apply-tensor-parallel" title="Link to this heading">#</a></h2>
<p>The PyTorch Fully Sharded Data Parallel (FSDP) already has the capability to scale model training to a specific
number of GPUs. However, when it comes to further scale the model training in terms of model size and GPU quantity,
many additional challenges arise that may require combining Tensor Parallel with FSDP.:</p>
<ol class="arabic simple">
<li><p>As the world size (number of GPUs) is becoming excessively large (exceeding 128/256 GPUs), the FSDP collectives (such as <code class="docutils literal notranslate"><span class="pre">allgather</span></code>) are being dominated by ring latency.
By implementing TP/SP on top of FSDP, the FSDP world size could be reduced by 8 by applying FSDP to be inter-host only, consequently decreasing the latency costs by the same amount.</p></li>
<li><p>Hit data parallelism limit where you can not raise the global batch size to be above the number of GPUs due to both convergence and GPU memory limitations, Tensor/Sequence Parallel
is the only known way to “ballpark” the global batch size and continue scaling with more GPUs. This means both model size and number of GPUs could continue to scale.</p></li>
<li><p>For certain types of models, when local batch size becomes smaller, TP/SP can yield matrix multiplication shapes that are more optimized for floating point operations (FLOPS).</p></li>
</ol>
<p>So, when pre-training, how easy is it to hit those limits? As of now, pre-training a Large Language Model (LLM) with billions or trillions of tokens could take months, even when using thousands of GPUs.</p>
<ul class="simple">
<li><p>It will always hit limitation 1 when training LLM on a large scale. For example, Llama 2 70B trained with 2k GPUs for 35 days, multi-dimensional parallelisms are needed at 2k scale.</p></li>
<li><p>When the Transformer model becomes larger (such as Llama2 70B), it will also quickly hit the limitation 2. One could not use FSDP alone with even local <code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code> due to memory
and convergence constraints. For example, Llama 2 global batch size is 1K, so data parallelism alone can not be used at 2K GPUs.</p></li>
</ul>
</section>
<section id="how-to-apply-tensor-parallel">
<h2>How to apply Tensor Parallel<a class="headerlink" href="#how-to-apply-tensor-parallel" title="Link to this heading">#</a></h2>
<p>PyTorch Tensor Parallel APIs offers a set of module level primitives (<code class="docutils literal notranslate"><span class="pre">ParallelStyle</span></code>) to configure the sharding for each individual layers of the model, including:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ColwiseParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">RowwiseParallel</span></code>: Shard the <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> in the column or row fashion.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SequenceParallel</span></code>: Perform sharded computations on <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">RMSNormPython</span></code>, etc.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PrepareModuleInput</span></code> and <code class="docutils literal notranslate"><span class="pre">PrepareModuleOutput</span></code>: Configure the module inputs/outputs sharding layouts with proper communication operations.</p></li>
</ul>
<p>To demonstrate how to use the PyTorch native Tensor Parallel APIs, let us look at a common Transformer model. In this tutorial, we use the most recent <a class="reference external" href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py">Llama2 model</a> as a reference Transformer model implementation, as it is also widely used in the community.</p>
<p>Since Tensor Parallel shard individual tensors over a set of devices, we would need to set up the distributed environment (such as NCCL communicators) first.
Tensor Parallelism is a Single-Program Multiple-Data (SPMD) sharding algorithm similar to PyTorch DDP/FSDP, and it under the hood leverages the PyTorch DTensor
to perform sharding. It also utilizes the DeviceMesh abstraction (which under the hood manages ProcessGroups) for device management and sharding.
To see how to utilize DeviceMesh to set up multi-dimensional parallelisms, please refer to <a class="reference external" href="https://pytorch.org/tutorials/recipes/distributed_device_mesh.html">this tutorial</a>. Tensor Parallel usually works within each host, so let us first initialize a DeviceMesh that connects 8 GPUs within a host.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_device_mesh</span>

<span class="n">tp_mesh</span> <span class="o">=</span> <span class="n">init_device_mesh</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,))</span>
</pre></div>
</div>
<p>Now that we have initialized DeviceMesh, let us take a detailed look at the Llama 2 model architecture and see how we should perform the Tensor Parallel sharding.
Here we focus on the core <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>, where the Transformer model stacks the identical <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> s to scale up the model.</p>
<p>The core <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> consists of an <code class="docutils literal notranslate"><span class="pre">Attention</span></code> layer and a <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> layer. Let us first look at the simpler <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> layer.
For the <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> Layer it consists of three Linear layers, where it performs a SwiGLU style MLP, looking at its forward function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward in the FeedForward layer</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>It performs <code class="docutils literal notranslate"><span class="pre">w1</span></code> and <code class="docutils literal notranslate"><span class="pre">w3</span></code> matmuls concurrently and followed by a <code class="docutils literal notranslate"><span class="pre">w2</span></code> matmul with the result of the combined w1/w3 linear projection results. This means we could
use the idea from the Tensor Parallelism paper to shard the w1/w3 Linear layers in the colwise fashion and shard the <code class="docutils literal notranslate"><span class="pre">w2</span></code> Linear layer in the rowwise fashion, so that
there is only one <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> communication happening at the end of all the three layers. With the PyTorch native Tensor Parallel, we can simply create a <code class="docutils literal notranslate"><span class="pre">parallelize_plan</span></code> for the <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> layer like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">parallelize_module</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># by default ColwiseParallel input layouts is replicated</span>
    <span class="c1"># and RowwiseParallel output layouts is replicated</span>
    <span class="s2">"feed_foward.w1"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w2"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w3"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>That’s simply how we configure the shardings for the <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> layer using the PyTorch Tensor Parallel APIs. Note that users would only need to specify how to shard the individual layers and the communications (for example, <code class="docutils literal notranslate"><span class="pre">allreduce</span></code>) will happen under the hood.</p>
<p>Moving on to the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> Layer. It consists of <code class="docutils literal notranslate"><span class="pre">wq</span></code>, <code class="docutils literal notranslate"><span class="pre">wk</span></code>, <code class="docutils literal notranslate"><span class="pre">wv</span></code> Linear layers to project input to <code class="docutils literal notranslate"><span class="pre">q</span></code>/ <code class="docutils literal notranslate"><span class="pre">k</span></code> / <code class="docutils literal notranslate"><span class="pre">v</span></code>, and then it performs attention and output projection with the <code class="docutils literal notranslate"><span class="pre">wo</span></code> Linear layer. Tensor Parallelism here intends to perform column-wise sharding for the
q/k/v projection and row-wise sharding for the <code class="docutils literal notranslate"><span class="pre">wo</span></code> linear projection. So we can add the Attention plan to the <code class="docutils literal notranslate"><span class="pre">tp_plan</span></code> that we just drafted up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># by default ColwiseParallel input layouts is replicated</span>
    <span class="c1"># and RowwiseParallel output layouts is replicated</span>
    <span class="s2">"attention.wq"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wk"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wv"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wo"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w1"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w2"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w3"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This is almost the <code class="docutils literal notranslate"><span class="pre">layer_tp_plan</span></code> we need to apply Tensor Parallelism to the <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>. However, one thing we should be aware is that when sharding the linear layer column-wise, the output of the linear layers would become sharded on the last tensor dimension, and the row-wise sharding linear layer directly accepts an input that shards on the last dimension.
If there are any more tensor operations (such as view operations) between the column-wise linear and the row-wise linear, we would need to adjust the relevant shape related ops to sharded shape.</p>
<p>For the Llama model, in the attention layer, there are several view operations related to shape. Specifically, for column-wise parallelism in the <code class="docutils literal notranslate"><span class="pre">wq</span></code>/<code class="docutils literal notranslate"><span class="pre">wk</span></code>/<code class="docutils literal notranslate"><span class="pre">wv</span></code> linear layers, the activation tensor is sharded on the <code class="docutils literal notranslate"><span class="pre">num_heads</span></code> dimension. To manage the difference between global and local <code class="docutils literal notranslate"><span class="pre">num_heads</span></code>, we should set <code class="docutils literal notranslate"><span class="pre">use_local_output=False</span></code> to ensure the output is a DTensor. Unlike a regular tensor, a DTensor is aware of the parallelism plans and will automatically handle changes in the <code class="docutils literal notranslate"><span class="pre">num_heads</span></code> dimension.</p>
<p>Finally, we need to call <code class="docutils literal notranslate"><span class="pre">parallelize_module</span></code> API to make the plan for each <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> effective. Under the hood, it distributes the model parameters inside <code class="docutils literal notranslate"><span class="pre">Attention</span></code> and <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> layers to DTensors, and registers communication hooks for model inputs and outputs (before and after each module respectively), if necessary:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="p">}</span>  <span class="c1"># i.e. the plan we just generated</span>

    <span class="n">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">tp_mesh</span><span class="p">,</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Now that we have elaborated the sharding plan for each <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>, there is usually a <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> in the first layer and a final <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> projection layer, where user could choose row-wise or column-wise sharding to the first <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> and column-wise sharding to the last <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> projection layer with proper input and output layouts specified.
Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">"tok_embeddings"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="n">Replicate</span><span class="p">(),</span>
        <span class="p">),</span>
        <span class="s2">"output"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="n">Replicate</span><span class="p">(),</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the model to be partitioned is too large to fit into CPU memory, one could either use <code class="docutils literal notranslate"><span class="pre">meta</span></code> device initialization (for example, initialize the model on meta device first, shard the layers, and the materialize the model), or parallelize the <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> layer by layer during the Transformer model initialization.</p>
</div>
</section>
<section id="apply-sequence-parallel-to-layernorm-rmsnorm-layers">
<h2>Apply Sequence Parallel to <code class="docutils literal notranslate"><span class="pre">LayerNorm/RMSNorm</span></code> layers<a class="headerlink" href="#apply-sequence-parallel-to-layernorm-rmsnorm-layers" title="Link to this heading">#</a></h2>
<p>Sequence Parallel works on top of the Tensor Parallel illustrated above. Compared with basic Tensor Parallel, which only shards tensors within the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> modules and <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> modules and keep their module inputs and outputs (namely activations in the forward pass and gradients in the backward pass) replicated, Sequence Parallel keeps them sharded on the sequence dimension.</p>
<p>In a typical <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>, the forward function combines norm layers (<code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> or <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code>), an attention layer, a feed forward layer, and residual connections. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward in a TransformerBlock</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_norm</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>In most use cases, the activations (and gradients) are of the shape <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">sequence</span> <span class="pre">length,</span> <span class="pre">hidden</span> <span class="pre">dimension]</span></code> outside the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> and <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> modules. In the DTensor’s language, Sequence Parallel performs activation computation using the <code class="docutils literal notranslate"><span class="pre">Shard(1)</span></code> layout for both forward/backward of the module.
Following the code example earlier, the code below demonstrates how we apply Sequence Parallel to the norm layers within a <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>:</p>
<p>First let’s import the required dependencies for Sequence Parallel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PrepareModuleInput</span><span class="p">,</span>
    <span class="n">SequenceParallel</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Next let’s adjust the <code class="docutils literal notranslate"><span class="pre">layer_tp_plan</span></code> to enable sequence parallel on the <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,</span>
    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension</span>
    <span class="s2">"attention_norm"</span><span class="p">:</span> <span class="n">SequenceParallel</span><span class="p">(),</span>
    <span class="s2">"attention"</span><span class="p">:</span> <span class="n">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="n">Replicate</span><span class="p">(),</span> <span class="n">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="s2">"attention.wq"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wk"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wv"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="s2">"attention.wo"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="s2">"ffn_norm"</span><span class="p">:</span> <span class="n">SequenceParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward"</span><span class="p">:</span> <span class="n">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="n">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="s2">"feed_forward.w1"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
    <span class="s2">"feed_forward.w2"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="s2">"feed_forward.w3"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>One can see we now use <code class="docutils literal notranslate"><span class="pre">PrepareModuleInput</span></code> to modify the module input layouts to the Attention and FeedForward layers from <code class="docutils literal notranslate"><span class="pre">Shard(1)</span></code> to <code class="docutils literal notranslate"><span class="pre">Replicate()</span></code>, and mark their output layouts as <code class="docutils literal notranslate"><span class="pre">Shard(1)</span></code>.
Just like what happens to Tensor Parallelism, one only needs to specify the tensor sharding layouts of the inputs and outputs, and the communication between layers will happen automatically.</p>
<p>Note that with Sequence Parallel, we assume the inputs and outputs of a <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> are always sharded on the sequence dimension, so that multiple <code class="docutils literal notranslate"><span class="pre">TransformerBlocks</span></code> can be concatenated seamlessly.
This can be facilitated by explicitly specifying the output of the beginning <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer and the input of the final <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> projection layer to be <code class="docutils literal notranslate"><span class="pre">Shard(1)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">"tok_embeddings"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="n">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="s2">"norm"</span><span class="p">:</span> <span class="n">SequenceParallel</span><span class="p">(),</span>
        <span class="s2">"output"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="n">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="apply-loss-parallel">
<h2>Apply Loss Parallel<a class="headerlink" href="#apply-loss-parallel" title="Link to this heading">#</a></h2>
<p>Loss Parallel is a related technique to save memory and communication when the loss function is computed, as model outputs are usually very large. In Loss Parallel, when the model outputs are sharded on the (often huge) vocabulary dimension, the cross-entropy loss can be computed efficiently, without gathering all the model outputs to every single GPU. This not only significantly reduces the memory consumption, but also improves training speed by reducing communication overhead and doing sharded computation in parallel. The picture below briefly illustrates how Loss Parallel avoids gathering all model outputs to every GPU by doing sharded computation.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/loss_parallel.png"><img alt="loss parallel" src="../_images/loss_parallel.png" style="width: 100%;"/></a>
<figcaption>
<p><span class="caption-text">Figure 2. Cross-entropy loss forward computation with loss parallel on one GPU. Blue represents sharded tensors; green represents replicated tensors; yellow represents tensors with partial values (to be all-reduced). Black arrows are local computations; red arrows are functional collectives among GPUs.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In the PyTorch Tensor Parallel API, Loss Parallel can be enabled via a context manager <code class="docutils literal notranslate"><span class="pre">loss_parallel</span></code>, with which one can directly use <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.cross_entropy</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code> without modifying other parts of their code.</p>
<p>To apply Loss Parallel, the model predictions, usually of the shape <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">sequence</span> <span class="pre">length,</span> <span class="pre">vocabulary</span> <span class="pre">size]</span></code>, should be sharded on the vocabulary dimension. This can be easily done via marking the output layouts of the last linear projection layer output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">"tok_embeddings"</span><span class="p">:</span> <span class="n">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="n">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="s2">"norm"</span><span class="p">:</span> <span class="n">SequenceParallel</span><span class="p">(),</span>
        <span class="s2">"output"</span><span class="p">:</span> <span class="n">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="c1"># use DTensor as the output</span>
            <span class="n">use_local_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<p>In the code above, we also apply Sequence Parallel to the norm layer before output. We apply <code class="docutils literal notranslate"><span class="pre">use_local_output=False</span></code> to let the output stay as a DTensor, to work with the <code class="docutils literal notranslate"><span class="pre">loss_parallel</span></code> context manager. After that, one can simply call the cross_entropy loss function as is shown below. Note that the backward computation also needs to happen within the context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">loss_parallel</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="k">with</span> <span class="n">loss_parallel</span><span class="p">():</span>
    <span class="c1"># assuming pred and labels are of the shape [batch, seq, vocab]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="combine-tensor-parallel-with-fully-sharded-data-parallel-together">
<h2>Combine Tensor Parallel with Fully Sharded Data Parallel together<a class="headerlink" href="#combine-tensor-parallel-with-fully-sharded-data-parallel-together" title="Link to this heading">#</a></h2>
<p>Now that we have shown how to apply Tensor/Sequence Parallel to the model, let us also take a look at how Tensor Parallel and Fully Sharded Data Parallel could work together.
Since Tensor Parallelism incurs communications that block the computation, we want to make sure it runs within a fast communication channel, such as NVLink.
In practice, we usually apply Tensor Parallel within each host, and apply Fully Sharded Data Parallel across the hosts.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/fsdp_tp.png"><img alt="fsdp + tp" src="../_images/fsdp_tp.png" style="width: 100%;"/></a>
<figcaption>
<p><span class="caption-text">Figure 3. FSDP and TP work on separate device dimensions, FSDP communication happens inter-host and TP communication happens intra-host.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This 2-D parallelism pattern can be easily expressed via a 2-D DeviceMesh, and we just need pass each “sub” DeviceMesh to each individual parallelism APIs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">parallelize_module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.fsdp</span><span class="w"> </span><span class="kn">import</span> <span class="n">fully_shard</span>

<span class="c1"># i.e. 2-D mesh is [dp, tp], training on 64 GPUs that performs 8 way DP and 8 way TP</span>
<span class="n">mesh_2d</span> <span class="o">=</span> <span class="n">init_device_mesh</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">tp_mesh</span> <span class="o">=</span> <span class="n">mesh_2d</span><span class="p">[</span><span class="s2">"tp"</span><span class="p">]</span> <span class="c1"># a submesh that connects intra-host devices</span>
<span class="n">dp_mesh</span> <span class="o">=</span> <span class="n">mesh_2d</span><span class="p">[</span><span class="s2">"dp"</span><span class="p">]</span> <span class="c1"># a submesh that connects inter-host devices</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">tp_plan</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="p">}</span>

<span class="c1"># apply Tensor Parallel intra-host on tp_mesh</span>
<span class="n">model_tp</span> <span class="o">=</span> <span class="n">parallelize_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tp_mesh</span><span class="p">,</span> <span class="n">tp_plan</span><span class="p">)</span>
<span class="c1"># apply FSDP inter-host on dp_mesh</span>
<span class="n">model_2d</span> <span class="o">=</span> <span class="n">fully_shard</span><span class="p">(</span><span class="n">model_tp</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">dp_mesh</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>This would allow us to easily apply Tensor Parallel within each host (intra-host) and apply FSDP across hosts (inter-hosts), with <strong>0-code changes</strong> to the Llama model.
The Tensor(Model) Parallel and Data Parallel techniques combined together provides the ability to continue increasing model size and training efficiently using a large number of GPUs.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This tutorial demonstrates how to train a large Transformer-like model across hundreds to thousands of GPUs using Tensor Parallel in combination with Fully Sharded Data Parallel.
It explains how to apply Tensor Parallel to different parts of the model, with <strong>no code changes</strong> to the model itself. Tensor Parallel is a efficient model parallelism technique for large scale training.</p>
<p>To see the complete end-to-end code example explained in this tutorial, please refer to the <a class="reference external" href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py">Tensor Parallel examples</a> in the pytorch/examples repository.</p>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="TCPStore_libuv_backend.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Introduction to Libuv TCPStore Backend</p>
</div>
</a>
<a class="right-next" href="pipelining_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Introduction to Distributed Pipeline Parallelism</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="TCPStore_libuv_backend.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Introduction to Libuv TCPStore Backend</p>
</div>
</a>
<a class="right-next" href="pipelining_tutorial.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Introduction to Distributed Pipeline Parallelism</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-tensor-parallel-works">How Tensor Parallel works?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-and-why-you-should-apply-tensor-parallel">When and Why you should apply Tensor Parallel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-apply-tensor-parallel">How to apply Tensor Parallel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-sequence-parallel-to-layernorm-rmsnorm-layers">Apply Sequence Parallel to <code class="docutils literal notranslate"><span class="pre">LayerNorm/RMSNorm</span></code> layers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-loss-parallel">Apply Loss Parallel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combine-tensor-parallel-with-fully-sharded-data-parallel-together">Combine Tensor Parallel with Fully Sharded Data Parallel together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "name": "Large Scale Transformer model training with Tensor Parallel (TP)",
         "headline": "Large Scale Transformer model training with Tensor Parallel (TP)",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/intermediate/TP_tutorial.html",
         "articleBody": "Large Scale Transformer model training with Tensor Parallel (TP)# Author: Wanchao Liang, Tianyu Liu Note View and edit this tutorial in github. This tutorial demonstrates how to train a large Transformer-like model across hundreds to thousands of GPUs using Tensor Parallel and Fully Sharded Data Parallel. Prerequisites: PyTorch 2.3.0 or later installed with CUDA/Linux Tensor Parallel APIs Getting Started with DeviceMesh Getting Started with Fully Sharded Data Parallel How Tensor Parallel works?# Tensor Parallel (TP) was originally proposed in the Megatron-LM paper, and it is an efficient model parallelism technique to train large scale Transformer models. Sequence Parallel (SP) we mention in this tutorial is a variant of Tensor Parallel that shards on the sequence dimension for nn.LayerNorm or RMSNorm to further save activation memory during training. As the model becomes larger, the activation memory becomes the bottleneck, so in Tensor Parallel training it usually applies Sequence Parallel to LayerNorm or RMSNorm layers. Figure 1. represents the sharding in Tensor Parallel style on a Transformer model\u2019s MLP and Self-Attention layer, where the matrix multiplications in both attention/MLP happens through sharded computations (image source)# At a high level, PyTorch Tensor Parallel works as follows: Sharding initialization Determine which ParallelStyle to apply to each layer and shard the initialized module by calling parallelize_module. The parallelized modules would have their model parameters be swapped to DTensors, and DTensor would be responsible to run the parallelized module using sharded computation. Runtime foward/backward Depending on the input/outputs DTensor layouts user specified for each ParallelStyle, it would run proper communication operation to transform the DTensor layouts for inputs/outputs (such as allreduce, allgather and reduce_scatter). Run sharded computation for the parallelized layers to save compute/memory (for example, nn.Linear, nn.Embedding). When and Why you should apply Tensor Parallel# The PyTorch Fully Sharded Data Parallel (FSDP) already has the capability to scale model training to a specific number of GPUs. However, when it comes to further scale the model training in terms of model size and GPU quantity, many additional challenges arise that may require combining Tensor Parallel with FSDP.: As the world size (number of GPUs) is becoming excessively large (exceeding 128/256 GPUs), the FSDP collectives (such as allgather) are being dominated by ring latency. By implementing TP/SP on top of FSDP, the FSDP world size could be reduced by 8 by applying FSDP to be inter-host only, consequently decreasing the latency costs by the same amount. Hit data parallelism limit where you can not raise the global batch size to be above the number of GPUs due to both convergence and GPU memory limitations, Tensor/Sequence Parallel is the only known way to \u201cballpark\u201d the global batch size and continue scaling with more GPUs. This means both model size and number of GPUs could continue to scale. For certain types of models, when local batch size becomes smaller, TP/SP can yield matrix multiplication shapes that are more optimized for floating point operations (FLOPS). So, when pre-training, how easy is it to hit those limits? As of now, pre-training a Large Language Model (LLM) with billions or trillions of tokens could take months, even when using thousands of GPUs. It will always hit limitation 1 when training LLM on a large scale. For example, Llama 2 70B trained with 2k GPUs for 35 days, multi-dimensional parallelisms are needed at 2k scale. When the Transformer model becomes larger (such as Llama2 70B), it will also quickly hit the limitation 2. One could not use FSDP alone with even local batch_size=1 due to memory and convergence constraints. For example, Llama 2 global batch size is 1K, so data parallelism alone can not be used at 2K GPUs. How to apply Tensor Parallel# PyTorch Tensor Parallel APIs offers a set of module level primitives (ParallelStyle) to configure the sharding for each individual layers of the model, including: ColwiseParallel and RowwiseParallel: Shard the nn.Linear and nn.Embedding in the column or row fashion. SequenceParallel: Perform sharded computations on nn.LayerNorm, nn.Dropout, RMSNormPython, etc. PrepareModuleInput and PrepareModuleOutput: Configure the module inputs/outputs sharding layouts with proper communication operations. To demonstrate how to use the PyTorch native Tensor Parallel APIs, let us look at a common Transformer model. In this tutorial, we use the most recent Llama2 model as a reference Transformer model implementation, as it is also widely used in the community. Since Tensor Parallel shard individual tensors over a set of devices, we would need to set up the distributed environment (such as NCCL communicators) first. Tensor Parallelism is a Single-Program Multiple-Data (SPMD) sharding algorithm similar to PyTorch DDP/FSDP, and it under the hood leverages the PyTorch DTensor to perform sharding. It also utilizes the DeviceMesh abstraction (which under the hood manages ProcessGroups) for device management and sharding. To see how to utilize DeviceMesh to set up multi-dimensional parallelisms, please refer to this tutorial. Tensor Parallel usually works within each host, so let us first initialize a DeviceMesh that connects 8 GPUs within a host. from torch.distributed.device_mesh import init_device_mesh tp_mesh = init_device_mesh(\"cuda\", (8,)) Now that we have initialized DeviceMesh, let us take a detailed look at the Llama 2 model architecture and see how we should perform the Tensor Parallel sharding. Here we focus on the core TransformerBlock, where the Transformer model stacks the identical TransformerBlock s to scale up the model. The core TransformerBlock consists of an Attention layer and a FeedForward layer. Let us first look at the simpler FeedForward layer. For the FeedForward Layer it consists of three Linear layers, where it performs a SwiGLU style MLP, looking at its forward function: # forward in the FeedForward layer def forward(self, x): return self.w2(F.silu(self.w1(x)) * self.w3(x)) It performs w1 and w3 matmuls concurrently and followed by a w2 matmul with the result of the combined w1/w3 linear projection results. This means we could use the idea from the Tensor Parallelism paper to shard the w1/w3 Linear layers in the colwise fashion and shard the w2 Linear layer in the rowwise fashion, so that there is only one allreduce communication happening at the end of all the three layers. With the PyTorch native Tensor Parallel, we can simply create a parallelize_plan for the FeedForward layer like below: from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module layer_tp_plan = { # by default ColwiseParallel input layouts is replicated # and RowwiseParallel output layouts is replicated \"feed_foward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(), \"feed_forward.w3\": ColwiseParallel(), } That\u2019s simply how we configure the shardings for the FeedForward layer using the PyTorch Tensor Parallel APIs. Note that users would only need to specify how to shard the individual layers and the communications (for example, allreduce) will happen under the hood. Moving on to the Attention Layer. It consists of wq, wk, wv Linear layers to project input to q/ k / v, and then it performs attention and output projection with the wo Linear layer. Tensor Parallelism here intends to perform column-wise sharding for the q/k/v projection and row-wise sharding for the wo linear projection. So we can add the Attention plan to the tp_plan that we just drafted up: layer_tp_plan = { # by default ColwiseParallel input layouts is replicated # and RowwiseParallel output layouts is replicated \"attention.wq\": ColwiseParallel(use_local_output=False), \"attention.wk\": ColwiseParallel(use_local_output=False), \"attention.wv\": ColwiseParallel(use_local_output=False), \"attention.wo\": RowwiseParallel(), \"feed_forward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(), \"feed_forward.w3\": ColwiseParallel(), } This is almost the layer_tp_plan we need to apply Tensor Parallelism to the TransformerBlock. However, one thing we should be aware is that when sharding the linear layer column-wise, the output of the linear layers would become sharded on the last tensor dimension, and the row-wise sharding linear layer directly accepts an input that shards on the last dimension. If there are any more tensor operations (such as view operations) between the column-wise linear and the row-wise linear, we would need to adjust the relevant shape related ops to sharded shape. For the Llama model, in the attention layer, there are several view operations related to shape. Specifically, for column-wise parallelism in the wq/wk/wv linear layers, the activation tensor is sharded on the num_heads dimension. To manage the difference between global and local num_heads, we should set use_local_output=False to ensure the output is a DTensor. Unlike a regular tensor, a DTensor is aware of the parallelism plans and will automatically handle changes in the num_heads dimension. Finally, we need to call parallelize_module API to make the plan for each TransformerBlock effective. Under the hood, it distributes the model parameters inside Attention and FeedForward layers to DTensors, and registers communication hooks for model inputs and outputs (before and after each module respectively), if necessary: for layer_id, transformer_block in enumerate(model.layers): layer_tp_plan = {...} # i.e. the plan we just generated parallelize_module( module=transformer_block, device_mesh=tp_mesh, parallelize_plan=layer_tp_plan, ) Now that we have elaborated the sharding plan for each TransformerBlock, there is usually a nn.Embedding in the first layer and a final nn.Linear projection layer, where user could choose row-wise or column-wise sharding to the first nn.Embedding and column-wise sharding to the last nn.Linear projection layer with proper input and output layouts specified. Here is an example: model = parallelize_module( model, tp_mesh, { \"tok_embeddings\": RowwiseParallel( input_layouts=Replicate(), ), \"output\": ColwiseParallel( output_layouts=Replicate(), ), } ) Note If the model to be partitioned is too large to fit into CPU memory, one could either use meta device initialization (for example, initialize the model on meta device first, shard the layers, and the materialize the model), or parallelize the TransformerBlock layer by layer during the Transformer model initialization. Apply Sequence Parallel to LayerNorm/RMSNorm layers# Sequence Parallel works on top of the Tensor Parallel illustrated above. Compared with basic Tensor Parallel, which only shards tensors within the Attention modules and FeedForward modules and keep their module inputs and outputs (namely activations in the forward pass and gradients in the backward pass) replicated, Sequence Parallel keeps them sharded on the sequence dimension. In a typical TransformerBlock, the forward function combines norm layers (LayerNorm or RMSNorm), an attention layer, a feed forward layer, and residual connections. For example: # forward in a TransformerBlock def forward(self, x): h = x + self.attention(self.attention_norm(x)) out = h + self.feed_forward(self.ffn_norm(h)) return out In most use cases, the activations (and gradients) are of the shape [batch size, sequence length, hidden dimension] outside the Attention and FeedForward modules. In the DTensor\u2019s language, Sequence Parallel performs activation computation using the Shard(1) layout for both forward/backward of the module. Following the code example earlier, the code below demonstrates how we apply Sequence Parallel to the norm layers within a TransformerBlock: First let\u2019s import the required dependencies for Sequence Parallel: from torch.distributed.tensor.parallel import ( PrepareModuleInput, SequenceParallel, ) Next let\u2019s adjust the layer_tp_plan to enable sequence parallel on the RMSNorm layers: layer_tp_plan = { # Now the input and output of SequenceParallel has Shard(1) layouts, # to represent the input/output tensors sharded on the sequence dimension \"attention_norm\": SequenceParallel(), \"attention\": PrepareModuleInput( input_layouts=(Shard(1), Replicate()), desired_input_layouts=(Replicate(), Replicate()), ), \"attention.wq\": ColwiseParallel(use_local_output=False), \"attention.wk\": ColwiseParallel(use_local_output=False), \"attention.wv\": ColwiseParallel(use_local_output=False), \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)), \"ffn_norm\": SequenceParallel(), \"feed_forward\": PrepareModuleInput( input_layouts=(Shard(1),), desired_input_layouts=(Replicate(),), ), \"feed_forward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)), \"feed_forward.w3\": ColwiseParallel(), } One can see we now use PrepareModuleInput to modify the module input layouts to the Attention and FeedForward layers from Shard(1) to Replicate(), and mark their output layouts as Shard(1). Just like what happens to Tensor Parallelism, one only needs to specify the tensor sharding layouts of the inputs and outputs, and the communication between layers will happen automatically. Note that with Sequence Parallel, we assume the inputs and outputs of a TransformerBlock are always sharded on the sequence dimension, so that multiple TransformerBlocks can be concatenated seamlessly. This can be facilitated by explicitly specifying the output of the beginning nn.Embedding layer and the input of the final nn.Linear projection layer to be Shard(1): model = parallelize_module( model, tp_mesh, { \"tok_embeddings\": RowwiseParallel( input_layouts=Replicate(), output_layouts=Shard(1), ), \"norm\": SequenceParallel(), \"output\": ColwiseParallel( input_layouts=Shard(1), output_layouts=Replicate() ), } ) Apply Loss Parallel# Loss Parallel is a related technique to save memory and communication when the loss function is computed, as model outputs are usually very large. In Loss Parallel, when the model outputs are sharded on the (often huge) vocabulary dimension, the cross-entropy loss can be computed efficiently, without gathering all the model outputs to every single GPU. This not only significantly reduces the memory consumption, but also improves training speed by reducing communication overhead and doing sharded computation in parallel. The picture below briefly illustrates how Loss Parallel avoids gathering all model outputs to every GPU by doing sharded computation. Figure 2. Cross-entropy loss forward computation with loss parallel on one GPU. Blue represents sharded tensors; green represents replicated tensors; yellow represents tensors with partial values (to be all-reduced). Black arrows are local computations; red arrows are functional collectives among GPUs.# In the PyTorch Tensor Parallel API, Loss Parallel can be enabled via a context manager loss_parallel, with which one can directly use torch.nn.functional.cross_entropy or torch.nn.CrossEntropyLoss without modifying other parts of their code. To apply Loss Parallel, the model predictions, usually of the shape [batch size, sequence length, vocabulary size], should be sharded on the vocabulary dimension. This can be easily done via marking the output layouts of the last linear projection layer output: model = parallelize_module( model, tp_mesh, { \"tok_embeddings\": RowwiseParallel( input_layouts=Replicate(), output_layouts=Shard(1), ), \"norm\": SequenceParallel(), \"output\": ColwiseParallel( input_layouts=Shard(1), # use DTensor as the output use_local_output=False, ), }, ) In the code above, we also apply Sequence Parallel to the norm layer before output. We apply use_local_output=False to let the output stay as a DTensor, to work with the loss_parallel context manager. After that, one can simply call the cross_entropy loss function as is shown below. Note that the backward computation also needs to happen within the context. import torch.nn.functional as F from torch.distributed.tensor.parallel import loss_parallel pred = model(input_ids) with loss_parallel(): # assuming pred and labels are of the shape [batch, seq, vocab] loss = F.cross_entropy(pred.flatten(0, 1), labels.flatten(0, 1)) loss.backward() Combine Tensor Parallel with Fully Sharded Data Parallel together# Now that we have shown how to apply Tensor/Sequence Parallel to the model, let us also take a look at how Tensor Parallel and Fully Sharded Data Parallel could work together. Since Tensor Parallelism incurs communications that block the computation, we want to make sure it runs within a fast communication channel, such as NVLink. In practice, we usually apply Tensor Parallel within each host, and apply Fully Sharded Data Parallel across the hosts. Figure 3. FSDP and TP work on separate device dimensions, FSDP communication happens inter-host and TP communication happens intra-host.# This 2-D parallelism pattern can be easily expressed via a 2-D DeviceMesh, and we just need pass each \u201csub\u201d DeviceMesh to each individual parallelism APIs: from torch.distributed.device_mesh import init_device_mesh from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module from torch.distributed.fsdp import fully_shard # i.e. 2-D mesh is [dp, tp], training on 64 GPUs that performs 8 way DP and 8 way TP mesh_2d = init_device_mesh(\"cuda\", (8, 8)) tp_mesh = mesh_2d[\"tp\"] # a submesh that connects intra-host devices dp_mesh = mesh_2d[\"dp\"] # a submesh that connects inter-host devices model = Model(...) tp_plan = {...} # apply Tensor Parallel intra-host on tp_mesh model_tp = parallelize_module(model, tp_mesh, tp_plan) # apply FSDP inter-host on dp_mesh model_2d = fully_shard(model_tp, mesh=dp_mesh, ...) This would allow us to easily apply Tensor Parallel within each host (intra-host) and apply FSDP across hosts (inter-hosts), with 0-code changes to the Llama model. The Tensor(Model) Parallel and Data Parallel techniques combined together provides the ability to continue increasing model size and training efficiently using a large number of GPUs. Conclusion# This tutorial demonstrates how to train a large Transformer-like model across hundreds to thousands of GPUs using Tensor Parallel in combination with Fully Sharded Data Parallel. It explains how to apply Tensor Parallel to different parts of the model, with no code changes to the model itself. Tensor Parallel is a efficient model parallelism technique for large scale training. To see the complete end-to-end code example explained in this tutorial, please refer to the Tensor Parallel examples in the pytorch/examples repository.",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/intermediate/TP_tutorial.html"
         },
         "datePublished": "2023-01-01T00:00:00Z",
         "dateModified": "2023-01-01T00:00:00Z"
       }
   </script>
</body>
</body></html>