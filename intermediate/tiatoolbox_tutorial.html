
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2023-12-19T06:57:37+00:00" property="article:modified_time"/>
<title>Whole Slide Image Classification Using PyTorch and TIAToolbox — PyTorch Tutorials 2.8.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=bffbcef7"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/tiatoolbox_tutorial';</script>
<link href="https://pytorch.org/tutorials/intermediate/tiatoolbox_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Dec 19, 2023" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/intermediate/tiatoolbox_tutorial.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname === 'docs.pytorch.org' ||
        window.location.hostname === 'docs-preview.pytorch.org' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Dec 19, 2023" name="docbuild:last-update">
</meta></head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.8.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li aria-current="page" class="breadcrumb-item active">Whole Slide...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="Whole Slide Image Classification Using PyTorch and TIAToolbox" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/tiatoolbox_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="whole-slide-image-classification-using-pytorch-and-tiatoolbox">
<h1>Whole Slide Image Classification Using PyTorch and TIAToolbox<a class="headerlink" href="#whole-slide-image-classification-using-pytorch-and-tiatoolbox" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Dec 19, 2023 | Last Updated: Jun 04, 2025 | Last Verified: Nov 05, 2024</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To get the most of this tutorial, we suggest using this
<a class="reference external" href="https://colab.research.google.com/github/pytorch/tutorials/blob/main/_static/tiatoolbox_tutorial.ipynb">Colab Version</a>. This will allow you to experiment with the information presented below.</p>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In this tutorial, we will show how to classify Whole Slide Images (WSIs)
using PyTorch deep learning models with help from TIAToolbox. A WSI
is an image of a sample of human tissue taken through a surgery or biopsy and
scanned using specialized scanners. They are used by pathologists and
computational pathology researchers to <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/">study diseases such as cancer at the microscopic
level</a> in
order to understand for example tumor growth and help improve treatment
for patients.</p>
<p>What makes WSIs challenging to process is their enormous size. For
example, a typical slide image has in the order of <a class="reference external" href="https://doi.org/10.1117%2F12.912388">100,000x100,000
pixels</a> where each pixel can
correspond to about 0.25x0.25 microns on the slide. This introduces
challenges in loading and processing such images, not to mention
hundreds or even thousands of WSIs in a single study (larger studies
produce better results)!</p>
<p>Conventional image processing pipelines are not suitable for WSI
processing so we need better tools. This is where
<a class="reference external" href="https://github.com/TissueImageAnalytics/tiatoolbox">TIAToolbox</a> can
help as it brings a set of useful tools to import and process tissue
slides in a fast and computationally efficient manner. Typically, WSIs
are saved in a pyramid structure with multiple copies of the same image
at various magnification levels optimized for visualization. The level 0
(or the bottom level) of the pyramid contains the image at the highest
magnification or zoom level, whereas the higher levels in the pyramid
have a lower resolution copy of the base image. The pyramid structure is
sketched below.</p>
<p><img alt="WSI pyramid stack" src="../_images/read_bounds_tissue.webp"/> <em>WSI pyramid stack
(</em><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#">source</a><em>)</em></p>
<p>TIAToolbox allows us to automate common downstream analysis tasks such
as <a class="reference external" href="https://doi.org/10.1016/j.media.2022.102685">tissue
classification</a>. In this
tutorial we show how you can: 1. Load WSI images using
TIAToolbox; and 2. Use different PyTorch models to classify slides at
the patch-level. In this tutorial, we will provide an example of using
TorchVision <code class="docutils literal notranslate"><span class="pre">ResNet18</span></code> model and custom
<cite>HistoEncoder</cite> &lt;<a class="github reference external" href="https://github.com/jopo666/HistoEncoder">jopo666/HistoEncoder</a>&gt;`__ model.</p>
<p>Let’s get started!</p>
</section>
<section id="setting-up-the-environment">
<h2>Setting up the environment<a class="headerlink" href="#setting-up-the-environment" title="Link to this heading">#</a></h2>
<p>To run the examples provided in this tutorial, the following packages
are required as prerequisites.</p>
<ol class="arabic simple">
<li><p>OpenJpeg</p></li>
<li><p>OpenSlide</p></li>
<li><p>Pixman</p></li>
<li><p>TIAToolbox</p></li>
<li><p>HistoEncoder (for a custom model example)</p></li>
</ol>
<p>Please run the following command in your terminal to install these
packages:</p>
<p><cite>apt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev</cite>
<cite>pip install -q ‘tiatoolbox&lt;1.5’ histoencoder &amp;&amp; echo “Installation is done.”</cite></p>
<p>Alternatively, you can run <code class="docutils literal notranslate"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">openjpeg</span> <span class="pre">openslide</span></code> to
install the prerequisite packages on MacOS instead of <code class="docutils literal notranslate"><span class="pre">apt-get</span></code>.
Further information on installation can be <a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/installation.html">found
here</a>.</p>
<section id="importing-related-libraries">
<h3>Importing related libraries<a class="headerlink" href="#importing-related-libraries" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""Import modules required to run the Jupyter notebook."""</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="c1"># Configure logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="k">if</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">hasHandlers</span><span class="p">():</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">".*The 'nopython' keyword.*"</span><span class="p">)</span>

<span class="c1"># Downloading data and files</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">zipfile</span><span class="w"> </span><span class="kn">import</span> <span class="n">ZipFile</span>

<span class="c1"># Data processing and visualization</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpl</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">PIL</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="c1"># TIAToolbox for WSI loading and processing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.models.architecture</span><span class="w"> </span><span class="kn">import</span> <span class="n">vanilla</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.models.engine.patch_predictor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">IOPatchPredictorConfig</span><span class="p">,</span>
    <span class="n">PatchPredictor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.utils.misc</span><span class="w"> </span><span class="kn">import</span> <span class="n">download_data</span><span class="p">,</span> <span class="n">grab_files_from_dir</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.utils.visualization</span><span class="w"> </span><span class="kn">import</span> <span class="n">overlay_prediction_mask</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.wsicore.wsireader</span><span class="w"> </span><span class="kn">import</span> <span class="n">WSIReader</span>

<span class="c1"># Torch-related</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>

<span class="c1"># Configure plotting</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">160</span>  <span class="c1"># for high resolution figure in notebook</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"figure.facecolor"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"white"</span>  <span class="c1"># To make sure text is visible in dark mode</span>

<span class="c1"># If you are not using GPU, change ON_GPU to False</span>
<span class="n">ON_GPU</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Function to suppress console output for overly verbose code blocks</span>
<span class="k">def</span><span class="w"> </span><span class="nf">suppress_console_output</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stderr</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="clean-up-before-a-run">
<h3>Clean-up before a run<a class="headerlink" href="#clean-up-before-a-run" title="Link to this heading">#</a></h3>
<p>To ensure proper clean-up (for example in abnormal termination), all
files downloaded or created in this run are saved in a single directory
<code class="docutils literal notranslate"><span class="pre">global_save_dir</span></code>, which we set equal to “./tmp/”. To simplify
maintenance, the name of the directory occurs only at this one place, so
that it can easily be changed, if desired.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>
<span class="n">global_save_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"./tmp/"</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">rmdir</span><span class="p">(</span><span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Helper function to delete directory."""</span>
    <span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Removing directory </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">dir_path</span><span class="p">)</span>


<span class="n">rmdir</span><span class="p">(</span><span class="n">global_save_dir</span><span class="p">)</span>  <span class="c1"># remove  directory if it exists from previous runs</span>
<span class="n">global_save_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Creating new directory </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">global_save_dir</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="downloading-the-data">
<h3>Downloading the data<a class="headerlink" href="#downloading-the-data" title="Link to this heading">#</a></h3>
<p>For our sample data, we will use one whole-slide image, and patches from
the validation subset of <a class="reference external" href="https://zenodo.org/record/1214456#.YJ-tn3mSkuU">Kather
100k</a> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wsi_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"sample_wsi.svs"</span>
<span class="n">patches_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"kather100k-validation-sample.zip"</span>
<span class="n">weights_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"resnet18-kather100k.pth"</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Download has started. Please wait..."</span><span class="p">)</span>

<span class="c1"># Downloading and unzip a sample whole-slide image</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs"</span><span class="p">,</span>
    <span class="n">wsi_path</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Download and unzip a sample of the validation set used to train the Kather 100K dataset</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip"</span><span class="p">,</span>
    <span class="n">patches_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">ZipFile</span><span class="p">(</span><span class="n">patches_path</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">zipfile</span><span class="p">:</span>
    <span class="n">zipfile</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">global_save_dir</span><span class="p">)</span>

<span class="c1"># Download pretrained model weights for WSI classification using ResNet18 architecture</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth"</span><span class="p">,</span>
    <span class="n">weights_path</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Download is complete."</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="reading-the-data">
<h2>Reading the data<a class="headerlink" href="#reading-the-data" title="Link to this heading">#</a></h2>
<p>We create a list of patches and a list of corresponding labels. For
example, the first label in <code class="docutils literal notranslate"><span class="pre">label_list</span></code> will indicate the class of
the first image patch in <code class="docutils literal notranslate"><span class="pre">patch_list</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read the patch data and create a list of patches and a list of corresponding labels</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"kather100k-validation-sample"</span>

<span class="c1"># Set the path to the dataset</span>
<span class="n">image_ext</span> <span class="o">=</span> <span class="s2">".tif"</span>  <span class="c1"># file extension of each image</span>

<span class="c1"># Obtain the mapping between the label ID and the class name</span>
<span class="n">label_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"BACK"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># Background (empty glass region)</span>
    <span class="s2">"NORM"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># Normal colon mucosa</span>
    <span class="s2">"DEB"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Debris</span>
    <span class="s2">"TUM"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># Colorectal adenocarcinoma epithelium</span>
    <span class="s2">"ADI"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>  <span class="c1"># Adipose</span>
    <span class="s2">"MUC"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>  <span class="c1"># Mucus</span>
    <span class="s2">"MUS"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>  <span class="c1"># Smooth muscle</span>
    <span class="s2">"STR"</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>  <span class="c1"># Cancer-associated stroma</span>
    <span class="s2">"LYM"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># Lymphocytes</span>
<span class="p">}</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">label_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">label_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="c1"># Generate a list of patches and generate the label from the filename</span>
<span class="n">patch_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">label_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">dataset_class_path</span> <span class="o">=</span> <span class="n">dataset_path</span> <span class="o">/</span> <span class="n">class_name</span>
    <span class="n">patch_list_single_class</span> <span class="o">=</span> <span class="n">grab_files_from_dir</span><span class="p">(</span>
        <span class="n">dataset_class_path</span><span class="p">,</span>
        <span class="n">file_types</span><span class="o">=</span><span class="s2">"*"</span> <span class="o">+</span> <span class="n">image_ext</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">patch_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">patch_list_single_class</span><span class="p">)</span>
    <span class="n">label_list</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">label</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">patch_list_single_class</span><span class="p">))</span>

<span class="c1"># Show some dataset statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="p">[</span><span class="n">label_list</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">class_labels</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Patch types"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Number of patches"</span><span class="p">)</span>

<span class="c1"># Count the number of examples per class</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">"Class ID: </span><span class="si">%d</span><span class="s2"> -- Class Name: </span><span class="si">%s</span><span class="s2"> -- Number of images: </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span>
        <span class="n">label</span><span class="p">,</span>
        <span class="n">class_name</span><span class="p">,</span>
        <span class="n">label_list</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">label</span><span class="p">),</span>
    <span class="p">)</span>

<span class="c1"># Overall dataset statistics</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Total number of patches: </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">patch_list</span><span class="p">)))</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_001.png" srcset="../_images/tiatoolbox_tutorial_001.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>|2023-11-14|13:15:59.299| [INFO] Class ID: 0 -- Class Name: BACK -- Number of images: 211
|2023-11-14|13:15:59.299| [INFO] Class ID: 1 -- Class Name: NORM -- Number of images: 176
|2023-11-14|13:15:59.299| [INFO] Class ID: 2 -- Class Name: DEB -- Number of images: 230
|2023-11-14|13:15:59.299| [INFO] Class ID: 3 -- Class Name: TUM -- Number of images: 286
|2023-11-14|13:15:59.299| [INFO] Class ID: 4 -- Class Name: ADI -- Number of images: 208
|2023-11-14|13:15:59.299| [INFO] Class ID: 5 -- Class Name: MUC -- Number of images: 178
|2023-11-14|13:15:59.299| [INFO] Class ID: 6 -- Class Name: MUS -- Number of images: 270
|2023-11-14|13:15:59.299| [INFO] Class ID: 7 -- Class Name: STR -- Number of images: 209
|2023-11-14|13:15:59.299| [INFO] Class ID: 8 -- Class Name: LYM -- Number of images: 232
|2023-11-14|13:15:59.299| [INFO] Total number of patches: 2000
</pre></div>
</div>
<p>As you can see for this patch dataset, we have 9 classes/labels with IDs
0-8 and associated class names. describing the dominant tissue type in
the patch:</p>
<ul class="simple">
<li><p>BACK ⟶ Background (empty glass region)</p></li>
<li><p>LYM ⟶ Lymphocytes</p></li>
<li><p>NORM ⟶ Normal colon mucosa</p></li>
<li><p>DEB ⟶ Debris</p></li>
<li><p>MUS ⟶ Smooth muscle</p></li>
<li><p>STR ⟶ Cancer-associated stroma</p></li>
<li><p>ADI ⟶ Adipose</p></li>
<li><p>MUC ⟶ Mucus</p></li>
<li><p>TUM ⟶ Colorectal adenocarcinoma epithelium</p></li>
</ul>
</section>
<section id="classify-image-patches">
<h2>Classify image patches<a class="headerlink" href="#classify-image-patches" title="Link to this heading">#</a></h2>
<p>We demonstrate how to obtain a prediction for each patch within a
digital slide first with the <code class="docutils literal notranslate"><span class="pre">patch</span></code> mode and then with a large slide
using <code class="docutils literal notranslate"><span class="pre">wsi</span></code> mode.</p>
<section id="define-patchpredictor-model">
<h3>Define <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> model<a class="headerlink" href="#define-patchpredictor-model" title="Link to this heading">#</a></h3>
<p>The PatchPredictor class runs a CNN-based classifier written in PyTorch.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> can be any trained PyTorch model with the constraint that
it should follow the
<code class="docutils literal notranslate"><span class="pre">tiatoolbox.models.abc.ModelABC</span></code> <cite>(docs)</cite> &lt;<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html">https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html</a>&gt;`__
class structure. For more information on this matter, please refer to
<a class="reference external" href="https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-modeling.ipynb">our example notebook on advanced model
techniques</a>.
In order to load a custom model, you need to write a small
preprocessing function, as in <code class="docutils literal notranslate"><span class="pre">preproc_func(img)</span></code>, which makes sure
the input tensors are in the right format for the loaded network.</p></li>
<li><p>Alternatively, you can pass <code class="docutils literal notranslate"><span class="pre">pretrained_model</span></code> as a string
argument. This specifies the CNN model that performs the prediction,
and it must be one of the models listed
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.models.architecture.get_pretrained_model.html#tiatoolbox.models.architecture.get_pretrained_model">here</a>.
The command will look like this:
<code class="docutils literal notranslate"><span class="pre">predictor</span> <span class="pre">=</span> <span class="pre">PatchPredictor(pretrained_model='resnet18-kather100k',</span> <span class="pre">pretrained_weights=weights_path,</span> <span class="pre">batch_size=32)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pretrained_weights</span></code>: When using a <code class="docutils literal notranslate"><span class="pre">pretrained_model</span></code>, the
corresponding pretrained weights will also be downloaded by default.
You can override the default with your own set of weights via the
<code class="docutils literal notranslate"><span class="pre">pretrained_weight</span></code> argument.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: Number of images fed into the model each time. Higher
values for this parameter require a larger (GPU) memory capacity.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a pretrained PyTorch model from TIAToolbox</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">PatchPredictor</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">=</span><span class="s1">'resnet18-kather100k'</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Users can load any PyTorch model architecture instead using the following script</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">vanilla</span><span class="o">.</span><span class="n">CNNModel</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="s2">"resnet18"</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span> <span class="c1"># Importing model from torchvision.models.resnet18</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">preproc_func</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">preproc_func</span> <span class="o">=</span> <span class="n">preproc_func</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">PatchPredictor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="predict-patch-labels">
<h3>Predict patch labels<a class="headerlink" href="#predict-patch-labels" title="Link to this heading">#</a></h3>
<p>We create a predictor object and then call the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method using
the <code class="docutils literal notranslate"><span class="pre">patch</span></code> mode. We then compute the classification accuracy and
confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">imgs</span><span class="o">=</span><span class="n">patch_list</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"patch"</span><span class="p">,</span> <span class="n">on_gpu</span><span class="o">=</span><span class="n">ON_GPU</span><span class="p">)</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">output</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">])</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Classification accuracy: </span><span class="si">%f</span><span class="s2">"</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>

<span class="c1"># Creating and visualizing the confusion matrix for patch classification results</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">output</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="s2">"true"</span><span class="p">)</span>
<span class="n">df_cm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">df_cm</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>|2023-11-14|13:16:03.215| [INFO] Classification accuracy: 0.993000
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>BACK</th>
<th>NORM</th>
<th>DEB</th>
<th>TUM</th>
<th>ADI</th>
<th>MUC</th>
<th>MUS</th>
<th>STR</th>
<th>LYM</th>
</tr>
</thead>
<tbody>
<tr>
<th>BACK</th>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>NORM</th>
<td>0.000000</td>
<td>0.988636</td>
<td>0.000000</td>
<td>0.011364</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>DEB</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.991304</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.008696</td>
<td>0.00000</td>
</tr>
<tr>
<th>TUM</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.996503</td>
<td>0.000000</td>
<td>0.003497</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>ADI</th>
<td>0.004808</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.990385</td>
<td>0.000000</td>
<td>0.004808</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>MUC</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.988764</td>
<td>0.000000</td>
<td>0.011236</td>
<td>0.00000</td>
</tr>
<tr>
<th>MUS</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.996296</td>
<td>0.003704</td>
<td>0.00000</td>
</tr>
<tr>
<th>STR</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004785</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004785</td>
<td>0.004785</td>
<td>0.985646</td>
<td>0.00000</td>
</tr>
<tr>
<th>LYM</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004310</td>
<td>0.99569</td>
</tr>
</tbody>
</table>
</div>
</div>
<br/>
<br/></section>
<section id="predict-patch-labels-for-a-whole-slide">
<h3>Predict patch labels for a whole slide<a class="headerlink" href="#predict-patch-labels-for-a-whole-slide" title="Link to this heading">#</a></h3>
<p>We now introduce <code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code>, a class that specifies the
configuration of image reading and prediction writing for the model
prediction engine. This is required to inform the classifier which level
of the WSI pyramid the classifier should read, process data and generate
output.</p>
<p>Parameters of <code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code> are defined as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_resolutions</span></code>: A list, in the form of a dictionary,
specifying the resolution of each input. List elements must be in the
same order as in the target <code class="docutils literal notranslate"><span class="pre">model.forward()</span></code>. If your model
accepts only one input, you just need to put one dictionary
specifying <code class="docutils literal notranslate"><span class="pre">'units'</span></code> and <code class="docutils literal notranslate"><span class="pre">'resolution'</span></code>. Note that TIAToolbox
supports a model with more than one input. For more information on
units and resolution, please see <a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect">TIAToolbox
documentation</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">patch_input_shape</span></code>: Shape of the largest input in (height, width)
format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride_shape</span></code>: The size of a stride (steps) between two
consecutive patches, used in the patch extraction process. If the
user sets <code class="docutils literal notranslate"><span class="pre">stride_shape</span></code> equal to <code class="docutils literal notranslate"><span class="pre">patch_input_shape</span></code>, patches
will be extracted and processed without any overlap.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wsi_ioconfig</span> <span class="o">=</span> <span class="n">IOPatchPredictorConfig</span><span class="p">(</span>
    <span class="n">input_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">stride_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">predict</span></code> method applies the CNN on the input patches and get the
results. Here are the arguments and their descriptions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: Type of input to be processed. Choose from <code class="docutils literal notranslate"><span class="pre">patch</span></code>,
<code class="docutils literal notranslate"><span class="pre">tile</span></code> or <code class="docutils literal notranslate"><span class="pre">wsi</span></code> according to your application.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imgs</span></code>: List of inputs, which should be a list of paths to the
input tiles or WSIs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">return_probabilities</span></code>: Set to <strong>True</strong> to get per class
probabilities alongside predicted labels of input patches. If you
wish to merge the predictions to generate prediction maps for
<code class="docutils literal notranslate"><span class="pre">tile</span></code> or <code class="docutils literal notranslate"><span class="pre">wsi</span></code> modes, you can set <code class="docutils literal notranslate"><span class="pre">return_probabilities=True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ioconfig</span></code>: set the IO configuration information using the
<code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code> class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resolution</span></code> and <code class="docutils literal notranslate"><span class="pre">unit</span></code> (not shown below): These arguments
specify the level or micron-per-pixel resolution of the WSI levels
from which we plan to extract patches and can be used instead of
<code class="docutils literal notranslate"><span class="pre">ioconfig</span></code>. Here we specify the WSI level as <code class="docutils literal notranslate"><span class="pre">'baseline'</span></code>,
which is equivalent to level 0. In general, this is the level of
greatest resolution. In this particular case, the image has only one
level. More information can be found in the
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect">documentation</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">masks</span></code>: A list of paths corresponding to the masks of WSIs in the
<code class="docutils literal notranslate"><span class="pre">imgs</span></code> list. These masks specify the regions in the original WSIs
from which we want to extract patches. If the mask of a particular
WSI is specified as <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the labels for all patches of that
WSI (even background regions) would be predicted. This could cause
unnecessary computation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code>: You can set this parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code> if it’s
required to generate a 2D map of patch classification results.
However, for large WSIs this will require large available memory. An
alternative (default) solution is to set <code class="docutils literal notranslate"><span class="pre">merge_predictions=False</span></code>,
and then generate the 2D prediction maps using the
<code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> function as you will see later on.</p></li>
</ul>
<p>Since we are using a large WSI the patch extraction and prediction
processes may take some time (make sure to set the <code class="docutils literal notranslate"><span class="pre">ON_GPU=True</span></code> if
you have access to Cuda enabled GPU and PyTorch+Cuda).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">wsi_output</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">imgs</span><span class="o">=</span><span class="p">[</span><span class="n">wsi_path</span><span class="p">],</span>
        <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">"wsi"</span><span class="p">,</span>
        <span class="n">merge_predictions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">ioconfig</span><span class="o">=</span><span class="n">wsi_ioconfig</span><span class="p">,</span>
        <span class="n">return_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="o">=</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_predictions"</span><span class="p">,</span>
        <span class="n">on_gpu</span><span class="o">=</span><span class="n">ON_GPU</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>We see how the prediction model works on our whole-slide images by
visualizing the <code class="docutils literal notranslate"><span class="pre">wsi_output</span></code>. We first need to merge patch prediction
outputs and then visualize them as an overlay on the original image. As
before, the <code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> method is used to merge the patch
predictions. Here we set the parameters
<code class="docutils literal notranslate"><span class="pre">resolution=1.25,</span> <span class="pre">units='power'</span></code> to generate the prediction map at
1.25x magnification. If you would like to have higher/lower resolution
(bigger/smaller) prediction maps, you need to change these parameters
accordingly. When the predictions are merged, use the
<code class="docutils literal notranslate"><span class="pre">overlay_patch_prediction</span></code> function to overlay the prediction map on
the WSI thumbnail, which should be extracted at the resolution used for
prediction merging.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">overview_resolution</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mi">4</span>  <span class="c1"># the resolution in which we desire to merge and visualize the patch predictions</span>
<span class="p">)</span>
<span class="c1"># the unit of the `resolution` parameter. Can be "power", "level", "mpp", or "baseline"</span>
<span class="n">overview_unit</span> <span class="o">=</span> <span class="s2">"mpp"</span>
<span class="n">wsi</span> <span class="o">=</span> <span class="n">WSIReader</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">wsi_path</span><span class="p">)</span>
<span class="n">wsi_overview</span> <span class="o">=</span> <span class="n">wsi</span><span class="o">.</span><span class="n">slide_thumbnail</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="n">overview_resolution</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">overview_unit</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(),</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wsi_overview</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_002.png" srcset="../_images/tiatoolbox_tutorial_002.png"/><p>Overlaying the prediction map on this image as below gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization of whole-slide image patch-level prediction</span>
<span class="c1"># first set up a label to color mapping</span>
<span class="n">label_color_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">label_color_dict</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"empty"</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"Set1"</span><span class="p">)</span><span class="o">.</span><span class="n">colors</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">label_color_dict</span><span class="p">[</span><span class="n">label</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">class_name</span><span class="p">,</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="n">label</span><span class="p">]))</span>

<span class="n">pred_map</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">merge_predictions</span><span class="p">(</span>
    <span class="n">wsi_path</span><span class="p">,</span>
    <span class="n">wsi_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">resolution</span><span class="o">=</span><span class="n">overview_resolution</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="n">overview_unit</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">overlay</span> <span class="o">=</span> <span class="n">overlay_prediction_mask</span><span class="p">(</span>
    <span class="n">wsi_overview</span><span class="p">,</span>
    <span class="n">pred_map</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label_info</span><span class="o">=</span><span class="n">label_color_dict</span><span class="p">,</span>
    <span class="n">return_ax</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_003.png" srcset="../_images/tiatoolbox_tutorial_003.png"/></section>
</section>
<section id="feature-extraction-with-a-pathology-specific-model">
<h2>Feature extraction with a pathology-specific model<a class="headerlink" href="#feature-extraction-with-a-pathology-specific-model" title="Link to this heading">#</a></h2>
<p>In this section, we will show how to extract features from a pretrained
PyTorch model that exists outside TIAToolbox, using the WSI inference
engines provided by TIAToolbox. To illustrate this we will use
HistoEncoder, a computational-pathology specific model that has been
trained in a self-supervised fashion to extract features from histology
images. The model has been made available here:</p>
<p>‘HistoEncoder: Foundation models for digital pathology’
(<a class="github reference external" href="https://github.com/jopo666/HistoEncoder">jopo666/HistoEncoder</a>) by Pohjonen, Joona and team at
the University of Helsinki.</p>
<p>We will plot a umap reduction into 3D (RGB) of the feature map to
visualize how the features capture the differences between some of the
above mentioned tissue types.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some extra modules</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">histoencoder.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.models.engine.semantic_segmentor</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeepFeatureExtractor</span><span class="p">,</span> <span class="n">IOSegmentorConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiatoolbox.models.models_abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelABC</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">umap</span>
</pre></div>
</div>
<p>TIAToolbox defines a ModelABC which is a class inheriting PyTorch
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a>
and specifies how a model should look in order to be used in the
TIAToolbox inference engines. The histoencoder model doesn’t follow this
structure, so we need to wrap it in a class whose output and methods are
those that the TIAToolbox engine expects.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">HistoEncWrapper</span><span class="p">(</span><span class="n">ModelABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">HistoEncWrapper</span><span class="p">,</span> <span class="n">encoder</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feat_extract</span> <span class="o">=</span> <span class="n">encoder</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">HistoEncWrapper</span><span class="p">,</span> <span class="n">imgs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Pass input data through the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            imgs (torch.Tensor):</span>
<span class="sd">                Model input.</span>

<span class="sd">        """</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">extract_features</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_extract</span><span class="p">,</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">avg_pool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">infer_batch</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">batch_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">on_gpu</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Run inference on an input batch.</span>

<span class="sd">        Contains logic for forward operation as well as i/o aggregation.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (nn.Module):</span>
<span class="sd">                PyTorch defined model.</span>
<span class="sd">            batch_data (torch.Tensor):</span>
<span class="sd">                A batch of data generated by</span>
<span class="sd">                `torch.utils.data.DataLoader`.</span>
<span class="sd">            on_gpu (bool):</span>
<span class="sd">                Whether to run inference on a GPU.</span>

<span class="sd">        """</span>
        <span class="n">img_patches_device</span> <span class="o">=</span> <span class="n">batch_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span> <span class="k">if</span> <span class="n">on_gpu</span> <span class="k">else</span> <span class="n">batch_data</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="c1"># Do not compute the gradient (not training)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img_patches_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
</pre></div>
</div>
<p>Now that we have our wrapper, we will create our feature extraction
model and instantiate a
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/v1.4.1/_autosummary/tiatoolbox.models.engine.semantic_segmentor.DeepFeatureExtractor.html">DeepFeatureExtractor</a>
to allow us to use this model over a WSI. We will use the same WSI as
above, but this time we will extract features from the patches of the
WSI using the HistoEncoder model, rather than predicting some label for
each patch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the model</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">create_encoder</span><span class="p">(</span><span class="s2">"prostate_medium"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HistoEncWrapper</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>

<span class="c1"># set the pre-processing function</span>
<span class="n">norm</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.662</span><span class="p">,</span> <span class="mf">0.446</span><span class="p">,</span> <span class="mf">0.605</span><span class="p">],</span><span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.169</span><span class="p">,</span> <span class="mf">0.190</span><span class="p">,</span> <span class="mf">0.155</span><span class="p">])</span>
<span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">norm</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">preproc_func</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="n">trans</span><span class="p">)</span>

<span class="n">wsi_ioconfig</span> <span class="o">=</span> <span class="n">IOSegmentorConfig</span><span class="p">(</span>
    <span class="n">input_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">output_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_output_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">stride_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When we create the <code class="docutils literal notranslate"><span class="pre">DeepFeatureExtractor</span></code>, we will pass the
<code class="docutils literal notranslate"><span class="pre">auto_generate_mask=True</span></code> argument. This will automatically create a
mask of the tissue region using otsu thresholding, so that the extractor
processes only those patches containing tissue.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the feature extractor and run it on the WSI</span>
<span class="n">extractor</span> <span class="o">=</span> <span class="n">DeepFeatureExtractor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">auto_generate_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_loader_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_postproc_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">imgs</span><span class="o">=</span><span class="p">[</span><span class="n">wsi_path</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"wsi"</span><span class="p">,</span> <span class="n">ioconfig</span><span class="o">=</span><span class="n">wsi_ioconfig</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span><span class="p">,)</span>
</pre></div>
</div>
<p>These features could be used to train a downstream model, but here in
order to get some intuition for what the features represent, we will use
a UMAP reduction to visualize the features in RGB space. The points
labeled in a similar color should have similar features, so we can check
if the features naturally separate out into the different tissue regions
when we overlay the UMAP reduction on the WSI thumbnail. We will plot it
along with the patch-level prediction map from above to see how the
features compare to the patch-level predictions in the following cells.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we define a function to calculate the umap reduction</span>
<span class="k">def</span><span class="w"> </span><span class="nf">umap_reducer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">nns</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""UMAP reduction of the input data."""</span>
    <span class="n">reducer</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">nns</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">dims</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">"manhattan"</span><span class="p">,</span> <span class="n">spread</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">=</span> <span class="n">reducer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">-=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">/=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduced</span>

<span class="c1"># load the features output by our feature extractor</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span> <span class="o">/</span> <span class="s2">"0.position.npy"</span><span class="p">)</span>
<span class="n">feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span> <span class="o">/</span> <span class="s2">"0.features.0.npy"</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">/</span> <span class="mi">8</span> <span class="c1"># as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp</span>

<span class="c1"># reduce the features into 3 dimensional (rgb) space</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">umap_reducer</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>

<span class="c1"># plot the prediction map the classifier again</span>
<span class="n">overlay</span> <span class="o">=</span> <span class="n">overlay_prediction_mask</span><span class="p">(</span>
    <span class="n">wsi_overview</span><span class="p">,</span>
    <span class="n">pred_map</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label_info</span><span class="o">=</span><span class="n">label_color_dict</span><span class="p">,</span>
    <span class="n">return_ax</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># plot the feature map reduction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wsi_overview</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pos</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">reduced</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"UMAP reduction of HistoEnc features"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="tiatoolbox tutorial" class="sphx-glr-multi-img" src="../_images/tiatoolbox_tutorial_004.png" srcset="../_images/tiatoolbox_tutorial_004.png"/></li>
<li><img alt="UMAP reduction of HistoEnc features" class="sphx-glr-multi-img" src="../_images/tiatoolbox_tutorial_005.png" srcset="../_images/tiatoolbox_tutorial_005.png"/></li>
</ul>
<p>We see that the prediction map from our patch-level predictor, and the
feature map from our self-supervised feature encoder, capture similar
information about the tissue types in the WSI. This is a good sanity
check that our models are working as expected. It also shows that the
features extracted by the HistoEncoder model are capturing the
differences between the tissue types, and so that they are encoding
histologically relevant information.</p>
</section>
<section id="where-to-go-from-here">
<h2>Where to Go From Here<a class="headerlink" href="#where-to-go-from-here" title="Link to this heading">#</a></h2>
<p>In this notebook, we show how we can use the <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> and
<code class="docutils literal notranslate"><span class="pre">DeepFeatureExtractor</span></code> classes and their <code class="docutils literal notranslate"><span class="pre">predict</span></code> method to predict
the label, or extract features, for patches of big tiles and WSIs. We
introduce <code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">overlay_prediction_mask</span></code> helper
functions that merge the patch prediction outputs and visualize the
resulting prediction map as an overlay on the input image/WSI.</p>
<p>All the processes take place within TIAToolbox and we can easily put the
pieces together, following our example code. Please make sure to set
inputs and options correctly. We encourage you to further investigate
the effect on the prediction output of changing <code class="docutils literal notranslate"><span class="pre">predict</span></code> function
parameters. We have demonstrated how to use your own pretrained model or
one provided by the research community for a specific task in the
TIAToolbox framework to do inference on large WSIs even if the model
structure is not defined in the TIAToolbox model class.</p>
<p>You can learn more through the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/07-advanced-modeling.html">Advanced model handling with PyTorch and
TIAToolbox</a></p></li>
<li><p><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/full-pipelines/slide-graph.html">Creating slide graphs for WSI with a custom PyTorch graph neural
network</a></p></li>
</ul>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-environment">Setting up the environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-related-libraries">Importing related libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clean-up-before-a-run">Clean-up before a run</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-the-data">Downloading the data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-the-data">Reading the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classify-image-patches">Classify image patches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-patchpredictor-model">Define <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-patch-labels">Predict patch labels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-patch-labels-for-a-whole-slide">Predict patch labels for a whole slide</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-extraction-with-a-pathology-specific-model">Feature extraction with a pathology-specific model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-to-go-from-here">Where to Go From Here</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "name": "Whole Slide Image Classification Using PyTorch and TIAToolbox",
         "headline": "Whole Slide Image Classification Using PyTorch and TIAToolbox",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/intermediate/tiatoolbox_tutorial.html",
         "articleBody": "Whole Slide Image Classification Using PyTorch and TIAToolbox# Tip To get the most of this tutorial, we suggest using this Colab Version. This will allow you to experiment with the information presented below. Introduction# In this tutorial, we will show how to classify Whole Slide Images (WSIs) using PyTorch deep learning models with help from TIAToolbox. A WSI is an image of a sample of human tissue taken through a surgery or biopsy and scanned using specialized scanners. They are used by pathologists and computational pathology researchers to study diseases such as cancer at the microscopic level in order to understand for example tumor growth and help improve treatment for patients. What makes WSIs challenging to process is their enormous size. For example, a typical slide image has in the order of 100,000x100,000 pixels where each pixel can correspond to about 0.25x0.25 microns on the slide. This introduces challenges in loading and processing such images, not to mention hundreds or even thousands of WSIs in a single study (larger studies produce better results)! Conventional image processing pipelines are not suitable for WSI processing so we need better tools. This is where TIAToolbox can help as it brings a set of useful tools to import and process tissue slides in a fast and computationally efficient manner. Typically, WSIs are saved in a pyramid structure with multiple copies of the same image at various magnification levels optimized for visualization. The level 0 (or the bottom level) of the pyramid contains the image at the highest magnification or zoom level, whereas the higher levels in the pyramid have a lower resolution copy of the base image. The pyramid structure is sketched below. WSI pyramid stack (source) TIAToolbox allows us to automate common downstream analysis tasks such as tissue classification. In this tutorial we show how you can: 1. Load WSI images using TIAToolbox; and 2. Use different PyTorch models to classify slides at the patch-level. In this tutorial, we will provide an example of using TorchVision ResNet18 model and custom HistoEncoder \u003cjopo666/HistoEncoder\u003e`__ model. Let\u2019s get started! Setting up the environment# To run the examples provided in this tutorial, the following packages are required as prerequisites. OpenJpeg OpenSlide Pixman TIAToolbox HistoEncoder (for a custom model example) Please run the following command in your terminal to install these packages: apt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev pip install -q \u2018tiatoolbox\u003c1.5\u2019 histoencoder \u0026\u0026 echo \u201cInstallation is done.\u201d Alternatively, you can run brew install openjpeg openslide to install the prerequisite packages on MacOS instead of apt-get. Further information on installation can be found here. Importing related libraries# \"\"\"Import modules required to run the Jupyter notebook.\"\"\" from __future__ import annotations # Configure logging import logging import warnings if logging.getLogger().hasHandlers(): logging.getLogger().handlers.clear() warnings.filterwarnings(\"ignore\", message=\".*The \u0027nopython\u0027 keyword.*\") # Downloading data and files import shutil from pathlib import Path from zipfile import ZipFile # Data processing and visualization import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import cm import PIL import contextlib import io from sklearn.metrics import accuracy_score, confusion_matrix # TIAToolbox for WSI loading and processing from tiatoolbox import logger from tiatoolbox.models.architecture import vanilla from tiatoolbox.models.engine.patch_predictor import ( IOPatchPredictorConfig, PatchPredictor, ) from tiatoolbox.utils.misc import download_data, grab_files_from_dir from tiatoolbox.utils.visualization import overlay_prediction_mask from tiatoolbox.wsicore.wsireader import WSIReader # Torch-related import torch from torchvision import transforms # Configure plotting mpl.rcParams[\"figure.dpi\"] = 160 # for high resolution figure in notebook mpl.rcParams[\"figure.facecolor\"] = \"white\" # To make sure text is visible in dark mode # If you are not using GPU, change ON_GPU to False ON_GPU = True # Function to suppress console output for overly verbose code blocks def suppress_console_output(): return contextlib.redirect_stderr(io.StringIO()) Clean-up before a run# To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory global_save_dir, which we set equal to \u201c./tmp/\u201d. To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired. warnings.filterwarnings(\"ignore\") global_save_dir = Path(\"./tmp/\") def rmdir(dir_path: str | Path) -\u003e None: \"\"\"Helper function to delete directory.\"\"\" if Path(dir_path).is_dir(): shutil.rmtree(dir_path) logger.info(\"Removing directory %s\", dir_path) rmdir(global_save_dir) # remove directory if it exists from previous runs global_save_dir.mkdir() logger.info(\"Creating new directory %s\", global_save_dir) Downloading the data# For our sample data, we will use one whole-slide image, and patches from the validation subset of Kather 100k dataset. wsi_path = global_save_dir / \"sample_wsi.svs\" patches_path = global_save_dir / \"kather100k-validation-sample.zip\" weights_path = global_save_dir / \"resnet18-kather100k.pth\" logger.info(\"Download has started. Please wait...\") # Downloading and unzip a sample whole-slide image download_data( \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\", wsi_path, ) # Download and unzip a sample of the validation set used to train the Kather 100K dataset download_data( \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\", patches_path, ) with ZipFile(patches_path, \"r\") as zipfile: zipfile.extractall(path=global_save_dir) # Download pretrained model weights for WSI classification using ResNet18 architecture download_data( \"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth\", weights_path, ) logger.info(\"Download is complete.\") Reading the data# We create a list of patches and a list of corresponding labels. For example, the first label in label_list will indicate the class of the first image patch in patch_list. # Read the patch data and create a list of patches and a list of corresponding labels dataset_path = global_save_dir / \"kather100k-validation-sample\" # Set the path to the dataset image_ext = \".tif\" # file extension of each image # Obtain the mapping between the label ID and the class name label_dict = { \"BACK\": 0, # Background (empty glass region) \"NORM\": 1, # Normal colon mucosa \"DEB\": 2, # Debris \"TUM\": 3, # Colorectal adenocarcinoma epithelium \"ADI\": 4, # Adipose \"MUC\": 5, # Mucus \"MUS\": 6, # Smooth muscle \"STR\": 7, # Cancer-associated stroma \"LYM\": 8, # Lymphocytes } class_names = list(label_dict.keys()) class_labels = list(label_dict.values()) # Generate a list of patches and generate the label from the filename patch_list = [] label_list = [] for class_name, label in label_dict.items(): dataset_class_path = dataset_path / class_name patch_list_single_class = grab_files_from_dir( dataset_class_path, file_types=\"*\" + image_ext, ) patch_list.extend(patch_list_single_class) label_list.extend([label] * len(patch_list_single_class)) # Show some dataset statistics plt.bar(class_names, [label_list.count(label) for label in class_labels]) plt.xlabel(\"Patch types\") plt.ylabel(\"Number of patches\") # Count the number of examples per class for class_name, label in label_dict.items(): logger.info( \"Class ID: %d -- Class Name: %s -- Number of images: %d\", label, class_name, label_list.count(label), ) # Overall dataset statistics logger.info(\"Total number of patches: %d\", (len(patch_list))) |2023-11-14|13:15:59.299| [INFO] Class ID: 0 -- Class Name: BACK -- Number of images: 211 |2023-11-14|13:15:59.299| [INFO] Class ID: 1 -- Class Name: NORM -- Number of images: 176 |2023-11-14|13:15:59.299| [INFO] Class ID: 2 -- Class Name: DEB -- Number of images: 230 |2023-11-14|13:15:59.299| [INFO] Class ID: 3 -- Class Name: TUM -- Number of images: 286 |2023-11-14|13:15:59.299| [INFO] Class ID: 4 -- Class Name: ADI -- Number of images: 208 |2023-11-14|13:15:59.299| [INFO] Class ID: 5 -- Class Name: MUC -- Number of images: 178 |2023-11-14|13:15:59.299| [INFO] Class ID: 6 -- Class Name: MUS -- Number of images: 270 |2023-11-14|13:15:59.299| [INFO] Class ID: 7 -- Class Name: STR -- Number of images: 209 |2023-11-14|13:15:59.299| [INFO] Class ID: 8 -- Class Name: LYM -- Number of images: 232 |2023-11-14|13:15:59.299| [INFO] Total number of patches: 2000 As you can see for this patch dataset, we have 9 classes/labels with IDs 0-8 and associated class names. describing the dominant tissue type in the patch: BACK \u27f6 Background (empty glass region) LYM \u27f6 Lymphocytes NORM \u27f6 Normal colon mucosa DEB \u27f6 Debris MUS \u27f6 Smooth muscle STR \u27f6 Cancer-associated stroma ADI \u27f6 Adipose MUC \u27f6 Mucus TUM \u27f6 Colorectal adenocarcinoma epithelium Classify image patches# We demonstrate how to obtain a prediction for each patch within a digital slide first with the patch mode and then with a large slide using wsi mode. Define PatchPredictor model# The PatchPredictor class runs a CNN-based classifier written in PyTorch. model can be any trained PyTorch model with the constraint that it should follow the tiatoolbox.models.abc.ModelABC (docs) \u003chttps://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html\u003e`__ class structure. For more information on this matter, please refer to our example notebook on advanced model techniques. In order to load a custom model, you need to write a small preprocessing function, as in preproc_func(img), which makes sure the input tensors are in the right format for the loaded network. Alternatively, you can pass pretrained_model as a string argument. This specifies the CNN model that performs the prediction, and it must be one of the models listed here. The command will look like this: predictor = PatchPredictor(pretrained_model=\u0027resnet18-kather100k\u0027, pretrained_weights=weights_path, batch_size=32). pretrained_weights: When using a pretrained_model, the corresponding pretrained weights will also be downloaded by default. You can override the default with your own set of weights via the pretrained_weight argument. batch_size: Number of images fed into the model each time. Higher values for this parameter require a larger (GPU) memory capacity. # Importing a pretrained PyTorch model from TIAToolbox predictor = PatchPredictor(pretrained_model=\u0027resnet18-kather100k\u0027, batch_size=32) # Users can load any PyTorch model architecture instead using the following script model = vanilla.CNNModel(backbone=\"resnet18\", num_classes=9) # Importing model from torchvision.models.resnet18 model.load_state_dict(torch.load(weights_path, map_location=\"cpu\", weights_only=True), strict=True) def preproc_func(img): img = PIL.Image.fromarray(img) img = transforms.ToTensor()(img) return img.permute(1, 2, 0) model.preproc_func = preproc_func predictor = PatchPredictor(model=model, batch_size=32) Predict patch labels# We create a predictor object and then call the predict method using the patch mode. We then compute the classification accuracy and confusion matrix. with suppress_console_output(): output = predictor.predict(imgs=patch_list, mode=\"patch\", on_gpu=ON_GPU) acc = accuracy_score(label_list, output[\"predictions\"]) logger.info(\"Classification accuracy: %f\", acc) # Creating and visualizing the confusion matrix for patch classification results conf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\") df_cm = pd.DataFrame(conf, index=class_names, columns=class_names) df_cm |2023-11-14|13:16:03.215| [INFO] Classification accuracy: 0.993000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BACK NORM DEB TUM ADI MUC MUS STR LYM BACK 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 NORM 0.000000 0.988636 0.000000 0.011364 0.000000 0.000000 0.000000 0.000000 0.00000 DEB 0.000000 0.000000 0.991304 0.000000 0.000000 0.000000 0.000000 0.008696 0.00000 TUM 0.000000 0.000000 0.000000 0.996503 0.000000 0.003497 0.000000 0.000000 0.00000 ADI 0.004808 0.000000 0.000000 0.000000 0.990385 0.000000 0.004808 0.000000 0.00000 MUC 0.000000 0.000000 0.000000 0.000000 0.000000 0.988764 0.000000 0.011236 0.00000 MUS 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.996296 0.003704 0.00000 STR 0.000000 0.000000 0.004785 0.000000 0.000000 0.004785 0.004785 0.985646 0.00000 LYM 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.004310 0.99569 Predict patch labels for a whole slide# We now introduce IOPatchPredictorConfig, a class that specifies the configuration of image reading and prediction writing for the model prediction engine. This is required to inform the classifier which level of the WSI pyramid the classifier should read, process data and generate output. Parameters of IOPatchPredictorConfig are defined as: input_resolutions: A list, in the form of a dictionary, specifying the resolution of each input. List elements must be in the same order as in the target model.forward(). If your model accepts only one input, you just need to put one dictionary specifying \u0027units\u0027 and \u0027resolution\u0027. Note that TIAToolbox supports a model with more than one input. For more information on units and resolution, please see TIAToolbox documentation. patch_input_shape: Shape of the largest input in (height, width) format. stride_shape: The size of a stride (steps) between two consecutive patches, used in the patch extraction process. If the user sets stride_shape equal to patch_input_shape, patches will be extracted and processed without any overlap. wsi_ioconfig = IOPatchPredictorConfig( input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}], patch_input_shape=[224, 224], stride_shape=[224, 224], ) The predict method applies the CNN on the input patches and get the results. Here are the arguments and their descriptions: mode: Type of input to be processed. Choose from patch, tile or wsi according to your application. imgs: List of inputs, which should be a list of paths to the input tiles or WSIs. return_probabilities: Set to True to get per class probabilities alongside predicted labels of input patches. If you wish to merge the predictions to generate prediction maps for tile or wsi modes, you can set return_probabilities=True. ioconfig: set the IO configuration information using the IOPatchPredictorConfig class. resolution and unit (not shown below): These arguments specify the level or micron-per-pixel resolution of the WSI levels from which we plan to extract patches and can be used instead of ioconfig. Here we specify the WSI level as \u0027baseline\u0027, which is equivalent to level 0. In general, this is the level of greatest resolution. In this particular case, the image has only one level. More information can be found in the documentation. masks: A list of paths corresponding to the masks of WSIs in the imgs list. These masks specify the regions in the original WSIs from which we want to extract patches. If the mask of a particular WSI is specified as None, then the labels for all patches of that WSI (even background regions) would be predicted. This could cause unnecessary computation. merge_predictions: You can set this parameter to True if it\u2019s required to generate a 2D map of patch classification results. However, for large WSIs this will require large available memory. An alternative (default) solution is to set merge_predictions=False, and then generate the 2D prediction maps using the merge_predictions function as you will see later on. Since we are using a large WSI the patch extraction and prediction processes may take some time (make sure to set the ON_GPU=True if you have access to Cuda enabled GPU and PyTorch+Cuda). with suppress_console_output(): wsi_output = predictor.predict( imgs=[wsi_path], masks=None, mode=\"wsi\", merge_predictions=False, ioconfig=wsi_ioconfig, return_probabilities=True, save_dir=global_save_dir / \"wsi_predictions\", on_gpu=ON_GPU, ) We see how the prediction model works on our whole-slide images by visualizing the wsi_output. We first need to merge patch prediction outputs and then visualize them as an overlay on the original image. As before, the merge_predictions method is used to merge the patch predictions. Here we set the parameters resolution=1.25, units=\u0027power\u0027 to generate the prediction map at 1.25x magnification. If you would like to have higher/lower resolution (bigger/smaller) prediction maps, you need to change these parameters accordingly. When the predictions are merged, use the overlay_patch_prediction function to overlay the prediction map on the WSI thumbnail, which should be extracted at the resolution used for prediction merging. overview_resolution = ( 4 # the resolution in which we desire to merge and visualize the patch predictions ) # the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\" overview_unit = \"mpp\" wsi = WSIReader.open(wsi_path) wsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit) plt.figure(), plt.imshow(wsi_overview) plt.axis(\"off\") Overlaying the prediction map on this image as below gives: # Visualization of whole-slide image patch-level prediction # first set up a label to color mapping label_color_dict = {} label_color_dict[0] = (\"empty\", (0, 0, 0)) colors = cm.get_cmap(\"Set1\").colors for class_name, label in label_dict.items(): label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label])) pred_map = predictor.merge_predictions( wsi_path, wsi_output[0], resolution=overview_resolution, units=overview_unit, ) overlay = overlay_prediction_mask( wsi_overview, pred_map, alpha=0.5, label_info=label_color_dict, return_ax=True, ) plt.show() Feature extraction with a pathology-specific model# In this section, we will show how to extract features from a pretrained PyTorch model that exists outside TIAToolbox, using the WSI inference engines provided by TIAToolbox. To illustrate this we will use HistoEncoder, a computational-pathology specific model that has been trained in a self-supervised fashion to extract features from histology images. The model has been made available here: \u2018HistoEncoder: Foundation models for digital pathology\u2019 (jopo666/HistoEncoder) by Pohjonen, Joona and team at the University of Helsinki. We will plot a umap reduction into 3D (RGB) of the feature map to visualize how the features capture the differences between some of the above mentioned tissue types. # Import some extra modules import histoencoder.functional as F import torch.nn as nn from tiatoolbox.models.engine.semantic_segmentor import DeepFeatureExtractor, IOSegmentorConfig from tiatoolbox.models.models_abc import ModelABC import umap TIAToolbox defines a ModelABC which is a class inheriting PyTorch nn.Module and specifies how a model should look in order to be used in the TIAToolbox inference engines. The histoencoder model doesn\u2019t follow this structure, so we need to wrap it in a class whose output and methods are those that the TIAToolbox engine expects. class HistoEncWrapper(ModelABC): \"\"\"Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface.\"\"\" def __init__(self: HistoEncWrapper, encoder) -\u003e None: super().__init__() self.feat_extract = encoder def forward(self: HistoEncWrapper, imgs: torch.Tensor) -\u003e torch.Tensor: \"\"\"Pass input data through the model. Args: imgs (torch.Tensor): Model input. \"\"\" out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True) return out @staticmethod def infer_batch( model: nn.Module, batch_data: torch.Tensor, *, on_gpu: bool, ) -\u003e list[np.ndarray]: \"\"\"Run inference on an input batch. Contains logic for forward operation as well as i/o aggregation. Args: model (nn.Module): PyTorch defined model. batch_data (torch.Tensor): A batch of data generated by `torch.utils.data.DataLoader`. on_gpu (bool): Whether to run inference on a GPU. \"\"\" img_patches_device = batch_data.to(\u0027cuda\u0027) if on_gpu else batch_data model.eval() # Do not compute the gradient (not training) with torch.inference_mode(): output = model(img_patches_device) return [output.cpu().numpy()] Now that we have our wrapper, we will create our feature extraction model and instantiate a DeepFeatureExtractor to allow us to use this model over a WSI. We will use the same WSI as above, but this time we will extract features from the patches of the WSI using the HistoEncoder model, rather than predicting some label for each patch. # create the model encoder = F.create_encoder(\"prostate_medium\") model = HistoEncWrapper(encoder) # set the pre-processing function norm=transforms.Normalize(mean=[0.662, 0.446, 0.605],std=[0.169, 0.190, 0.155]) trans = [ transforms.ToTensor(), norm, ] model.preproc_func = transforms.Compose(trans) wsi_ioconfig = IOSegmentorConfig( input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}], patch_input_shape=[224, 224], output_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}], patch_output_shape=[224, 224], stride_shape=[224, 224], ) When we create the DeepFeatureExtractor, we will pass the auto_generate_mask=True argument. This will automatically create a mask of the tissue region using otsu thresholding, so that the extractor processes only those patches containing tissue. # create the feature extractor and run it on the WSI extractor = DeepFeatureExtractor(model=model, auto_generate_mask=True, batch_size=32, num_loader_workers=4, num_postproc_workers=4) with suppress_console_output(): out = extractor.predict(imgs=[wsi_path], mode=\"wsi\", ioconfig=wsi_ioconfig, save_dir=global_save_dir / \"wsi_features\",) These features could be used to train a downstream model, but here in order to get some intuition for what the features represent, we will use a UMAP reduction to visualize the features in RGB space. The points labeled in a similar color should have similar features, so we can check if the features naturally separate out into the different tissue regions when we overlay the UMAP reduction on the WSI thumbnail. We will plot it along with the patch-level prediction map from above to see how the features compare to the patch-level predictions in the following cells. # First we define a function to calculate the umap reduction def umap_reducer(x, dims=3, nns=10): \"\"\"UMAP reduction of the input data.\"\"\" reducer = umap.UMAP(n_neighbors=nns, n_components=dims, metric=\"manhattan\", spread=0.5, random_state=2) reduced = reducer.fit_transform(x) reduced -= reduced.min(axis=0) reduced /= reduced.max(axis=0) return reduced # load the features output by our feature extractor pos = np.load(global_save_dir / \"wsi_features\" / \"0.position.npy\") feats = np.load(global_save_dir / \"wsi_features\" / \"0.features.0.npy\") pos = pos / 8 # as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp # reduce the features into 3 dimensional (rgb) space reduced = umap_reducer(feats) # plot the prediction map the classifier again overlay = overlay_prediction_mask( wsi_overview, pred_map, alpha=0.5, label_info=label_color_dict, return_ax=True, ) # plot the feature map reduction plt.figure() plt.imshow(wsi_overview) plt.scatter(pos[:,0], pos[:,1], c=reduced, s=1, alpha=0.5) plt.axis(\"off\") plt.title(\"UMAP reduction of HistoEnc features\") plt.show() We see that the prediction map from our patch-level predictor, and the feature map from our self-supervised feature encoder, capture similar information about the tissue types in the WSI. This is a good sanity check that our models are working as expected. It also shows that the features extracted by the HistoEncoder model are capturing the differences between the tissue types, and so that they are encoding histologically relevant information. Where to Go From Here# In this notebook, we show how we can use the PatchPredictor and DeepFeatureExtractor classes and their predict method to predict the label, or extract features, for patches of big tiles and WSIs. We introduce merge_predictions and overlay_prediction_mask helper functions that merge the patch prediction outputs and visualize the resulting prediction map as an overlay on the input image/WSI. All the processes take place within TIAToolbox and we can easily put the pieces together, following our example code. Please make sure to set inputs and options correctly. We encourage you to further investigate the effect on the prediction output of changing predict function parameters. We have demonstrated how to use your own pretrained model or one provided by the research community for a specific task in the TIAToolbox framework to do inference on large WSIs even if the model structure is not defined in the TIAToolbox model class. You can learn more through the following resources: Advanced model handling with PyTorch and TIAToolbox Creating slide graphs for WSI with a custom PyTorch graph neural network",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/intermediate/tiatoolbox_tutorial.html"
         },
         "datePublished": "2023-01-01T00:00:00Z",
         "dateModified": "2023-01-01T00:00:00Z"
       }
   </script>
</body>
</body></html>