
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>torch.export Tutorial — PyTorch Tutorials 2.9.0+cu128 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=047068a3" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=c2809cec"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/torch_export_tutorial';</script>
<link href="https://docs.pytorch.org/tutorials/intermediate/torch_export_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../recipes/torch_export_aoti_python.html" rel="next" title="torch.export AOTInductor Tutorial for Python runtime (Beta)"/>
<link href="../recipes/regional_compilation.html" rel="prev" title="Reducing torch.compile cold start compilation time with regional compilation"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="tutorials" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.9.0+cu128');
</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->
<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "pytorch/tutorials",
    github_branch: "main",
    colab_repo: "pytorch/tutorials",
    colab_branch: ""
  };
</script>
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
<span class="dropdown-title">RAY</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
<span class="dropdown-title">Brand Guidelines</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started/locally">Get Started</a>
</li>
<li>
<a href="https://docs.pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.9.0+cu128</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_full_example.html"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> End-to-End Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiled_autograd_tutorial.html">Compiled Autograd: Capturing a larger backward graph for <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compiler_set_stance_tutorial.html">Dynamic Compilation Control with <code class="docutils literal notranslate"><span class="pre">torch.compiler.set_stance</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.export</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">torch.export Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">Export a PyTorch model to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/onnx_registry_tutorial.html">Extending the ONNX Exporter Operator Support</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">Export a model with control flow to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_conv_bn_fuser.html">Building a Convolution/Batch Norm fuser with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../compilers_index.html">Compilers</a></li>
<li aria-current="page" class="breadcrumb-item active">torch.export...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../compilers_index.html" itemprop="item"/>
<meta content="Compilers" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="torch.export Tutorial" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/torch_export_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-torch-export-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torch-export-tutorial">
<span id="sphx-glr-intermediate-torch-export-tutorial-py"></span><h1>torch.export Tutorial<a class="headerlink" href="#torch-export-tutorial" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Oct 02, 2023 | Last Updated: Jan 27, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and its related features are in prototype status and are subject to backwards compatibility
breaking changes. This tutorial provides a snapshot of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> usage as of PyTorch 2.5.</p>
</div>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> is the PyTorch 2.X way to export PyTorch models into
standardized model representations, intended
to be run on different (i.e. Python-less) environments. The official
documentation can be found <a class="reference external" href="https://pytorch.org/docs/main/export.html">here</a>.</p>
<p>In this tutorial, you will learn how to use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> to extract
<code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>’s (i.e. single-graph representations) from PyTorch programs.
We also detail some considerations/modifications that you may need
to make in order to make your model compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p><strong>Contents</strong></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#basic-usage" id="id2">Basic Usage</a></p></li>
<li><p><a class="reference internal" href="#graph-breaks" id="id3">Graph Breaks</a></p></li>
<li><p><a class="reference internal" href="#non-strict-export" id="id4">Non-Strict Export</a></p></li>
<li><p><a class="reference internal" href="#control-flow-ops" id="id5">Control Flow Ops</a></p></li>
<li><p><a class="reference internal" href="#constraints-dynamic-shapes" id="id6">Constraints/Dynamic Shapes</a></p>
<ul>
<li><p><a class="reference internal" href="#basic-concepts-symbols-and-guards" id="id7">Basic concepts: symbols and guards</a></p></li>
<li><p><a class="reference internal" href="#specialization" id="id8">0/1 specialization</a></p></li>
<li><p><a class="reference internal" href="#named-dims" id="id9">Named Dims</a></p></li>
<li><p><a class="reference internal" href="#constraint-violations-suggested-fixes" id="id10">Constraint violations, suggested fixes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-dependent-errors" id="id11">Data-dependent errors</a></p>
<ul>
<li><p><a class="reference internal" href="#guards-torch-check" id="id12">Guards, torch._check()</a></p></li>
<li><p><a class="reference internal" href="#specialized-values" id="id13">Specialized values</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#custom-ops" id="id14">Custom Ops</a></p></li>
<li><p><a class="reference internal" href="#ir-decompositions" id="id15">IR/Decompositions</a></p></li>
<li><p><a class="reference internal" href="#exportdb" id="id16">ExportDB</a></p></li>
<li><p><a class="reference internal" href="#running-the-exported-program" id="id17">Running the Exported Program</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id18">Conclusion</a></p></li>
</ul>
</nav>
<section id="basic-usage">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Basic Usage</a><a class="headerlink" href="#basic-usage" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> extracts single-graph representations from PyTorch programs
by tracing the target function, given example inputs.
<code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is the main entry point for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p>In this tutorial, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> are practically synonymous,
though <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> generally refers to the PyTorch 2.X export process, and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code>
generally refers to the actual function call.</p>
<p>The signature of <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> traces the tensor computation graph from calling <code class="docutils literal notranslate"><span class="pre">mod(*args,</span> <span class="pre">**kwargs)</span></code>
and wraps it in an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, which can be serialized or executed later with
different inputs. To execute the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> we can call <code class="docutils literal notranslate"><span class="pre">.module()</span></code>
on it to return a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> which is callable, just like the
original program.
We will detail the <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument later in the tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MyModule</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_mod</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class 'torch.export.exported_program.ExportedProgram'&gt;
tensor([[0.2226, 1.1915, 0.0000, 0.2458, 1.3452, 0.0000, 0.1659, 0.0000, 0.4901,
         0.3078],
        [1.1002, 0.1275, 0.0000, 0.0000, 1.1690, 1.1518, 0.2871, 0.0160, 0.0000,
         0.0000],
        [0.7178, 0.0000, 0.0000, 1.0897, 0.0000, 0.6323, 0.4352, 0.0000, 0.1490,
         2.1381],
        [0.0000, 0.0000, 2.0230, 0.3646, 0.6718, 0.0000, 0.0000, 0.9920, 0.4406,
         0.9623],
        [0.0000, 0.0000, 0.0000, 0.6759, 0.0000, 0.0000, 0.0000, 1.4586, 0.4583,
         0.0000],
        [0.1861, 0.0000, 0.2774, 1.1439, 0.0000, 1.8380, 0.0000, 1.3985, 0.0000,
         0.0000],
        [1.3288, 0.0604, 0.0639, 0.0000, 0.0000, 0.0000, 1.0302, 0.0000, 0.0000,
         0.0000],
        [0.5052, 0.0000, 0.0000, 0.8917, 0.0000, 0.5097, 1.0726, 0.0000, 0.0000,
         0.0000]], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
<p>Let’s review some attributes of <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> that are of interest.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph</span></code> attribute is an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.Graph">FX graph</a>
traced from the function we exported, that is, the computation graph of all PyTorch operations.
The FX graph is in “ATen IR” meaning that it contains only “ATen-level” operations.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph_signature</span></code> attribute gives a more detailed description of the
input and output nodes in the exported graph, describing which ones are
parameters, buffers, user inputs, or user outputs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code> attributes will be covered later.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_lin_weight: "f32[10, 100]", p_lin_bias: "f32[10]", x: "f32[8, 100]", y: "f32[8, 100]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            add: "f32[8, 100]" = torch.ops.aten.add.Tensor(x, y);  x = y = None

             # File: /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: "f32[8, 10]" = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias);  add = p_lin_weight = p_lin_bias = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            relu_: "f32[8, 10]" = torch.ops.aten.relu_.default(linear);  linear = None
            return (relu_,)

Graph signature:
    # inputs
    p_lin_weight: PARAMETER target='lin.weight'
    p_lin_bias: PARAMETER target='lin.bias'
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    relu_: USER_OUTPUT

Range constraints: {}
</pre></div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.export">documentation</a>
for more details.</p>
</section>
<section id="graph-breaks">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Graph Breaks</a><a class="headerlink" href="#graph-breaks" title="Link to this heading">#</a></h2>
<p>Although <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> shares components with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
the key limitation of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, especially when compared to
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, is that it does not support graph breaks. This is because
handling graph breaks involves interpreting the unsupported operation with
default Python evaluation, which is incompatible with the export use case.
Therefore, in order to make your model code compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>,
you will need to modify your code to remove graph breaks.</p>
<p>A graph break is necessary in cases such as:</p>
<ul class="simple">
<li><p>data-dependent control flow</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tb</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>def forward(self, arg0_1: "f32[3, 3]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
    sum_1: "f32[]" = torch.ops.aten.sum.default(arg0_1);  arg0_1 = None
    gt: "b8[]" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: "Sym(Eq(u0, 1))" = torch.ops.aten.item.default(ne);  ne = item = None




def forward(self, arg0_1: "f32[3, 3]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
    sum_1: "f32[]" = torch.ops.aten.sum.default(arg0_1);  arg0_1 = None
    gt: "b8[]" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: "Sym(Eq(u0, 1))" = torch.ops.aten.item.default(ne);  ne = item = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 122, in &lt;module&gt;
    export(Bad1(), (torch.randn(3, 3),))
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 311, in export
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
    export_artifact = export_func(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1793, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1922, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1706, in _make_fx_helper
    gm = make_fx(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2429, in wrapped
    return make_fx_tracer.trace(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2356, in trace
    return self._trace_inner(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2318, in _trace_inner
    t = dispatch_trace(
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1303, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1908, in trace
    res = super().trace(root, concrete_args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 868, in trace
    (self.create_arg(fn(*args)),),
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1361, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File "&lt;string&gt;", line 1, in &lt;lambda&gt;
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1593, in wrapped_fn
    return tuple(flat_fn(*args))
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 187, in flat_fn
    tree_out = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py", line 1354, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1906, in forward
    tree_out = mod(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 116, in forward
    if x.sum() &gt; 0:
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1409, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1479, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 1066, in __torch_function__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 538, in guard_bool
    r = self.evaluate()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 512, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7233, in evaluate_sym_node
    return self.evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7333, in evaluate_expr
    return self._inner_evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7356, in _inner_evaluate_expr
    return self._evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7574, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)

consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 116, in forward
    if x.sum() &gt; 0:


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<ul class="simple">
<li><p>accessing tensor data with <code class="docutils literal notranslate"><span class="pre">.data</span></code></p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad2</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span><span class="o">.</span><span class="n">data</span></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad2</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>calling unsupported functions (such as many built-in functions)</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="non-strict-export">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Non-Strict Export</a><a class="headerlink" href="#non-strict-export" title="Link to this heading">#</a></h2>
<p>To trace the program, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> uses TorchDynamo by default, a byte
code analysis engine, to symbolically analyze the Python code and build a
graph based on the results. This analysis allows <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> to provide
stronger guarantees about safety, but not all Python code is supported,
causing these graph breaks.</p>
<p>To address this issue, in PyTorch 2.3, we introduced a new mode of
exporting called non-strict mode, where we trace through the program using the
Python interpreter executing it exactly as it would in eager mode, allowing us
to skip over unsupported Python features. This is done through adding a
<code class="docutils literal notranslate"><span class="pre">strict=False</span></code> flag.</p>
<p>Looking at some of the previous examples which resulted in graph breaks:</p>
<ul class="simple">
<li><p>Calling unsupported functions (such as many built-in functions) traces</p></li>
</ul>
<p>through, but in this case, <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> gets specialized as a constant integer in
the graph. This is because <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> is not a tensor operation, so the
operation is not recorded in the graph.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">bad3_nonstrict</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1
            add: "f32[3, 3]" = torch.ops.aten.add.Tensor(x, 1);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x)
            add_1: "f32[3, 3]" = torch.ops.aten.add.Tensor(add, 140334371813264);  add = None
            return (add_1,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    add_1: USER_OUTPUT

Range constraints: {}

tensor([[1.4033e+14, 1.4033e+14, 1.4033e+14],
        [1.4033e+14, 1.4033e+14, 1.4033e+14],
        [1.4033e+14, 1.4033e+14, 1.4033e+14]])
</pre></div>
</div>
<p>However, there are still some features that require rewrites to the original
module:</p>
</section>
<section id="control-flow-ops">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Control Flow Ops</a><a class="headerlink" href="#control-flow-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> actually does support data-dependent control flow.
But these need to be expressed using control flow ops. For example,
we can fix the control flow example above using the <code class="docutils literal notranslate"><span class="pre">cond</span></code> op, like so:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1Fixed</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">true_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">false_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cond.html#torch.cond" title="torch.cond"><span class="n">torch</span><span class="o">.</span><span class="n">cond</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1Fixed</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">-</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() &gt; 0, true_fn, false_fn, [x])
            sum_1: "f32[]" = torch.ops.aten.sum.default(x)
            gt: "b8[]" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None

             # File: &lt;eval_with_key&gt;.35:9 in forward, code: cond = torch.ops.higher_order.cond(l_args_0_, cond_true_0, cond_false_0, (l_args_3_0_,));  l_args_0_ = cond_true_0 = cond_false_0 = l_args_3_0_ = None
            true_graph_0 = self.true_graph_0
            false_graph_0 = self.false_graph_0
            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x,));  gt = true_graph_0 = false_graph_0 = x = None
            getitem: "f32[3, 3]" = cond[0];  cond = None
            return (getitem,)

        class true_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: &lt;eval_with_key&gt;.32:6 in forward, code: sin = torch.sin(l_args_3_0__1);  l_args_3_0__1 = None
                sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None
                return (sin,)

        class false_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: &lt;eval_with_key&gt;.33:6 in forward, code: cos = torch.cos(l_args_3_0__1);  l_args_3_0__1 = None
                cos: "f32[3, 3]" = torch.ops.aten.cos.default(x);  x = None
                return (cos,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    getitem: USER_OUTPUT

Range constraints: {}

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]])
tensor([[0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403]])
</pre></div>
</div>
<p>There are limitations to <code class="docutils literal notranslate"><span class="pre">cond</span></code> that one should be aware of:</p>
<ul class="simple">
<li><p>The predicate (i.e. <code class="docutils literal notranslate"><span class="pre">x.sum()</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>) must result in a boolean or a single-element tensor.</p></li>
<li><p>The operands (i.e. <code class="docutils literal notranslate"><span class="pre">[x]</span></code>) must be tensors.</p></li>
<li><p>The branch function (i.e. <code class="docutils literal notranslate"><span class="pre">true_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">false_fn</span></code>) signature must match with the
operands and they must both return a single tensor with the same metadata (for example, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code>, etc.).</p></li>
<li><p>Branch functions cannot mutate input or global variables.</p></li>
<li><p>Branch functions cannot access closure variables, except for <code class="docutils literal notranslate"><span class="pre">self</span></code> if the function is
defined in the scope of a method.</p></li>
</ul>
<p>For more details about <code class="docutils literal notranslate"><span class="pre">cond</span></code>, check out the <a class="reference external" href="https://pytorch.org/docs/main/cond.html">cond documentation</a>.</p>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">map</span></code>, which applies a function across the first dimension
of the first tensor argument.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch._higher_order_ops.map</span><span class="w"> </span><span class="kn">import</span> <span class="nb">map</span> <span class="k">as</span> <span class="n">torch_map</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MapModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>

        <span class="k">return</span> <span class="n">torch_map</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MapModule</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_map_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">*</span><span class="n">inps</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, xs: "f32[6, 4]", y: "i64[]", z: "i64[]"):
             # File: &lt;eval_with_key&gt;.64:9 in forward, code: map_impl = torch.ops.higher_order.map_impl(map_body_0, [l_flat_xs_0_], [l_flat_args_0_, l_flat_args_1_]);  map_body_0 = l_flat_xs_0_ = l_flat_args_0_ = l_flat_args_1_ = None
            body_graph_0 = self.body_graph_0
            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]);  body_graph_0 = xs = y = z = None
            getitem: "f32[6, 4]" = map_impl[0];  map_impl = None
            return (getitem,)

        class body_graph_0(torch.nn.Module):
            def forward(self, xs: "f32[4]", y: "i64[]", z: "i64[]"):
                 # File: &lt;eval_with_key&gt;.62:5 in forward, code: add = child + l_flat_args_0_;  child = l_flat_args_0_ = None
                add: "f32[4]" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None

                 # File: &lt;eval_with_key&gt;.62:6 in forward, code: add_1 = add + l_flat_args_1_;  add = l_flat_args_1_ = None
                add_1: "f32[4]" = torch.ops.aten.add.Tensor(add, z);  add = z = None
                return (add_1,)

Graph signature:
    # inputs
    xs: USER_INPUT
    y: USER_INPUT
    z: USER_INPUT

    # outputs
    getitem: USER_OUTPUT

Range constraints: {}

tensor([[10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.]])
</pre></div>
</div>
<p>Other control flow ops include <code class="docutils literal notranslate"><span class="pre">while_loop</span></code>, <code class="docutils literal notranslate"><span class="pre">associative_scan</span></code>, and
<code class="docutils literal notranslate"><span class="pre">scan</span></code>. For more documentation on each operator, please refer to
<a class="reference external" href="https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops">this page</a>.</p>
</section>
<section id="constraints-dynamic-shapes">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Constraints/Dynamic Shapes</a><a class="headerlink" href="#constraints-dynamic-shapes" title="Link to this heading">#</a></h2>
<p>This section covers dynamic behavior and representation of exported programs. Dynamic behavior is
subjective to the particular model being exported, so for the most part of this tutorial, we’ll focus
on this particular toy model (with the resulting tensor shapes annotated):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [6, 5]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [8, 4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [32]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># [8, 4]</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># [6, 3]</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># [32]</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># [32]</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a static program. One consequence of this is that at runtime,
the program won’t work on inputs with different shapes, even if they’re valid in eager mode.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">))</span>
<span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 286, in &lt;module&gt;
    ep.module()(w, x, torch.randn(3, 4), torch.randn(12))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 413, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 400, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1881, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1829, in inner
    result = forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.95", line 8, in forward
    _guards_fn = self._guards_fn(w, x, y, z);  _guards_fn = None
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 209, in inner
    return func(*args, **kwargs)
  File "&lt;string&gt;", line 6, in _
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 2185, in _assert
    assert condition, message
AssertionError: Guard failed: y.size()[0] == 8
</pre></div>
</div>
<section id="basic-concepts-symbols-and-guards">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Basic concepts: symbols and guards</a><a class="headerlink" href="#basic-concepts-symbols-and-guards" title="Link to this heading">#</a></h3>
<p>To enable dynamism, <code class="docutils literal notranslate"><span class="pre">export()</span></code> provides a <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument. The easiest way to work with
dynamic shapes is using <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> and looking at the program that’s returned. Dynamic behavior is specified
at a input dimension-level; for each input we can specify a tuple of values:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export.dynamic_shapes</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<p>Before we look at the program that’s produced, let’s understand what specifying <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> entails,
and how that interacts with export. For every input dimension where a <code class="docutils literal notranslate"><span class="pre">Dim</span></code> object is specified, a symbol is
<a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">allocated</a>,
taking on a range of <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">inf]</span></code> (why not <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">inf]</span></code> or <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">inf]</span></code>? we’ll explain later in the
0/1 specialization section).</p>
<p>Export then runs model tracing, looking at each operation that’s performed by the model. Each individual operation can emit
what’s called “guards”; basically boolean condition that are required to be true for the program to be valid.
When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid;
i.e. the program’s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards
and producing a final program representation that adheres to all of these guards. Before we see this “final representation” in
an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, let’s look at the guards emitted by the toy model we’re tracing.</p>
<p>Here, each forward input tensor is annotated with the symbol allocated at the start of tracing:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s0, s1]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s2]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s3, s4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s5]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># guard: s2 == s4</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># guard: s1 == 5</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># no guard added here</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># guard: s3 * s4 == s5</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>Let’s understand each of the operations and the emitted guards:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>: This is an element-wise add with broadcasting, since <code class="docutils literal notranslate"><span class="pre">x</span></code> is a 1-d tensor and <code class="docutils literal notranslate"><span class="pre">y</span></code> a 2-d tensor. <code class="docutils literal notranslate"><span class="pre">x</span></code> is broadcasted along the last dimension of <code class="docutils literal notranslate"><span class="pre">y</span></code>, emitting the guard <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">==</span> <span class="pre">s4</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x1</span> <span class="pre">=</span> <span class="pre">self.l(w)</span></code>: Calling <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (<code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">s1]</span></code>), and a statically-shaped tensor. This emits the guard <code class="docutils literal notranslate"><span class="pre">s1</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x2</span> <span class="pre">=</span> <span class="pre">x0.flatten()</span></code>: This call actually doesn’t emit any guards! (at least none relevant to input shapes)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x3</span> <span class="pre">=</span> <span class="pre">x2</span> <span class="pre">+</span> <span class="pre">z</span></code>: <code class="docutils literal notranslate"><span class="pre">x2</span></code> has shape <code class="docutils literal notranslate"><span class="pre">[s3*s4]</span></code> after flattening, and this element-wise add emits <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">*</span> <span class="pre">s4</span> <span class="pre">==</span> <span class="pre">s5</span></code>.</p></li>
</ul>
<p>Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes
subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">5]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x:</span> <span class="pre">[s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y:</span> <span class="pre">[s3,</span> <span class="pre">s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z:</span> <span class="pre">[s2*s3]</span></code></p></li>
</ul>
<p>And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the
corresponding inputs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_l_weight: "f32[3, 5]", p_l_bias: "f32[3]", w: "f32[s15, 5]", x: "f32[s77]", y: "f32[s17, s77]", z: "f32[s17*s77]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y  # [8, 4]
            add: "f32[s17, s77]" = torch.ops.aten.add.Tensor(x, y);  x = y = None

             # File: /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: "f32[s15, 3]" = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias);  w = p_l_weight = p_l_bias = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten()  # [32]
            flatten: "f32[s17*s77]" = torch.ops.aten.flatten.using_ints(add);  add = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z  # [32]
            add_1: "f32[s17*s77]" = torch.ops.aten.add.Tensor(flatten, z);  flatten = z = None
            return (linear, add_1)

Graph signature:
    # inputs
    p_l_weight: PARAMETER target='l.weight'
    p_l_bias: PARAMETER target='l.bias'
    w: USER_INPUT
    x: USER_INPUT
    y: USER_INPUT
    z: USER_INPUT

    # outputs
    linear: USER_OUTPUT
    add_1: USER_OUTPUT

Range constraints: {s15: VR[2, int_oo], s77: VR[2, int_oo], s17: VR[2, int_oo], s17*s77: VR[4, int_oo]}
</pre></div>
</div>
<p>Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn’t
so interesting currently, since this export call doesn’t emit any guards related to symbol bounds and each base symbol has
a generic bound, but this will come up later.</p>
<p>So far, because we’ve been exporting this toy model, this experience has not been representative of how hard
it typically is to debug dynamic shapes guards &amp; issues. In most cases it isn’t obvious what guards are being emitted,
and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards
are rather intuitive.</p>
<p>In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment
variable <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS="+dynamic"</span></code>, or interactively with <code class="docutils literal notranslate"><span class="pre">torch._logging.set_logs(dynamic=10)</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-_logging sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs" title="torch._logging.set_logs"><span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span></a><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:26.945000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:26.947000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L['w'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s15" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:26.948000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L['w'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s21" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:26.950000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L['x'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s77" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:26.951000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L['y'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s17" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:26.952000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L['y'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s94" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:26.954000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L['z'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s68" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0108 21:26:26.960000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.960000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.961000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.962000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.963000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.964000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.965000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.966000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.967000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.967000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:26.970000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s77, s94)"
I0108 21:26:26.971000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[2, int_oo]
V0108 21:26:26.972000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:26.979000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s21, 5)"
V0108 21:26:26.979000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update)
I0108 21:26:26.980000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V0108 21:26:26.992000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:26.995000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:26.997000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s17*s77, s68)"
V0108 21:26:26.998000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[4, int_oo] (update)
I0108 21:26:26.999000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[4, int_oo]
I0108 21:26:27.004000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[0] s15 None
V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[1] 5 None
V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[0] 5 None
V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[1] 1 None
V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].storage_offset() 0 None
V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] s77 None
V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 1 None
V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] s17 None
V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[1] s77 None
V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] s77 None
V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[1] 1 None
V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].size()[0] s17*s77 None
V0108 21:26:27.009000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].stride()[0] 1 None
V0108 21:26:27.009000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].storage_offset() 0 None
V0108 21:26:27.021000 21876 torch/fx/experimental/symbolic_shapes.py:7471] eval 5 [trivial]
</pre></div>
</div>
<p>This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end
to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above;
e.g. the allocation of symbols:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">runtime_assert True == True [statically known]</span>
<span class="sd">create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>"\ncreate_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\nruntime_assert True == True [statically known]\ncreate_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\n"
</pre></div>
</div>
<p>The lines with <cite>create_symbol</cite> show when a new symbol has been allocated, and the logs also identify the tensor variable names
and dimensions they’ve been allocated for. In other lines we can also see the guards emitted:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"</span>
<span class="sd">runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"</span>
<span class="sd">runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>'\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"\n'
</pre></div>
</div>
<p>Next to the <code class="docutils literal notranslate"><span class="pre">[guard</span> <span class="pre">added]</span></code> messages, we also see the responsible user lines of code - luckily here the model is simple enough.
In many real-world cases it’s not so straightforward: high-level torch operations can have complicated fake-kernel implementations
or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate
is to follow the logs’ suggestion, and re-run with environment variable <code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="..."</span></code>, to further
attribute the guard of interest.</p>
<p><code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> is just one of the available options for interacting with <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code>; as of writing this 2 other options are available:
<code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code> simply marks a dimension static, while <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code> is similar to <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> in all
ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a
static guard is emitted on a dynamically-marked dimension:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.025000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.027000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L['w'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s15" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.027000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L['w'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s21" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.029000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L['x'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s77" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.031000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L['y'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s17" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.031000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L['y'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s94" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.033000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L['z'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s68" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0108 21:26:27.039000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.040000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.040000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.042000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.042000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.044000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.044000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.045000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.046000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.047000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.049000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s77, s94)"
I0108 21:26:27.050000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[2, int_oo]
V0108 21:26:27.052000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.058000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s21, 5)"
V0108 21:26:27.058000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update)
I0108 21:26:27.059000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V0108 21:26:27.071000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.074000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.076000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s17*s77, s68)"
V0108 21:26:27.077000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[4, int_oo] (update)
I0108 21:26:27.078000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[4, int_oo]
I0108 21:26:27.083000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.083000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[0] s15 None
V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[1] 5 RelaxedUnspecConstraint(warn_only=False)
V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[0] 5 None
V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[1] 1 None
V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].storage_offset() 0 None
V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] s77 None
V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 1 None
V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] s17 None
V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[1] s77 None
V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] s77 None
V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[1] 1 None
V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].size()[0] s17*s77 None
V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].stride()[0] 1 None
V0108 21:26:27.088000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].storage_offset() 0 None
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1821, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1968, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 533, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 500, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5204, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=("python",))[0].exprs
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5928, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - You marked L['w'].size()[1] as dynamic but your code specialized it to be a constant (5). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 311, in export
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
    export_artifact = export_func(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1823, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - You marked L['w'].size()[1] as dynamic but your code specialized it to be a constant (5). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>Static guards also aren’t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape
specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is
raised when this is the case for <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"x"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">,)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"y"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.098000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.100000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L['w'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s15" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.100000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L['w'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s21" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.103000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L['y'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s17" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.103000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L['y'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s94" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.105000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L['z'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s68" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0108 21:26:27.111000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.112000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.113000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.115000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.115000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.116000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.117000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.118000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.123000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s94, 4) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s94, 4)"
V0108 21:26:27.124000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 4] (update)
I0108 21:26:27.124000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = 4 (range_refined_to_singleton) VR[4, 4]
I0108 21:26:27.132000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s21, 5)"
V0108 21:26:27.132000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update)
I0108 21:26:27.133000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
I0108 21:26:27.153000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(4*s17, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(4*s17, s68)"
V0108 21:26:27.154000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[8, int_oo] (update)
I0108 21:26:27.158000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = 4*s17 (solve) VR[8, int_oo]
I0108 21:26:27.163000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.163000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[0] s15 None
V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[1] 5 None
V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[0] 5 None
V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[1] 1 None
V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].storage_offset() 0 None
V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] 4 None
V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 1 None
V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] s17 None
V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[1] 4 RelaxedUnspecConstraint(warn_only=False)
V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 4 None
V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[1] 1 None
V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].size()[0] 4*s17 None
V0108 21:26:27.168000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].stride()[0] 1 None
V0108 21:26:27.168000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].storage_offset() 0 None
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1821, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1968, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 533, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 500, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5204, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=("python",))[0].exprs
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5928, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - You marked L['y'].size()[1] as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 311, in export
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
    export_artifact = export_func(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1823, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - You marked L['y'].size()[1] as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>Here you might ask why export “specializes”, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because
of the symbolic shapes system described above, of symbols and guards. When <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is marked static, we don’t allocate a symbol, and compile
treating this shape as a concrete integer 4. A symbol is allocated for <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>, and so we finally emit the guard <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">==</span> <span class="pre">4</span></code>, leading to
specialization.</p>
<p>One feature of export is that during tracing, statements like asserts, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code>, and <code class="docutils literal notranslate"><span class="pre">if/else</span></code> conditions will also emit guards.
See what happens when we augment the existing model with such statements:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">assert</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">512</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
            <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>
            <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.177000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.179000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L['w'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s15" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.179000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L['w'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s21" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.181000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L['x'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s77" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.182000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L['y'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s17" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.183000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L['y'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s94" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.185000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L['z'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s68" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0108 21:26:27.191000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.192000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.192000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.193000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.194000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.195000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.196000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.197000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.198000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.198000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.204000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval s15 &lt;= 512 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:450 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s15 &lt;= 512"
V0108 21:26:27.205000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s15 = VR[2, 512] (update)
I0108 21:26:27.208000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval s77 &gt;= 4 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:451 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s77 &gt;= 4"
V0108 21:26:27.209000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s77 = VR[4, int_oo] (update)
I0108 21:26:27.213000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s15, s77 + 2) [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:452 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s15, s77 + 2)"
V0108 21:26:27.214000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s15 = VR[6, 512] (update)
V0108 21:26:27.217000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s77 = VR[4, 510] (update)
I0108 21:26:27.218000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s15 = s77 + 2 (solve) VR[6, 512]
I0108 21:26:27.222000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s77, s94)"
V0108 21:26:27.223000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 510] (update)
I0108 21:26:27.223000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[4, 510]
V0108 21:26:27.227000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.234000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s21, 5)"
V0108 21:26:27.235000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update)
I0108 21:26:27.235000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V0108 21:26:27.250000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.253000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.262000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s17*s77, s68)"
V0108 21:26:27.263000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[8, int_oo] (update)
I0108 21:26:27.264000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[8, int_oo]
I0108 21:26:27.269000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.270000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[0] s77 + 2 None
V0108 21:26:27.270000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].size()[1] 5 None
V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[0] 5 None
V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].stride()[1] 1 None
V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['w'].storage_offset() 0 None
V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] s77 None
V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 1 None
V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] s17 None
V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[1] s77 None
V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] s77 None
V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[1] 1 None
V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].size()[0] s17*s77 None
V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].stride()[0] 1 None
V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['z'].storage_offset() 0 None
V0108 21:26:27.290000 21876 torch/fx/experimental/symbolic_shapes.py:7471] eval 5 [trivial]
</pre></div>
</div>
<p>Each of these statements emits an additional guard, and the exported program shows the changes; <code class="docutils literal notranslate"><span class="pre">s0</span></code> is eliminated in favor of <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">+</span> <span class="pre">2</span></code>,
and <code class="docutils literal notranslate"><span class="pre">s2</span></code> now contains lower and upper bounds, reflected in <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code>.</p>
<p>For the if/else condition, you might ask why the True branch was taken, and why it wasn’t the <code class="docutils literal notranslate"><span class="pre">w.shape[0]</span> <span class="pre">!=</span> <span class="pre">x.shape[0]</span> <span class="pre">+</span> <span class="pre">2</span></code> guard that
got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken.
If different sample input shapes were provided that fail the <code class="docutils literal notranslate"><span class="pre">if</span></code> condition, export would trace and emit guards corresponding to the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch.
Additionally, you might ask why we traced only the <code class="docutils literal notranslate"><span class="pre">if</span></code> branch, and if it’s possible to maintain control-flow in your program and keep both branches
alive. For that, refer to rewriting your model code following the <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Flow</span> <span class="pre">Ops</span></code> section above.</p>
</section>
<section id="specialization">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">0/1 specialization</a><a class="headerlink" href="#specialization" title="Link to this heading">#</a></h3>
<p>Since we’re talking about guards and specializations, it’s a good time to talk about the 0/1 specialization issue we brought up earlier.
The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that
don’t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 … . This just means that you should
specify 0/1 sample inputs when you’d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens
at runtime when we export this linear layer:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),),</span>
    <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"input"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.295000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['input'].size()[0] 1 None
V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['input'].size()[1] 4 None
V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['input'].stride()[0] 4 None
V0108 21:26:27.309000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['input'].stride()[1] 1 None
V0108 21:26:27.309000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['input'].storage_offset() 0 None
W0108 21:26:27.311000 21876 torch/_export/non_strict_utils.py:564] dimension inputs['input'].shape[0] 0/1 specialized; Dim.AUTO was specified along with a sample input with hint = 1.
Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 500, in &lt;module&gt;
    ep.module()(torch.randn(2, 4))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 413, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 400, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1881, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1829, in inner
    result = forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.125", line 9, in forward
    _guards_fn = self._guards_fn(input_1);  _guards_fn = None
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 209, in inner
    return func(*args, **kwargs)
  File "&lt;string&gt;", line 3, in _
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 2185, in _assert
    assert condition, message
AssertionError: Guard failed: input.size()[0] == 1
</pre></div>
</div>
</section>
<section id="named-dims">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Named Dims</a><a class="headerlink" href="#named-dims" title="Link to this heading">#</a></h3>
<p>So far we’ve only been talking about 3 ways to specify dynamic shapes: <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code>, <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. The attraction of these is the
low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic
dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a “discovery” process, summarizing these guards
and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or
beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at
all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels.
These changes won’t be detected and the <code class="docutils literal notranslate"><span class="pre">export()</span></code> call will most likely succeed, unless tests are in place that check the resulting <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> representation.</p>
<p>For such cases, our stance is to recommend the “traditional” way of specifying dynamic shapes, which longer-term users of export might be familiar with: named <code class="docutils literal notranslate"><span class="pre">Dims</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dh</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dh"</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dh</span></a><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the
dynamic behavior of the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> produced; <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic
specifications given. For example, in the above specification, the following is asserted:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is to have range <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">256]</span></code>, and related to <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span></code> by <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span> <span class="pre">==</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">x.shape[0]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[1]</span></code> is static.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code> has range <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">512]</span></code>, and is unrelated to any other dimension.</p></li>
</ul>
<p>In this design, we allow relations between dimensions to be specified with univariate linear expressions: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">dim</span> <span class="pre">+</span> <span class="pre">B</span></code> can be specified for any dimension. This allows users
to specify more complex constraints like integer divisibility for dynamic dimensions:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># x.shape[0] has range [16, 2048], and is divisible by 4.</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="constraint-violations-suggested-fixes">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Constraint violations, suggested fixes</a><a class="headerlink" href="#constraint-violations-suggested-fixes" title="Link to this heading">#</a></h3>
<p>One common issue with this specification style (before <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> was introduced), is that the specification would often be mismatched with what was produced by model tracing.
That would lead to <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors and export suggested fixes - see for example with this model &amp; specification, where the model inherently requires equality between
dimensions 0 of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and requires dimension 1 to be static.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dy</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">d1</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">dims</span><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="s2">"dy"</span><span class="p">,</span> <span class="s2">"d1"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span>
        <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dx</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">d1</span></a><span class="p">),</span>
            <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">dy</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">d1</span></a><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.320000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.321000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 6 for L['x'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s77" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.323000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s27 = 4 for L['x'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s27" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.326000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 6 for L['y'].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s17" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0108 21:26:27.327000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L['y'].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s94" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0108 21:26:27.333000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.334000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.335000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.337000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.338000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
V0108 21:26:27.338000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.342000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s27, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s27, s94)"
I0108 21:26:27.343000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s27 (solve) VR[2, int_oo]
I0108 21:26:27.345000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s17) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s77, s17)"
I0108 21:26:27.346000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s77 = s17 (solve) VR[2, int_oo]
V0108 21:26:27.349000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known]
I0108 21:26:27.358000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s27, 4) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s27, 4)"
V0108 21:26:27.359000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s27 = VR[4, 4] (update)
I0108 21:26:27.360000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s27 = 4 (range_refined_to_singleton) VR[4, 4]
I0108 21:26:27.365000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 4] (update)
I0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = 4 (find) VR[4, 4]
V0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 4 None
V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[1] 1 None
V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 4 None
V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[1] 1 None
V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1821, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1968, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 533, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 500, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5204, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=("python",))[0].exprs
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5928, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.
Suggested fixes:
  d1 = 4
  dy = dx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 557, in &lt;module&gt;
    ep = export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 311, in export
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
    export_artifact = export_func(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1823, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.
Suggested fixes:
  d1 = 4
  dy = dx

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards.</p>
<p>Lastly, there’s couple nice-to-knows about the options for specification:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> is a good option for static behavior:
- <code class="docutils literal notranslate"><span class="pre">dynamic_shapes=None</span></code> (default) exports with the entire model being static.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at a dimension-level specializes that dimension, though this is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>.</p></li>
<li><p>specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification.</p></li>
</ul>
<p>These options are combined in the inputs &amp; dynamic shapes spec below:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="mi">16</span><span class="p">,</span>
    <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"tensor_0"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"tensor_1"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"int_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"bool_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="data-dependent-errors">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Data-dependent errors</a><a class="headerlink" href="#data-dependent-errors" title="Link to this heading">#</a></h2>
<p>While trying to export models, you have may have encountered errors like “Could not guard on data-dependent expression”, or Could not extract specialized integer from data-dependent expression”.
These errors exist because <code class="docutils literal notranslate"><span class="pre">torch.export()</span></code> compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties
(e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be
unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out,
complaining that the value is not available.</p>
<p>Data-dependent values appear in many places, and common sources are calls like <code class="docutils literal notranslate"><span class="pre">item()</span></code>, <code class="docutils literal notranslate"><span class="pre">tolist()</span></code>, or <code class="docutils literal notranslate"><span class="pre">torch.unbind()</span></code> that extract scalar values from tensors.
How are these values represented in the exported program? In the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a>
section, we talked about allocating symbols to represent dynamic input dimensions.
The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are “unbacked” symbols,
in contrast to the “backed” symbols allocated for input dimensions. The <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">“backed/unbacked”</a>
nomenclature refers to the presence/absence of a “hint” for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed.</p>
<p>In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties.
For data-dependent values, the symbols are taken from FakeTensor “data” during tracing, and so the compiler doesn’t know the actual values (hints) that these symbols would take on.</p>
<p>Let’s see how these show up in exported programs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.377000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.384000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.384000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
I0108 21:26:27.388000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.388000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u1]
I0108 21:26:27.389000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u2 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.389000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u2]
I0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] 2 None
V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 1 None
V0108 21:26:27.393000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "i64[2]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist()
            unbind = torch.ops.aten.unbind.int(y);  y = None
            getitem: "i64[]" = unbind[0]
            getitem_1: "i64[]" = unbind[1];  unbind = None
            item_1: "Sym(u1)" = torch.ops.aten.item.default(getitem);  getitem = None
            item_2: "Sym(u2)" = torch.ops.aten.item.default(getitem_1);  getitem_1 = None
            return (item_1, item_2, item)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    item_1: USER_OUTPUT
    item_2: USER_OUTPUT
    item: USER_OUTPUT

Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo]}
</pre></div>
</div>
<p>The result is that 3 unbacked symbols (notice they’re prefixed with “u”, instead of the usual “s” for input shape/backed symbols) are allocated and returned:
1 for the <code class="docutils literal notranslate"><span class="pre">item()</span></code> call, and 1 for each of the elements of <code class="docutils literal notranslate"><span class="pre">y</span></code> with the <code class="docutils literal notranslate"><span class="pre">tolist()</span></code> call.
Note from the range constraints field that these take on ranges of <code class="docutils literal notranslate"><span class="pre">[-int_oo,</span> <span class="pre">int_oo]</span></code>, not the default <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">int_oo]</span></code> range allocated to input shape symbols,
since we have no information on what these values are - they don’t represent sizes, so don’t necessarily have positive values.</p>
<section id="guards-torch-check">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Guards, torch._check()</a><a class="headerlink" href="#guards-torch-check" title="Link to this heading">#</a></h3>
<p>But the case above is easy to export, because the concrete values of these symbols aren’t used in any compiler decision-making; all that’s relevant is that the return values are unbacked symbols.
The data-dependent errors highlighted in this section are cases like the following, where <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#control-flow-static-vs-dynamic">data-dependent guards</a> are encountered:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Here we actually need the “hint”, or the concrete value of <code class="docutils literal notranslate"><span class="pre">a</span></code> for the compiler to decide whether to trace <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> or <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code> as the output.
Because we trace with FakeTensors, we don’t know what <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span></code> actually evaluates to, and export errors out with “Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span> <span class="pre">(unhinted)</span></code>”.</p>
<p>So how do we export this toy model? Unlike <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, export requires full graph compilation, and we can’t just graph break on this. Here are some basic options:</p>
<ol class="arabic simple">
<li><p>Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using <code class="docutils literal notranslate"><span class="pre">torch.compiler.is_compiling()</span></code> to guard what’s traced at compile-time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code>: we could rewrite the control-flow code to use <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> so we don’t specialize on a branch.</p></li>
</ol>
<p>While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> is not a comprehensive system for handling data-dependent errors.
As we will see, there are data-dependent errors that do not involve control-flow.</p>
<p>The generally recommended approach is to start with <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols.
While a <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true,
are stored as symbol properties (provided it’s smart enough to infer those properties). So even if unbacked symbols don’t have hints, if we’re able to communicate properties that are generally true for these symbols via
<code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls, we can potentially bypass data-dependent guards without rewriting the offending model code.</p>
<p>For example in the model above, inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&gt;=</span> <span class="pre">10)</span></code> would tell the compiler that <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> can always be returned, and <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">4)</span></code> tells it to return <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code>.
See what happens when we re-export this model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.399000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.405000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.405000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
I0108 21:26:27.407000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 &gt;= 10 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:673 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 10"
V0108 21:26:27.408000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[10, int_oo] (update)
I0108 21:26:27.412000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 &lt;= 60 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:674 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt;= 60"
V0108 21:26:27.413000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[10, 60] (update)
V0108 21:26:27.418000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known]
I0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] 4 None
V0108 21:26:27.423000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 1 None
V0108 21:26:27.423000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.425000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &gt;= 10 == True [statically known]
V0108 21:26:27.426000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &lt;= 60 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[4]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_2: "Sym(u0 &gt;= 10)" = item &gt;= 10
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_2, "Runtime assertion failed for expression u0 &gt;= 10 on node 'ge_2'");  ge_2 = _assert_scalar_default = None
            le_1: "Sym(u0 &lt;= 60)" = item &lt;= 60;  item = None
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u0 &lt;= 60 on node 'le_1'");  le_1 = _assert_scalar_default_1 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2
            add: "f32[4]" = torch.ops.aten.add.Tensor(y, 2);  y = None
            return (add,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    add: USER_OUTPUT

Range constraints: {u0: VR[10, 60]}
</pre></div>
</div>
<p>Export succeeds, and note from the range constraints field that <code class="docutils literal notranslate"><span class="pre">u0</span></code> takes on a range of <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">60]</span></code>.</p>
<p>So what information do <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true:</p>
<ol class="arabic simple">
<li><p>Equality with non-data-dependent expressions: <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls that communicate equalities like <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">s0</span> <span class="pre">+</span> <span class="pre">4</span></code> or <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p>Range refinement: calls that provide lower or upper bounds for symbols, like the above.</p></li>
<li><p>Some basic reasoning around more complicated expressions: inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&lt;</span> <span class="pre">4)</span></code> will typically tell the compiler that <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;=</span> <span class="pre">4</span></code> is false. Checks on complex expressions like <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">-</span> <span class="pre">3</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">&lt;=</span> <span class="pre">10)</span></code> will typically get you past identical guards.</p></li>
</ol>
<p>As mentioned previously, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls have applicability outside of data-dependent control flow. For example, here’s a model where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion
prevails while manual specialization &amp; <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> do not:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.432000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.438000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.438000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
I0108 21:26:27.440000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u0 &gt;= 0 due to data dependency, it was assumed to be False with no runtime assertions (_subclasses/fake_impls.py:388 in meta_select)
I0108 21:26:27.440000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u0 &lt; 0 due to data dependency, it was assumed to be False with no runtime assertions (_subclasses/fake_impls.py:390 in meta_select)
I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:402 in meta_select)
I0108 21:26:27.443000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u1 &gt;= 0 due to data dependency, it was assumed to be True with no runtime assertions (utils/_stats.py:28 in wrapper)
I0108 21:26:27.443000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.444000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u1]
I0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] 60 None
V0108 21:26:27.447000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 1 None
V0108 21:26:27.447000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
</pre></div>
</div>
<p>Here is a scenario where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion is required simply to prevent an operation from failing. The export call will fail with
“Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">-u0</span> <span class="pre">&gt;</span> <span class="pre">60</span></code>”, implying that the compiler doesn’t know if this is a valid indexing operation -
if the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> is out-of-bounds for <code class="docutils literal notranslate"><span class="pre">y</span></code> or not. Here, manual specialization is too prohibitive, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> has no place.
Instead, informing the compiler of <code class="docutils literal notranslate"><span class="pre">u0</span></code>’s range is sufficient:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">y</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.451000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.456000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.457000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
I0108 21:26:27.458000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 &gt;= 0 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:722 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0108 21:26:27.458000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, int_oo] (update)
I0108 21:26:27.461000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 &lt; 60 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:723 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt; 60"
V0108 21:26:27.462000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, 59] (update)
V0108 21:26:27.464000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known]
V0108 21:26:27.465000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known]
I0108 21:26:27.468000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] 60 None
V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 1 None
V0108 21:26:27.470000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.471000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &gt;= 0 == True [statically known]
V0108 21:26:27.473000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &lt;= 59 == True [statically known]
V0108 21:26:27.474000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &lt; 60 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_1: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge_1'");  ge_1 = _assert_scalar_default = None
            le: "Sym(u0 &lt;= 59)" = item &lt;= 59
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, "Runtime assertion failed for expression u0 &lt;= 59 on node 'le'");  le = _assert_scalar_default_1 = None

             #
            lt_1: "Sym(u0 &lt; 60)" = item &lt; 60
            _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, "Runtime assertion failed for expression u0 &lt; 60 on node 'lt_1'");  lt_1 = _assert_scalar_default_2 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a]
            select: "f32[]" = torch.ops.aten.select.int(y, 0, item);  y = item = None
            return (select,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    select: USER_OUTPUT

Range constraints: {u0: VR[0, 59]}
</pre></div>
</div>
</section>
<section id="specialized-values">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Specialized values</a><a class="headerlink" href="#specialized-values" title="Link to this heading">#</a></h3>
<p>Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value
while tracing. This looks something like “Could not extract specialized integer from data-dependent expression”, and is analogous to
the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise
with evaluating concrete boolean values.</p>
<p>This error typically occurs when there is an explicit or implicit <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on a data-dependent expression. For example, this list comprehension
has a <cite>range()</cite> call that implicitly does an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on the size of the list:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">([</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.481000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.487000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.487000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] Data dependent variable 'u0' allocated at:
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/bin/sphinx-build", line 7, in &lt;module&gt;
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     sys.exit(main())
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 339, in main
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return make_main(argv)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 213, in make_main
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return make_mode.run_make_mode(argv[1:])
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 181, in run_make_mode
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return make.run_generic_build(args[0])
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 169, in run_generic_build
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return build_main(args + opts)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 293, in build_main
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 272, in __init__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self._init_builder()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 343, in _init_builder
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self.events.emit('builder-inited')
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx/events.py", line 97, in emit
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     results.append(listener.handler(self.app, *args))
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 757, in generate_gallery_rst
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ) = generate_dir_rst(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 606, in generate_dir_rst
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     results = parallel(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 607, in &lt;genexpr&gt;
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/var/lib/workspace/conf.py", line 85, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     p.start()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self._popen = self._Popen(self)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return _default_context.get_context().Process._Popen(process_obj)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return Popen(process_obj)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self._launch(process_obj)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     code = process_obj._bootstrap(parent_sentinel=child_r)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self.run()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     self._target(*self._args, **self._kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/var/lib/workspace/conf.py", line 73, in call_fn
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     result = func(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1374, in generate_file_rst
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     output_blocks, time_elapsed = execute_script(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1192, in execute_script
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     execute_code_block(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1048, in execute_code_block
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     is_last_expr, mem_max = _exec_and_get_memory(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 876, in _exec_and_get_memory
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     mem_max, _ = call_memory(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1725, in _sg_call_memory_noop
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return 0.0, func()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 794, in __call__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     exec(self.code, self.fake_main.__dict__)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     export(Foo(), inps, strict=False)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return _export(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ep = fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ep = _export_for_training(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ep = fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     export_artifact = export_func(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     aten_export_artifact = _to_aten_func(  # type: ignore[operator]
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1793, in _export_to_aten_ir_make_fx
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     gm, graph_signature = transform(_make_fx_helper)(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1922, in _aot_export_non_strict
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1706, in _make_fx_helper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     gm = make_fx(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2429, in wrapped
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return make_fx_tracer.trace(f, *args)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2356, in trace
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._trace_inner(f, *args)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2318, in _trace_inner
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     t = dispatch_trace(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 53, in inner
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return disable_fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1303, in dispatch_trace
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1908, in trace
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     res = super().trace(root, concrete_args)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 868, in trace
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     (self.create_arg(fn(*args)),),
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1361, in wrapped
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     out = f(*tensors)  # type:ignore[call-arg]
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "&lt;string&gt;", line 1, in &lt;lambda&gt;
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1593, in wrapped_fn
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return tuple(flat_fn(*args))
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 187, in flat_fn
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     tree_out = fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py", line 1354, in functional_call
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     out = mod(*args[params_len:], **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self.call_module(mod, forward, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return Tracer.call_module(self, m, forward, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ret_val = forward(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return _orig_module_call(mod, *args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._call_impl(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return forward_call(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1906, in forward
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     tree_out = mod(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self.call_module(mod, forward, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return Tracer.call_module(self, m, forward, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     ret_val = forward(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return _orig_module_call(mod, *args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._call_impl(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return forward_call(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 747, in forward
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     a = x.item()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1409, in __torch_function__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return func(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1479, in __torch_function__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return func(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 1066, in __torch_function__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return func(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 962, in handler
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return torch._library.utils.handle_dispatch_mode(
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_library/utils.py", line 286, in handle_dispatch_mode
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 28, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1534, in __torch_dispatch__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return proxy_call(self, func, self.pre_dispatch, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 994, in proxy_call
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     out = func(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 841, in __call__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._op(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 28, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return fn(*args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1376, in __torch_dispatch__
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self.dispatch(func, types, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2096, in dispatch
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._cached_dispatch_impl(func, types, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1498, in _cached_dispatch_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return self._dispatch_impl(func, types, args, kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2725, in _dispatch_impl
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     op_impl_out = op_impl(self, func, *args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 169, in dispatch_to_op_implementations_dict
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 651, in local_scalar_dense
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     r = fake_mode.shape_env.create_unbacked_symint()
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 272, in wrapper
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]     return retlog(fn(*args, **kwargs))
V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532]



def forward(self, arg0_1: "i64[]", arg1_1: "f32[60]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: "Sym(u0)" = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None




def forward(self, arg0_1: "i64[]", arg1_1: "f32[60]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: "Sym(u0)" = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
    export(Foo(), inps, strict=False)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 311, in export
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 277, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2255, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1163, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1129, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 124, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2071, in _export_for_training
    export_artifact = export_func(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2002, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1793, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1922, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1706, in _make_fx_helper
    gm = make_fx(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2429, in wrapped
    return make_fx_tracer.trace(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2356, in trace
    return self._trace_inner(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2318, in _trace_inner
    t = dispatch_trace(
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1303, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1908, in trace
    res = super().trace(root, concrete_args)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 868, in trace
    (self.create_arg(fn(*args)),),
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1361, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File "&lt;string&gt;", line 1, in &lt;lambda&gt;
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1593, in wrapped_fn
    return tuple(flat_fn(*args))
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 187, in flat_fn
    tree_out = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py", line 1354, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1906, in forward
    tree_out = mod(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 843, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1997, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 560, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 836, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 748, in forward
    b = torch.cat([y for y in range(a)], dim=0)
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 449, in __index__
    return self.node.int_()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 468, in int_
    return self.guard_int("", 0)  # NB: uses Python backtrace
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 518, in guard_int
    r = self.evaluate()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 512, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7233, in evaluate_sym_node
    return self.evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7333, in evaluate_expr
    return self._inner_evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7356, in _inner_evaluate_expr
    return self._evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 7574, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)

Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>For these errors, some basic options you have are:</p>
<ol class="arabic simple">
<li><p>Avoid unnecessary <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast calls, in this case the <code class="docutils literal notranslate"><span class="pre">int(a)</span></code> in the return statement.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls; unfortunately all you may be able to do in this case is specialize (with <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">60)</span></code>).</p></li>
<li><p>Rewrite the offending code at a higher level. For example, the list comprehension is semantically a <code class="docutils literal notranslate"><span class="pre">repeat()</span></code> op, which doesn’t involve an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast. The following rewrite avoids data-dependent errors:</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.505000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.510000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense)
I0108 21:26:27.511000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0]
I0108 21:26:27.514000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 &gt;= 0 [guard added] (_meta_registrations.py:4109 in meta_repeat), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0108 21:26:27.515000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, int_oo] (update)
V0108 21:26:27.516000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &gt;= 0 == True [statically known]
I0108 21:26:27.519000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate Eq(u0, 0) due to data dependency, it was assumed to be False with no runtime assertions (utils/_stats.py:28 in wrapper)
I0108 21:26:27.519000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.525000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate 60*u0 &lt; 2 due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:310 in is_contiguous)
I0108 21:26:27.525000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.527000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate Eq(u0, 1) due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:276 in check_contiguous_sizes_strides)
I0108 21:26:27.527000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
V0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].size()[0] 60 None
V0108 21:26:27.535000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].stride()[0] 1 None
V0108 21:26:27.535000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['y'].storage_offset() 0 None
V0108 21:26:27.537000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 &gt;= 0 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             #
            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            ge: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge'");  ge = _assert_scalar_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1)
            unsqueeze: "f32[1, 60]" = torch.ops.aten.unsqueeze.default(y, 0);  y = None
            repeat: "f32[u0, 60]" = torch.ops.aten.repeat.default(unsqueeze, [item, 1]);  unsqueeze = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a
            add: "f32[u0, 60]" = torch.ops.aten.add.Tensor(repeat, item);  repeat = item = None
            return (add,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    add: USER_OUTPUT

Range constraints: {u0: VR[0, int_oo]}
</pre></div>
</div>
<p>Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: <code class="docutils literal notranslate"><span class="pre">torch._check_is_size()</span></code>, <code class="docutils literal notranslate"><span class="pre">guard_size_oblivious()</span></code>, or real-tensor tracing, as starters.
For more in-depth guides, please refer to the <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html">Export Programming Model</a>,
or <a class="reference external" href="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs">Dealing with GuardOnDataDependentSymNode errors</a>.</p>
</section>
</section>
<section id="custom-ops">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Custom Ops</a><a class="headerlink" href="#custom-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> can export PyTorch programs with custom operators. Please
refer to <a class="reference external" href="https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html">this page</a>
on how to author a custom operator in either C++ or Python.</p>
<p>The following is an example of registering a custom operator in python to be
used by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. The important thing to note is that the custom op
must have a <a class="reference external" href="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290">FakeTensor kernel</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">"my_custom_library::custom_op"</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_op called!"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="nd">@custom_op</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op_meta</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
    <span class="c1"># Returns an empty tensor with the same shape as the expected output</span>
    <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<p>Here is an example of exporting a program with the custom op.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomOpExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">my_custom_library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">CustomOpExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_custom_op_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:27.613000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.624000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] 3 None
V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[1] 3 None
V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 3 None
V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[1] 1 None
V0108 21:26:27.626000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x)
            sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x)
            custom_op: "f32[3, 3]" = torch.ops.my_custom_library.custom_op.default(sin);  sin = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x)
            cos: "f32[3, 3]" = torch.ops.aten.cos.default(custom_op);  custom_op = None
            return (cos,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    cos: USER_OUTPUT

Range constraints: {}

custom_op called!
tensor([[1.0000, 0.9431, 0.6418],
        [1.0000, 0.5546, 0.9050],
        [0.5543, 1.0000, 1.0000]])
</pre></div>
</div>
<p>Note that in the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, the custom operator is included in the graph.</p>
</section>
<section id="ir-decompositions">
<h2><a class="toc-backref" href="#id15" role="doc-backlink">IR/Decompositions</a><a class="headerlink" href="#ir-decompositions" title="Link to this heading">#</a></h2>
<p>The graph produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> returns a graph containing only
<a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen operators</a>, which are the
basic unit of computation in PyTorch. As there are over 3000 ATen operators,
export provides a way to narrow down the operator set used in the graph based
on certain characteristics, creating different IRs.</p>
<p>By default, export produces the most generic IR which contains all ATen
operators, including both functional and non-functional operators. A functional
operator is one that does not contain any mutations or aliasing of the inputs.
You can find a list of all ATen operators
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">here</a>
and you can inspect if an operator is functional by checking
<code class="docutils literal notranslate"><span class="pre">op._schema.is_mutable</span></code>, for example:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add_</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>False
True
</pre></div>
</div>
<p>This generic IR can be used to train in eager PyTorch Autograd. This IR can be
more explicitly reached through the API <code class="docutils literal notranslate"><span class="pre">torch.export.export_for_training</span></code>,
which was introduced in PyTorch 2.5, but calling <code class="docutils literal notranslate"><span class="pre">torch.export.export</span></code>
should produce the same graph as of PyTorch 2.6.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DecompExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span></a><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_training</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DecompExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.graph" title="torch.export.ExportedProgram.graph"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">graph</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/var/lib/workspace/intermediate_source/torch_export_tutorial.py:862: FutureWarning:

`torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.

I0108 21:26:27.636000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] 1 None
V0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[1] 1 None
V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[2] 3 None
V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[3] 3 None
V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 9 None
V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[1] 9 None
V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[2] 3 None
V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[3] 1 None
V0108 21:26:27.673000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {})
    return (batch_norm,)
</pre></div>
</div>
<p>We can then lower this exported program to an operator set which only contains
functional ATen operators through the API <code class="docutils literal notranslate"><span class="pre">run_decompositions</span></code>, which
decomposes the ATen operators into the ones specified in the decomposition
table, and functionalizes the graph. By specifying an empty set, we’re only
performing functionalization, and does not do any additional decompositions.
This results in an IR which contains ~2000 operators (instead of the 3000
operators above), and is ideal for inference cases.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_inference</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.graph" title="torch.export.ExportedProgram.graph"><span class="n">ep_for_inference</span><span class="o">.</span><span class="n">graph</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>As we can see, the previously mutable operator,
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add_.default</span></code> has now been replaced with
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.default</span></code>, a l operator.</p>
<p>We can also further lower this exported program to an operator set which only
contains the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir">Core ATen Operator Set</a>,
which is a collection of only ~180 operators. This IR is optimal for backends
who do not want to reimplement all ATen operators.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">default_decompositions</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a> <span class="o">=</span> <span class="n">default_decompositions</span><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">core_aten_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.graph" title="torch.export.ExportedProgram.graph"><span class="n">core_aten_ep</span><span class="o">.</span><span class="n">graph</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>We now see that <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> has been decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>. This is because <code class="docutils literal notranslate"><span class="pre">convolution</span></code>
is a more “core” operator, as operations like <code class="docutils literal notranslate"><span class="pre">conv1d</span></code> and <code class="docutils literal notranslate"><span class="pre">conv2d</span></code> can be
implemented using the same op.</p>
<p>We can also specify our own decomposition behaviors:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">default_decompositions</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_awesome_custom_conv2d_function</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dilation</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">groups</span><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">conv2d</span><span class="o">.</span><span class="n">default</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_awesome_custom_conv2d_function</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">my_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-property" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.graph" title="torch.export.ExportedProgram.graph"><span class="n">my_ep</span><span class="o">.</span><span class="n">graph</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>Notice that instead of <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> being decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>, it is now decomposed into
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mul.Tensor</span></code>,
which matches our custom decomposition rule.</p>
</section>
<section id="exportdb">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">ExportDB</a><a class="headerlink" href="#exportdb" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will only ever export a single computation graph from a PyTorch program. Because of this requirement,
there will be Python or PyTorch features that are not compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, which will require users to
rewrite parts of their model code. We have seen examples of this earlier in the tutorial – for example, rewriting
if-statements using <code class="docutils literal notranslate"><span class="pre">cond</span></code>.</p>
<p><a class="reference external" href="https://pytorch.org/docs/main/generated/exportdb/index.html">ExportDB</a> is the standard reference that documents
supported and unsupported Python/PyTorch features for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. It is essentially a list a program samples, each
of which represents the usage of one particular Python/PyTorch feature and its interaction with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.
Examples are also tagged by category so that they can be more easily searched.</p>
<p>For example, let’s use ExportDB to get a better understanding of how the predicate works in the <code class="docutils literal notranslate"><span class="pre">cond</span></code> operator.
We can look at the example called <code class="docutils literal notranslate"><span class="pre">cond_predicate</span></code>, which has a <code class="docutils literal notranslate"><span class="pre">torch.cond</span></code> tag. The example code looks like:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cond_predicate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    The conditional statement (aka predicate) passed to ``cond()`` must be one of the following:</span>
<span class="sd">    - ``torch.Tensor`` with a single element</span>
<span class="sd">    - boolean expression</span>
<span class="sd">    NOTE: If the `pred` is test on a dim with batch size &lt; 2, it will be specialized.</span>
<span class="sd">    """</span>
    <span class="n">pred</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>
</pre></div>
</div>
<p>More generally, ExportDB can be used as a reference when one of the following occurs:</p>
<ol class="arabic simple">
<li><p>Before attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, you know ahead of time that your model uses some tricky Python/PyTorch features
and you want to know if <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> covers that feature.</p></li>
<li><p>When attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, there is a failure and it’s unclear how to work around it.</p></li>
</ol>
<p>ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach
out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
</section>
<section id="running-the-exported-program">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Running the Exported Program</a><a class="headerlink" href="#running-the-exported-program" title="Link to this heading">#</a></h2>
<p>As <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is only a graph capturing mechanism, calling the artifact
produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> eagerly will be equivalent to running the eager
module. To optimize the execution of the Exported Program, we can pass this
exported artifact to backends such as Inductor through <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AOTInductor</a>,
or <a class="reference external" href="https://pytorch.org/TensorRT/dynamo/dynamo_export.html">TensorRT</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">M</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">M</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">,))</span>

<span class="c1"># Run it eagerly</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>

<span class="c1"># Run it with torch.compile</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">(),</span> <span class="n">backend</span><span class="o">=</span><span class="s2">"inductor"</span><span class="p">)(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0108 21:26:28.652000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env
I0108 21:26:28.665000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards
V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[0] 2 None
V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].size()[1] 3 None
V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[0] 3 None
V0108 21:26:28.667000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].stride()[1] 1 None
V0108 21:26:28.667000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L['x'].storage_offset() 0 None
tensor([[-7.3250e-01,  4.7538e-01,  4.9275e-01],
        [-2.8545e-04,  3.1366e-02, -5.0228e-02]], device='cuda:0',
       grad_fn=&lt;AddmmBackward0&gt;)
I0108 21:26:29.812000 21876 torch/fx/experimental/symbolic_shapes.py:3769] [2/0] create_env
/usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning:

Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)

/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:312: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.

I0108 21:26:30.826000 21876 torch/fx/experimental/symbolic_shapes.py:5242] [2/0] produce_guards
I0108 21:26:30.837000 21876 torch/fx/experimental/symbolic_shapes.py:5242] [2/0] produce_guards
V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['x'].size()[0] 2 None
V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['x'].size()[1] 3 None
V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['x'].stride()[0] 3 None
V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['x'].stride()[1] 1 None
V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['x'].storage_offset() 0 None
V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 3 None
V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 3 None
V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 3 None
V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None
V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None
V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 3 None
V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None
V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None
V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['x'].size()[0] == 2
V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['x'].size()[1] == 3
V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['x'].stride()[0] == 3
V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['x'].stride()[1] == 1
V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['x'].storage_offset() == 0
V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 3
V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 3
V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 3
V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1
V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0
V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 3
V0108 21:26:30.845000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1
V0108 21:26:30.845000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0
tensor([[-7.3250e-01,  4.7538e-01,  4.9275e-01],
        [-2.8545e-04,  3.1366e-02, -5.0228e-02]], device='cuda:0',
       grad_fn=&lt;CompiledFunctionBackward&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._inductor</span>

<span class="c1"># Note: these APIs are subject to change</span>
<span class="c1"># Compile the exported program to a PT2 archive using ``AOTInductor``</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt2_path</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_compile_and_package</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>

<span class="c1"># Load and run the .so file in Python.</span>
<span class="c1"># To load and run it in a C++ environment, see:</span>
<span class="c1"># https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</span>
<span class="n">aoti_compiled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_load_package</span><span class="p">(</span><span class="n">pt2_path</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <span class="n">aoti_compiled</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We introduced <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, the new PyTorch 2.X way to export single computation
graphs from PyTorch programs. In particular, we demonstrate several code modifications
and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 8.508 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-torch-export-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fb8083290582c4f473d970913a4186c4/torch_export_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torch_export_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b865be3c401c1b4fbdb03f49916ac8e8/torch_export_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torch_export_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/eb8d1a44ecbb6f9b3f0732396942ffcd/torch_export_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">torch_export_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../recipes/regional_compilation.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
</div>
</a>
<a class="right-next" href="../recipes/torch_export_aoti_python.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../recipes/regional_compilation.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
</div>
</a>
<a class="right-next" href="../recipes/torch_export_aoti_python.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-breaks">Graph Breaks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-strict-export">Non-Strict Export</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-flow-ops">Control Flow Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts-symbols-and-guards">Basic concepts: symbols and guards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialization">0/1 specialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#named-dims">Named Dims</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constraint-violations-suggested-fixes">Constraint violations, suggested fixes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-dependent-errors">Data-dependent errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guards-torch-check">Guards, torch._check()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-values">Specialized values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-ops">Custom Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-decompositions">IR/Decompositions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exportdb">ExportDB</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-exported-program">Running the Exported Program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg">
<path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg">
<path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg">
<path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path>
</svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg">
<rect fill="currentColor" height="512" rx="0" width="512"></rect>
<circle cx="142" cy="138" fill="#000" r="37"></circle>
<path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path>
<path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path>
</svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg">
<path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor">
</path>
</svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg">
<path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor">
</path>
<path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor">
</path>
</svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
          © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torch.export Tutorial",
       "headline": "torch.export Tutorial",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/torch_export_tutorial.html",
       "articleBody": "Note Go to the end to download the full example code. torch.export Tutorial# Author: William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan Warning torch.export and its related features are in prototype status and are subject to backwards compatibility breaking changes. This tutorial provides a snapshot of torch.export usage as of PyTorch 2.5. torch.export() is the PyTorch 2.X way to export PyTorch models into standardized model representations, intended to be run on different (i.e. Python-less) environments. The official documentation can be found here. In this tutorial, you will learn how to use torch.export() to extract ExportedProgram\u2019s (i.e. single-graph representations) from PyTorch programs. We also detail some considerations/modifications that you may need to make in order to make your model compatible with torch.export. Contents Basic Usage Graph Breaks Non-Strict Export Control Flow Ops Constraints/Dynamic Shapes Basic concepts: symbols and guards 0/1 specialization Named Dims Constraint violations, suggested fixes Data-dependent errors Guards, torch._check() Specialized values Custom Ops IR/Decompositions ExportDB Running the Exported Program Conclusion Basic Usage# torch.export extracts single-graph representations from PyTorch programs by tracing the target function, given example inputs. torch.export.export() is the main entry point for torch.export. In this tutorial, torch.export and torch.export.export() are practically synonymous, though torch.export generally refers to the PyTorch 2.X export process, and torch.export.export() generally refers to the actual function call. The signature of torch.export.export() is: export( mod: torch.nn.Module, args: Tuple[Any, ...], kwargs: Optional[Dict[str, Any]] = None, *, dynamic_shapes: Optional[Dict[str, Dict[int, Dim]]] = None ) -\u003e ExportedProgram torch.export.export() traces the tensor computation graph from calling mod(*args, **kwargs) and wraps it in an ExportedProgram, which can be serialized or executed later with different inputs. To execute the ExportedProgram we can call .module() on it to return a torch.nn.Module which is callable, just like the original program. We will detail the dynamic_shapes argument later in the tutorial. import torch from torch.export import export class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.lin = torch.nn.Linear(100, 10) def forward(self, x, y): return torch.nn.functional.relu(self.lin(x + y), inplace=True) mod = MyModule() exported_mod = export(mod, (torch.randn(8, 100), torch.randn(8, 100))) print(type(exported_mod)) print(exported_mod.module()(torch.randn(8, 100), torch.randn(8, 100))) \u003cclass \u0027torch.export.exported_program.ExportedProgram\u0027\u003e tensor([[0.2226, 1.1915, 0.0000, 0.2458, 1.3452, 0.0000, 0.1659, 0.0000, 0.4901, 0.3078], [1.1002, 0.1275, 0.0000, 0.0000, 1.1690, 1.1518, 0.2871, 0.0160, 0.0000, 0.0000], [0.7178, 0.0000, 0.0000, 1.0897, 0.0000, 0.6323, 0.4352, 0.0000, 0.1490, 2.1381], [0.0000, 0.0000, 2.0230, 0.3646, 0.6718, 0.0000, 0.0000, 0.9920, 0.4406, 0.9623], [0.0000, 0.0000, 0.0000, 0.6759, 0.0000, 0.0000, 0.0000, 1.4586, 0.4583, 0.0000], [0.1861, 0.0000, 0.2774, 1.1439, 0.0000, 1.8380, 0.0000, 1.3985, 0.0000, 0.0000], [1.3288, 0.0604, 0.0639, 0.0000, 0.0000, 0.0000, 1.0302, 0.0000, 0.0000, 0.0000], [0.5052, 0.0000, 0.0000, 0.8917, 0.0000, 0.5097, 1.0726, 0.0000, 0.0000, 0.0000]], grad_fn=\u003cReluBackward0\u003e) Let\u2019s review some attributes of ExportedProgram that are of interest. The graph attribute is an FX graph traced from the function we exported, that is, the computation graph of all PyTorch operations. The FX graph is in \u201cATen IR\u201d meaning that it contains only \u201cATen-level\u201d operations. The graph_signature attribute gives a more detailed description of the input and output nodes in the exported graph, describing which ones are parameters, buffers, user inputs, or user outputs. The range_constraints attributes will be covered later. print(exported_mod) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_lin_weight: \"f32[10, 100]\", p_lin_bias: \"f32[10]\", x: \"f32[8, 100]\", y: \"f32[8, 100]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True) add: \"f32[8, 100]\" = torch.ops.aten.add.Tensor(x, y); x = y = None # File: /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias) linear: \"f32[8, 10]\" = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias); add = p_lin_weight = p_lin_bias = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True) relu_: \"f32[8, 10]\" = torch.ops.aten.relu_.default(linear); linear = None return (relu_,) Graph signature: # inputs p_lin_weight: PARAMETER target=\u0027lin.weight\u0027 p_lin_bias: PARAMETER target=\u0027lin.bias\u0027 x: USER_INPUT y: USER_INPUT # outputs relu_: USER_OUTPUT Range constraints: {} See the torch.export documentation for more details. Graph Breaks# Although torch.export shares components with torch.compile, the key limitation of torch.export, especially when compared to torch.compile, is that it does not support graph breaks. This is because handling graph breaks involves interpreting the unsupported operation with default Python evaluation, which is incompatible with the export use case. Therefore, in order to make your model code compatible with torch.export, you will need to modify your code to remove graph breaks. A graph break is necessary in cases such as: data-dependent control flow class Bad1(torch.nn.Module): def forward(self, x): if x.sum() \u003e 0: return torch.sin(x) return torch.cos(x) import traceback as tb try: export(Bad1(), (torch.randn(3, 3),)) except Exception: tb.print_exc() def forward(self, arg0_1: \"f32[3, 3]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() \u003e 0: sum_1: \"f32[]\" = torch.ops.aten.sum.default(arg0_1); arg0_1 = None gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None ne: \"b8[]\" = torch.ops.aten.ne.Scalar(gt, 0); gt = None item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne); ne = item = None def forward(self, arg0_1: \"f32[3, 3]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() \u003e 0: sum_1: \"f32[]\" = torch.ops.aten.sum.default(arg0_1); arg0_1 = None gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None ne: \"b8[]\" = torch.ops.aten.ne.Scalar(gt, 0); gt = None item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne); ne = item = None Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 122, in \u003cmodule\u003e export(Bad1(), (torch.randn(3, 3),)) File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 311, in export raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export return _export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export ep = _export_for_training( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training export_artifact = export_func( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1793, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1922, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1706, in _make_fx_helper gm = make_fx( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2429, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2356, in trace return self._trace_inner(f, *args) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in _trace_inner t = dispatch_trace( File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1303, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1908, in trace res = super().trace(root, concrete_args) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 868, in trace (self.create_arg(fn(*args)),), File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1361, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1593, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 187, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py\", line 1354, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1906, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 116, in forward if x.sum() \u003e 0: File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1409, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1479, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 1066, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py\", line 538, in guard_bool r = self.evaluate() File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate return self.shape_env.evaluate_sym_node(self, size_oblivious) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7233, in evaluate_sym_node return self.evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7333, in evaluate_expr return self._inner_evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7356, in _inner_evaluate_expr return self._evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7574, in _evaluate_expr raise self._make_data_dependent_error( torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)). (Size-like symbols: none) consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__) For more information, run with TORCH_LOGS=\"dynamic\" For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 The following call raised this error: File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 116, in forward if x.sum() \u003e 0: The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. accessing tensor data with .data class Bad2(torch.nn.Module): def forward(self, x): x.data[0, 0] = 3 return x try: export(Bad2(), (torch.randn(3, 3),)) except Exception: tb.print_exc() calling unsupported functions (such as many built-in functions) class Bad3(torch.nn.Module): def forward(self, x): x = x + 1 return x + id(x) try: export(Bad3(), (torch.randn(3, 3),)) except Exception: tb.print_exc() Non-Strict Export# To trace the program, torch.export uses TorchDynamo by default, a byte code analysis engine, to symbolically analyze the Python code and build a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported, causing these graph breaks. To address this issue, in PyTorch 2.3, we introduced a new mode of exporting called non-strict mode, where we trace through the program using the Python interpreter executing it exactly as it would in eager mode, allowing us to skip over unsupported Python features. This is done through adding a strict=False flag. Looking at some of the previous examples which resulted in graph breaks: Calling unsupported functions (such as many built-in functions) traces through, but in this case, id(x) gets specialized as a constant integer in the graph. This is because id(x) is not a tensor operation, so the operation is not recorded in the graph. class Bad3(torch.nn.Module): def forward(self, x): x = x + 1 return x + id(x) bad3_nonstrict = export(Bad3(), (torch.randn(3, 3),), strict=False) print(bad3_nonstrict) print(bad3_nonstrict.module()(torch.ones(3, 3))) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1 add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, 1); x = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x) add_1: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(add, 140334371813264); add = None return (add_1,) Graph signature: # inputs x: USER_INPUT # outputs add_1: USER_OUTPUT Range constraints: {} tensor([[1.4033e+14, 1.4033e+14, 1.4033e+14], [1.4033e+14, 1.4033e+14, 1.4033e+14], [1.4033e+14, 1.4033e+14, 1.4033e+14]]) However, there are still some features that require rewrites to the original module: Control Flow Ops# torch.export actually does support data-dependent control flow. But these need to be expressed using control flow ops. For example, we can fix the control flow example above using the cond op, like so: class Bad1Fixed(torch.nn.Module): def forward(self, x): def true_fn(x): return torch.sin(x) def false_fn(x): return torch.cos(x) return torch.cond(x.sum() \u003e 0, true_fn, false_fn, [x]) exported_bad1_fixed = export(Bad1Fixed(), (torch.randn(3, 3),)) print(exported_bad1_fixed) print(exported_bad1_fixed.module()(torch.ones(3, 3))) print(exported_bad1_fixed.module()(-torch.ones(3, 3))) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() \u003e 0, true_fn, false_fn, [x]) sum_1: \"f32[]\" = torch.ops.aten.sum.default(x) gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None # File: \u003ceval_with_key\u003e.35:9 in forward, code: cond = torch.ops.higher_order.cond(l_args_0_, cond_true_0, cond_false_0, (l_args_3_0_,)); l_args_0_ = cond_true_0 = cond_false_0 = l_args_3_0_ = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x,)); gt = true_graph_0 = false_graph_0 = x = None getitem: \"f32[3, 3]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: \u003ceval_with_key\u003e.32:6 in forward, code: sin = torch.sin(l_args_3_0__1); l_args_3_0__1 = None sin: \"f32[3, 3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: \u003ceval_with_key\u003e.33:6 in forward, code: cos = torch.cos(l_args_3_0__1); l_args_3_0__1 = None cos: \"f32[3, 3]\" = torch.ops.aten.cos.default(x); x = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {} tensor([[0.8415, 0.8415, 0.8415], [0.8415, 0.8415, 0.8415], [0.8415, 0.8415, 0.8415]]) tensor([[0.5403, 0.5403, 0.5403], [0.5403, 0.5403, 0.5403], [0.5403, 0.5403, 0.5403]]) There are limitations to cond that one should be aware of: The predicate (i.e. x.sum() \u003e 0) must result in a boolean or a single-element tensor. The operands (i.e. [x]) must be tensors. The branch function (i.e. true_fn and false_fn) signature must match with the operands and they must both return a single tensor with the same metadata (for example, dtype, shape, etc.). Branch functions cannot mutate input or global variables. Branch functions cannot access closure variables, except for self if the function is defined in the scope of a method. For more details about cond, check out the cond documentation. We can also use map, which applies a function across the first dimension of the first tensor argument. from torch._higher_order_ops.map import map as torch_map class MapModule(torch.nn.Module): def forward(self, xs, y, z): def body(x, y, z): return x + y + z return torch_map(body, xs, y, z) inps = (torch.ones(6, 4), torch.tensor(5), torch.tensor(4)) exported_map_example = export(MapModule(), inps) print(exported_map_example) print(exported_map_example.module()(*inps)) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, xs: \"f32[6, 4]\", y: \"i64[]\", z: \"i64[]\"): # File: \u003ceval_with_key\u003e.64:9 in forward, code: map_impl = torch.ops.higher_order.map_impl(map_body_0, [l_flat_xs_0_], [l_flat_args_0_, l_flat_args_1_]); map_body_0 = l_flat_xs_0_ = l_flat_args_0_ = l_flat_args_1_ = None body_graph_0 = self.body_graph_0 map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]); body_graph_0 = xs = y = z = None getitem: \"f32[6, 4]\" = map_impl[0]; map_impl = None return (getitem,) class body_graph_0(torch.nn.Module): def forward(self, xs: \"f32[4]\", y: \"i64[]\", z: \"i64[]\"): # File: \u003ceval_with_key\u003e.62:5 in forward, code: add = child + l_flat_args_0_; child = l_flat_args_0_ = None add: \"f32[4]\" = torch.ops.aten.add.Tensor(xs, y); xs = y = None # File: \u003ceval_with_key\u003e.62:6 in forward, code: add_1 = add + l_flat_args_1_; add = l_flat_args_1_ = None add_1: \"f32[4]\" = torch.ops.aten.add.Tensor(add, z); add = z = None return (add_1,) Graph signature: # inputs xs: USER_INPUT y: USER_INPUT z: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {} tensor([[10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.]]) Other control flow ops include while_loop, associative_scan, and scan. For more documentation on each operator, please refer to this page. Constraints/Dynamic Shapes# This section covers dynamic behavior and representation of exported programs. Dynamic behavior is subjective to the particular model being exported, so for the most part of this tutorial, we\u2019ll focus on this particular toy model (with the resulting tensor shapes annotated): class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward( self, w: torch.Tensor, # [6, 5] x: torch.Tensor, # [4] y: torch.Tensor, # [8, 4] z: torch.Tensor, # [32] ): x0 = x + y # [8, 4] x1 = self.l(w) # [6, 3] x2 = x0.flatten() # [32] x3 = x2 + z # [32] return x1, x3 By default, torch.export produces a static program. One consequence of this is that at runtime, the program won\u2019t work on inputs with different shapes, even if they\u2019re valid in eager mode. w = torch.randn(6, 5) x = torch.randn(4) y = torch.randn(8, 4) z = torch.randn(32) model = DynamicModel() ep = export(model, (w, x, y, z)) model(w, x, torch.randn(3, 4), torch.randn(12)) try: ep.module()(w, x, torch.randn(3, 4), torch.randn(12)) except Exception: tb.print_exc() Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 286, in \u003cmodule\u003e ep.module()(w, x, torch.randn(3, 4), torch.randn(12)) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 837, in call_wrapped return self._wrapped_call(self, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 413, in __call__ raise e File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 400, in __call__ return super(self.cls, obj).__call__(*args, **kwargs) # type: ignore[misc] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1881, in _call_impl return inner() File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1829, in inner result = forward_call(*args, **kwargs) File \"\u003ceval_with_key\u003e.95\", line 8, in forward _guards_fn = self._guards_fn(w, x, y, z); _guards_fn = None File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 209, in inner return func(*args, **kwargs) File \"\u003cstring\u003e\", line 6, in _ File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2185, in _assert assert condition, message AssertionError: Guard failed: y.size()[0] == 8 Basic concepts: symbols and guards# To enable dynamism, export() provides a dynamic_shapes argument. The easiest way to work with dynamic shapes is using Dim.AUTO and looking at the program that\u2019s returned. Dynamic behavior is specified at a input dimension-level; for each input we can specify a tuple of values: from torch.export.dynamic_shapes import Dim dynamic_shapes = { \"w\": (Dim.AUTO, Dim.AUTO), \"x\": (Dim.AUTO,), \"y\": (Dim.AUTO, Dim.AUTO), \"z\": (Dim.AUTO,), } ep = export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) Before we look at the program that\u2019s produced, let\u2019s understand what specifying dynamic_shapes entails, and how that interacts with export. For every input dimension where a Dim object is specified, a symbol is allocated, taking on a range of [2, inf] (why not [0, inf] or [1, inf]? we\u2019ll explain later in the 0/1 specialization section). Export then runs model tracing, looking at each operation that\u2019s performed by the model. Each individual operation can emit what\u2019s called \u201cguards\u201d; basically boolean condition that are required to be true for the program to be valid. When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid; i.e. the program\u2019s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards and producing a final program representation that adheres to all of these guards. Before we see this \u201cfinal representation\u201d in an ExportedProgram, let\u2019s look at the guards emitted by the toy model we\u2019re tracing. Here, each forward input tensor is annotated with the symbol allocated at the start of tracing: class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward( self, w: torch.Tensor, # [s0, s1] x: torch.Tensor, # [s2] y: torch.Tensor, # [s3, s4] z: torch.Tensor, # [s5] ): x0 = x + y # guard: s2 == s4 x1 = self.l(w) # guard: s1 == 5 x2 = x0.flatten() # no guard added here x3 = x2 + z # guard: s3 * s4 == s5 return x1, x3 Let\u2019s understand each of the operations and the emitted guards: x0 = x + y: This is an element-wise add with broadcasting, since x is a 1-d tensor and y a 2-d tensor. x is broadcasted along the last dimension of y, emitting the guard s2 == s4. x1 = self.l(w): Calling nn.Linear() performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (w: [s0, s1]), and a statically-shaped tensor. This emits the guard s1 == 5. x2 = x0.flatten(): This call actually doesn\u2019t emit any guards! (at least none relevant to input shapes) x3 = x2 + z: x2 has shape [s3*s4] after flattening, and this element-wise add emits s3 * s4 == s5. Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid: w: [s0, 5] x: [s2] y: [s3, s2] z: [s2*s3] And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the corresponding inputs: print(ep) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_l_weight: \"f32[3, 5]\", p_l_bias: \"f32[3]\", w: \"f32[s15, 5]\", x: \"f32[s77]\", y: \"f32[s17, s77]\", z: \"f32[s17*s77]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y # [8, 4] add: \"f32[s17, s77]\" = torch.ops.aten.add.Tensor(x, y); x = y = None # File: /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias) linear: \"f32[s15, 3]\" = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias); w = p_l_weight = p_l_bias = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten() # [32] flatten: \"f32[s17*s77]\" = torch.ops.aten.flatten.using_ints(add); add = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z # [32] add_1: \"f32[s17*s77]\" = torch.ops.aten.add.Tensor(flatten, z); flatten = z = None return (linear, add_1) Graph signature: # inputs p_l_weight: PARAMETER target=\u0027l.weight\u0027 p_l_bias: PARAMETER target=\u0027l.bias\u0027 w: USER_INPUT x: USER_INPUT y: USER_INPUT z: USER_INPUT # outputs linear: USER_OUTPUT add_1: USER_OUTPUT Range constraints: {s15: VR[2, int_oo], s77: VR[2, int_oo], s17: VR[2, int_oo], s17*s77: VR[4, int_oo]} Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn\u2019t so interesting currently, since this export call doesn\u2019t emit any guards related to symbol bounds and each base symbol has a generic bound, but this will come up later. So far, because we\u2019ve been exporting this toy model, this experience has not been representative of how hard it typically is to debug dynamic shapes guards \u0026 issues. In most cases it isn\u2019t obvious what guards are being emitted, and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards are rather intuitive. In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment variable TORCH_LOGS=\"+dynamic\", or interactively with torch._logging.set_logs(dynamic=10): torch._logging.set_logs(dynamic=10) ep = export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) I0108 21:26:26.945000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:26.947000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:26.948000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:26.950000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:26.951000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:26.952000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:26.954000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V0108 21:26:26.960000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.960000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.961000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.962000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.963000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.964000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.965000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.966000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.967000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.967000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:26.970000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" I0108 21:26:26.971000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[2, int_oo] V0108 21:26:26.972000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:26.979000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V0108 21:26:26.979000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update) I0108 21:26:26.980000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V0108 21:26:26.992000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:26.995000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:26.997000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V0108 21:26:26.998000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[4, int_oo] (update) I0108 21:26:26.999000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[4, int_oo] I0108 21:26:27.004000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[0] s15 None V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[1] 5 None V0108 21:26:27.005000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[0] 5 None V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[1] 1 None V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].storage_offset() 0 None V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] s77 None V0108 21:26:27.006000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 1 None V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] s17 None V0108 21:26:27.007000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[1] s77 None V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] s77 None V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[1] 1 None V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.008000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V0108 21:26:27.009000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].stride()[0] 1 None V0108 21:26:27.009000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].storage_offset() 0 None V0108 21:26:27.021000 21876 torch/fx/experimental/symbolic_shapes.py:7471] eval 5 [trivial] This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above; e.g. the allocation of symbols: \"\"\" create_symbol s0 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s1 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) runtime_assert True == True [statically known] create_symbol s2 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s3 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s4 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s5 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) \"\"\" \"\\ncreate_symbol s0 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s1 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\nruntime_assert True == True [statically known]\\ncreate_symbol s2 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s3 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s4 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s5 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\n\" The lines with create_symbol show when a new symbol has been allocated, and the logs also identify the tensor variable names and dimensions they\u2019ve been allocated for. In other lines we can also see the guards emitted: \"\"\" runtime_assert Eq(s2, s4) [guard added] x0 = x + y # output shape: [8, 4] # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2, s4)\" runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w) # [6, 3] # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s1, 5)\" runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z # [32] # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2*s3, s5)\" \"\"\" \u0027\\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y # output shape: [8, 4] # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2, s4)\"\\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w) # [6, 3] # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s1, 5)\"\\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z # [32] # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2*s3, s5)\"\\n\u0027 Next to the [guard added] messages, we also see the responsible user lines of code - luckily here the model is simple enough. In many real-world cases it\u2019s not so straightforward: high-level torch operations can have complicated fake-kernel implementations or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate is to follow the logs\u2019 suggestion, and re-run with environment variable TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"...\", to further attribute the guard of interest. Dim.AUTO is just one of the available options for interacting with dynamic_shapes; as of writing this 2 other options are available: Dim.DYNAMIC, and Dim.STATIC. Dim.STATIC simply marks a dimension static, while Dim.DYNAMIC is similar to Dim.AUTO in all ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a static guard is emitted on a dynamically-marked dimension: dynamic_shapes[\"w\"] = (Dim.AUTO, Dim.DYNAMIC) try: export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I0108 21:26:27.025000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.027000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.027000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.029000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.031000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.031000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.033000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V0108 21:26:27.039000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.040000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.040000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.042000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.042000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.044000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.044000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.045000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.046000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.047000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.049000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" I0108 21:26:27.050000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[2, int_oo] V0108 21:26:27.052000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.058000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V0108 21:26:27.058000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update) I0108 21:26:27.059000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V0108 21:26:27.071000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.074000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.076000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V0108 21:26:27.077000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[4, int_oo] (update) I0108 21:26:27.078000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[4, int_oo] I0108 21:26:27.083000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.083000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[0] s15 None V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[1] 5 RelaxedUnspecConstraint(warn_only=False) V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[0] 5 None V0108 21:26:27.084000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[1] 1 None V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].storage_offset() 0 None V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] s77 None V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 1 None V0108 21:26:27.085000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] s17 None V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[1] s77 None V0108 21:26:27.086000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] s77 None V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[1] 1 None V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V0108 21:26:27.087000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].stride()[0] 1 None V0108 21:26:27.088000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1821, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1968, in _produce_guards_callback return produce_guards_and_solve_constraints( File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 533, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 500, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5204, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5928, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[\u0027w\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027w\u0027].size()[1] as dynamic but your code specialized it to be a constant (5). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 418, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 311, in export raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export return _export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export ep = _export_for_training( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training export_artifact = export_func( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1823, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 torch._dynamo.exc.UserError: Constraints violated (L[\u0027w\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027w\u0027].size()[1] as dynamic but your code specialized it to be a constant (5). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. Static guards also aren\u2019t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is raised when this is the case for x.shape[0] and y.shape[1]: dynamic_shapes[\"w\"] = (Dim.AUTO, Dim.AUTO) dynamic_shapes[\"x\"] = (Dim.STATIC,) dynamic_shapes[\"y\"] = (Dim.AUTO, Dim.DYNAMIC) try: export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I0108 21:26:27.098000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.100000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.100000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.103000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.103000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.105000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V0108 21:26:27.111000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.112000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.113000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.115000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.115000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.116000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.117000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.118000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.123000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s94, 4) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s94, 4)\" V0108 21:26:27.124000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 4] (update) I0108 21:26:27.124000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = 4 (range_refined_to_singleton) VR[4, 4] I0108 21:26:27.132000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V0108 21:26:27.132000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update) I0108 21:26:27.133000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] I0108 21:26:27.153000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(4*s17, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(4*s17, s68)\" V0108 21:26:27.154000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[8, int_oo] (update) I0108 21:26:27.158000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = 4*s17 (solve) VR[8, int_oo] I0108 21:26:27.163000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.163000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[0] s15 None V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[1] 5 None V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[0] 5 None V0108 21:26:27.164000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[1] 1 None V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].storage_offset() 0 None V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] 4 None V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 1 None V0108 21:26:27.165000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] s17 None V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[1] 4 RelaxedUnspecConstraint(warn_only=False) V0108 21:26:27.166000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 4 None V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[1] 1 None V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.167000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].size()[0] 4*s17 None V0108 21:26:27.168000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].stride()[0] 1 None V0108 21:26:27.168000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1821, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1968, in _produce_guards_callback return produce_guards_and_solve_constraints( File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 533, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 500, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5204, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5928, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[\u0027y\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027y\u0027].size()[1] as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 431, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 311, in export raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export return _export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export ep = _export_for_training( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training export_artifact = export_func( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1823, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 torch._dynamo.exc.UserError: Constraints violated (L[\u0027y\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027y\u0027].size()[1] as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. Here you might ask why export \u201cspecializes\u201d, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because of the symbolic shapes system described above, of symbols and guards. When x.shape[0] is marked static, we don\u2019t allocate a symbol, and compile treating this shape as a concrete integer 4. A symbol is allocated for y.shape[1], and so we finally emit the guard s3 == 4, leading to specialization. One feature of export is that during tracing, statements like asserts, torch._check(), and if/else conditions will also emit guards. See what happens when we augment the existing model with such statements: class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward(self, w, x, y, z): assert w.shape[0] \u003c= 512 torch._check(x.shape[0] \u003e= 4) if w.shape[0] == x.shape[0] + 2: x0 = x + y x1 = self.l(w) x2 = x0.flatten() x3 = x2 + z return x1, x3 else: return w dynamic_shapes = { \"w\": (Dim.AUTO, Dim.AUTO), \"x\": (Dim.AUTO,), \"y\": (Dim.AUTO, Dim.AUTO), \"z\": (Dim.AUTO,), } try: ep = export(DynamicModel(), (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I0108 21:26:27.177000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.179000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.179000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.181000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.182000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.183000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.185000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V0108 21:26:27.191000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.192000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.192000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.193000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.194000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.195000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.196000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.197000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.198000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.198000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.204000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval s15 \u003c= 512 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:450 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"s15 \u003c= 512\" V0108 21:26:27.205000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s15 = VR[2, 512] (update) I0108 21:26:27.208000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval s77 \u003e= 4 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:451 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"s77 \u003e= 4\" V0108 21:26:27.209000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s77 = VR[4, int_oo] (update) I0108 21:26:27.213000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s15, s77 + 2) [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:452 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s15, s77 + 2)\" V0108 21:26:27.214000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s15 = VR[6, 512] (update) V0108 21:26:27.217000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s77 = VR[4, 510] (update) I0108 21:26:27.218000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s15 = s77 + 2 (solve) VR[6, 512] I0108 21:26:27.222000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" V0108 21:26:27.223000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 510] (update) I0108 21:26:27.223000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s77 (solve) VR[4, 510] V0108 21:26:27.227000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.234000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s21, 5) [guard added] (_meta_registrations.py:2248 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V0108 21:26:27.235000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s21 = VR[5, 5] (update) I0108 21:26:27.235000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V0108 21:26:27.250000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.253000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.262000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V0108 21:26:27.263000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s68 = VR[8, int_oo] (update) I0108 21:26:27.264000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s68 = s17*s77 (solve) VR[8, int_oo] I0108 21:26:27.269000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.270000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[0] s77 + 2 None V0108 21:26:27.270000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].size()[1] 5 None V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[0] 5 None V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].stride()[1] 1 None V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027w\u0027].storage_offset() 0 None V0108 21:26:27.271000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] s77 None V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 1 None V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] s17 None V0108 21:26:27.272000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[1] s77 None V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] s77 None V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[1] 1 None V0108 21:26:27.273000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].stride()[0] 1 None V0108 21:26:27.274000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027z\u0027].storage_offset() 0 None V0108 21:26:27.290000 21876 torch/fx/experimental/symbolic_shapes.py:7471] eval 5 [trivial] Each of these statements emits an additional guard, and the exported program shows the changes; s0 is eliminated in favor of s2 + 2, and s2 now contains lower and upper bounds, reflected in range_constraints. For the if/else condition, you might ask why the True branch was taken, and why it wasn\u2019t the w.shape[0] != x.shape[0] + 2 guard that got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken. If different sample input shapes were provided that fail the if condition, export would trace and emit guards corresponding to the else branch. Additionally, you might ask why we traced only the if branch, and if it\u2019s possible to maintain control-flow in your program and keep both branches alive. For that, refer to rewriting your model code following the Control Flow Ops section above. 0/1 specialization# Since we\u2019re talking about guards and specializations, it\u2019s a good time to talk about the 0/1 specialization issue we brought up earlier. The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that don\u2019t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 \u2026 . This just means that you should specify 0/1 sample inputs when you\u2019d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens at runtime when we export this linear layer: ep = export( torch.nn.Linear(4, 3), (torch.randn(1, 4),), dynamic_shapes={ \"input\": (Dim.AUTO, Dim.STATIC), }, ) try: ep.module()(torch.randn(2, 4)) except Exception: tb.print_exc() I0108 21:26:27.295000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027input\u0027].size()[0] 1 None V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027input\u0027].size()[1] 4 None V0108 21:26:27.308000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027input\u0027].stride()[0] 4 None V0108 21:26:27.309000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027input\u0027].stride()[1] 1 None V0108 21:26:27.309000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027input\u0027].storage_offset() 0 None W0108 21:26:27.311000 21876 torch/_export/non_strict_utils.py:564] dimension inputs[\u0027input\u0027].shape[0] 0/1 specialized; Dim.AUTO was specified along with a sample input with hint = 1. Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 500, in \u003cmodule\u003e ep.module()(torch.randn(2, 4)) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 837, in call_wrapped return self._wrapped_call(self, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 413, in __call__ raise e File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 400, in __call__ return super(self.cls, obj).__call__(*args, **kwargs) # type: ignore[misc] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1881, in _call_impl return inner() File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1829, in inner result = forward_call(*args, **kwargs) File \"\u003ceval_with_key\u003e.125\", line 9, in forward _guards_fn = self._guards_fn(input_1); _guards_fn = None File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 209, in inner return func(*args, **kwargs) File \"\u003cstring\u003e\", line 3, in _ File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2185, in _assert assert condition, message AssertionError: Guard failed: input.size()[0] == 1 Named Dims# So far we\u2019ve only been talking about 3 ways to specify dynamic shapes: Dim.AUTO, Dim.DYNAMIC, and Dim.STATIC. The attraction of these is the low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a \u201cdiscovery\u201d process, summarizing these guards and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels. These changes won\u2019t be detected and the export() call will most likely succeed, unless tests are in place that check the resulting ExportedProgram representation. For such cases, our stance is to recommend the \u201ctraditional\u201d way of specifying dynamic shapes, which longer-term users of export might be familiar with: named Dims: dx = Dim(\"dx\", min=4, max=256) dh = Dim(\"dh\", max=512) dynamic_shapes = { \"x\": (dx, None), \"y\": (2 * dx, dh), } This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the dynamic behavior of the ExportedProgram produced; ConstraintViolation errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic specifications given. For example, in the above specification, the following is asserted: x.shape[0] is to have range [4, 256], and related to y.shape[0] by y.shape[0] == 2 * x.shape[0]. x.shape[1] is static. y.shape[1] has range [2, 512], and is unrelated to any other dimension. In this design, we allow relations between dimensions to be specified with univariate linear expressions: A * dim + B can be specified for any dimension. This allows users to specify more complex constraints like integer divisibility for dynamic dimensions: dx = Dim(\"dx\", min=4, max=512) dynamic_shapes = { \"x\": (4 * dx, None) # x.shape[0] has range [16, 2048], and is divisible by 4. } Constraint violations, suggested fixes# One common issue with this specification style (before Dim.AUTO was introduced), is that the specification would often be mismatched with what was produced by model tracing. That would lead to ConstraintViolation errors and export suggested fixes - see for example with this model \u0026 specification, where the model inherently requires equality between dimensions 0 of x and y, and requires dimension 1 to be static. class Foo(torch.nn.Module): def forward(self, x, y): w = x + y return w + torch.ones(4) dx, dy, d1 = torch.export.dims(\"dx\", \"dy\", \"d1\") try: ep = export( Foo(), (torch.randn(6, 4), torch.randn(6, 4)), dynamic_shapes={ \"x\": (dx, d1), \"y\": (dy, d1), }, ) except Exception: tb.print_exc() I0108 21:26:27.320000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.321000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s77 = 6 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.323000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s27 = 4 for L[\u0027x\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s27\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.326000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s17 = 6 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I0108 21:26:27.327000 21876 torch/fx/experimental/symbolic_shapes.py:5114] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:232 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V0108 21:26:27.333000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.334000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.335000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.337000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.338000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] V0108 21:26:27.338000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.342000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s27, s94) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s27, s94)\" I0108 21:26:27.343000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = s27 (solve) VR[2, int_oo] I0108 21:26:27.345000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s77, s17) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s17)\" I0108 21:26:27.346000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s77 = s17 (solve) VR[2, int_oo] V0108 21:26:27.349000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == False [statically known] I0108 21:26:27.358000 21876 torch/fx/experimental/symbolic_shapes.py:7207] eval Eq(s27, 4) [guard added] (_subclasses/fake_impls.py:1148 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s27, 4)\" V0108 21:26:27.359000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s27 = VR[4, 4] (update) I0108 21:26:27.360000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s27 = 4 (range_refined_to_singleton) VR[4, 4] I0108 21:26:27.365000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range s94 = VR[4, 4] (update) I0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:6786] set_replacement s94 = 4 (find) VR[4, 4] V0108 21:26:27.366000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 4 None V0108 21:26:27.367000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[1] 1 None V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V0108 21:26:27.368000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 4 None V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[1] 1 None V0108 21:26:27.369000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1821, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1968, in _produce_guards_callback return produce_guards_and_solve_constraints( File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 533, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 500, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5204, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 5928, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. - The values of dy = L[\u0027y\u0027].size()[0] and dx = L[\u0027x\u0027].size()[0] must always be equal. Suggested fixes: d1 = 4 dy = dx During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 311, in export raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export return _export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export ep = _export_for_training( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training export_artifact = export_func( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1823, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. - The values of dy = L[\u0027y\u0027].size()[0] and dx = L[\u0027x\u0027].size()[0] must always be equal. Suggested fixes: d1 = 4 dy = dx The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards. Lastly, there\u2019s couple nice-to-knows about the options for specification: None is a good option for static behavior: - dynamic_shapes=None (default) exports with the entire model being static. - specifying None at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs. - specifying None at a dimension-level specializes that dimension, though this is deprecated in favor of Dim.STATIC. specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification. These options are combined in the inputs \u0026 dynamic shapes spec below: inputs = ( torch.randn(4, 4), torch.randn(3, 3), 16, False, ) dynamic_shapes = { \"tensor_0\": (Dim.AUTO, None), \"tensor_1\": None, \"int_val\": None, \"bool_val\": None, } Data-dependent errors# While trying to export models, you have may have encountered errors like \u201cCould not guard on data-dependent expression\u201d, or Could not extract specialized integer from data-dependent expression\u201d. These errors exist because torch.export() compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties (e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out, complaining that the value is not available. Data-dependent values appear in many places, and common sources are calls like item(), tolist(), or torch.unbind() that extract scalar values from tensors. How are these values represented in the exported program? In the Constraints/Dynamic Shapes section, we talked about allocating symbols to represent dynamic input dimensions. The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are \u201cunbacked\u201d symbols, in contrast to the \u201cbacked\u201d symbols allocated for input dimensions. The \u201cbacked/unbacked\u201d nomenclature refers to the presence/absence of a \u201chint\u201d for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed. In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties. For data-dependent values, the symbols are taken from FakeTensor \u201cdata\u201d during tracing, and so the compiler doesn\u2019t know the actual values (hints) that these symbols would take on. Let\u2019s see how these show up in exported programs: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = y.tolist() return b + [a] inps = ( torch.tensor(1), torch.tensor([2, 3]), ) ep = export(Foo(), inps) print(ep) I0108 21:26:27.377000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.384000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.384000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] I0108 21:26:27.388000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.388000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u1] I0108 21:26:27.389000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u2 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.389000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u2] I0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] 2 None V0108 21:26:27.392000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 1 None V0108 21:26:27.393000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"i64[2]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist() unbind = torch.ops.aten.unbind.int(y); y = None getitem: \"i64[]\" = unbind[0] getitem_1: \"i64[]\" = unbind[1]; unbind = None item_1: \"Sym(u1)\" = torch.ops.aten.item.default(getitem); getitem = None item_2: \"Sym(u2)\" = torch.ops.aten.item.default(getitem_1); getitem_1 = None return (item_1, item_2, item) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs item_1: USER_OUTPUT item_2: USER_OUTPUT item: USER_OUTPUT Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo]} The result is that 3 unbacked symbols (notice they\u2019re prefixed with \u201cu\u201d, instead of the usual \u201cs\u201d for input shape/backed symbols) are allocated and returned: 1 for the item() call, and 1 for each of the elements of y with the tolist() call. Note from the range constraints field that these take on ranges of [-int_oo, int_oo], not the default [0, int_oo] range allocated to input shape symbols, since we have no information on what these values are - they don\u2019t represent sizes, so don\u2019t necessarily have positive values. Guards, torch._check()# But the case above is easy to export, because the concrete values of these symbols aren\u2019t used in any compiler decision-making; all that\u2019s relevant is that the return values are unbacked symbols. The data-dependent errors highlighted in this section are cases like the following, where data-dependent guards are encountered: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() if a // 2 \u003e= 5: return y + 2 else: return y * 5 Here we actually need the \u201chint\u201d, or the concrete value of a for the compiler to decide whether to trace return y + 2 or return y * 5 as the output. Because we trace with FakeTensors, we don\u2019t know what a // 2 \u003e= 5 actually evaluates to, and export errors out with \u201cCould not guard on data-dependent expression u0 // 2 \u003e= 5 (unhinted)\u201d. So how do we export this toy model? Unlike torch.compile(), export requires full graph compilation, and we can\u2019t just graph break on this. Here are some basic options: Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using torch.compiler.is_compiling() to guard what\u2019s traced at compile-time. torch.cond(): we could rewrite the control-flow code to use torch.cond() so we don\u2019t specialize on a branch. While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and torch.cond() is not a comprehensive system for handling data-dependent errors. As we will see, there are data-dependent errors that do not involve control-flow. The generally recommended approach is to start with torch._check() calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols. While a torch._check() call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true, are stored as symbol properties (provided it\u2019s smart enough to infer those properties). So even if unbacked symbols don\u2019t have hints, if we\u2019re able to communicate properties that are generally true for these symbols via torch._check() calls, we can potentially bypass data-dependent guards without rewriting the offending model code. For example in the model above, inserting torch._check(a \u003e= 10) would tell the compiler that y + 2 can always be returned, and torch._check(a == 4) tells it to return y * 5. See what happens when we re-export this model. class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() torch._check(a \u003e= 10) torch._check(a \u003c= 60) if a // 2 \u003e= 5: return y + 2 else: return y * 5 inps = ( torch.tensor(32), torch.randn(4), ) ep = export(Foo(), inps) print(ep) I0108 21:26:27.399000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.405000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.405000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] I0108 21:26:27.407000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 \u003e= 10 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:673 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 10\" V0108 21:26:27.408000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[10, int_oo] (update) I0108 21:26:27.412000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 \u003c= 60 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:674 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003c= 60\" V0108 21:26:27.413000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[10, 60] (update) V0108 21:26:27.418000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known] I0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.422000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] 4 None V0108 21:26:27.423000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 1 None V0108 21:26:27.423000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.425000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003e= 10 == True [statically known] V0108 21:26:27.426000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003c= 60 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[4]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_2: \"Sym(u0 \u003e= 10)\" = item \u003e= 10 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u0 \u003e= 10 on node \u0027ge_2\u0027\"); ge_2 = _assert_scalar_default = None le_1: \"Sym(u0 \u003c= 60)\" = item \u003c= 60; item = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 \u003c= 60 on node \u0027le_1\u0027\"); le_1 = _assert_scalar_default_1 = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2 add: \"f32[4]\" = torch.ops.aten.add.Tensor(y, 2); y = None return (add,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {u0: VR[10, 60]} Export succeeds, and note from the range constraints field that u0 takes on a range of [10, 60]. So what information do torch._check() calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true: Equality with non-data-dependent expressions: torch._check() calls that communicate equalities like u0 == s0 + 4 or u0 == 5. Range refinement: calls that provide lower or upper bounds for symbols, like the above. Some basic reasoning around more complicated expressions: inserting torch._check(a \u003c 4) will typically tell the compiler that a \u003e= 4 is false. Checks on complex expressions like torch._check(a ** 2 - 3 * a \u003c= 10) will typically get you past identical guards. As mentioned previously, torch._check() calls have applicability outside of data-dependent control flow. For example, here\u2019s a model where torch._check() insertion prevails while manual specialization \u0026 torch.cond() do not: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() return y[a] inps = ( torch.tensor(32), torch.randn(60), ) try: export(Foo(), inps) except Exception: tb.print_exc() I0108 21:26:27.432000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.438000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.438000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] I0108 21:26:27.440000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u0 \u003e= 0 due to data dependency, it was assumed to be False with no runtime assertions (_subclasses/fake_impls.py:388 in meta_select) I0108 21:26:27.440000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u0 \u003c 0 due to data dependency, it was assumed to be False with no runtime assertions (_subclasses/fake_impls.py:390 in meta_select) I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.441000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:402 in meta_select) I0108 21:26:27.443000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate u1 \u003e= 0 due to data dependency, it was assumed to be True with no runtime assertions (utils/_stats.py:28 in wrapper) I0108 21:26:27.443000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.444000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u1] I0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.446000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] 60 None V0108 21:26:27.447000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 1 None V0108 21:26:27.447000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None Here is a scenario where torch._check() insertion is required simply to prevent an operation from failing. The export call will fail with \u201cCould not guard on data-dependent expression -u0 \u003e 60\u201d, implying that the compiler doesn\u2019t know if this is a valid indexing operation - if the value of x is out-of-bounds for y or not. Here, manual specialization is too prohibitive, and torch.cond() has no place. Instead, informing the compiler of u0\u2019s range is sufficient: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() torch._check(a \u003e= 0) torch._check(a \u003c y.shape[0]) return y[a] inps = ( torch.tensor(32), torch.randn(60), ) ep = export(Foo(), inps) print(ep) I0108 21:26:27.451000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.456000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.457000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] I0108 21:26:27.458000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 \u003e= 0 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:722 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 0\" V0108 21:26:27.458000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, int_oo] (update) I0108 21:26:27.461000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 \u003c 60 [guard added] (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:723 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003c 60\" V0108 21:26:27.462000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, 59] (update) V0108 21:26:27.464000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known] V0108 21:26:27.465000 21876 torch/fx/experimental/symbolic_shapes.py:7485] eval False == True [statically known] I0108 21:26:27.468000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] 60 None V0108 21:26:27.469000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 1 None V0108 21:26:27.470000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.471000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003e= 0 == True [statically known] V0108 21:26:27.473000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003c= 59 == True [statically known] V0108 21:26:27.474000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003c 60 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[60]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_1: \"Sym(u0 \u003e= 0)\" = item \u003e= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 \u003e= 0 on node \u0027ge_1\u0027\"); ge_1 = _assert_scalar_default = None le: \"Sym(u0 \u003c= 59)\" = item \u003c= 59 _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 \u003c= 59 on node \u0027le\u0027\"); le = _assert_scalar_default_1 = None # lt_1: \"Sym(u0 \u003c 60)\" = item \u003c 60 _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, \"Runtime assertion failed for expression u0 \u003c 60 on node \u0027lt_1\u0027\"); lt_1 = _assert_scalar_default_2 = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a] select: \"f32[]\" = torch.ops.aten.select.int(y, 0, item); y = item = None return (select,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs select: USER_OUTPUT Range constraints: {u0: VR[0, 59]} Specialized values# Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value while tracing. This looks something like \u201cCould not extract specialized integer from data-dependent expression\u201d, and is analogous to the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise with evaluating concrete boolean values. This error typically occurs when there is an explicit or implicit int() cast on a data-dependent expression. For example, this list comprehension has a range() call that implicitly does an int() cast on the size of the list: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = torch.cat([y for y in range(a)], dim=0) return b + int(a) inps = ( torch.tensor(32), torch.randn(60), ) try: export(Foo(), inps, strict=False) except Exception: tb.print_exc() I0108 21:26:27.481000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.487000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.487000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] Data dependent variable \u0027u0\u0027 allocated at: V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/bin/sphinx-build\", line 7, in \u003cmodule\u003e V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] sys.exit(main()) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 339, in main V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return make_main(argv) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 213, in make_main V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return make_mode.run_make_mode(argv[1:]) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return make.run_generic_build(args[0]) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return build_main(args + opts) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 293, in build_main V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] app = Sphinx(args.sourcedir, args.confdir, args.outputdir, V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/application.py\", line 272, in __init__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self._init_builder() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/application.py\", line 343, in _init_builder V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self.events.emit(\u0027builder-inited\u0027) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx/events.py\", line 97, in emit V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] results.append(listener.handler(self.app, *args)) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ) = generate_dir_rst( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] results = parallel( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/var/lib/workspace/conf.py\", line 85, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] p.start() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/process.py\", line 121, in start V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self._popen = self._Popen(self) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return _default_context.get_context().Process._Popen(process_obj) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/context.py\", line 281, in _Popen V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return Popen(process_obj) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self._launch(process_obj) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 71, in _launch V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] code = process_obj._bootstrap(parent_sentinel=child_r) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self.run() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] self._target(*self._args, **self._kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/var/lib/workspace/conf.py\", line 73, in call_fn V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] result = func(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] output_blocks, time_elapsed = execute_script( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] execute_code_block( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] is_last_expr, mem_max = _exec_and_get_memory( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] mem_max, _ = call_memory( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return 0.0, func() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] exec(self.code, self.fake_main.__dict__) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 756, in \u003cmodule\u003e V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] export(Foo(), inps, strict=False) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return _export( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ep = fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ep = _export_for_training( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ep = fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] export_artifact = export_func( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] aten_export_artifact = _to_aten_func( # type: ignore[operator] V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1793, in _export_to_aten_ir_make_fx V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] gm, graph_signature = transform(_make_fx_helper)( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1922, in _aot_export_non_strict V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1706, in _make_fx_helper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] gm = make_fx( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2429, in wrapped V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return make_fx_tracer.trace(f, *args) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2356, in trace V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._trace_inner(f, *args) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in _trace_inner V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] t = dispatch_trace( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 53, in inner V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return disable_fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1303, in dispatch_trace V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1908, in trace V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] res = super().trace(root, concrete_args) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 868, in trace V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] (self.create_arg(fn(*args)),), V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1361, in wrapped V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] out = f(*tensors) # type:ignore[call-arg] V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1593, in wrapped_fn V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return tuple(flat_fn(*args)) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 187, in flat_fn V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] tree_out = fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py\", line 1354, in functional_call V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] out = mod(*args[params_len:], **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self.call_module(mod, forward, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return Tracer.call_module(self, m, forward, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ret_val = forward(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return _orig_module_call(mod, *args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._call_impl(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return forward_call(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1906, in forward V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] tree_out = mod(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self.call_module(mod, forward, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return Tracer.call_module(self, m, forward, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] ret_val = forward(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return _orig_module_call(mod, *args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._call_impl(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return forward_call(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 747, in forward V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] a = x.item() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1409, in __torch_function__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return func(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1479, in __torch_function__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return func(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py\", line 1066, in __torch_function__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return func(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 962, in handler V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return torch._library.utils.handle_dispatch_mode( V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_library/utils.py\", line 286, in handle_dispatch_mode V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 28, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1534, in __torch_dispatch__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return proxy_call(self, func, self.pre_dispatch, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 994, in proxy_call V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] out = func(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 841, in __call__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._op(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 28, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return fn(*args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1376, in __torch_dispatch__ V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self.dispatch(func, types, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2096, in dispatch V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._cached_dispatch_impl(func, types, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1498, in _cached_dispatch_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return self._dispatch_impl(func, types, args, kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2725, in _dispatch_impl V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] op_impl_out = op_impl(self, func, *args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 169, in dispatch_to_op_implementations_dict V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return op_implementations_dict[func](fake_mode, func, *args, **kwargs) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 651, in local_scalar_dense V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] r = fake_mode.shape_env.create_unbacked_symint() V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py\", line 272, in wrapper V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] return retlog(fn(*args, **kwargs)) V0108 21:26:27.488000 21876 torch/fx/experimental/symbolic_shapes.py:6532] def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = item = None def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = item = None Traceback (most recent call last): File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 756, in \u003cmodule\u003e export(Foo(), inps, strict=False) File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 311, in export raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 277, in export return _export( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2255, in _export ep = _export_for_training( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1163, in wrapper raise e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1129, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\", line 124, in wrapper return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training export_artifact = export_func( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1793, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1922, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1706, in _make_fx_helper gm = make_fx( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2429, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2356, in trace return self._trace_inner(f, *args) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in _trace_inner t = dispatch_trace( File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1303, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1908, in trace res = super().trace(root, concrete_args) File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn return fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 868, in trace (self.create_arg(fn(*args)),), File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1361, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1593, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 187, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py\", line 1354, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py\", line 1906, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl return forward_call(*args, **kwargs) File \"/var/lib/workspace/intermediate_source/torch_export_tutorial.py\", line 748, in forward b = torch.cat([y for y in range(a)], dim=0) File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 449, in __index__ return self.node.int_() File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py\", line 468, in int_ return self.guard_int(\"\", 0) # NB: uses Python backtrace File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py\", line 518, in guard_int r = self.evaluate() File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate return self.shape_env.evaluate_sym_node(self, size_oblivious) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7233, in evaluate_sym_node return self.evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7333, in evaluate_expr return self._inner_evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7356, in _inner_evaluate_expr return self._evaluate_expr( File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7574, in _evaluate_expr raise self._make_data_dependent_error( torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0). (Size-like symbols: none) Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward) For more information, run with TORCH_LOGS=\"dynamic\" For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. For these errors, some basic options you have are: Avoid unnecessary int() cast calls, in this case the int(a) in the return statement. Use torch._check() calls; unfortunately all you may be able to do in this case is specialize (with torch._check(a == 60)). Rewrite the offending code at a higher level. For example, the list comprehension is semantically a repeat() op, which doesn\u2019t involve an int() cast. The following rewrite avoids data-dependent errors: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = y.unsqueeze(0).repeat(a, 1) return b + a inps = ( torch.tensor(32), torch.randn(60), ) ep = export(Foo(), inps, strict=False) print(ep) I0108 21:26:27.505000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.510000 21876 torch/fx/experimental/symbolic_shapes.py:4780] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:651 in local_scalar_dense) I0108 21:26:27.511000 21876 torch/fx/experimental/symbolic_shapes.py:1289] compute_unbacked_bindings [u0] I0108 21:26:27.514000 21876 torch/fx/experimental/symbolic_shapes.py:7207] runtime_assert u0 \u003e= 0 [guard added] (_meta_registrations.py:4109 in meta_repeat), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 0\" V0108 21:26:27.515000 21876 torch/fx/experimental/symbolic_shapes.py:6619] _update_var_to_range u0 = VR[0, int_oo] (update) V0108 21:26:27.516000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003e= 0 == True [statically known] I0108 21:26:27.519000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate Eq(u0, 0) due to data dependency, it was assumed to be False with no runtime assertions (utils/_stats.py:28 in wrapper) I0108 21:26:27.519000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.525000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate 60*u0 \u003c 2 due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:310 in is_contiguous) I0108 21:26:27.525000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.527000 21876 torch/fx/experimental/symbolic_shapes.py:7379] could not evaluate Eq(u0, 1) due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:276 in check_contiguous_sizes_strides) I0108 21:26:27.527000 21876 torch/fx/experimental/symbolic_shapes.py:7379] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:27.534000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].size()[0] 60 None V0108 21:26:27.535000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].stride()[0] 1 None V0108 21:26:27.535000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027y\u0027].storage_offset() 0 None V0108 21:26:27.537000 21876 torch/fx/experimental/symbolic_shapes.py:7700] runtime_assert u0 \u003e= 0 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[60]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item); sym_constrain_range_for_size_default = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item() ge: \"Sym(u0 \u003e= 0)\" = item \u003e= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 \u003e= 0 on node \u0027ge\u0027\"); ge = _assert_scalar_default = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1) unsqueeze: \"f32[1, 60]\" = torch.ops.aten.unsqueeze.default(y, 0); y = None repeat: \"f32[u0, 60]\" = torch.ops.aten.repeat.default(unsqueeze, [item, 1]); unsqueeze = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a add: \"f32[u0, 60]\" = torch.ops.aten.add.Tensor(repeat, item); repeat = item = None return (add,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {u0: VR[0, int_oo]} Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: torch._check_is_size(), guard_size_oblivious(), or real-tensor tracing, as starters. For more in-depth guides, please refer to the Export Programming Model, or Dealing with GuardOnDataDependentSymNode errors. Custom Ops# torch.export can export PyTorch programs with custom operators. Please refer to this page on how to author a custom operator in either C++ or Python. The following is an example of registering a custom operator in python to be used by torch.export. The important thing to note is that the custom op must have a FakeTensor kernel. @torch.library.custom_op(\"my_custom_library::custom_op\", mutates_args={}) def custom_op(x: torch.Tensor) -\u003e torch.Tensor: print(\"custom_op called!\") return torch.relu(x) @custom_op.register_fake def custom_op_meta(x): # Returns an empty tensor with the same shape as the expected output return torch.empty_like(x) Here is an example of exporting a program with the custom op. class CustomOpExample(torch.nn.Module): def forward(self, x): x = torch.sin(x) x = torch.ops.my_custom_library.custom_op(x) x = torch.cos(x) return x exported_custom_op_example = export(CustomOpExample(), (torch.randn(3, 3),)) print(exported_custom_op_example) print(exported_custom_op_example.module()(torch.randn(3, 3))) I0108 21:26:27.613000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.624000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] 3 None V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[1] 3 None V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 3 None V0108 21:26:27.625000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[1] 1 None V0108 21:26:27.626000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x) sin: \"f32[3, 3]\" = torch.ops.aten.sin.default(x); x = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x) custom_op: \"f32[3, 3]\" = torch.ops.my_custom_library.custom_op.default(sin); sin = None # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x) cos: \"f32[3, 3]\" = torch.ops.aten.cos.default(custom_op); custom_op = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {} custom_op called! tensor([[1.0000, 0.9431, 0.6418], [1.0000, 0.5546, 0.9050], [0.5543, 1.0000, 1.0000]]) Note that in the ExportedProgram, the custom operator is included in the graph. IR/Decompositions# The graph produced by torch.export returns a graph containing only ATen operators, which are the basic unit of computation in PyTorch. As there are over 3000 ATen operators, export provides a way to narrow down the operator set used in the graph based on certain characteristics, creating different IRs. By default, export produces the most generic IR which contains all ATen operators, including both functional and non-functional operators. A functional operator is one that does not contain any mutations or aliasing of the inputs. You can find a list of all ATen operators here and you can inspect if an operator is functional by checking op._schema.is_mutable, for example: print(torch.ops.aten.add.Tensor._schema.is_mutable) print(torch.ops.aten.add_.Tensor._schema.is_mutable) False True This generic IR can be used to train in eager PyTorch Autograd. This IR can be more explicitly reached through the API torch.export.export_for_training, which was introduced in PyTorch 2.5, but calling torch.export.export should produce the same graph as of PyTorch 2.6. class DecompExample(torch.nn.Module): def __init__(self) -\u003e None: super().__init__() self.conv = torch.nn.Conv2d(1, 3, 1, 1) self.bn = torch.nn.BatchNorm2d(3) def forward(self, x): x = self.conv(x) x = self.bn(x) return (x,) ep_for_training = torch.export.export_for_training(DecompExample(), (torch.randn(1, 1, 3, 3),)) print(ep_for_training.graph) /var/lib/workspace/intermediate_source/torch_export_tutorial.py:862: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent. I0108 21:26:27.636000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] 1 None V0108 21:26:27.670000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[1] 1 None V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[2] 3 None V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[3] 3 None V0108 21:26:27.671000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 9 None V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[1] 9 None V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[2] 3 None V0108 21:26:27.672000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[3] 1 None V0108 21:26:27.673000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {}) %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {}) return (batch_norm,) We can then lower this exported program to an operator set which only contains functional ATen operators through the API run_decompositions, which decomposes the ATen operators into the ones specified in the decomposition table, and functionalizes the graph. By specifying an empty set, we\u2019re only performing functionalization, and does not do any additional decompositions. This results in an IR which contains ~2000 operators (instead of the 3000 operators above), and is ideal for inference cases. ep_for_inference = ep_for_training.run_decompositions(decomp_table={}) print(ep_for_inference.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) As we can see, the previously mutable operator, torch.ops.aten.add_.default has now been replaced with torch.ops.aten.add.default, a l operator. We can also further lower this exported program to an operator set which only contains the Core ATen Operator Set, which is a collection of only ~180 operators. This IR is optimal for backends who do not want to reimplement all ATen operators. from torch.export import default_decompositions core_aten_decomp_table = default_decompositions() core_aten_ep = ep_for_training.run_decompositions(decomp_table=core_aten_decomp_table) print(core_aten_ep.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) We now see that torch.ops.aten.conv2d.default has been decomposed into torch.ops.aten.convolution.default. This is because convolution is a more \u201ccore\u201d operator, as operations like conv1d and conv2d can be implemented using the same op. We can also specify our own decomposition behaviors: my_decomp_table = torch.export.default_decompositions() def my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1): return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups) my_decomp_table[torch.ops.aten.conv2d.default] = my_awesome_custom_conv2d_function my_ep = ep_for_training.run_decompositions(my_decomp_table) print(my_ep.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {}) %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) Notice that instead of torch.ops.aten.conv2d.default being decomposed into torch.ops.aten.convolution.default, it is now decomposed into torch.ops.aten.convolution.default and torch.ops.aten.mul.Tensor, which matches our custom decomposition rule. ExportDB# torch.export will only ever export a single computation graph from a PyTorch program. Because of this requirement, there will be Python or PyTorch features that are not compatible with torch.export, which will require users to rewrite parts of their model code. We have seen examples of this earlier in the tutorial \u2013 for example, rewriting if-statements using cond. ExportDB is the standard reference that documents supported and unsupported Python/PyTorch features for torch.export. It is essentially a list a program samples, each of which represents the usage of one particular Python/PyTorch feature and its interaction with torch.export. Examples are also tagged by category so that they can be more easily searched. For example, let\u2019s use ExportDB to get a better understanding of how the predicate works in the cond operator. We can look at the example called cond_predicate, which has a torch.cond tag. The example code looks like: def cond_predicate(x): \"\"\" The conditional statement (aka predicate) passed to ``cond()`` must be one of the following: - ``torch.Tensor`` with a single element - boolean expression NOTE: If the `pred` is test on a dim with batch size \u003c 2, it will be specialized. \"\"\" pred = x.dim() \u003e 2 and x.shape[2] \u003e 10 return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x]) More generally, ExportDB can be used as a reference when one of the following occurs: Before attempting torch.export, you know ahead of time that your model uses some tricky Python/PyTorch features and you want to know if torch.export covers that feature. When attempting torch.export, there is a failure and it\u2019s unclear how to work around it. ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by torch.export. Running the Exported Program# As torch.export is only a graph capturing mechanism, calling the artifact produced by torch.export eagerly will be equivalent to running the eager module. To optimize the execution of the Exported Program, we can pass this exported artifact to backends such as Inductor through torch.compile, AOTInductor, or TensorRT. class M(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(3, 3) def forward(self, x): x = self.linear(x) return x inp = torch.randn(2, 3, device=\"cuda\") m = M().to(device=\"cuda\") ep = torch.export.export(m, (inp,)) # Run it eagerly res = ep.module()(inp) print(res) # Run it with torch.compile res = torch.compile(ep.module(), backend=\"inductor\")(inp) print(res) I0108 21:26:28.652000 21876 torch/fx/experimental/symbolic_shapes.py:3769] create_env I0108 21:26:28.665000 21876 torch/fx/experimental/symbolic_shapes.py:5242] produce_guards V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[0] 2 None V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].size()[1] 3 None V0108 21:26:28.666000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[0] 3 None V0108 21:26:28.667000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].stride()[1] 1 None V0108 21:26:28.667000 21876 torch/fx/experimental/symbolic_shapes.py:5462] track_symint L[\u0027x\u0027].storage_offset() 0 None tensor([[-7.3250e-01, 4.7538e-01, 4.9275e-01], [-2.8545e-04, 3.1366e-02, -5.0228e-02]], device=\u0027cuda:0\u0027, grad_fn=\u003cAddmmBackward0\u003e) I0108 21:26:29.812000 21876 torch/fx/experimental/symbolic_shapes.py:3769] [2/0] create_env /usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = \u0027tf32\u0027 or torch.backends.cuda.matmul.fp32_precision = \u0027ieee\u0027. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.) /usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(\u0027high\u0027)` for better performance. I0108 21:26:30.826000 21876 torch/fx/experimental/symbolic_shapes.py:5242] [2/0] produce_guards I0108 21:26:30.837000 21876 torch/fx/experimental/symbolic_shapes.py:5242] [2/0] produce_guards V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027x\u0027].size()[0] 2 None V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027x\u0027].size()[1] 3 None V0108 21:26:30.838000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027x\u0027].stride()[0] 3 None V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027x\u0027].stride()[1] 1 None V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027x\u0027].storage_offset() 0 None V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[0] 3 None V0108 21:26:30.839000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[1] 3 None V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[0] 3 None V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[1] 1 None V0108 21:26:30.840000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].storage_offset() 0 None V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].size()[0] 3 None V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].stride()[0] 1 None V0108 21:26:30.841000 21876 torch/fx/experimental/symbolic_shapes.py:5462] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].storage_offset() 0 None V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027x\u0027].size()[0] == 2 V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027x\u0027].size()[1] == 3 V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027x\u0027].stride()[0] == 3 V0108 21:26:30.842000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027x\u0027].stride()[1] == 1 V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027x\u0027].storage_offset() == 0 V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[0] == 3 V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[1] == 3 V0108 21:26:30.843000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[0] == 3 V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[1] == 1 V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].storage_offset() == 0 V0108 21:26:30.844000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].size()[0] == 3 V0108 21:26:30.845000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].stride()[0] == 1 V0108 21:26:30.845000 21876 torch/fx/experimental/symbolic_shapes.py:5675] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].storage_offset() == 0 tensor([[-7.3250e-01, 4.7538e-01, 4.9275e-01], [-2.8545e-04, 3.1366e-02, -5.0228e-02]], device=\u0027cuda:0\u0027, grad_fn=\u003cCompiledFunctionBackward\u003e) import torch._inductor # Note: these APIs are subject to change # Compile the exported program to a PT2 archive using ``AOTInductor`` with torch.no_grad(): pt2_path = torch._inductor.aoti_compile_and_package(ep) # Load and run the .so file in Python. # To load and run it in a C++ environment, see: # https://pytorch.org/docs/main/torch.compiler_aot_inductor.html aoti_compiled = torch._inductor.aoti_load_package(pt2_path) res = aoti_compiled(inp) Conclusion# We introduced torch.export, the new PyTorch 2.X way to export single computation graphs from PyTorch programs. In particular, we demonstrate several code modifications and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph. Total running time of the script: (0 minutes 8.508 seconds) Download Jupyter notebook: torch_export_tutorial.ipynb Download Python source code: torch_export_tutorial.py Download zipped: torch_export_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/torch_export_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
<script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
</body>
</body></html>