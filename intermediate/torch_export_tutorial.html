
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="2022-07-20T23:02:43+00:00" property="article:modified_time"/>
<title>torch.export Tutorial — PyTorch Tutorials 2.7.0+cu126 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css?v=536c50fe" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css?v=c9393ea6" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css?v=91adb8b6" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/documentation_options.js?v=07b0cd76"></script>
<script src="../_static/doctools.js?v=888ff710"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=f281be69"></script>
<script src="../_static/katex.min.js?v=be8ff15f"></script>
<script src="../_static/auto-render.min.js?v=ad136472"></script>
<script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/torch_export_tutorial';</script>
<link href="https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html" rel="canonical"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../recipes/torch_export_aoti_python.html" rel="next" title="torch.export AOTInductor Tutorial for Python runtime (Beta)"/>
<link href="../recipes/regional_compilation.html" rel="prev" title="Reducing torch.compile cold start compilation time with regional compilation"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
<link crossorigin="anonymous" href="/intermediate/torch_export_tutorial.html" rel="canonical"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects.
  document.addEventListener('DOMContentLoaded', function() {
    // Hide cookie banner on local environments
    if (window.location.hostname === 'localhost' ||
        window.location.hostname === '0.0.0.0' ||
        window.location.hostname === '127.0.0.1' ||
        window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<link crossorigin="anonymous" href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&amp;display=swap" rel="stylesheet"/>
<meta content="../_static/img/pytorch_seo.png" property="og:image"/>
<link crossorigin="anonymous" href="../_static/webfonts/all.min.css" rel="stylesheet"/>
<meta content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;" http-equiv="Content-Security-Policy"/>
<meta content="" name="pytorch_project"/>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<script>
   document.documentElement.setAttribute('data-version', 'v2.7.0+cu126');
 </script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView&amp;noscript=1" width="1"/>
</noscript>
<script>
   function gtag() {
    window.dataLayer.push(arguments);
   }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Jul 20, 2022" name="docbuild:last-update"/>
</head>
<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/tutorials">
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="header-container-wrapper">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Learn</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
<span class="dropdown-title">Webinars</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Community</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
<span class="dropdown-title">Landscape</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
<span class="dropdown-title">Join the Ecosystem</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
<span class="dropdown-title">Community Hub</span>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
<span class="dropdown-title">Forums</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
<span class="dropdown-title">Contributor Awards</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
<span class="dropdown-title">Community Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
<span class="dropdown-title">PyTorch Ambassadors</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Projects</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
<span class="dropdown-title">vLLM</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
<span class="dropdown-title">DeepSpeed</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
<span class="dropdown-title">Host Your Project</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span> Docs</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">Domains</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>Blogs &amp; News</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">Blog</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/announcements">
<span class="dropdown-title">Announcements</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
<span class="dropdown-title">Case Studies</span>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
</a>
</a></div>
</div></li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
<span>About</span>
</a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/members">
<span class="dropdown-title">Members</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact">
<span class="dropdown-title">Contact</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown main-menu-button">
<a data-cta="join" href="https://pytorch.org/join">
                JOIN
              </a>
</div>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
<i class="fa-solid fa-ellipsis"></i>
</a>
</div>
</div>
</div>
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="header-container-wrapper">
<div class="mobile-main-menu-header-container">
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
</a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
<li>
<a href="https://pytorch.org/webinars/">Webinars</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Community</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://landscape.pytorch.org/">Landscape</a>
</li>
<li>
<a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/community-hub/">Community Hub</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
</li>
<li>
<a href="https://pytorch.org/community-events/">Community Events</a>
</li>
<li>
<a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Projects</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/projects/vllm/">vLLM</a>
</li>
<li>
<a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
</li>
<li>
<a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li>
<a href="https://pytorch.org/announcements">Announcements</a>
</li>
<li>
<a href="https://pytorch.org/case-studies/">Case Studies</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/members">Members</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact">Contact</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="version" href="../index.html">v2.7.0+cu126</a>
</div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../unstable_index.html">
    Unstable
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://x.com/PyTorch" rel="noopener" target="_blank" title="X"><i aria-hidden="true" class="fa-brands fa-x-twitter fa-lg"></i>
<span class="sr-only">X</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/pytorch/tutorials" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://dev-discuss.pytorch.org/" rel="noopener" target="_blank" title="Discourse"><i aria-hidden="true" class="fa-brands fa-discourse fa-lg"></i>
<span class="sr-only">Discourse</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/torch/" rel="noopener" target="_blank" title="PyPi"><i aria-hidden="true" class="fa-brands fa-python fa-lg"></i>
<span class="sr-only">PyPi</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="compiled_autograd_tutorial.html">Compiled Autograd: Capturing a larger backward graph for <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compiler_set_stance_tutorial.html">Dynamic Compilation Control with <code class="docutils literal notranslate"><span class="pre">torch.compiler.set_stance</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.export</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">torch.export Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">Export a PyTorch model to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/onnx_registry_tutorial.html">Extending the ONNX Exporter Operator Support</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">Export a model with control flow to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_conv_bn_fuser.html">Building a Convolution/Batch Norm fuser with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../compilers_index.html">Compilers</a></li>
<li aria-current="page" class="breadcrumb-item active">torch.export...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" id="pytorch-article">
<!-- Hidden breadcrumb schema for SEO only -->
<div itemscope="" itemtype="https://schema.org/BreadcrumbList" style="display:none;">
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<link href="../compilers_index.html" itemprop="item"/>
<meta content="Compilers" itemprop="name"/>
<meta content="1" itemprop="position"/>
</div>
<div itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<meta content="torch.export Tutorial" itemprop="name"/>
<meta content="2" itemprop="position"/>
</div>
</div>
<script>
      if((window.location.href.indexOf("/unstable/")!= -1) && (window.location.href.indexOf("/unstable/unstable_index")< 1))
        {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function() {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/torch_export_tutorial</div>
<a data-behavior="call-to-action-event" data-response="Run in Google Colab" id="colab-link" target="_blank">
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/img/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="Download Notebook" id="notebook-link">
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
</a>
<a data-behavior="call-to-action-event" data-response="View on Github" id="github-link" target="_blank">
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/img/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</a>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-torch-export-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torch-export-tutorial">
<span id="sphx-glr-intermediate-torch-export-tutorial-py"></span><h1>torch.export Tutorial<a class="headerlink" href="#torch-export-tutorial" title="Link to this heading">#</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Oct 02, 2023 | Last Updated: Jan 27, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and its related features are in prototype status and are subject to backwards compatibility
breaking changes. This tutorial provides a snapshot of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> usage as of PyTorch 2.5.</p>
</div>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> is the PyTorch 2.X way to export PyTorch models into
standardized model representations, intended
to be run on different (i.e. Python-less) environments. The official
documentation can be found <a class="reference external" href="https://pytorch.org/docs/main/export.html">here</a>.</p>
<p>In this tutorial, you will learn how to use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> to extract
<code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>’s (i.e. single-graph representations) from PyTorch programs.
We also detail some considerations/modifications that you may need
to make in order to make your model compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p><strong>Contents</strong></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#basic-usage" id="id2">Basic Usage</a></p></li>
<li><p><a class="reference internal" href="#graph-breaks" id="id3">Graph Breaks</a></p></li>
<li><p><a class="reference internal" href="#non-strict-export" id="id4">Non-Strict Export</a></p></li>
<li><p><a class="reference internal" href="#control-flow-ops" id="id5">Control Flow Ops</a></p></li>
<li><p><a class="reference internal" href="#constraints-dynamic-shapes" id="id6">Constraints/Dynamic Shapes</a></p>
<ul>
<li><p><a class="reference internal" href="#basic-concepts-symbols-and-guards" id="id7">Basic concepts: symbols and guards</a></p></li>
<li><p><a class="reference internal" href="#specialization" id="id8">0/1 specialization</a></p></li>
<li><p><a class="reference internal" href="#named-dims" id="id9">Named Dims</a></p></li>
<li><p><a class="reference internal" href="#constraint-violations-suggested-fixes" id="id10">Constraint violations, suggested fixes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-dependent-errors" id="id11">Data-dependent errors</a></p>
<ul>
<li><p><a class="reference internal" href="#guards-torch-check" id="id12">Guards, torch._check()</a></p></li>
<li><p><a class="reference internal" href="#specialized-values" id="id13">Specialized values</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#custom-ops" id="id14">Custom Ops</a></p></li>
<li><p><a class="reference internal" href="#ir-decompositions" id="id15">IR/Decompositions</a></p></li>
<li><p><a class="reference internal" href="#exportdb" id="id16">ExportDB</a></p></li>
<li><p><a class="reference internal" href="#running-the-exported-program" id="id17">Running the Exported Program</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id18">Conclusion</a></p></li>
</ul>
</nav>
<section id="basic-usage">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Basic Usage</a><a class="headerlink" href="#basic-usage" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> extracts single-graph representations from PyTorch programs
by tracing the target function, given example inputs.
<code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is the main entry point for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p>In this tutorial, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> are practically synonymous,
though <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> generally refers to the PyTorch 2.X export process, and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code>
generally refers to the actual function call.</p>
<p>The signature of <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> traces the tensor computation graph from calling <code class="docutils literal notranslate"><span class="pre">mod(*args,</span> <span class="pre">**kwargs)</span></code>
and wraps it in an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, which can be serialized or executed later with
different inputs. To execute the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> we can call <code class="docutils literal notranslate"><span class="pre">.module()</span></code>
on it to return a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> which is callable, just like the
original program.
We will detail the <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument later in the tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MyModule</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_mod</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class 'torch.export.exported_program.ExportedProgram'&gt;
tensor([[8.3606e-02, 9.9602e-01, 4.8319e-01, 0.0000e+00, 1.0635e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00, 5.5685e-01, 2.8575e-01],
        [4.7759e-01, 4.1704e-02, 0.0000e+00, 1.2967e+00, 1.3874e+00, 3.1448e-01,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7466e-01],
        [0.0000e+00, 0.0000e+00, 2.5761e-01, 3.1085e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [6.5117e-01, 0.0000e+00, 0.0000e+00, 7.9677e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 1.4381e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1752e+00, 4.4295e-01, 0.0000e+00, 5.2602e-01,
         0.0000e+00, 6.4774e-01, 1.7094e-01, 0.0000e+00],
        [1.8159e+00, 0.0000e+00, 1.1569e-01, 1.1906e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00, 1.1688e+00, 0.0000e+00],
        [2.3181e-01, 7.0379e-01, 0.0000e+00, 0.0000e+00, 2.9194e-01, 1.2860e+00,
         7.4120e-01, 9.6724e-01, 0.0000e+00, 3.7153e-01],
        [1.4333e-04, 1.7629e+00, 0.0000e+00, 0.0000e+00, 1.6567e+00, 0.0000e+00,
         1.0776e+00, 9.6315e-01, 0.0000e+00, 0.0000e+00]],
       grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
<p>Let’s review some attributes of <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> that are of interest.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph</span></code> attribute is an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.Graph">FX graph</a>
traced from the function we exported, that is, the computation graph of all PyTorch operations.
The FX graph is in “ATen IR” meaning that it contains only “ATen-level” operations.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph_signature</span></code> attribute gives a more detailed description of the
input and output nodes in the exported graph, describing which ones are
parameters, buffers, user inputs, or user outputs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code> attributes will be covered later.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_lin_weight: "f32[10, 100]", p_lin_bias: "f32[10]", x: "f32[8, 100]", y: "f32[8, 100]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            add: "f32[8, 100]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
            linear: "f32[8, 10]" = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias);  add = p_lin_weight = p_lin_bias = None
            relu_: "f32[8, 10]" = torch.ops.aten.relu_.default(linear);  linear = None
            return (relu_,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_lin_weight'), target='lin.weight', persistent=None), InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_lin_bias'), target='lin.bias', persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='relu_'), target=None)])
Range constraints: {}
</pre></div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.export">documentation</a>
for more details.</p>
</section>
<section id="graph-breaks">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Graph Breaks</a><a class="headerlink" href="#graph-breaks" title="Link to this heading">#</a></h2>
<p>Although <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> shares components with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
the key limitation of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, especially when compared to
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, is that it does not support graph breaks. This is because
handling graph breaks involves interpreting the unsupported operation with
default Python evaluation, which is incompatible with the export use case.
Therefore, in order to make your model code compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>,
you will need to modify your code to remove graph breaks.</p>
<p>A graph break is necessary in cases such as:</p>
<ul class="simple">
<li><p>data-dependent control flow</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tb</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "f32[3, 3][3, 1]cpu"):
        l_x_ = L_x_

         # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
        sum_1: "f32[][]cpu" = l_x_.sum();  l_x_ = None
        gt: "b8[][]cpu" = sum_1 &gt; 0;  sum_1 = gt = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 122, in &lt;module&gt;
    export(Bad1(), (torch.randn(3, 3),))
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.Unsupported: Data-dependent branching
  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.
  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.
  Hint: Use `torch.cond` to express dynamic control flow.

  Developer debug context: attempted to jump with TensorVariable()


from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 116, in forward
    if x.sum() &gt; 0:

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
<ul class="simple">
<li><p>accessing tensor data with <code class="docutils literal notranslate"><span class="pre">.data</span></code></p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad2</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span><span class="o">.</span><span class="n">data</span></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad2</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>calling unsupported functions (such as many built-in functions)</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 148, in &lt;module&gt;
    export(Bad3(), (torch.randn(3, 3),))
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.Unsupported: call_id not supported for sourceless TensorVariable

from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 145, in forward
    return x + id(x)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
</section>
<section id="non-strict-export">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Non-Strict Export</a><a class="headerlink" href="#non-strict-export" title="Link to this heading">#</a></h2>
<p>To trace the program, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> uses TorchDynamo by default, a byte
code analysis engine, to symbolically analyze the Python code and build a
graph based on the results. This analysis allows <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> to provide
stronger guarantees about safety, but not all Python code is supported,
causing these graph breaks.</p>
<p>To address this issue, in PyTorch 2.3, we introduced a new mode of
exporting called non-strict mode, where we trace through the program using the
Python interpreter executing it exactly as it would in eager mode, allowing us
to skip over unsupported Python features. This is done through adding a
<code class="docutils literal notranslate"><span class="pre">strict=False</span></code> flag.</p>
<p>Looking at some of the previous examples which resulted in graph breaks:</p>
<ul class="simple">
<li><p>Calling unsupported functions (such as many built-in functions) traces</p></li>
</ul>
<p>through, but in this case, <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> gets specialized as a constant integer in
the graph. This is because <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> is not a tensor operation, so the
operation is not recorded in the graph.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">bad3_nonstrict</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1
            add: "f32[3, 3]" = torch.ops.aten.add.Tensor(x, 1);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x)
            add_1: "f32[3, 3]" = torch.ops.aten.add.Tensor(add, 140444573385008);  add = None
            return (add_1,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add_1'), target=None)])
Range constraints: {}

tensor([[1.4044e+14, 1.4044e+14, 1.4044e+14],
        [1.4044e+14, 1.4044e+14, 1.4044e+14],
        [1.4044e+14, 1.4044e+14, 1.4044e+14]])
</pre></div>
</div>
<p>However, there are still some features that require rewrites to the original
module:</p>
</section>
<section id="control-flow-ops">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Control Flow Ops</a><a class="headerlink" href="#control-flow-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> actually does support data-dependent control flow.
But these need to be expressed using control flow ops. For example,
we can fix the control flow example above using the <code class="docutils literal notranslate"><span class="pre">cond</span></code> op, like so:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1Fixed</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">true_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">false_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cond.html#torch.cond" title="torch.cond"><span class="n">torch</span><span class="o">.</span><span class="n">cond</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1Fixed</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">-</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() &gt; 0, true_fn, false_fn, [x])
            sum_1: "f32[]" = torch.ops.aten.sum.default(x)
            gt: "b8[]" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None

             # File: /usr/local/lib/python3.10/dist-packages/torch/_higher_order_ops/cond.py:137 in cond, code: return cond_op(pred, true_fn, false_fn, operands)
            true_graph_0 = self.true_graph_0
            false_graph_0 = self.false_graph_0
            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x]);  gt = true_graph_0 = false_graph_0 = x = None
            getitem: "f32[3, 3]" = cond[0];  cond = None
            return (getitem,)

        class true_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:202 in true_fn, code: return torch.sin(x)
                sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None
                return (sin,)

        class false_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:204 in false_fn, code: return torch.cos(x)
                cos: "f32[3, 3]" = torch.ops.aten.cos.default(x);  x = None
                return (cos,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='getitem'), target=None)])
Range constraints: {}

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]])
tensor([[0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403]])
</pre></div>
</div>
<p>There are limitations to <code class="docutils literal notranslate"><span class="pre">cond</span></code> that one should be aware of:</p>
<ul class="simple">
<li><p>The predicate (i.e. <code class="docutils literal notranslate"><span class="pre">x.sum()</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>) must result in a boolean or a single-element tensor.</p></li>
<li><p>The operands (i.e. <code class="docutils literal notranslate"><span class="pre">[x]</span></code>) must be tensors.</p></li>
<li><p>The branch function (i.e. <code class="docutils literal notranslate"><span class="pre">true_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">false_fn</span></code>) signature must match with the
operands and they must both return a single tensor with the same metadata (for example, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code>, etc.).</p></li>
<li><p>Branch functions cannot mutate input or global variables.</p></li>
<li><p>Branch functions cannot access closure variables, except for <code class="docutils literal notranslate"><span class="pre">self</span></code> if the function is
defined in the scope of a method.</p></li>
</ul>
<p>For more details about <code class="docutils literal notranslate"><span class="pre">cond</span></code>, check out the <a class="reference external" href="https://pytorch.org/docs/main/cond.html">cond documentation</a>.</p>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">map</span></code>, which applies a function across the first dimension
of the first tensor argument.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch._higher_order_ops.map</span><span class="w"> </span><span class="kn">import</span> <span class="nb">map</span> <span class="k">as</span> <span class="n">torch_map</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MapModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>

        <span class="k">return</span> <span class="n">torch_map</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MapModule</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_map_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">*</span><span class="n">inps</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, xs: "f32[6, 4]", y: "i64[]", z: "i64[]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:236 in forward, code: return torch_map(body, xs, y, z)
            body_graph_0 = self.body_graph_0
            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]);  body_graph_0 = xs = y = z = None
            getitem: "f32[6, 4]" = map_impl[0];  map_impl = None
            return (getitem,)

        class body_graph_0(torch.nn.Module):
            def forward(self, xs: "f32[4]", y: "i64[]", z: "i64[]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:234 in body, code: return x + y + z
                add: "f32[4]" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None
                add_1: "f32[4]" = torch.ops.aten.add.Tensor(add, z);  add = z = None
                return (add_1,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='xs'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='z'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='getitem'), target=None)])
Range constraints: {}

tensor([[10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.]])
</pre></div>
</div>
<p>Other control flow ops include <code class="docutils literal notranslate"><span class="pre">while_loop</span></code>, <code class="docutils literal notranslate"><span class="pre">associative_scan</span></code>, and
<code class="docutils literal notranslate"><span class="pre">scan</span></code>. For more documentation on each operator, please refer to
<a class="reference external" href="https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops">this page</a>.</p>
</section>
<section id="constraints-dynamic-shapes">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Constraints/Dynamic Shapes</a><a class="headerlink" href="#constraints-dynamic-shapes" title="Link to this heading">#</a></h2>
<p>This section covers dynamic behavior and representation of exported programs. Dynamic behavior is
subjective to the particular model being exported, so for the most part of this tutorial, we’ll focus
on this particular toy model (with the resulting tensor shapes annotated):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [6, 5]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [8, 4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [32]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># [8, 4]</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># [6, 3]</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># [32]</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># [32]</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a static program. One consequence of this is that at runtime,
the program won’t work on inputs with different shapes, even if they’re valid in eager mode.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">))</span>
<span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 286, in &lt;module&gt;
    ep.module()(w, x, torch.randn(3, 4), torch.randn(12))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1857, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_unlift.py", line 55, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/utils.py", line 398, in _check_input_constraints_for_graph
    raise RuntimeError(
RuntimeError: Expected input at *args[2].shape[0] to be equal to 8, but got 3
</pre></div>
</div>
<section id="basic-concepts-symbols-and-guards">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Basic concepts: symbols and guards</a><a class="headerlink" href="#basic-concepts-symbols-and-guards" title="Link to this heading">#</a></h3>
<p>To enable dynamism, <code class="docutils literal notranslate"><span class="pre">export()</span></code> provides a <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument. The easiest way to work with
dynamic shapes is using <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> and looking at the program that’s returned. Dynamic behavior is specified
at a input dimension-level; for each input we can specify a tuple of values:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export.dynamic_shapes</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<p>Before we look at the program that’s produced, let’s understand what specifying <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> entails,
and how that interacts with export. For every input dimension where a <code class="docutils literal notranslate"><span class="pre">Dim</span></code> object is specified, a symbol is
<a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">allocated</a>,
taking on a range of <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">inf]</span></code> (why not <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">inf]</span></code> or <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">inf]</span></code>? we’ll explain later in the
0/1 specialization section).</p>
<p>Export then runs model tracing, looking at each operation that’s performed by the model. Each individual operation can emit
what’s called “guards”; basically boolean condition that are required to be true for the program to be valid.
When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid;
i.e. the program’s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards
and producing a final program representation that adheres to all of these guards. Before we see this “final representation” in
an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, let’s look at the guards emitted by the toy model we’re tracing.</p>
<p>Here, each forward input tensor is annotated with the symbol allocated at the start of tracing:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s0, s1]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s2]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s3, s4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s5]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># guard: s2 == s4</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># guard: s1 == 5</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># no guard added here</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># guard: s3 * s4 == s5</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>Let’s understand each of the operations and the emitted guards:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>: This is an element-wise add with broadcasting, since <code class="docutils literal notranslate"><span class="pre">x</span></code> is a 1-d tensor and <code class="docutils literal notranslate"><span class="pre">y</span></code> a 2-d tensor. <code class="docutils literal notranslate"><span class="pre">x</span></code> is broadcasted along the last dimension of <code class="docutils literal notranslate"><span class="pre">y</span></code>, emitting the guard <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">==</span> <span class="pre">s4</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x1</span> <span class="pre">=</span> <span class="pre">self.l(w)</span></code>: Calling <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (<code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">s1]</span></code>), and a statically-shaped tensor. This emits the guard <code class="docutils literal notranslate"><span class="pre">s1</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x2</span> <span class="pre">=</span> <span class="pre">x0.flatten()</span></code>: This call actually doesn’t emit any guards! (at least none relevant to input shapes)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x3</span> <span class="pre">=</span> <span class="pre">x2</span> <span class="pre">+</span> <span class="pre">z</span></code>: <code class="docutils literal notranslate"><span class="pre">x2</span></code> has shape <code class="docutils literal notranslate"><span class="pre">[s3*s4]</span></code> after flattening, and this element-wise add emits <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">*</span> <span class="pre">s4</span> <span class="pre">==</span> <span class="pre">s5</span></code>.</p></li>
</ul>
<p>Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes
subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">5]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x:</span> <span class="pre">[s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y:</span> <span class="pre">[s3,</span> <span class="pre">s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z:</span> <span class="pre">[s2*s3]</span></code></p></li>
</ul>
<p>And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the
corresponding inputs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_l_weight: "f32[3, 5]", p_l_bias: "f32[3]", w: "f32[s0, 5]", x: "f32[s2]", y: "f32[s3, s2]", z: "f32[s2*s3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y  # [8, 4]
            add: "f32[s3, s2]" = torch.ops.aten.add.Tensor(x, y);  x = y = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward, code: x1 = self.l(w)  # [6, 3]
            linear: "f32[s0, 3]" = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias);  w = p_l_weight = p_l_bias = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten()  # [32]
            flatten: "f32[s2*s3]" = torch.ops.aten.flatten.using_ints(add);  add = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z  # [32]
            add_1: "f32[s2*s3]" = torch.ops.aten.add.Tensor(flatten, z);  flatten = z = None
            return (linear, add_1)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_l_weight'), target='l.weight', persistent=None), InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_l_bias'), target='l.bias', persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='w'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='z'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='linear'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add_1'), target=None)])
Range constraints: {s0: VR[2, int_oo], s2: VR[2, int_oo], s3: VR[2, int_oo], s2*s3: VR[4, int_oo]}
</pre></div>
</div>
<p>Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn’t
so interesting currently, since this export call doesn’t emit any guards related to symbol bounds and each base symbol has
a generic bound, but this will come up later.</p>
<p>So far, because we’ve been exporting this toy model, this experience has not been representative of how hard
it typically is to debug dynamic shapes guards &amp; issues. In most cases it isn’t obvious what guards are being emitted,
and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards
are rather intuitive.</p>
<p>In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment
variable <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS="+dynamic"</span></code>, or interactively with <code class="docutils literal notranslate"><span class="pre">torch._logging.set_logs(dynamic=10)</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-_logging sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs" title="torch._logging.set_logs"><span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span></a><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.079000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [8/0] create_env
I0806 16:46:42.081000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.082000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.083000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [8/0] runtime_assert True == True [statically known]
I0806 16:46:42.085000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.087000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.088000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.091000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.093000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0806 16:46:42.093000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [8/0] runtime_assert True == True [statically known]
V0806 16:46:42.094000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0806 16:46:42.095000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
I0806 16:46:42.096000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s4 = s2 (solve) VR[2, int_oo]
V0806 16:46:42.097000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0806 16:46:42.098000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0806 16:46:42.104000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0806 16:46:42.105000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [8/0] _update_var_to_range s1 = VR[5, 5] (update)
I0806 16:46:42.106000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0806 16:46:42.108000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0806 16:46:42.113000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0806 16:46:42.114000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0806 16:46:42.115000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0806 16:46:42.116000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [8/0] _update_var_to_range s5 = VR[4, int_oo] (update)
I0806 16:46:42.117000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s5 = s2*s3 (solve) VR[4, int_oo]
V0806 16:46:42.119000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
I0806 16:46:42.124000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [8/0] produce_guards
V0806 16:46:42.125000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].size()[0] s0 None
V0806 16:46:42.125000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].size()[1] 5 None
V0806 16:46:42.125000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].stride()[0] 5 None
V0806 16:46:42.126000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].stride()[1] 1 None
V0806 16:46:42.126000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].storage_offset() 0 None
V0806 16:46:42.126000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].size()[0] s2 None
V0806 16:46:42.127000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].stride()[0] 1 None
V0806 16:46:42.127000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.127000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].size()[0] s3 None
V0806 16:46:42.128000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].size()[1] s2 None
V0806 16:46:42.128000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].stride()[0] s2 None
V0806 16:46:42.128000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].stride()[1] 1 None
V0806 16:46:42.129000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].storage_offset() 0 None
V0806 16:46:42.129000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].size()[0] s2*s3 None
V0806 16:46:42.130000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].stride()[0] 1 None
V0806 16:46:42.130000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].storage_offset() 0 None
V0806 16:46:42.160000 25964 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Ne(s0, 1)) == True [statically known]
</pre></div>
</div>
<p>This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end
to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above;
e.g. the allocation of symbols:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">runtime_assert True == True [statically known]</span>
<span class="sd">create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>"\ncreate_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\nruntime_assert True == True [statically known]\ncreate_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\n"
</pre></div>
</div>
<p>The lines with <cite>create_symbol</cite> show when a new symbol has been allocated, and the logs also identify the tensor variable names
and dimensions they’ve been allocated for. In other lines we can also see the guards emitted:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"</span>
<span class="sd">runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"</span>
<span class="sd">runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>'\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"\n'
</pre></div>
</div>
<p>Next to the <code class="docutils literal notranslate"><span class="pre">[guard</span> <span class="pre">added]</span></code> messages, we also see the responsible user lines of code - luckily here the model is simple enough.
In many real-world cases it’s not so straightforward: high-level torch operations can have complicated fake-kernel implementations
or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate
is to follow the logs’ suggestion, and re-run with environment variable <code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="..."</span></code>, to further
attribute the guard of interest.</p>
<p><code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> is just one of the available options for interacting with <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code>; as of writing this 2 other options are available:
<code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code> simply marks a dimension static, while <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code> is similar to <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> in all
ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a
static guard is emitted on a dynamically-marked dimension:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.180000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [9/0] create_env
I0806 16:46:42.182000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.182000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.184000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [9/0] runtime_assert True == True [statically known]
I0806 16:46:42.186000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.188000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.188000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.191000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.193000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0806 16:46:42.194000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [9/0] runtime_assert True == True [statically known]
V0806 16:46:42.194000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0806 16:46:42.195000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
I0806 16:46:42.196000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s4 = s2 (solve) VR[2, int_oo]
V0806 16:46:42.198000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0806 16:46:42.199000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0806 16:46:42.204000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0806 16:46:42.205000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [9/0] _update_var_to_range s1 = VR[5, 5] (update)
I0806 16:46:42.206000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0806 16:46:42.208000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0806 16:46:42.213000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0806 16:46:42.214000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0806 16:46:42.214000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0806 16:46:42.215000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [9/0] _update_var_to_range s5 = VR[4, int_oo] (update)
I0806 16:46:42.217000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s5 = s2*s3 (solve) VR[4, int_oo]
V0806 16:46:42.218000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
I0806 16:46:42.224000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [9/0] produce_guards
V0806 16:46:42.224000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].size()[0] s0 None
V0806 16:46:42.224000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].size()[1] 5 RelaxedUnspecConstraint(warn_only=False)
V0806 16:46:42.225000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].stride()[0] 5 None
V0806 16:46:42.225000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].stride()[1] 1 None
V0806 16:46:42.225000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].storage_offset() 0 None
V0806 16:46:42.226000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].size()[0] s2 None
V0806 16:46:42.226000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].stride()[0] 1 None
V0806 16:46:42.226000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.227000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].size()[0] s3 None
V0806 16:46:42.227000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].size()[1] s2 None
V0806 16:46:42.227000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].stride()[0] s2 None
V0806 16:46:42.228000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].stride()[1] 1 None
V0806 16:46:42.228000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].storage_offset() 0 None
V0806 16:46:42.228000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].size()[0] s2*s3 None
V0806 16:46:42.229000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].stride()[0] 1 None
V0806 16:46:42.229000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].storage_offset() 0 None
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0] Error while creating guard:
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0] Name: ''
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Source: shape_env
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Create Function: SHAPE_ENV
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Guard Types: None
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Code List: None
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Object Weakref: None
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     Guarded Class Weakref: None
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0] Traceback (most recent call last):
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     return self.create_fn(builder, self)
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     return output_graph.shape_env.produce_guards_verbose(
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]     raise ConstraintViolationError(
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
E0806 16:46:42.231000 25964 torch/_guards.py:359] [9/0]   - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0] Created at:
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]     tracer = InstructionTranslator(
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]     output=OutputGraph(
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]     self.init_ambient_guards()
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0806 16:46:42.234000 25964 torch/_guards.py:361] [9/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).
</pre></div>
</div>
<p>Static guards also aren’t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape
specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is
raised when this is the case for <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"x"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">,)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"y"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.251000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [10/0] create_env
I0806 16:46:42.253000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.254000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.255000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [10/0] runtime_assert True == True [statically known]
I0806 16:46:42.258000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s2 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.258000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s3 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.261000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s4 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.264000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s3, 1)) == False [statically known]
I0806 16:46:42.268000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(s3, 4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s3, 4)"
V0806 16:46:42.269000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s3 = VR[4, 4] (update)
I0806 16:46:42.269000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s3 = 4 (range_refined_to_singleton) VR[4, 4]
V0806 16:46:42.271000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
I0806 16:46:42.277000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0806 16:46:42.278000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s1 = VR[5, 5] (update)
I0806 16:46:42.279000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0806 16:46:42.280000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0806 16:46:42.281000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [10/0] runtime_assert True == True [statically known]
V0806 16:46:42.287000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0806 16:46:42.292000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(4*s2, s4) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(4*s2, s4)"
V0806 16:46:42.293000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s4 = VR[8, int_oo] (update)
I0806 16:46:42.296000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s4 = 4*s2 (solve) VR[8, int_oo]
I0806 16:46:42.302000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [10/0] produce_guards
V0806 16:46:42.302000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].size()[0] s0 None
V0806 16:46:42.303000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].size()[1] 5 None
V0806 16:46:42.303000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].stride()[0] 5 None
V0806 16:46:42.303000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].stride()[1] 1 None
V0806 16:46:42.304000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].storage_offset() 0 None
V0806 16:46:42.304000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].size()[0] 4 None
V0806 16:46:42.304000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].stride()[0] 1 None
V0806 16:46:42.305000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.305000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].size()[0] s2 None
V0806 16:46:42.305000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].size()[1] 4 RelaxedUnspecConstraint(warn_only=False)
V0806 16:46:42.306000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].stride()[0] 4 None
V0806 16:46:42.306000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].stride()[1] 1 None
V0806 16:46:42.306000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].storage_offset() 0 None
V0806 16:46:42.307000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].size()[0] 4*s2 None
V0806 16:46:42.307000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].stride()[0] 1 None
V0806 16:46:42.307000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].storage_offset() 0 None
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0] Error while creating guard:
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0] Name: ''
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Source: shape_env
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Create Function: SHAPE_ENV
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Guard Types: None
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Code List: None
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Object Weakref: None
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     Guarded Class Weakref: None
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0] Traceback (most recent call last):
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     return self.create_fn(builder, self)
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     return output_graph.shape_env.produce_guards_verbose(
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]     raise ConstraintViolationError(
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
E0806 16:46:42.309000 25964 torch/_guards.py:359] [10/0]   - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0] Created at:
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]     tracer = InstructionTranslator(
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]     output=OutputGraph(
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]     self.init_ambient_guards()
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0806 16:46:42.311000 25964 torch/_guards.py:361] [10/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).
</pre></div>
</div>
<p>Here you might ask why export “specializes”, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because
of the symbolic shapes system described above, of symbols and guards. When <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is marked static, we don’t allocate a symbol, and compile
treating this shape as a concrete integer 4. A symbol is allocated for <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>, and so we finally emit the guard <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">==</span> <span class="pre">4</span></code>, leading to
specialization.</p>
<p>One feature of export is that during tracing, statements like asserts, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code>, and <code class="docutils literal notranslate"><span class="pre">if/else</span></code> conditions will also emit guards.
See what happens when we augment the existing model with such statements:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">assert</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">512</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
            <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>
            <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.326000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [11/0] create_env
I0806 16:46:42.328000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.329000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.330000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert True == True [statically known]
I0806 16:46:42.332000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.334000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.335000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.337000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.344000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert s0 &lt;= 512 [guard added] assert w.shape[0] &lt;= 512  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:450 in forward (_dynamo/symbolic_convert.py:669 in inner), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s0 &lt;= 512"
V0806 16:46:42.344000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s0 = VR[2, 512] (update)
I0806 16:46:42.349000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert s2 &gt;= 4 [guard added] torch._check(x.shape[0] &gt;= 4)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:451 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s2 &gt;= 4"
V0806 16:46:42.350000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s2 = VR[4, int_oo] (update)
I0806 16:46:42.356000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] eval Eq(s0, s2 + 2) [guard added] if w.shape[0] == x.shape[0] + 2:  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:452 in forward (_dynamo/variables/tensor.py:1245 in evaluate_expr), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s0, s2 + 2)"
V0806 16:46:42.357000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s0 = VR[6, 512] (update)
V0806 16:46:42.360000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s2 = VR[4, 510] (update)
I0806 16:46:42.361000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s0 = s2 + 2 (solve) VR[6, 512]
V0806 16:46:42.362000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0806 16:46:42.363000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert True == True [statically known]
V0806 16:46:42.364000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0806 16:46:42.366000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:453 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
V0806 16:46:42.367000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s4 = VR[4, 510] (update)
I0806 16:46:42.368000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s4 = s2 (solve) VR[4, 510]
V0806 16:46:42.369000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0806 16:46:42.370000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0806 16:46:42.376000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:454 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0806 16:46:42.377000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s1 = VR[5, 5] (update)
I0806 16:46:42.378000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0806 16:46:42.387000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0806 16:46:42.388000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0806 16:46:42.395000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:456 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0806 16:46:42.396000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s5 = VR[8, int_oo] (update)
I0806 16:46:42.397000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s5 = s2*s3 (solve) VR[8, int_oo]
V0806 16:46:42.398000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
V0806 16:46:42.402000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert s2 &gt;= 4 == True [statically known]
I0806 16:46:42.407000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [11/0] produce_guards
V0806 16:46:42.408000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].size()[0] s2 + 2 None
V0806 16:46:42.408000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].size()[1] 5 None
V0806 16:46:42.408000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].stride()[0] 5 None
V0806 16:46:42.409000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].stride()[1] 1 None
V0806 16:46:42.409000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].storage_offset() 0 None
V0806 16:46:42.409000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].size()[0] s2 None
V0806 16:46:42.409000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].stride()[0] 1 None
V0806 16:46:42.410000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.410000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].size()[0] s3 None
V0806 16:46:42.410000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].size()[1] s2 None
V0806 16:46:42.410000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].stride()[0] s2 None
V0806 16:46:42.411000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].stride()[1] 1 None
V0806 16:46:42.411000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].storage_offset() 0 None
V0806 16:46:42.411000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].size()[0] s2*s3 None
V0806 16:46:42.412000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].stride()[0] 1 None
V0806 16:46:42.412000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].storage_offset() 0 None
</pre></div>
</div>
<p>Each of these statements emits an additional guard, and the exported program shows the changes; <code class="docutils literal notranslate"><span class="pre">s0</span></code> is eliminated in favor of <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">+</span> <span class="pre">2</span></code>,
and <code class="docutils literal notranslate"><span class="pre">s2</span></code> now contains lower and upper bounds, reflected in <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code>.</p>
<p>For the if/else condition, you might ask why the True branch was taken, and why it wasn’t the <code class="docutils literal notranslate"><span class="pre">w.shape[0]</span> <span class="pre">!=</span> <span class="pre">x.shape[0]</span> <span class="pre">+</span> <span class="pre">2</span></code> guard that
got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken.
If different sample input shapes were provided that fail the <code class="docutils literal notranslate"><span class="pre">if</span></code> condition, export would trace and emit guards corresponding to the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch.
Additionally, you might ask why we traced only the <code class="docutils literal notranslate"><span class="pre">if</span></code> branch, and if it’s possible to maintain control-flow in your program and keep both branches
alive. For that, refer to rewriting your model code following the <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Flow</span> <span class="pre">Ops</span></code> section above.</p>
</section>
<section id="specialization">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">0/1 specialization</a><a class="headerlink" href="#specialization" title="Link to this heading">#</a></h3>
<p>Since we’re talking about guards and specializations, it’s a good time to talk about the 0/1 specialization issue we brought up earlier.
The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that
don’t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 … . This just means that you should
specify 0/1 sample inputs when you’d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens
at runtime when we export this linear layer:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),),</span>
    <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"input"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.472000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [12/0] create_env
I0806 16:46:42.485000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [12/0] produce_guards
V0806 16:46:42.485000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].size()[0] 1 None
V0806 16:46:42.486000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].size()[1] 4 None
V0806 16:46:42.486000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].stride()[0] 4 None
V0806 16:46:42.486000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].stride()[1] 1 None
V0806 16:46:42.486000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].storage_offset() 0 None
Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 500, in &lt;module&gt;
    ep.module()(torch.randn(2, 4))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1857, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_unlift.py", line 55, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/utils.py", line 398, in _check_input_constraints_for_graph
    raise RuntimeError(
RuntimeError: Expected input at *args[0].shape[0] to be equal to 1, but got 2
</pre></div>
</div>
</section>
<section id="named-dims">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Named Dims</a><a class="headerlink" href="#named-dims" title="Link to this heading">#</a></h3>
<p>So far we’ve only been talking about 3 ways to specify dynamic shapes: <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code>, <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. The attraction of these is the
low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic
dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a “discovery” process, summarizing these guards
and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or
beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at
all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels.
These changes won’t be detected and the <code class="docutils literal notranslate"><span class="pre">export()</span></code> call will most likely succeed, unless tests are in place that check the resulting <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> representation.</p>
<p>For such cases, our stance is to recommend the “traditional” way of specifying dynamic shapes, which longer-term users of export might be familiar with: named <code class="docutils literal notranslate"><span class="pre">Dims</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">dh</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dh"</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dh</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the
dynamic behavior of the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> produced; <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic
specifications given. For example, in the above specification, the following is asserted:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is to have range <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">256]</span></code>, and related to <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span></code> by <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span> <span class="pre">==</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">x.shape[0]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[1]</span></code> is static.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code> has range <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">512]</span></code>, and is unrelated to any other dimension.</p></li>
</ul>
<p>In this design, we allow relations between dimensions to be specified with univariate linear expressions: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">dim</span> <span class="pre">+</span> <span class="pre">B</span></code> can be specified for any dimension. This allows users
to specify more complex constraints like integer divisibility for dynamic dimensions:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># x.shape[0] has range [16, 2048], and is divisible by 4.</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="constraint-violations-suggested-fixes">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Constraint violations, suggested fixes</a><a class="headerlink" href="#constraint-violations-suggested-fixes" title="Link to this heading">#</a></h3>
<p>One common issue with this specification style (before <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> was introduced), is that the specification would often be mismatched with what was produced by model tracing.
That would lead to <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors and export suggested fixes - see for example with this model &amp; specification, where the model inherently requires equality between
dimensions 0 of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and requires dimension 1 to be static.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">d1</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dims" title="torch.export.dims"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">dims</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="s2">"dy"</span><span class="p">,</span> <span class="s2">"d1"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span>
        <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
            <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.613000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [13/0] create_env
I0806 16:46:42.615000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s0 = 6 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.616000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s1 = 4 for L['x'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.617000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [13/0] runtime_assert True == True [statically known]
I0806 16:46:42.619000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s2 = 6 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0806 16:46:42.620000 25964 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s3 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0806 16:46:42.624000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s1, 1)) == False [statically known]
V0806 16:46:42.624000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [13/0] runtime_assert True == True [statically known]
V0806 16:46:42.625000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0806 16:46:42.626000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s3, 1)) == False [statically known]
I0806 16:46:42.628000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s1, s3) [guard added] w = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:552 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, s3)"
I0806 16:46:42.629000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s3 = s1 (solve) VR[2, int_oo]
V0806 16:46:42.630000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
I0806 16:46:42.631000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s0, s2) [guard added] w = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:552 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s0, s2)"
I0806 16:46:42.632000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s2 = s0 (solve) VR[2, int_oo]
V0806 16:46:42.634000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Ne(s1, 1)) == True [statically known]
V0806 16:46:42.634000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Ne(s0, 1)) == True [statically known]
I0806 16:46:42.641000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s1, 4) [guard added] return w + torch.ones(4)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:553 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 4)"
V0806 16:46:42.642000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [13/0] _update_var_to_range s1 = VR[4, 4] (update)
I0806 16:46:42.642000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s1 = 4 (range_refined_to_singleton) VR[4, 4]
V0806 16:46:42.646000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [13/0] _update_var_to_range s3 = VR[4, 4] (update)
I0806 16:46:42.646000 25964 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s3 = 4 (find) VR[4, 4]
I0806 16:46:42.649000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [13/0] produce_guards
V0806 16:46:42.649000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].size()[0] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0806 16:46:42.649000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0806 16:46:42.650000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].stride()[0] 4 None
V0806 16:46:42.650000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].stride()[1] 1 None
V0806 16:46:42.650000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.651000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].size()[0] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0806 16:46:42.651000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0806 16:46:42.651000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].stride()[0] 4 None
V0806 16:46:42.652000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].stride()[1] 1 None
V0806 16:46:42.652000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].storage_offset() 0 None
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0] Error while creating guard:
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0] Name: ''
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Source: shape_env
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Create Function: SHAPE_ENV
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Guard Types: None
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Code List: None
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Object Weakref: None
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     Guarded Class Weakref: None
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0] Traceback (most recent call last):
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     return self.create_fn(builder, self)
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     return output_graph.shape_env.produce_guards_verbose(
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]     raise ConstraintViolationError(
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
E0806 16:46:42.653000 25964 torch/_guards.py:359] [13/0]   - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0] Created at:
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]     tracer = InstructionTranslator(
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]     output=OutputGraph(
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]     self.init_ambient_guards()
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0806 16:46:42.655000 25964 torch/_guards.py:361] [13/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.

Suggested fixes:
  d1 = 4
  dy = dx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 557, in &lt;module&gt;
    ep = export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.

Suggested fixes:
  d1 = 4
  dy = dx
</pre></div>
</div>
<p>The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards.</p>
<p>Lastly, there’s couple nice-to-knows about the options for specification:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> is a good option for static behavior:
- <code class="docutils literal notranslate"><span class="pre">dynamic_shapes=None</span></code> (default) exports with the entire model being static.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at a dimension-level specializes that dimension, though this is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>.</p></li>
<li><p>specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification.</p></li>
</ul>
<p>These options are combined in the inputs &amp; dynamic shapes spec below:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="mi">16</span><span class="p">,</span>
    <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"tensor_0"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"tensor_1"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"int_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"bool_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="data-dependent-errors">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Data-dependent errors</a><a class="headerlink" href="#data-dependent-errors" title="Link to this heading">#</a></h2>
<p>While trying to export models, you have may have encountered errors like “Could not guard on data-dependent expression”, or Could not extract specialized integer from data-dependent expression”.
These errors exist because <code class="docutils literal notranslate"><span class="pre">torch.export()</span></code> compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties
(e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be
unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out,
complaining that the value is not available.</p>
<p>Data-dependent values appear in many places, and common sources are calls like <code class="docutils literal notranslate"><span class="pre">item()</span></code>, <code class="docutils literal notranslate"><span class="pre">tolist()</span></code>, or <code class="docutils literal notranslate"><span class="pre">torch.unbind()</span></code> that extract scalar values from tensors.
How are these values represented in the exported program? In the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a>
section, we talked about allocating symbols to represent dynamic input dimensions.
The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are “unbacked” symbols,
in contrast to the “backed” symbols allocated for input dimensions. The <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">“backed/unbacked”</a>
nomenclature refers to the presence/absence of a “hint” for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed.</p>
<p>In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties.
For data-dependent values, the symbols are taken from FakeTensor “data” during tracing, and so the compiler doesn’t know the actual values (hints) that these symbols would take on.</p>
<p>Let’s see how these show up in exported programs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.668000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [14/0] create_env
I0806 16:46:42.673000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.673000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u0]
I0806 16:46:42.675000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u1 [-int_oo, int_oo] b = y.tolist()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.676000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u1]
I0806 16:46:42.677000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u2 [-int_oo, int_oo] b = y.tolist()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.678000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u2]
I0806 16:46:42.681000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [14/0] produce_guards
V0806 16:46:42.682000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.682000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].size()[0] 2 None
V0806 16:46:42.682000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].stride()[0] 1 None
V0806 16:46:42.683000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].storage_offset() 0 None
I0806 16:46:42.688000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u3 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.689000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u4 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.695000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u5 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.695000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u5]
I0806 16:46:42.696000 25964 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u5 = u0 (rename_unbacked_to) VR[-int_oo, int_oo]
I0806 16:46:42.697000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u6 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.698000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u6]
I0806 16:46:42.698000 25964 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u6 = u1 (rename_unbacked_to) VR[-int_oo, int_oo]
I0806 16:46:42.700000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u7 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.700000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u7]
I0806 16:46:42.701000 25964 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u7 = u2 (rename_unbacked_to) VR[-int_oo, int_oo]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "i64[2]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist()
            select: "i64[]" = torch.ops.aten.select.int(y, 0, 0)
            item_1: "Sym(u1)" = torch.ops.aten.item.default(select);  select = None
            select_1: "i64[]" = torch.ops.aten.select.int(y, 0, 1);  y = None
            item_2: "Sym(u2)" = torch.ops.aten.item.default(select_1);  select_1 = None
            return (item_1, item_2, item)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item_1'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item_2'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item'), target=None)])
Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo], u3: VR[-int_oo, int_oo], u4: VR[-int_oo, int_oo], u5: VR[-int_oo, int_oo], u6: VR[-int_oo, int_oo], u7: VR[-int_oo, int_oo]}
</pre></div>
</div>
<p>The result is that 3 unbacked symbols (notice they’re prefixed with “u”, instead of the usual “s” for input shape/backed symbols) are allocated and returned:
1 for the <code class="docutils literal notranslate"><span class="pre">item()</span></code> call, and 1 for each of the elements of <code class="docutils literal notranslate"><span class="pre">y</span></code> with the <code class="docutils literal notranslate"><span class="pre">tolist()</span></code> call.
Note from the range constraints field that these take on ranges of <code class="docutils literal notranslate"><span class="pre">[-int_oo,</span> <span class="pre">int_oo]</span></code>, not the default <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">int_oo]</span></code> range allocated to input shape symbols,
since we have no information on what these values are - they don’t represent sizes, so don’t necessarily have positive values.</p>
<section id="guards-torch-check">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Guards, torch._check()</a><a class="headerlink" href="#guards-torch-check" title="Link to this heading">#</a></h3>
<p>But the case above is easy to export, because the concrete values of these symbols aren’t used in any compiler decision-making; all that’s relevant is that the return values are unbacked symbols.
The data-dependent errors highlighted in this section are cases like the following, where <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#control-flow-static-vs-dynamic">data-dependent guards</a> are encountered:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Here we actually need the “hint”, or the concrete value of <code class="docutils literal notranslate"><span class="pre">a</span></code> for the compiler to decide whether to trace <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> or <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code> as the output.
Because we trace with FakeTensors, we don’t know what <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span></code> actually evaluates to, and export errors out with “Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span> <span class="pre">(unhinted)</span></code>”.</p>
<p>So how do we export this toy model? Unlike <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, export requires full graph compilation, and we can’t just graph break on this. Here are some basic options:</p>
<ol class="arabic simple">
<li><p>Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using <code class="docutils literal notranslate"><span class="pre">torch.compiler.is_compiling()</span></code> to guard what’s traced at compile-time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code>: we could rewrite the control-flow code to use <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> so we don’t specialize on a branch.</p></li>
</ol>
<p>While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> is not a comprehensive system for handling data-dependent errors.
As we will see, there are data-dependent errors that do not involve control-flow.</p>
<p>The generally recommended approach is to start with <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols.
While a <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true,
are stored as symbol properties (provided it’s smart enough to infer those properties). So even if unbacked symbols don’t have hints, if we’re able to communicate properties that are generally true for these symbols via
<code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls, we can potentially bypass data-dependent guards without rewriting the offending model code.</p>
<p>For example in the model above, inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&gt;=</span> <span class="pre">10)</span></code> would tell the compiler that <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> can always be returned, and <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">4)</span></code> tells it to return <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code>.
See what happens when we re-export this model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.710000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [15/0] create_env
I0806 16:46:42.713000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [15/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.714000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [15/0] compute_unbacked_bindings [u0]
I0806 16:46:42.717000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [15/0] runtime_assert u0 &gt;= 10 [guard added] torch._check(a &gt;= 10)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:673 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 10"
V0806 16:46:42.717000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [15/0] _update_var_to_range u0 = VR[10, int_oo] (update)
I0806 16:46:42.722000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [15/0] runtime_assert u0 &lt;= 60 [guard added] torch._check(a &lt;= 60)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:674 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt;= 60"
V0806 16:46:42.723000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [15/0] _update_var_to_range u0 = VR[10, 60] (update)
V0806 16:46:42.728000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [15/0] eval False == True [statically known]
V0806 16:46:42.731000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [15/0] runtime_assert u0 &gt;= 10 == True [statically known]
V0806 16:46:42.732000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [15/0] runtime_assert u0 &lt;= 60 == True [statically known]
I0806 16:46:42.734000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [15/0] produce_guards
V0806 16:46:42.735000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.735000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].size()[0] 4 None
V0806 16:46:42.735000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].stride()[0] 1 None
V0806 16:46:42.736000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].storage_offset() 0 None
I0806 16:46:42.747000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.747000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u1]
V0806 16:46:42.748000 25964 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u1 = VR[10, 60] (update)
I0806 16:46:42.748000 25964 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u1 = u0 (rename_unbacked_to) VR[10, 60]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[4]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_1: "Sym(u0 &gt;= 10)" = item &gt;= 10
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, "Runtime assertion failed for expression u0 &gt;= 10 on node 'ge_1'");  ge_1 = _assert_scalar_default = None
            le_1: "Sym(u0 &lt;= 60)" = item &lt;= 60;  item = None
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u0 &lt;= 60 on node 'le_1'");  le_1 = _assert_scalar_default_1 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2
            add: "f32[4]" = torch.ops.aten.add.Tensor(y, 2);  y = None
            return (add,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add'), target=None)])
Range constraints: {u0: VR[10, 60], u1: VR[10, 60]}
</pre></div>
</div>
<p>Export succeeds, and note from the range constraints field that <code class="docutils literal notranslate"><span class="pre">u0</span></code> takes on a range of <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">60]</span></code>.</p>
<p>So what information do <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true:</p>
<ol class="arabic simple">
<li><p>Equality with non-data-dependent expressions: <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls that communicate equalities like <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">s0</span> <span class="pre">+</span> <span class="pre">4</span></code> or <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p>Range refinement: calls that provide lower or upper bounds for symbols, like the above.</p></li>
<li><p>Some basic reasoning around more complicated expressions: inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&lt;</span> <span class="pre">4)</span></code> will typically tell the compiler that <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;=</span> <span class="pre">4</span></code> is false. Checks on complex expressions like <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">-</span> <span class="pre">3</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">&lt;=</span> <span class="pre">10)</span></code> will typically get you past identical guards.</p></li>
</ol>
<p>As mentioned previously, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls have applicability outside of data-dependent control flow. For example, here’s a model where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion
prevails while manual specialization &amp; <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> do not:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.761000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [16/0] create_env
I0806 16:46:42.765000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [16/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:701 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.765000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [16/0] compute_unbacked_bindings [u0]
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0] Data dependent variable 'u0' allocated at:
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/bin/sphinx-build", line 7, in &lt;module&gt;
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     sys.exit(main())
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 339, in main
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make_main(argv)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 213, in make_main
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make_mode.run_make_mode(argv[1:])
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 181, in run_make_mode
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make.run_generic_build(args[0])
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 169, in run_generic_build
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return build_main(args + opts)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 293, in build_main
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 272, in __init__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._init_builder()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 343, in _init_builder
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.events.emit('builder-inited')
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/events.py", line 97, in emit
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     results.append(listener.handler(self.app, *args))
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 757, in generate_gallery_rst
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ) = generate_dir_rst(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 606, in generate_dir_rst
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     results = parallel(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 607, in &lt;genexpr&gt;
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/conf.py", line 85, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     p.start()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._popen = self._Popen(self)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _default_context.get_context().Process._Popen(process_obj)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return Popen(process_obj)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._launch(process_obj)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     code = process_obj._bootstrap(parent_sentinel=child_r)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.run()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._target(*self._args, **self._kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/conf.py", line 73, in call_fn
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     result = func(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1374, in generate_file_rst
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     output_blocks, time_elapsed = execute_script(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1192, in execute_script
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     execute_code_block(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1048, in execute_code_block
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     is_last_expr, mem_max = _exec_and_get_memory(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 876, in _exec_and_get_memory
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     mem_max, _ = call_memory(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1725, in _sg_call_memory_noop
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return 0.0, func()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 794, in __call__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     exec(self.code, self.fake_main.__dict__)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 709, in &lt;module&gt;
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     export(Foo(), inps)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _export(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = _export_for_training(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     export_artifact = export_func(  # type: ignore[operator]
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     gm_torch_level = _export_to_torch_ir(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     gm_torch_level, _ = torch._dynamo.export(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     result_traced = opt_f(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._call_impl(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return forward_call(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._call_impl(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return forward_call(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._torchdynamo_orig_callable(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _compile(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     guarded_code = compile_inner(code, one_graph, hooks, transform)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return function(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _compile_inner(code, one_graph, hooks, transform)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 797, in _compile_inner
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     out_code = transform_code_object(code, transform)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1422, in transform_code_object
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     transformations(instructions, code_options)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 257, in _fn
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 715, in transform
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     tracer.run()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3500, in run
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     super().run()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     while self.step():
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.dispatch_table[inst.opcode](self, inst)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 819, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return inner_fn(self, inst)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2168, in CALL_FUNCTION
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.call_function(fn, args, {})
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1170, in call_function
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/misc.py", line 903, in call_function
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self.obj.call_method(tx, self.name, args, kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/tensor.py", line 632, in call_method
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return wrap_fx_proxy(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2302, in wrap_fx_proxy
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2368, in wrap_fx_proxy_cls
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _wrap_fx_proxy(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2464, in _wrap_fx_proxy
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3127, in get_fake_value
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ret_val = wrap_fake_exception(
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 2641, in wrap_fake_exception
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3128, in &lt;lambda&gt;
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3295, in run_node
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return getattr(args[0], node.target)(*args[1:], **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1282, in __torch_dispatch__
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self.dispatch(func, types, args, kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1823, in dispatch
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._cached_dispatch_impl(func, types, args, kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1393, in _cached_dispatch_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     output = self._dispatch_impl(func, types, args, kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2397, in _dispatch_impl
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     op_impl_out = op_impl(self, func, *args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 160, in dispatch_to_op_implementations_dict
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 422, in local_scalar_dense
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     r = fake_mode.shape_env.create_unbacked_symint()
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return retlog(fn(*args, **kwargs))
V0806 16:46:42.768000 25964 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]
W0806 16:46:42.778000 25964 torch/fx/experimental/symbolic_shapes.py:6679] [16/0] failed during evaluate_expr(-u0 &gt; 60, hint=None, size_oblivious=True, forcing_spec=False
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] failed while running evaluate_expr(*(-u0 &gt; 60, None, False, True), **{})
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] Traceback (most recent call last):
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]     return retlog(fn(*args, **kwargs))
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]     return self._evaluate_expr(
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]     raise self._make_data_dependent_error(
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] For more information, run with TORCH_LOGS="dynamic"
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] User Stack (most recent call last):
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]   (snipped, see stack below for prefix)
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]     return y[a]
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0]
E0806 16:46:42.778000 25964 torch/fx/experimental/recording.py:299] [16/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] failed while attempting to run meta for aten.select.int
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] Traceback (most recent call last):
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2427, in _dispatch_impl
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     r = func(*args, **kwargs)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 756, in __call__
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self._op(*args, **kwargs)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py", line 5278, in meta_select
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     guard_size_oblivious(-index &gt; size) or guard_size_oblivious(index &gt;= size)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 408, in guard_size_oblivious
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return expr.node.guard_size_oblivious("", 0)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 588, in guard_size_oblivious
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     r = self.evaluate(size_oblivious=True)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 510, in evaluate
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self.shape_env.evaluate_sym_node(self, size_oblivious)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6655, in evaluate_sym_node
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self.evaluate_expr(
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return retlog(fn(*args, **kwargs))
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self._evaluate_expr(
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     raise self._make_data_dependent_error(
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] For more information, run with TORCH_LOGS="dynamic"
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] User Stack (most recent call last):
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   (snipped, see stack below for prefix)
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]     return y[a]
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0806 16:46:42.780000 25964 torch/_subclasses/fake_tensor.py:2431] [16/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 709, in &lt;module&gt;
    export(Foo(), inps)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.UserError: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)

Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

User Stack (most recent call last):
  (snipped, see stack below for prefix)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
    return y[a]

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#constrain-as-size-example

from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
    return y[a]

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
<p>Here is a scenario where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion is required simply to prevent an operation from failing. The export call will fail with
“Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">-u0</span> <span class="pre">&gt;</span> <span class="pre">60</span></code>”, implying that the compiler doesn’t know if this is a valid indexing operation -
if the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> is out-of-bounds for <code class="docutils literal notranslate"><span class="pre">y</span></code> or not. Here, manual specialization is too prohibitive, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> has no place.
Instead, informing the compiler of <code class="docutils literal notranslate"><span class="pre">u0</span></code>’s range is sufficient:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">y</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.794000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [17/0] create_env
I0806 16:46:42.799000 25964 torch/fx/experimental/symbolic_shapes.py:4276] [17/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.799000 25964 torch/fx/experimental/symbolic_shapes.py:1130] [17/0] compute_unbacked_bindings [u0]
I0806 16:46:42.802000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [17/0] runtime_assert u0 &gt;= 0 [guard added] torch._check(a &gt;= 0)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:722 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0806 16:46:42.802000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [17/0] _update_var_to_range u0 = VR[0, int_oo] (update)
I0806 16:46:42.807000 25964 torch/fx/experimental/symbolic_shapes.py:6630] [17/0] runtime_assert u0 &lt; 60 [guard added] torch._check(a &lt; y.shape[0])  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:723 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt; 60"
V0806 16:46:42.808000 25964 torch/fx/experimental/symbolic_shapes.py:6071] [17/0] _update_var_to_range u0 = VR[0, 59] (update)
V0806 16:46:42.811000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval size_oblivious(-u0 &gt; 60) == False [statically known]
V0806 16:46:42.811000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval size_oblivious(u0 &gt;= 60) == False [statically known]
V0806 16:46:42.812000 25964 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval False == True [statically known]
V0806 16:46:42.815000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &gt;= 0 == True [statically known]
V0806 16:46:42.817000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &lt;= 59 == True [statically known]
V0806 16:46:42.817000 25964 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &lt; 60 == True [statically known]
I0806 16:46:42.820000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [17/0] produce_guards
V0806 16:46:42.820000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:42.821000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].size()[0] 60 None
V0806 16:46:42.821000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].stride()[0] 1 None
V0806 16:46:42.821000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].storage_offset() 0 None
I0806 16:46:42.834000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.834000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u1]
V0806 16:46:42.835000 25964 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u1 = VR[0, 59] (update)
I0806 16:46:42.835000 25964 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u1 = u0 (rename_unbacked_to) VR[0, 59]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_1: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge_1'");  ge_1 = _assert_scalar_default = None
            le_1: "Sym(u0 &lt;= 59)" = item &lt;= 59
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u0 &lt;= 59 on node 'le_1'");  le_1 = _assert_scalar_default_1 = None

             #
            lt_1: "Sym(u0 &lt; 60)" = item &lt; 60
            _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, "Runtime assertion failed for expression u0 &lt; 60 on node 'lt_1'");  lt_1 = _assert_scalar_default_2 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a]
            select: "f32[]" = torch.ops.aten.select.int(y, 0, item);  y = item = None
            return (select,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='select'), target=None)])
Range constraints: {u0: VR[0, 59], u1: VR[0, 59]}
</pre></div>
</div>
</section>
<section id="specialized-values">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Specialized values</a><a class="headerlink" href="#specialized-values" title="Link to this heading">#</a></h3>
<p>Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value
while tracing. This looks something like “Could not extract specialized integer from data-dependent expression”, and is analogous to
the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise
with evaluating concrete boolean values.</p>
<p>This error typically occurs when there is an explicit or implicit <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on a data-dependent expression. For example, this list comprehension
has a <cite>range()</cite> call that implicitly does an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on the size of the list:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">([</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.850000 25964 torch/fx/experimental/symbolic_shapes.py:3334] create_env
I0806 16:46:42.855000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.855000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u0]
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984] Data dependent variable 'u0' allocated at:
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/bin/sphinx-build", line 7, in &lt;module&gt;
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     sys.exit(main())
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 339, in main
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return make_main(argv)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 213, in make_main
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return make_mode.run_make_mode(argv[1:])
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 181, in run_make_mode
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return make.run_generic_build(args[0])
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 169, in run_generic_build
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return build_main(args + opts)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 293, in build_main
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 272, in __init__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self._init_builder()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 343, in _init_builder
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self.events.emit('builder-inited')
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/events.py", line 97, in emit
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     results.append(listener.handler(self.app, *args))
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 757, in generate_gallery_rst
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ) = generate_dir_rst(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 606, in generate_dir_rst
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     results = parallel(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 607, in &lt;genexpr&gt;
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/conf.py", line 85, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     p.start()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self._popen = self._Popen(self)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return _default_context.get_context().Process._Popen(process_obj)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return Popen(process_obj)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self._launch(process_obj)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     code = process_obj._bootstrap(parent_sentinel=child_r)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self.run()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     self._target(*self._args, **self._kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/conf.py", line 73, in call_fn
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     result = func(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1374, in generate_file_rst
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     output_blocks, time_elapsed = execute_script(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1192, in execute_script
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     execute_code_block(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1048, in execute_code_block
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     is_last_expr, mem_max = _exec_and_get_memory(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 876, in _exec_and_get_memory
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     mem_max, _ = call_memory(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1725, in _sg_call_memory_noop
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return 0.0, func()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 794, in __call__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     exec(self.code, self.fake_main.__dict__)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     export(Foo(), inps, strict=False)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return _export(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ep = fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ep = _export_for_training(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ep = fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     export_artifact = export_func(  # type: ignore[operator]
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1910, in _non_strict_export
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     aten_export_artifact = _to_aten_func(  # type: ignore[operator]
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1696, in _export_to_aten_ir_make_fx
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     gm, graph_signature = transform(_make_fx_helper)(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1840, in _aot_export_non_strict
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1616, in _make_fx_helper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     gm = make_fx(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2240, in wrapped
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return make_fx_tracer.trace(f, *args)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2178, in trace
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self._trace_inner(f, *args)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2149, in _trace_inner
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     t = dispatch_trace(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 51, in inner
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return disable_fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1174, in dispatch_trace
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1738, in trace
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     res = super().trace(root, concrete_args)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 838, in trace
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     (self.create_arg(fn(*args)),),
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1229, in wrapped
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     out = f(*tensors)  # type:ignore[call-arg]
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "&lt;string&gt;", line 1, in &lt;lambda&gt;
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1520, in wrapped_fn
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return tuple(flat_fn(*args))
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 184, in flat_fn
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     tree_out = fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 903, in functional_call
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     out = mod(*args[params_len:], **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self.call_module(mod, forward, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return Tracer.call_module(self, m, forward, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ret_val = forward(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return _orig_module_call(mod, *args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self._call_impl(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return forward_call(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1824, in forward
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     tree_out = mod(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self.call_module(mod, forward, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return Tracer.call_module(self, m, forward, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     ret_val = forward(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return _orig_module_call(mod, *args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self._call_impl(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return forward_call(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 747, in forward
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     a = x.item()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1277, in __torch_function__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1324, in __torch_function__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 683, in __torch_function__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 875, in handler
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return torch._library.utils.handle_dispatch_mode(
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_library/utils.py", line 296, in handle_dispatch_mode
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1379, in __torch_dispatch__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return proxy_call(self, func, self.pre_dispatch, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 914, in proxy_call
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     out = func(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 756, in __call__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self._op(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1282, in __torch_dispatch__
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self.dispatch(func, types, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1823, in dispatch
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return self._cached_dispatch_impl(func, types, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1393, in _cached_dispatch_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     output = self._dispatch_impl(func, types, args, kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2397, in _dispatch_impl
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     op_impl_out = op_impl(self, func, *args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 160, in dispatch_to_op_implementations_dict
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 422, in local_scalar_dense
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     r = fake_mode.shape_env.create_unbacked_symint()
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]     return retlog(fn(*args, **kwargs))
V0806 16:46:42.856000 25964 torch/fx/experimental/symbolic_shapes.py:5984]
W0806 16:46:42.865000 25964 torch/fx/experimental/symbolic_shapes.py:6679] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] failed while running evaluate_expr(*(u0, None, False, False), **{})
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] Traceback (most recent call last):
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]     return retlog(fn(*args, **kwargs))
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]     return self._evaluate_expr(
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]     raise self._make_data_dependent_error(
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward)
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] For more information, run with TORCH_LOGS="dynamic"
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299]
E0806 16:46:42.865000 25964 torch/fx/experimental/recording.py:299] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1




def forward(self, arg0_1: "i64[]", arg1_1: "f32[60]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: "Sym(u0)" = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
    export(Foo(), inps, strict=False)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1910, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1696, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1840, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1616, in _make_fx_helper
    gm = make_fx(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2240, in wrapped
    return make_fx_tracer.trace(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2178, in trace
    return self._trace_inner(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2149, in _trace_inner
    t = dispatch_trace(
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1174, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1738, in trace
    res = super().trace(root, concrete_args)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 838, in trace
    (self.create_arg(fn(*args)),),
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1229, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File "&lt;string&gt;", line 1, in &lt;lambda&gt;
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1520, in wrapped_fn
    return tuple(flat_fn(*args))
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 903, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1824, in forward
    tree_out = mod(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 748, in forward
    b = torch.cat([y for y in range(a)], dim=0)
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 431, in __index__
    return self.node.int_()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 466, in int_
    return self.guard_int("", 0)  # NB: uses Python backtrace
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 516, in guard_int
    r = self.evaluate()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 510, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6655, in evaluate_sym_node
    return self.evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
    return retlog(fn(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
    return self._evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)

Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
</pre></div>
</div>
<p>For these errors, some basic options you have are:</p>
<ol class="arabic simple">
<li><p>Avoid unnecessary <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast calls, in this case the <code class="docutils literal notranslate"><span class="pre">int(a)</span></code> in the return statement.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls; unfortunately all you may be able to do in this case is specialize (with <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">60)</span></code>).</p></li>
<li><p>Rewrite the offending code at a higher level. For example, the list comprehension is semantically a <code class="docutils literal notranslate"><span class="pre">repeat()</span></code> op, which doesn’t involve an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast. The following rewrite avoids data-dependent errors:</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.875000 25964 torch/fx/experimental/symbolic_shapes.py:3334] create_env
I0806 16:46:42.880000 25964 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0806 16:46:42.881000 25964 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u0]
I0806 16:46:42.885000 25964 torch/fx/experimental/symbolic_shapes.py:6630] runtime_assert u0 &gt;= 0 [guard added] (_refs/__init__.py:4796 in new_empty), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0806 16:46:42.885000 25964 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u0 = VR[0, int_oo] (update)
V0806 16:46:42.888000 25964 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Eq(u0, 0)) == False [statically known]
V0806 16:46:42.890000 25964 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Eq(u0, 1)) == False [statically known]
V0806 16:46:42.891000 25964 torch/fx/experimental/symbolic_shapes.py:7018] runtime_assert True == True [statically known]
I0806 16:46:42.894000 25964 torch/fx/experimental/symbolic_shapes.py:4734] produce_guards
V0806 16:46:42.894000 25964 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][0].storage_offset() 0 None
V0806 16:46:42.895000 25964 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].size()[0] 60 None
V0806 16:46:42.895000 25964 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].stride()[0] 1 None
V0806 16:46:42.895000 25964 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].storage_offset() 0 None
V0806 16:46:42.897000 25964 torch/fx/experimental/symbolic_shapes.py:7018] runtime_assert u0 &gt;= 0 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             #
            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            ge: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge'");  ge = _assert_scalar_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1)
            unsqueeze: "f32[1, 60]" = torch.ops.aten.unsqueeze.default(y, 0);  y = None
            repeat: "f32[u0, 60]" = torch.ops.aten.repeat.default(unsqueeze, [item, 1]);  unsqueeze = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a
            add: "f32[u0, 60]" = torch.ops.aten.add.Tensor(repeat, item);  repeat = item = None
            return (add,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add'), target=None)])
Range constraints: {u0: VR[0, int_oo]}
</pre></div>
</div>
<p>Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: <code class="docutils literal notranslate"><span class="pre">torch._check_is_size()</span></code>, <code class="docutils literal notranslate"><span class="pre">guard_size_oblivious()</span></code>, or real-tensor tracing, as starters.
For more in-depth guides, please refer to the <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html">Export Programming Model</a>,
or <a class="reference external" href="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs">Dealing with GuardOnDataDependentSymNode errors</a>.</p>
</section>
</section>
<section id="custom-ops">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Custom Ops</a><a class="headerlink" href="#custom-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> can export PyTorch programs with custom operators. Please
refer to <a class="reference external" href="https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html">this page</a>
on how to author a custom operator in either C++ or Python.</p>
<p>The following is an example of registering a custom operator in python to be
used by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. The important thing to note is that the custom op
must have a <a class="reference external" href="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290">FakeTensor kernel</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">"my_custom_library::custom_op"</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_op called!"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="nd">@custom_op</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op_meta</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
    <span class="c1"># Returns an empty tensor with the same shape as the expected output</span>
    <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<p>Here is an example of exporting a program with the custom op.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomOpExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">my_custom_library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">CustomOpExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_custom_op_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.911000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [18/0] create_env
I0806 16:46:42.920000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [18/0] produce_guards
V0806 16:46:42.921000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].size()[0] 3 None
V0806 16:46:42.921000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].size()[1] 3 None
V0806 16:46:42.921000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].stride()[0] 3 None
V0806 16:46:42.921000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].stride()[1] 1 None
V0806 16:46:42.922000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x)
            sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x)
            custom_op: "f32[3, 3]" = torch.ops.my_custom_library.custom_op.default(sin);  sin = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x)
            cos: "f32[3, 3]" = torch.ops.aten.cos.default(custom_op);  custom_op = None
            return (cos,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='cos'), target=None)])
Range constraints: {}

custom_op called!
tensor([[1.0000, 1.0000, 1.0000],
        [0.7787, 1.0000, 0.6369],
        [1.0000, 0.6198, 1.0000]])
</pre></div>
</div>
<p>Note that in the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, the custom operator is included in the graph.</p>
</section>
<section id="ir-decompositions">
<h2><a class="toc-backref" href="#id15" role="doc-backlink">IR/Decompositions</a><a class="headerlink" href="#ir-decompositions" title="Link to this heading">#</a></h2>
<p>The graph produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> returns a graph containing only
<a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen operators</a>, which are the
basic unit of computation in PyTorch. As there are over 3000 ATen operators,
export provides a way to narrow down the operator set used in the graph based
on certain characteristics, creating different IRs.</p>
<p>By default, export produces the most generic IR which contains all ATen
operators, including both functional and non-functional operators. A functional
operator is one that does not contain any mutations or aliasing of the inputs.
You can find a list of all ATen operators
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">here</a>
and you can inspect if an operator is functional by checking
<code class="docutils literal notranslate"><span class="pre">op._schema.is_mutable</span></code>, for example:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add_</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>False
True
</pre></div>
</div>
<p>This generic IR can be used to train in eager PyTorch Autograd. This IR can be
more explicitly reached through the API <code class="docutils literal notranslate"><span class="pre">torch.export.export_for_training</span></code>,
which was introduced in PyTorch 2.5, but calling <code class="docutils literal notranslate"><span class="pre">torch.export.export</span></code>
should produce the same graph as of PyTorch 2.6.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DecompExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span></a><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_training</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DecompExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_training</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:42.944000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [19/0] create_env
I0806 16:46:42.975000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [19/0] produce_guards
V0806 16:46:42.975000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[0] 1 None
V0806 16:46:42.976000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[1] 1 None
V0806 16:46:42.976000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[2] 3 None
V0806 16:46:42.976000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[3] 3 None
V0806 16:46:42.977000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[0] 9 None
V0806 16:46:42.977000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[1] 9 None
V0806 16:46:42.977000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[2] 3 None
V0806 16:46:42.978000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[3] 1 None
V0806 16:46:42.978000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].storage_offset() 0 None
graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {})
    return (batch_norm,)
</pre></div>
</div>
<p>We can then lower this exported program to an operator set which only contains
functional ATen operators through the API <code class="docutils literal notranslate"><span class="pre">run_decompositions</span></code>, which
decomposes the ATen operators into the ones specified in the decomposition
table, and functionalizes the graph. By specifying an empty set, we’re only
performing functionalization, and does not do any additional decompositions.
This results in an IR which contains ~2000 operators (instead of the 3000
operators above), and is ideal for inference cases.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_inference</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_inference</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>As we can see, the previously mutable operator,
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add_.default</span></code> has now been replaced with
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.default</span></code>, a l operator.</p>
<p>We can also further lower this exported program to an operator set which only
contains the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir">Core ATen Operator Set</a>,
which is a collection of only ~180 operators. This IR is optimal for backends
who do not want to reimplement all ATen operators.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">default_decompositions</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a> <span class="o">=</span> <span class="n">default_decompositions</span><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">core_aten_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">core_aten_ep</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>We now see that <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> has been decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>. This is because <code class="docutils literal notranslate"><span class="pre">convolution</span></code>
is a more “core” operator, as operations like <code class="docutils literal notranslate"><span class="pre">conv1d</span></code> and <code class="docutils literal notranslate"><span class="pre">conv2d</span></code> can be
implemented using the same op.</p>
<p>We can also specify our own decomposition behaviors:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">default_decompositions</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_awesome_custom_conv2d_function</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dilation</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">groups</span><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">conv2d</span><span class="o">.</span><span class="n">default</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_awesome_custom_conv2d_function</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">my_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">my_ep</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>Notice that instead of <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> being decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>, it is now decomposed into
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mul.Tensor</span></code>,
which matches our custom decomposition rule.</p>
</section>
<section id="exportdb">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">ExportDB</a><a class="headerlink" href="#exportdb" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will only ever export a single computation graph from a PyTorch program. Because of this requirement,
there will be Python or PyTorch features that are not compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, which will require users to
rewrite parts of their model code. We have seen examples of this earlier in the tutorial – for example, rewriting
if-statements using <code class="docutils literal notranslate"><span class="pre">cond</span></code>.</p>
<p><a class="reference external" href="https://pytorch.org/docs/main/generated/exportdb/index.html">ExportDB</a> is the standard reference that documents
supported and unsupported Python/PyTorch features for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. It is essentially a list a program samples, each
of which represents the usage of one particular Python/PyTorch feature and its interaction with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.
Examples are also tagged by category so that they can be more easily searched.</p>
<p>For example, let’s use ExportDB to get a better understanding of how the predicate works in the <code class="docutils literal notranslate"><span class="pre">cond</span></code> operator.
We can look at the example called <code class="docutils literal notranslate"><span class="pre">cond_predicate</span></code>, which has a <code class="docutils literal notranslate"><span class="pre">torch.cond</span></code> tag. The example code looks like:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cond_predicate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    The conditional statement (aka predicate) passed to ``cond()`` must be one of the following:</span>
<span class="sd">    - ``torch.Tensor`` with a single element</span>
<span class="sd">    - boolean expression</span>
<span class="sd">    NOTE: If the `pred` is test on a dim with batch size &lt; 2, it will be specialized.</span>
<span class="sd">    """</span>
    <span class="n">pred</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>
</pre></div>
</div>
<p>More generally, ExportDB can be used as a reference when one of the following occurs:</p>
<ol class="arabic simple">
<li><p>Before attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, you know ahead of time that your model uses some tricky Python/PyTorch features
and you want to know if <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> covers that feature.</p></li>
<li><p>When attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, there is a failure and it’s unclear how to work around it.</p></li>
</ol>
<p>ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach
out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
</section>
<section id="running-the-exported-program">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Running the Exported Program</a><a class="headerlink" href="#running-the-exported-program" title="Link to this heading">#</a></h2>
<p>As <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is only a graph capturing mechanism, calling the artifact
produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> eagerly will be equivalent to running the eager
module. To optimize the execution of the Exported Program, we can pass this
exported artifact to backends such as Inductor through <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AOTInductor</a>,
or <a class="reference external" href="https://pytorch.org/TensorRT/dynamo/dynamo_export.html">TensorRT</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">M</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">M</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">,))</span>

<span class="c1"># Run it eagerly</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>

<span class="c1"># Run it with torch.compile</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">(),</span> <span class="n">backend</span><span class="o">=</span><span class="s2">"inductor"</span><span class="p">)(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0806 16:46:43.899000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [20/0] create_env
I0806 16:46:43.914000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [20/0] produce_guards
V0806 16:46:43.914000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].size()[0] 2 None
V0806 16:46:43.914000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].size()[1] 3 None
V0806 16:46:43.915000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].stride()[0] 3 None
V0806 16:46:43.915000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].stride()[1] 1 None
V0806 16:46:43.915000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].storage_offset() 0 None
tensor([[0.0837, 1.4412, 0.0568],
        [0.5612, 0.1055, 0.0308]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)
I0806 16:46:45.059000 25964 torch/fx/experimental/symbolic_shapes.py:3334] [21/0] create_env
/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:236: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.

I0806 16:46:45.921000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [21/0] produce_guards
I0806 16:46:45.930000 25964 torch/fx/experimental/symbolic_shapes.py:4734] [21/0] produce_guards
V0806 16:46:45.931000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].size()[0] 2 None
V0806 16:46:45.931000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].size()[1] 3 None
V0806 16:46:45.931000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].stride()[0] 3 None
V0806 16:46:45.931000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].stride()[1] 1 None
V0806 16:46:45.932000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].storage_offset() 0 None
V0806 16:46:45.932000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 3 None
V0806 16:46:45.932000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 3 None
V0806 16:46:45.932000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 3 None
V0806 16:46:45.933000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None
V0806 16:46:45.933000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None
V0806 16:46:45.933000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 3 None
V0806 16:46:45.933000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None
V0806 16:46:45.934000 25964 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None
V0806 16:46:45.934000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].size()[0] == 2
V0806 16:46:45.934000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].size()[1] == 3
V0806 16:46:45.934000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].stride()[0] == 3
V0806 16:46:45.935000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].stride()[1] == 1
V0806 16:46:45.935000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].storage_offset() == 0
V0806 16:46:45.935000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 3
V0806 16:46:45.936000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 3
V0806 16:46:45.936000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 3
V0806 16:46:45.936000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1
V0806 16:46:45.936000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0
V0806 16:46:45.937000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 3
V0806 16:46:45.937000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1
V0806 16:46:45.937000 25964 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0
tensor([[0.0837, 1.4412, 0.0568],
        [0.5612, 0.1055, 0.0308]], device='cuda:0',
       grad_fn=&lt;CompiledFunctionBackward&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._inductor</span>

<span class="c1"># Note: these APIs are subject to change</span>
<span class="c1"># Compile the exported program to a PT2 archive using ``AOTInductor``</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt2_path</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_compile_and_package</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>

<span class="c1"># Load and run the .so file in Python.</span>
<span class="c1"># To load and run it in a C++ environment, see:</span>
<span class="c1"># https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</span>
<span class="n">aoti_compiled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_load_package</span><span class="p">(</span><span class="n">pt2_path</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <span class="n">aoti_compiled</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We introduced <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, the new PyTorch 2.X way to export single computation
graphs from PyTorch programs. In particular, we demonstrate several code modifications
and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 4.549 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-torch-export-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fb8083290582c4f473d970913a4186c4/torch_export_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torch_export_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b865be3c401c1b4fbdb03f49916ac8e8/torch_export_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torch_export_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/eb8d1a44ecbb6f9b3f0732396942ffcd/torch_export_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">torch_export_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</article>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item">
<div class="feedback">
<div class="rating">
    Rate this Page
    <div class="stars">
<span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
<span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
</div>
</div>
<div class="feedback-send">
<button class="feedback-btn" data-bs-placement="bottom" data-bs-title="Create a GitHub Issue" data-bs-toggle="tooltip" data-gtm="feedback-btn-click" onclick="openGitHubIssue()">Send Feedback
    </button>
</div>
</div>
<div class="prev-next-area">
<a class="left-prev" href="../recipes/regional_compilation.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
</div>
</a>
<a class="right-next" href="../recipes/torch_export_aoti_python.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
<div class="footer-info">
<p class="copyright">
    
      
        © Copyright 2024, PyTorch.
      
      <br/>
</p>
<p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
</div>
</footer>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../recipes/regional_compilation.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
</div>
</a>
<a class="right-next" href="../recipes/torch_export_aoti_python.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-breaks">Graph Breaks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-strict-export">Non-Strict Export</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-flow-ops">Control Flow Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts-symbols-and-guards">Basic concepts: symbols and guards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialization">0/1 specialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#named-dims">Named Dims</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constraint-violations-suggested-fixes">Constraint violations, suggested fixes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-dependent-errors">Data-dependent errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guards-torch-check">Guards, torch._check()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-values">Specialized values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-ops">Custom Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-decompositions">IR/Decompositions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exportdb">ExportDB</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-exported-program">Running the Exported Program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="sidebar-heading">PyTorch Libraries</div>
<ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
<li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
</ul>
</div>
</div>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="newsletter" id="newsletter">
<p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
<script charset="utf-8" src="//js.hsforms.net/forms/embed/v2.js" type="text/javascript"></script>
<script>
            hbspt.forms.create({
              region: "na1",
              portalId: "8112310",
              formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
            });
          </script>
<p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
</div>
<div class="lf-grid">
<ul class="social-links">
<li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
<svg aria-label="Facebook" viewbox="-0.51 -0.26 26.45 26.45" xmlns="http://www.w3.org/2000/svg"><path d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
<svg aria-label="X" viewbox="0 0 300 300" xmlns="http://www.w3.org/2000/svg"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
<svg aria-label="YouTube" viewbox="0.21 0.27 34.45 25.07" xmlns="http://www.w3.org/2000/svg"><path d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
<svg aria-label="LinkedIn" viewbox="-10.23 -10.23 531.96 531.96" xmlns="http://www.w3.org/2000/svg"><rect fill="currentColor" height="512" rx="0" width="512"></rect><circle cx="142" cy="138" fill="#000" r="37"></circle><path d="M244 194v198M142 194v198" stroke="#000" stroke-width="66"></path><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" fill="#000"></path></svg>
</a></li>
<li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
<svg aria-label="Slack" viewbox="0.16 -0.03 21.19 21.19" xmlns="http://www.w3.org/2000/svg"><path d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z" fill="currentColor"></path></svg>
</a></li>
<li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
<svg aria-label="WeChat" viewbox="0.14 -0.17 38.02 33.02" xmlns="http://www.w3.org/2000/svg"><path d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z" fill="currentColor"></path><path d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z" fill="currentColor"></path></svg>
</a></li>
</ul>
</div>
<div class="privacy-policy">
<div class="copyright">
<p>
            © PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
          </p>
</div>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/img/pytorch-x.svg"/>
</div>
</div>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024, PyTorch.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
</div>
</div>
</footer>
<script type="application/ld+json">
      {
         "@context": "https://schema.org",
         "@type": "Article",
         "headline": "torch.export Tutorial",
         "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
         "url": "/intermediate/torch_export_tutorial.html",
         "author": {
           "@type": "Organization",
           "name": "PyTorch Contributors",
           "url": "https://pytorch.org"
         },
         "image": "../_static/img/pytorch_seo.png",
         "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "/intermediate/torch_export_tutorial.html"
         },
        "datePublished": "",
         "dateModified": "",
         "articleBody": "Note Go to the end to download the full example code. torch.export Tutorial# Author: William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan Warning torch.export and its related features are in prototype status and are subject to backwards compatibility breaking changes. This tutorial provides a snapshot of torch.export usage as of PyTorch 2.5. torch.export() is the PyTorch 2.X way to export PyTorch models into standardized model representations, intended to be run on different (i.e. Python-less) environments. The official documentation can be found here. In this tutorial, you will learn how to use torch.export() to extract ExportedProgram’s (i.e. single-graph representations) from PyTorch programs. We also detail some considerations/modifications that you may need to make in order to make your model compatible with torch.export. Contents Basic Usage Graph Breaks Non-Strict Export Control Flow Ops Constraints/Dynamic Shapes Basic concepts: symbols and guards 0/1 specialization Named Dims Constraint violations, suggested fixes Data-dependent errors Guards, torch._check() Specialized values Custom Ops IR/Decompositions ExportDB Running the Exported Program Conclusion Basic Usage# torch.export extracts single-graph representations from PyTorch programs by tracing the target function, given example inputs. torch.export.export() is the main entry point for torch.export. In this tutorial, torch.export and torch.export.export() are practically synonymous, though torch.export generally refers to the PyTorch 2.X export process, and torch.export.export() generally refers to the actual function call. The signature of torch.export.export() is: export( mod: torch.nn.Module, args: Tuple[Any, ...], kwargs: Optional[Dict[str, Any]] = None, *, dynamic_shapes: Optional[Dict[str, Dict[int, Dim]]] = None ) -&gt; ExportedProgram torch.export.export() traces the tensor computation graph from calling mod(*args, **kwargs) and wraps it in an ExportedProgram, which can be serialized or executed later with different inputs. To execute the ExportedProgram we can call .module() on it to return a torch.nn.Module which is callable, just like the original program. We will detail the dynamic_shapes argument later in the tutorial. import torch from torch.export import export class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.lin = torch.nn.Linear(100, 10) def forward(self, x, y): return torch.nn.functional.relu(self.lin(x + y), inplace=True) mod = MyModule() exported_mod = export(mod, (torch.randn(8, 100), torch.randn(8, 100))) print(type(exported_mod)) print(exported_mod.module()(torch.randn(8, 100), torch.randn(8, 100))) &lt;class &#39;torch.export.exported_program.ExportedProgram&#39;&gt; tensor([[8.3606e-02, 9.9602e-01, 4.8319e-01, 0.0000e+00, 1.0635e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5685e-01, 2.8575e-01], [4.7759e-01, 4.1704e-02, 0.0000e+00, 1.2967e+00, 1.3874e+00, 3.1448e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7466e-01], [0.0000e+00, 0.0000e+00, 2.5761e..."
       }
   </script>
</body>
</body></html>