
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>torch.export Tutorial — PyTorch Tutorials 2.7.0+cu126 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom2.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
<!-- End Google Tag Manager -->
<script src="../_static/js/modernizr.min.js"></script>
<!-- Preload the theme fonts -->
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
<!-- Preload the katex fonts -->
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
</head>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="container">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                Learn
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/get-started">
<span class="dropdown-title">Get Started</span>
<p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
<span class="dropdown-title">Tutorials</span>
<p>Whats new in PyTorch tutorials</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
<span class="dropdown-title">Learn the Basics</span>
<p>Familiarize yourself with PyTorch concepts and modules</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
<span class="dropdown-title">PyTorch Recipes</span>
<p>Bite-size, ready-to-deploy PyTorch code examples</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
<span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
<p>Master PyTorch basics with our engaging YouTube tutorial series</p>
</a>
</div>
</div>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                Ecosystem
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
<span class="dropdown-title">Tools</span>
<p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
<span class="dropdown-title">Community</span>
<p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
<span class="dropdown-title">Forums</span>
<p>A place to discuss PyTorch code, issues, install, research</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
<p>Find resources and get questions answered</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
<span class="dropdown-title">Contributor Awards - 2024</span>
<p>Award winners announced at this year's PyTorch Conference</p>
</a>
</div>
</div>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                Edge
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/edge">
<span class="dropdown-title">About PyTorch Edge</span>
<p>Build innovative and privacy-aware AI experiences for edge devices</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
<span class="dropdown-title">ExecuTorch</span>
<p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
<span class="dropdown-title">ExecuTorch Docs</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                Docs
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
<p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
<span class="dropdown-title">PyTorch Domains</span>
<p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
</a>
</div>
</div>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                Blogs &amp; News 
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/blog/">
<span class="dropdown-title">PyTorch Blog</span>
<p>Catch up on the latest technical news and happenings</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
<span class="dropdown-title">Community Blog</span>
<p>Stories from the PyTorch ecosystem</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/videos">
<span class="dropdown-title">Videos</span>
<p>Learn about the latest PyTorch tutorials, new, and more </p>
<a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
<span class="dropdown-title">Community Stories</span>
<p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
<p>Find events, webinars, and podcasts</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
<span class="dropdown-title">Newsletter</span>
<p>Stay up-to-date with the latest updates</p>
</a>
</a></div>
</div></li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="with-down-arrow">
                About
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
<p>Learn more about the PyTorch Foundation</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
<span class="dropdown-title">Governing Board</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/credits">
<span class="dropdown-title">Cloud Credit Program</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/tac">
<span class="dropdown-title">Technical Advisory Council</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/staff">
<span class="dropdown-title">Staff</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
<span class="dropdown-title">Contact Us</span>
</a>
</div>
</div>
</li>
<li class="main-menu-item">
<div class="no-dropdown">
<a data-cta="join" href="https://pytorch.org/join">
                Become a Member
              </a>
</div>
</li>
<li>
<div class="main-menu-item">
<a class="github-icon" href="https://github.com/pytorch/pytorch">
</a>
</div>
</li>
<!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#"></a>
</div>
</div>
</div>
<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div class="version">
        2.7.0+cu126
      </div>
<!-- Search box -->
<div id="searchBox">
<div class="searchbox" id="googleSearchBox">
<script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<div class="gcse-search"></div>
</div>
<div id="sphinxSearchBox" style="display: none;">
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
</div>
<form id="searchForm">
<label style="margin-bottom: 1rem">
<input checked="" name="searchType" type="radio" value="google"/>
      Google Search
    </label>
<label style="margin-bottom: 1rem">
<input name="searchType" type="radio" value="sphinx"/>
      Classic Search
    </label>
</form>
<script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType = 'google';
      if (window.location.href.startsWith('https://docs-preview.pytorch.org/')) {
        defaultSearchType = 'sphinx';
      } else {
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>
</div>
<p class="caption" role="heading"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">See All Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prototype/prototype_index.html">See All Prototype Recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/intro.html">Learn the Basics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/quickstart_tutorial.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/tensorqs_tutorial.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/transforms_tutorial.html">Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/buildmodel_tutorial.html">Build the Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/autogradqs_tutorial.html">Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/optimization_tutorial.html">Optimizing Model Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/basics/saveloadrun_tutorial.html">Save and Load the Model</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/introyt_index.html">Introduction to PyTorch - YouTube Series</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/introyt1_tutorial.html">Introduction to PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/tensors_deeper_tutorial.html">Introduction to PyTorch Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_from_scratch_index.html">NLP from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="pinmem_nonblock.html">A guide on good usage of <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> and <code class="docutils literal notranslate"><span class="pre">pin_memory()</span></code> in PyTorch</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="tiatoolbox_tutorial.html">Whole Slide Image Classification Using PyTorch and TIAToolbox</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Audio</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="forced_alignment_with_torchaudio_tutorial.html">Forced Alignment with Wav2Vec2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backends</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="realtime_rpi.html">Real Time Inference on Raspberry Pi 4 (30 fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Profiling PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_trace_diff_tutorial.html">Trace Diff using Holistic Trace Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="fx_conv_bn_fuser.html">(beta) Building a Convolution/Batch Norm fuser in FX</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/custom_ops_landing_page.html">PyTorch Custom Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/python_custom_ops.html">Custom Python Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_custom_ops.html">Custom C++ and CUDA Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hyperparameter_tuning_tutorial.html">Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="compiled_autograd_tutorial.html">Compiled Autograd: Capturing a larger backward graph for <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel (FSDP2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_advanced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="TCPStore_libuv_backend.html">Introduction to Libuv TCPStore Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelining_tutorial.html">Introduction to Distributed Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Edge with ExecuTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html">Exporting to ExecuTorch Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href=" https://pytorch.org/executorch/stable/running-a-model-cpp-tutorial.html">Running an ExecuTorch Model in C++ Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.pytorch.org/executorch/main/tutorials/devtools-integration-tutorial.html">Using the ExecuTorch SDK to Profile a Model</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/pytorch-labs/executorch-examples/tree/main/mv3/apple/ExecuTorchDemo">Building an ExecuTorch iOS Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/pytorch-labs/executorch-examples/tree/main/dl3/android/DeepLabV3Demo#executorch-android-demo-app">Building an ExecuTorch Android Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html">Lowering a Model as a Delegate</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchrec_intro_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/flava_finetuning_tutorial.html">TorchMultimodal Tutorial: Finetuning FLAVA</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>
<li>torch.export Tutorial</li>
<li class="pytorch-breadcrumbs-aside">
<a href="../_sources/intermediate/torch_export_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"/></a>
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/torch_export_tutorial</div>
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</div>
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</div>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div class="rst-content">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-intermediate-torch-export-tutorial-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="torch-export-tutorial">
<span id="sphx-glr-intermediate-torch-export-tutorial-py"></span><h1>torch.export Tutorial<a class="headerlink" href="#torch-export-tutorial" title="Permalink to this heading">¶</a></h1><p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Oct 02, 2023 | Last Updated: Jan 27, 2025 | Last Verified: Nov 05, 2024</p>
<p><strong>Author:</strong> William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and its related features are in prototype status and are subject to backwards compatibility
breaking changes. This tutorial provides a snapshot of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> usage as of PyTorch 2.5.</p>
</div>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> is the PyTorch 2.X way to export PyTorch models into
standardized model representations, intended
to be run on different (i.e. Python-less) environments. The official
documentation can be found <a class="reference external" href="https://pytorch.org/docs/main/export.html">here</a>.</p>
<p>In this tutorial, you will learn how to use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> to extract
<code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>’s (i.e. single-graph representations) from PyTorch programs.
We also detail some considerations/modifications that you may need
to make in order to make your model compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p><strong>Contents</strong></p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#basic-usage" id="id2">Basic Usage</a></p></li>
<li><p><a class="reference internal" href="#graph-breaks" id="id3">Graph Breaks</a></p></li>
<li><p><a class="reference internal" href="#non-strict-export" id="id4">Non-Strict Export</a></p></li>
<li><p><a class="reference internal" href="#control-flow-ops" id="id5">Control Flow Ops</a></p></li>
<li><p><a class="reference internal" href="#constraints-dynamic-shapes" id="id6">Constraints/Dynamic Shapes</a></p>
<ul>
<li><p><a class="reference internal" href="#basic-concepts-symbols-and-guards" id="id7">Basic concepts: symbols and guards</a></p></li>
<li><p><a class="reference internal" href="#specialization" id="id8">0/1 specialization</a></p></li>
<li><p><a class="reference internal" href="#named-dims" id="id9">Named Dims</a></p></li>
<li><p><a class="reference internal" href="#constraint-violations-suggested-fixes" id="id10">Constraint violations, suggested fixes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-dependent-errors" id="id11">Data-dependent errors</a></p>
<ul>
<li><p><a class="reference internal" href="#guards-torch-check" id="id12">Guards, torch._check()</a></p></li>
<li><p><a class="reference internal" href="#specialized-values" id="id13">Specialized values</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#custom-ops" id="id14">Custom Ops</a></p></li>
<li><p><a class="reference internal" href="#ir-decompositions" id="id15">IR/Decompositions</a></p></li>
<li><p><a class="reference internal" href="#exportdb" id="id16">ExportDB</a></p></li>
<li><p><a class="reference internal" href="#running-the-exported-program" id="id17">Running the Exported Program</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id18">Conclusion</a></p></li>
</ul>
</div>
<div class="section" id="basic-usage">
<h2><a class="toc-backref" href="#id2">Basic Usage</a><a class="headerlink" href="#basic-usage" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> extracts single-graph representations from PyTorch programs
by tracing the target function, given example inputs.
<code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is the main entry point for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p>In this tutorial, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> are practically synonymous,
though <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> generally refers to the PyTorch 2.X export process, and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code>
generally refers to the actual function call.</p>
<p>The signature of <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> traces the tensor computation graph from calling <code class="docutils literal notranslate"><span class="pre">mod(*args,</span> <span class="pre">**kwargs)</span></code>
and wraps it in an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, which can be serialized or executed later with
different inputs. To execute the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> we can call <code class="docutils literal notranslate"><span class="pre">.module()</span></code>
on it to return a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> which is callable, just like the
original program.
We will detail the <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument later in the tutorial.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MyModule</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_mod</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class 'torch.export.exported_program.ExportedProgram'&gt;
tensor([[0.0603, 0.1089, 0.0000, 0.9743, 0.0000, 0.0000, 0.2439, 0.9234, 0.4352,
         0.0000],
        [0.0024, 0.5681, 0.0000, 0.5269, 0.0145, 0.7523, 0.2326, 1.2928, 0.0000,
         0.0000],
        [0.8073, 1.0314, 0.0000, 0.2576, 0.0000, 0.0000, 0.7143, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0843,
         0.0000],
        [0.1064, 0.9259, 0.7808, 0.0000, 0.2491, 0.0000, 1.5726, 0.0000, 0.0000,
         0.3021],
        [0.2130, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1342, 0.0000, 0.5455,
         0.0000],
        [0.1459, 0.0000, 0.0000, 0.0267, 0.3038, 0.5913, 0.8987, 0.6035, 0.6807,
         1.4537],
        [0.2762, 0.0000, 0.0000, 0.2972, 0.0000, 0.3529, 0.0000, 0.5298, 0.0016,
         0.0000]], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
<p>Let’s review some attributes of <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> that are of interest.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph</span></code> attribute is an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.Graph">FX graph</a>
traced from the function we exported, that is, the computation graph of all PyTorch operations.
The FX graph is in “ATen IR” meaning that it contains only “ATen-level” operations.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph_signature</span></code> attribute gives a more detailed description of the
input and output nodes in the exported graph, describing which ones are
parameters, buffers, user inputs, or user outputs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code> attributes will be covered later.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_mod</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_lin_weight: "f32[10, 100]", p_lin_bias: "f32[10]", x: "f32[8, 100]", y: "f32[8, 100]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            add: "f32[8, 100]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
            linear: "f32[8, 10]" = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias);  add = p_lin_weight = p_lin_bias = None
            relu_: "f32[8, 10]" = torch.ops.aten.relu_.default(linear);  linear = None
            return (relu_,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_lin_weight'), target='lin.weight', persistent=None), InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_lin_bias'), target='lin.bias', persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='relu_'), target=None)])
Range constraints: {}
</pre></div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.export">documentation</a>
for more details.</p>
</div>
<div class="section" id="graph-breaks">
<h2><a class="toc-backref" href="#id3">Graph Breaks</a><a class="headerlink" href="#graph-breaks" title="Permalink to this heading">¶</a></h2>
<p>Although <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> shares components with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
the key limitation of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, especially when compared to
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, is that it does not support graph breaks. This is because
handling graph breaks involves interpreting the unsupported operation with
default Python evaluation, which is incompatible with the export use case.
Therefore, in order to make your model code compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>,
you will need to modify your code to remove graph breaks.</p>
<p>A graph break is necessary in cases such as:</p>
<ul class="simple">
<li><p>data-dependent control flow</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tb</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "f32[3, 3][3, 1]cpu"):
        l_x_ = L_x_

         # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
        sum_1: "f32[][]cpu" = l_x_.sum();  l_x_ = None
        gt: "b8[][]cpu" = sum_1 &gt; 0;  sum_1 = gt = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 122, in &lt;module&gt;
    export(Bad1(), (torch.randn(3, 3),))
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.Unsupported: Data-dependent branching
  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.
  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.
  Hint: Use `torch.cond` to express dynamic control flow.

  Developer debug context: attempted to jump with TensorVariable()


from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 116, in forward
    if x.sum() &gt; 0:

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
<ul class="simple">
<li><p>accessing tensor data with <code class="docutils literal notranslate"><span class="pre">.data</span></code></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad2</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span><span class="o">.</span><span class="n">data</span></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad2</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>calling unsupported functions (such as many built-in functions)</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 148, in &lt;module&gt;
    export(Bad3(), (torch.randn(3, 3),))
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.Unsupported: call_id not supported for sourceless TensorVariable

from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 145, in forward
    return x + id(x)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
</div>
<div class="section" id="non-strict-export">
<h2><a class="toc-backref" href="#id4">Non-Strict Export</a><a class="headerlink" href="#non-strict-export" title="Permalink to this heading">¶</a></h2>
<p>To trace the program, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> uses TorchDynamo by default, a byte
code analysis engine, to symbolically analyze the Python code and build a
graph based on the results. This analysis allows <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> to provide
stronger guarantees about safety, but not all Python code is supported,
causing these graph breaks.</p>
<p>To address this issue, in PyTorch 2.3, we introduced a new mode of
exporting called non-strict mode, where we trace through the program using the
Python interpreter executing it exactly as it would in eager mode, allowing us
to skip over unsupported Python features. This is done through adding a
<code class="docutils literal notranslate"><span class="pre">strict=False</span></code> flag.</p>
<p>Looking at some of the previous examples which resulted in graph breaks:</p>
<ul class="simple">
<li><p>Calling unsupported functions (such as many built-in functions) traces</p></li>
</ul>
<p>through, but in this case, <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> gets specialized as a constant integer in
the graph. This is because <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> is not a tensor operation, so the
operation is not recorded in the graph.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad3</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">bad3_nonstrict</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">bad3_nonstrict</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1
            add: "f32[3, 3]" = torch.ops.aten.add.Tensor(x, 1);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x)
            add_1: "f32[3, 3]" = torch.ops.aten.add.Tensor(add, 140396155731168);  add = None
            return (add_1,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add_1'), target=None)])
Range constraints: {}

tensor([[1.4040e+14, 1.4040e+14, 1.4040e+14],
        [1.4040e+14, 1.4040e+14, 1.4040e+14],
        [1.4040e+14, 1.4040e+14, 1.4040e+14]])
</pre></div>
</div>
<p>However, there are still some features that require rewrites to the original
module:</p>
</div>
<div class="section" id="control-flow-ops">
<h2><a class="toc-backref" href="#id5">Control Flow Ops</a><a class="headerlink" href="#control-flow-ops" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> actually does support data-dependent control flow.
But these need to be expressed using control flow ops. For example,
we can fix the control flow example above using the <code class="docutils literal notranslate"><span class="pre">cond</span></code> op, like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1Fixed</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">true_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">false_fn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cond.html#torch.cond" title="torch.cond"><span class="n">torch</span><span class="o">.</span><span class="n">cond</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Bad1Fixed</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_bad1_fixed</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">-</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() &gt; 0, true_fn, false_fn, [x])
            sum_1: "f32[]" = torch.ops.aten.sum.default(x)
            gt: "b8[]" = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None

             # File: /usr/local/lib/python3.10/dist-packages/torch/_higher_order_ops/cond.py:137 in cond, code: return cond_op(pred, true_fn, false_fn, operands)
            true_graph_0 = self.true_graph_0
            false_graph_0 = self.false_graph_0
            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x]);  gt = true_graph_0 = false_graph_0 = x = None
            getitem: "f32[3, 3]" = cond[0];  cond = None
            return (getitem,)

        class true_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:202 in true_fn, code: return torch.sin(x)
                sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None
                return (sin,)

        class false_graph_0(torch.nn.Module):
            def forward(self, x: "f32[3, 3]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:204 in false_fn, code: return torch.cos(x)
                cos: "f32[3, 3]" = torch.ops.aten.cos.default(x);  x = None
                return (cos,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='getitem'), target=None)])
Range constraints: {}

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]])
tensor([[0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403]])
</pre></div>
</div>
<p>There are limitations to <code class="docutils literal notranslate"><span class="pre">cond</span></code> that one should be aware of:</p>
<ul class="simple">
<li><p>The predicate (i.e. <code class="docutils literal notranslate"><span class="pre">x.sum()</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>) must result in a boolean or a single-element tensor.</p></li>
<li><p>The operands (i.e. <code class="docutils literal notranslate"><span class="pre">[x]</span></code>) must be tensors.</p></li>
<li><p>The branch function (i.e. <code class="docutils literal notranslate"><span class="pre">true_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">false_fn</span></code>) signature must match with the
operands and they must both return a single tensor with the same metadata (for example, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code>, etc.).</p></li>
<li><p>Branch functions cannot mutate input or global variables.</p></li>
<li><p>Branch functions cannot access closure variables, except for <code class="docutils literal notranslate"><span class="pre">self</span></code> if the function is
defined in the scope of a method.</p></li>
</ul>
<p>For more details about <code class="docutils literal notranslate"><span class="pre">cond</span></code>, check out the <a class="reference external" href="https://pytorch.org/docs/main/cond.html">cond documentation</a>.</p>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">map</span></code>, which applies a function across the first dimension
of the first tensor argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch._higher_order_ops.map</span><span class="w"> </span><span class="kn">import</span> <span class="nb">map</span> <span class="k">as</span> <span class="n">torch_map</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MapModule</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>

        <span class="k">return</span> <span class="n">torch_map</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">MapModule</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_map_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_map_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><span class="o">*</span><span class="n">inps</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, xs: "f32[6, 4]", y: "i64[]", z: "i64[]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:236 in forward, code: return torch_map(body, xs, y, z)
            body_graph_0 = self.body_graph_0
            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]);  body_graph_0 = xs = y = z = None
            getitem: "f32[6, 4]" = map_impl[0];  map_impl = None
            return (getitem,)

        class body_graph_0(torch.nn.Module):
            def forward(self, xs: "f32[4]", y: "i64[]", z: "i64[]"):
                 # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:234 in body, code: return x + y + z
                add: "f32[4]" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None
                add_1: "f32[4]" = torch.ops.aten.add.Tensor(add, z);  add = z = None
                return (add_1,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='xs'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='z'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='getitem'), target=None)])
Range constraints: {}

tensor([[10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.]])
</pre></div>
</div>
<p>Other control flow ops include <code class="docutils literal notranslate"><span class="pre">while_loop</span></code>, <code class="docutils literal notranslate"><span class="pre">associative_scan</span></code>, and
<code class="docutils literal notranslate"><span class="pre">scan</span></code>. For more documentation on each operator, please refer to
<a class="reference external" href="https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops">this page</a>.</p>
</div>
<div class="section" id="constraints-dynamic-shapes">
<h2><a class="toc-backref" href="#id6">Constraints/Dynamic Shapes</a><a class="headerlink" href="#constraints-dynamic-shapes" title="Permalink to this heading">¶</a></h2>
<p>This section covers dynamic behavior and representation of exported programs. Dynamic behavior is
subjective to the particular model being exported, so for the most part of this tutorial, we’ll focus
on this particular toy model (with the resulting tensor shapes annotated):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [6, 5]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [8, 4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [32]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># [8, 4]</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># [6, 3]</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># [32]</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># [32]</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a static program. One consequence of this is that at runtime,
the program won’t work on inputs with different shapes, even if they’re valid in eager mode.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">))</span>
<span class="n">model</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 286, in &lt;module&gt;
    ep.module()(w, x, torch.randn(3, 4), torch.randn(12))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1857, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_unlift.py", line 55, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/utils.py", line 398, in _check_input_constraints_for_graph
    raise RuntimeError(
RuntimeError: Expected input at *args[2].shape[0] to be equal to 8, but got 3
</pre></div>
</div>
<div class="section" id="basic-concepts-symbols-and-guards">
<h3><a class="toc-backref" href="#id7">Basic concepts: symbols and guards</a><a class="headerlink" href="#basic-concepts-symbols-and-guards" title="Permalink to this heading">¶</a></h3>
<p>To enable dynamism, <code class="docutils literal notranslate"><span class="pre">export()</span></code> provides a <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument. The easiest way to work with
dynamic shapes is using <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> and looking at the program that’s returned. Dynamic behavior is specified
at a input dimension-level; for each input we can specify a tuple of values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export.dynamic_shapes</span><span class="w"> </span><span class="kn">import</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<p>Before we look at the program that’s produced, let’s understand what specifying <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> entails,
and how that interacts with export. For every input dimension where a <code class="docutils literal notranslate"><span class="pre">Dim</span></code> object is specified, a symbol is
<a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">allocated</a>,
taking on a range of <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">inf]</span></code> (why not <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">inf]</span></code> or <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">inf]</span></code>? we’ll explain later in the
0/1 specialization section).</p>
<p>Export then runs model tracing, looking at each operation that’s performed by the model. Each individual operation can emit
what’s called “guards”; basically boolean condition that are required to be true for the program to be valid.
When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid;
i.e. the program’s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards
and producing a final program representation that adheres to all of these guards. Before we see this “final representation” in
an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, let’s look at the guards emitted by the toy model we’re tracing.</p>
<p>Here, each forward input tensor is annotated with the symbol allocated at the start of tracing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s0, s1]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s2]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s3, s4]</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">,</span>  <span class="c1"># [s5]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>  <span class="c1"># guard: s2 == s4</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>  <span class="c1"># guard: s1 == 5</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># no guard added here</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>  <span class="c1"># guard: s3 * s4 == s5</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>Let’s understand each of the operations and the emitted guards:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>: This is an element-wise add with broadcasting, since <code class="docutils literal notranslate"><span class="pre">x</span></code> is a 1-d tensor and <code class="docutils literal notranslate"><span class="pre">y</span></code> a 2-d tensor. <code class="docutils literal notranslate"><span class="pre">x</span></code> is broadcasted along the last dimension of <code class="docutils literal notranslate"><span class="pre">y</span></code>, emitting the guard <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">==</span> <span class="pre">s4</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x1</span> <span class="pre">=</span> <span class="pre">self.l(w)</span></code>: Calling <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (<code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">s1]</span></code>), and a statically-shaped tensor. This emits the guard <code class="docutils literal notranslate"><span class="pre">s1</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x2</span> <span class="pre">=</span> <span class="pre">x0.flatten()</span></code>: This call actually doesn’t emit any guards! (at least none relevant to input shapes)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x3</span> <span class="pre">=</span> <span class="pre">x2</span> <span class="pre">+</span> <span class="pre">z</span></code>: <code class="docutils literal notranslate"><span class="pre">x2</span></code> has shape <code class="docutils literal notranslate"><span class="pre">[s3*s4]</span></code> after flattening, and this element-wise add emits <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">*</span> <span class="pre">s4</span> <span class="pre">==</span> <span class="pre">s5</span></code>.</p></li>
</ul>
<p>Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes
subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">5]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x:</span> <span class="pre">[s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y:</span> <span class="pre">[s3,</span> <span class="pre">s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z:</span> <span class="pre">[s2*s3]</span></code></p></li>
</ul>
<p>And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the
corresponding inputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_l_weight: "f32[3, 5]", p_l_bias: "f32[3]", w: "f32[s0, 5]", x: "f32[s2]", y: "f32[s3, s2]", z: "f32[s2*s3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y  # [8, 4]
            add: "f32[s3, s2]" = torch.ops.aten.add.Tensor(x, y);  x = y = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward, code: x1 = self.l(w)  # [6, 3]
            linear: "f32[s0, 3]" = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias);  w = p_l_weight = p_l_bias = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten()  # [32]
            flatten: "f32[s2*s3]" = torch.ops.aten.flatten.using_ints(add);  add = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z  # [32]
            add_1: "f32[s2*s3]" = torch.ops.aten.add.Tensor(flatten, z);  flatten = z = None
            return (linear, add_1)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_l_weight'), target='l.weight', persistent=None), InputSpec(kind=&lt;InputKind.PARAMETER: 2&gt;, arg=TensorArgument(name='p_l_bias'), target='l.bias', persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='w'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='z'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='linear'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add_1'), target=None)])
Range constraints: {s0: VR[2, int_oo], s2: VR[2, int_oo], s3: VR[2, int_oo], s2*s3: VR[4, int_oo]}
</pre></div>
</div>
<p>Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn’t
so interesting currently, since this export call doesn’t emit any guards related to symbol bounds and each base symbol has
a generic bound, but this will come up later.</p>
<p>So far, because we’ve been exporting this toy model, this experience has not been representative of how hard
it typically is to debug dynamic shapes guards &amp; issues. In most cases it isn’t obvious what guards are being emitted,
and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards
are rather intuitive.</p>
<p>In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment
variable <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS="+dynamic"</span></code>, or interactively with <code class="docutils literal notranslate"><span class="pre">torch._logging.set_logs(dynamic=10)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-_logging sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs" title="torch._logging.set_logs"><span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span></a><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.088000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [8/0] create_env
I0701 17:27:08.090000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.091000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.092000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [8/0] runtime_assert True == True [statically known]
I0701 17:27:08.094000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.096000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.097000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.099000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [8/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.101000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0701 17:27:08.102000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [8/0] runtime_assert True == True [statically known]
V0701 17:27:08.103000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0701 17:27:08.103000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
I0701 17:27:08.104000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s4 = s2 (solve) VR[2, int_oo]
V0701 17:27:08.106000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0701 17:27:08.107000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0701 17:27:08.113000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0701 17:27:08.113000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [8/0] _update_var_to_range s1 = VR[5, 5] (update)
I0701 17:27:08.114000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0701 17:27:08.116000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0701 17:27:08.121000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0701 17:27:08.122000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0701 17:27:08.123000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [8/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0701 17:27:08.124000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [8/0] _update_var_to_range s5 = VR[4, int_oo] (update)
I0701 17:27:08.125000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [8/0] set_replacement s5 = s2*s3 (solve) VR[4, int_oo]
V0701 17:27:08.126000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [8/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
I0701 17:27:08.132000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [8/0] produce_guards
V0701 17:27:08.132000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].size()[0] s0 None
V0701 17:27:08.132000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].size()[1] 5 None
V0701 17:27:08.133000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].stride()[0] 5 None
V0701 17:27:08.133000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].stride()[1] 1 None
V0701 17:27:08.133000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['w'].storage_offset() 0 None
V0701 17:27:08.133000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].size()[0] s2 None
V0701 17:27:08.134000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].stride()[0] 1 None
V0701 17:27:08.134000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.134000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].size()[0] s3 None
V0701 17:27:08.135000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].size()[1] s2 None
V0701 17:27:08.135000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].stride()[0] s2 None
V0701 17:27:08.135000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].stride()[1] 1 None
V0701 17:27:08.136000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['y'].storage_offset() 0 None
V0701 17:27:08.136000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].size()[0] s2*s3 None
V0701 17:27:08.136000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].stride()[0] 1 None
V0701 17:27:08.136000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [8/0] track_symint L['z'].storage_offset() 0 None
V0701 17:27:08.168000 31114 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Ne(s0, 1)) == True [statically known]
</pre></div>
</div>
<p>This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end
to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above;
e.g. the allocation of symbols:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">runtime_assert True == True [statically known]</span>
<span class="sd">create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>"\ncreate_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\nruntime_assert True == True [statically known]\ncreate_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\n"
</pre></div>
</div>
<p>The lines with <cite>create_symbol</cite> show when a new symbol has been allocated, and the logs also identify the tensor variable names
and dimensions they’ve been allocated for. In other lines we can also see the guards emitted:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">"""</span>
<span class="sd">runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"</span>
<span class="sd">runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"</span>
<span class="sd">runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"</span>
<span class="sd">"""</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>'\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"\n'
</pre></div>
</div>
<p>Next to the <code class="docutils literal notranslate"><span class="pre">[guard</span> <span class="pre">added]</span></code> messages, we also see the responsible user lines of code - luckily here the model is simple enough.
In many real-world cases it’s not so straightforward: high-level torch operations can have complicated fake-kernel implementations
or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate
is to follow the logs’ suggestion, and re-run with environment variable <code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="..."</span></code>, to further
attribute the guard of interest.</p>
<p><code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> is just one of the available options for interacting with <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code>; as of writing this 2 other options are available:
<code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code> simply marks a dimension static, while <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code> is similar to <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> in all
ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a
static guard is emitted on a dynamically-marked dimension:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.187000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [9/0] create_env
I0701 17:27:08.189000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.190000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.191000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [9/0] runtime_assert True == True [statically known]
I0701 17:27:08.193000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.195000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.196000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.198000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [9/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.200000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0701 17:27:08.201000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [9/0] runtime_assert True == True [statically known]
V0701 17:27:08.202000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0701 17:27:08.202000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
I0701 17:27:08.203000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s4 = s2 (solve) VR[2, int_oo]
V0701 17:27:08.205000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0701 17:27:08.206000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0701 17:27:08.212000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0701 17:27:08.212000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [9/0] _update_var_to_range s1 = VR[5, 5] (update)
I0701 17:27:08.213000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0701 17:27:08.215000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0701 17:27:08.220000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0701 17:27:08.221000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0701 17:27:08.222000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [9/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0701 17:27:08.223000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [9/0] _update_var_to_range s5 = VR[4, int_oo] (update)
I0701 17:27:08.224000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [9/0] set_replacement s5 = s2*s3 (solve) VR[4, int_oo]
V0701 17:27:08.225000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [9/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
I0701 17:27:08.231000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [9/0] produce_guards
V0701 17:27:08.231000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].size()[0] s0 None
V0701 17:27:08.231000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].size()[1] 5 RelaxedUnspecConstraint(warn_only=False)
V0701 17:27:08.232000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].stride()[0] 5 None
V0701 17:27:08.232000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].stride()[1] 1 None
V0701 17:27:08.232000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['w'].storage_offset() 0 None
V0701 17:27:08.232000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].size()[0] s2 None
V0701 17:27:08.233000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].stride()[0] 1 None
V0701 17:27:08.233000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.233000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].size()[0] s3 None
V0701 17:27:08.234000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].size()[1] s2 None
V0701 17:27:08.234000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].stride()[0] s2 None
V0701 17:27:08.234000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].stride()[1] 1 None
V0701 17:27:08.235000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['y'].storage_offset() 0 None
V0701 17:27:08.235000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].size()[0] s2*s3 None
V0701 17:27:08.235000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].stride()[0] 1 None
V0701 17:27:08.236000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [9/0] track_symint L['z'].storage_offset() 0 None
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0] Error while creating guard:
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0] Name: ''
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Source: shape_env
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Create Function: SHAPE_ENV
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Guard Types: None
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Code List: None
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Object Weakref: None
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     Guarded Class Weakref: None
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0] Traceback (most recent call last):
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     return self.create_fn(builder, self)
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     return output_graph.shape_env.produce_guards_verbose(
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]     raise ConstraintViolationError(
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
E0701 17:27:08.237000 31114 torch/_guards.py:359] [9/0]   - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0] Created at:
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]     tracer = InstructionTranslator(
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]     output=OutputGraph(
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]     self.init_ambient_guards()
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0701 17:27:08.240000 31114 torch/_guards.py:361] [9/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['w'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['w'].size()[1]) are valid because L['w'].size()[1] was inferred to be a constant (5).
</pre></div>
</div>
<p>Static guards also aren’t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape
specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is
raised when this is the case for <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"w"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"x"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">,)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">"y"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.257000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [10/0] create_env
I0701 17:27:08.259000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.260000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.261000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [10/0] runtime_assert True == True [statically known]
I0701 17:27:08.264000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s2 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.264000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s3 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.267000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [10/0] create_symbol s4 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.270000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s3, 1)) == False [statically known]
I0701 17:27:08.273000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(s3, 4) [guard added] x0 = x + y  # [8, 4]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:268 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s3, 4)"
V0701 17:27:08.274000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s3 = VR[4, 4] (update)
I0701 17:27:08.275000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s3 = 4 (range_refined_to_singleton) VR[4, 4]
V0701 17:27:08.276000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
I0701 17:27:08.282000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:269 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0701 17:27:08.283000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s1 = VR[5, 5] (update)
I0701 17:27:08.284000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0701 17:27:08.285000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0701 17:27:08.286000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [10/0] runtime_assert True == True [statically known]
V0701 17:27:08.292000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [10/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0701 17:27:08.297000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [10/0] runtime_assert Eq(4*s2, s4) [guard added] x3 = x2 + z  # [32]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:271 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(4*s2, s4)"
V0701 17:27:08.298000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [10/0] _update_var_to_range s4 = VR[8, int_oo] (update)
I0701 17:27:08.300000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [10/0] set_replacement s4 = 4*s2 (solve) VR[8, int_oo]
I0701 17:27:08.306000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [10/0] produce_guards
V0701 17:27:08.307000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].size()[0] s0 None
V0701 17:27:08.307000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].size()[1] 5 None
V0701 17:27:08.307000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].stride()[0] 5 None
V0701 17:27:08.308000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].stride()[1] 1 None
V0701 17:27:08.308000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['w'].storage_offset() 0 None
V0701 17:27:08.308000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].size()[0] 4 None
V0701 17:27:08.309000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].stride()[0] 1 None
V0701 17:27:08.309000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.309000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].size()[0] s2 None
V0701 17:27:08.309000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].size()[1] 4 RelaxedUnspecConstraint(warn_only=False)
V0701 17:27:08.310000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].stride()[0] 4 None
V0701 17:27:08.310000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].stride()[1] 1 None
V0701 17:27:08.310000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['y'].storage_offset() 0 None
V0701 17:27:08.310000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].size()[0] 4*s2 None
V0701 17:27:08.311000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].stride()[0] 1 None
V0701 17:27:08.311000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [10/0] track_symint L['z'].storage_offset() 0 None
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0] Error while creating guard:
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0] Name: ''
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Source: shape_env
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Create Function: SHAPE_ENV
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Guard Types: None
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Code List: None
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Object Weakref: None
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     Guarded Class Weakref: None
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0] Traceback (most recent call last):
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     return self.create_fn(builder, self)
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     return output_graph.shape_env.produce_guards_verbose(
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]     raise ConstraintViolationError(
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
E0701 17:27:08.312000 31114 torch/_guards.py:359] [10/0]   - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0] Created at:
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]     tracer = InstructionTranslator(
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]     output=OutputGraph(
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]     self.init_ambient_guards()
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0701 17:27:08.314000 31114 torch/_guards.py:361] [10/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (L['y'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of RelaxedUnspecConstraint(L['y'].size()[1]) are valid because L['y'].size()[1] was inferred to be a constant (4).
</pre></div>
</div>
<p>Here you might ask why export “specializes”, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because
of the symbolic shapes system described above, of symbols and guards. When <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is marked static, we don’t allocate a symbol, and compile
treating this shape as a concrete integer 4. A symbol is allocated for <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>, and so we finally emit the guard <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">==</span> <span class="pre">4</span></code>, leading to
specialization.</p>
<p>One feature of export is that during tracing, statements like asserts, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code>, and <code class="docutils literal notranslate"><span class="pre">if/else</span></code> conditions will also emit guards.
See what happens when we augment the existing model with such statements:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">):</span>
        <span class="k">assert</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">512</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">w</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
            <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">)</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a>
            <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"w"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">"z"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DynamicModel</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">z</span></a><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.329000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [11/0] create_env
I0701 17:27:08.331000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s0 = 6 for L['w'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.332000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s1 = 5 for L['w'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.333000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert True == True [statically known]
I0701 17:27:08.335000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s2 = 4 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.337000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s3 = 8 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.337000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s4 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s4" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.340000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [11/0] create_symbol s5 = 32 for L['z'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s5" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.346000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert s0 &lt;= 512 [guard added] assert w.shape[0] &lt;= 512  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:450 in forward (_dynamo/symbolic_convert.py:669 in inner), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s0 &lt;= 512"
V0701 17:27:08.347000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s0 = VR[2, 512] (update)
I0701 17:27:08.351000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert s2 &gt;= 4 [guard added] torch._check(x.shape[0] &gt;= 4)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:451 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="s2 &gt;= 4"
V0701 17:27:08.352000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s2 = VR[4, int_oo] (update)
I0701 17:27:08.358000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] eval Eq(s0, s2 + 2) [guard added] if w.shape[0] == x.shape[0] + 2:  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:452 in forward (_dynamo/variables/tensor.py:1245 in evaluate_expr), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s0, s2 + 2)"
V0701 17:27:08.361000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s2 = VR[4, 510] (update)
V0701 17:27:08.362000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s0 = VR[6, 512] (update)
I0701 17:27:08.362000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s0 = s2 + 2 (solve) VR[6, 512]
V0701 17:27:08.364000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
V0701 17:27:08.365000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert True == True [statically known]
V0701 17:27:08.366000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s4, 1)) == False [statically known]
I0701 17:27:08.368000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:453 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2, s4)"
V0701 17:27:08.369000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s4 = VR[4, 510] (update)
I0701 17:27:08.370000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s4 = s2 (solve) VR[4, 510]
V0701 17:27:08.371000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s2, 1)) == True [statically known]
V0701 17:27:08.372000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s3, 1)) == True [statically known]
I0701 17:27:08.378000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:454 in forward (_meta_registrations.py:2236 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 5)"
V0701 17:27:08.379000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s1 = VR[5, 5] (update)
I0701 17:27:08.380000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s1 = 5 (range_refined_to_singleton) VR[5, 5]
V0701 17:27:08.388000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s2*s3, 1)) == False [statically known]
V0701 17:27:08.389000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Eq(s5, 1)) == False [statically known]
I0701 17:27:08.396000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [11/0] runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:456 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s2*s3, s5)"
V0701 17:27:08.397000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [11/0] _update_var_to_range s5 = VR[8, int_oo] (update)
I0701 17:27:08.398000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [11/0] set_replacement s5 = s2*s3 (solve) VR[8, int_oo]
V0701 17:27:08.400000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [11/0] eval size_oblivious(Ne(s2*s3, 1)) == True [statically known]
V0701 17:27:08.403000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [11/0] runtime_assert s2 &gt;= 4 == True [statically known]
I0701 17:27:08.408000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [11/0] produce_guards
V0701 17:27:08.409000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].size()[0] s2 + 2 None
V0701 17:27:08.409000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].size()[1] 5 None
V0701 17:27:08.409000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].stride()[0] 5 None
V0701 17:27:08.410000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].stride()[1] 1 None
V0701 17:27:08.410000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['w'].storage_offset() 0 None
V0701 17:27:08.410000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].size()[0] s2 None
V0701 17:27:08.410000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].stride()[0] 1 None
V0701 17:27:08.411000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.411000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].size()[0] s3 None
V0701 17:27:08.411000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].size()[1] s2 None
V0701 17:27:08.412000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].stride()[0] s2 None
V0701 17:27:08.412000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].stride()[1] 1 None
V0701 17:27:08.412000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['y'].storage_offset() 0 None
V0701 17:27:08.412000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].size()[0] s2*s3 None
V0701 17:27:08.413000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].stride()[0] 1 None
V0701 17:27:08.413000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [11/0] track_symint L['z'].storage_offset() 0 None
</pre></div>
</div>
<p>Each of these statements emits an additional guard, and the exported program shows the changes; <code class="docutils literal notranslate"><span class="pre">s0</span></code> is eliminated in favor of <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">+</span> <span class="pre">2</span></code>,
and <code class="docutils literal notranslate"><span class="pre">s2</span></code> now contains lower and upper bounds, reflected in <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code>.</p>
<p>For the if/else condition, you might ask why the True branch was taken, and why it wasn’t the <code class="docutils literal notranslate"><span class="pre">w.shape[0]</span> <span class="pre">!=</span> <span class="pre">x.shape[0]</span> <span class="pre">+</span> <span class="pre">2</span></code> guard that
got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken.
If different sample input shapes were provided that fail the <code class="docutils literal notranslate"><span class="pre">if</span></code> condition, export would trace and emit guards corresponding to the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch.
Additionally, you might ask why we traced only the <code class="docutils literal notranslate"><span class="pre">if</span></code> branch, and if it’s possible to maintain control-flow in your program and keep both branches
alive. For that, refer to rewriting your model code following the <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Flow</span> <span class="pre">Ops</span></code> section above.</p>
</div>
<div class="section" id="specialization">
<h3><a class="toc-backref" href="#id8">0/1 specialization</a><a class="headerlink" href="#specialization" title="Permalink to this heading">¶</a></h3>
<p>Since we’re talking about guards and specializations, it’s a good time to talk about the 0/1 specialization issue we brought up earlier.
The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that
don’t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 … . This just means that you should
specify 0/1 sample inputs when you’d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens
at runtime when we export this linear layer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
    <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),),</span>
    <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"input"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">STATIC</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.473000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [12/0] create_env
I0701 17:27:08.486000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [12/0] produce_guards
V0701 17:27:08.486000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].size()[0] 1 None
V0701 17:27:08.487000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].size()[1] 4 None
V0701 17:27:08.487000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].stride()[0] 4 None
V0701 17:27:08.487000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].stride()[1] 1 None
V0701 17:27:08.487000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [12/0] track_symint L['args'][0].storage_offset() 0 None
Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 500, in &lt;module&gt;
    ep.module()(torch.randn(2, 4))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1857, in _call_impl
    return inner()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_unlift.py", line 55, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File "/usr/local/lib/python3.10/dist-packages/torch/_export/utils.py", line 398, in _check_input_constraints_for_graph
    raise RuntimeError(
RuntimeError: Expected input at *args[0].shape[0] to be equal to 1, but got 2
</pre></div>
</div>
</div>
<div class="section" id="named-dims">
<h3><a class="toc-backref" href="#id9">Named Dims</a><a class="headerlink" href="#named-dims" title="Permalink to this heading">¶</a></h3>
<p>So far we’ve only been talking about 3 ways to specify dynamic shapes: <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code>, <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. The attraction of these is the
low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic
dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a “discovery” process, summarizing these guards
and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or
beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at
all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels.
These changes won’t be detected and the <code class="docutils literal notranslate"><span class="pre">export()</span></code> call will most likely succeed, unless tests are in place that check the resulting <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> representation.</p>
<p>For such cases, our stance is to recommend the “traditional” way of specifying dynamic shapes, which longer-term users of export might be familiar with: named <code class="docutils literal notranslate"><span class="pre">Dims</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">dh</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dh"</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dh</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the
dynamic behavior of the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> produced; <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic
specifications given. For example, in the above specification, the following is asserted:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is to have range <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">256]</span></code>, and related to <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span></code> by <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span> <span class="pre">==</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">x.shape[0]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[1]</span></code> is static.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code> has range <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">512]</span></code>, and is unrelated to any other dimension.</p></li>
</ul>
<p>In this design, we allow relations between dimensions to be specified with univariate linear expressions: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">dim</span> <span class="pre">+</span> <span class="pre">B</span></code> can be specified for any dimension. This allows users
to specify more complex constraints like integer divisibility for dynamic dimensions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># x.shape[0] has range [16, 2048], and is divisible by 4.</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="constraint-violations-suggested-fixes">
<h3><a class="toc-backref" href="#id10">Constraint violations, suggested fixes</a><a class="headerlink" href="#constraint-violations-suggested-fixes" title="Permalink to this heading">¶</a></h3>
<p>One common issue with this specification style (before <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> was introduced), is that the specification would often be mismatched with what was produced by model tracing.
That would lead to <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors and export suggested fixes - see for example with this model &amp; specification, where the model inherently requires equality between
dimensions 0 of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and requires dimension 1 to be static.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">w</span></a> <span class="o">+</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">d1</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dims" title="torch.export.dims"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">dims</span></a><span class="p">(</span><span class="s2">"dx"</span><span class="p">,</span> <span class="s2">"dy"</span><span class="p">,</span> <span class="s2">"d1"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span>
        <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"x"</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
            <span class="s2">"y"</span><span class="p">:</span> <span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.612000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [13/0] create_env
I0701 17:27:08.615000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s0 = 6 for L['x'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s0" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.615000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s1 = 4 for L['x'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s1" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.616000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [13/0] runtime_assert True == True [statically known]
I0701 17:27:08.619000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s2 = 6 for L['y'].size()[0] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s2" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
I0701 17:27:08.620000 31114 torch/fx/experimental/symbolic_shapes.py:4606] [13/0] create_symbol s3 = 4 for L['y'].size()[1] [2, int_oo] (_dynamo/variables/builder.py:3033 in &lt;lambda&gt;), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="s3" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE="0"
V0701 17:27:08.624000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s1, 1)) == False [statically known]
V0701 17:27:08.624000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [13/0] runtime_assert True == True [statically known]
V0701 17:27:08.625000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]
V0701 17:27:08.626000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s3, 1)) == False [statically known]
I0701 17:27:08.628000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s1, s3) [guard added] w = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:552 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, s3)"
I0701 17:27:08.629000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s3 = s1 (solve) VR[2, int_oo]
V0701 17:27:08.630000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Eq(s2, 1)) == False [statically known]
I0701 17:27:08.632000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s0, s2) [guard added] w = x + y  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:552 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s0, s2)"
I0701 17:27:08.633000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s2 = s0 (solve) VR[2, int_oo]
V0701 17:27:08.635000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Ne(s1, 1)) == True [statically known]
V0701 17:27:08.635000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [13/0] eval size_oblivious(Ne(s0, 1)) == True [statically known]
I0701 17:27:08.642000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [13/0] runtime_assert Eq(s1, 4) [guard added] return w + torch.ones(4)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:553 in forward (_subclasses/fake_impls.py:881 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="Eq(s1, 4)"
V0701 17:27:08.643000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [13/0] _update_var_to_range s1 = VR[4, 4] (update)
I0701 17:27:08.644000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s1 = 4 (range_refined_to_singleton) VR[4, 4]
V0701 17:27:08.647000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [13/0] _update_var_to_range s3 = VR[4, 4] (update)
I0701 17:27:08.648000 31114 torch/fx/experimental/symbolic_shapes.py:6234] [13/0] set_replacement s3 = 4 (find) VR[4, 4]
I0701 17:27:08.650000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [13/0] produce_guards
V0701 17:27:08.651000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].size()[0] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0701 17:27:08.651000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0701 17:27:08.651000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].stride()[0] 4 None
V0701 17:27:08.652000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].stride()[1] 1 None
V0701 17:27:08.652000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.652000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].size()[0] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0701 17:27:08.653000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V0701 17:27:08.653000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].stride()[0] 4 None
V0701 17:27:08.654000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].stride()[1] 1 None
V0701 17:27:08.654000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [13/0] track_symint L['y'].storage_offset() 0 None
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0] Error while creating guard:
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0] Name: ''
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Source: shape_env
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Create Function: SHAPE_ENV
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Guard Types: None
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Code List: None
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Object Weakref: None
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     Guarded Class Weakref: None
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0] Traceback (most recent call last):
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     return self.create_fn(builder, self)
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     python_code_parts, verbose_code_parts = _get_code_parts(
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     return output_graph.shape_env.produce_guards_verbose(
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]     raise ConstraintViolationError(
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
E0701 17:27:08.655000 31114 torch/_guards.py:359] [13/0]   - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0] Created at:
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 694, in transform
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]     tracer = InstructionTranslator(
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3329, in __init__
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]     output=OutputGraph(
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 358, in __init__
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]     self.init_ambient_guards()
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 512, in init_ambient_guards
E0701 17:27:08.657000 31114 torch/_guards.py:361] [13/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1722, in inner
    raise constraint_violation_error
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 906, in _compile_inner
    check_fn = CheckFunctionManager(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 2481, in __init__
    guard.create(builder)
  File "/usr/local/lib/python3.10/dist-packages/torch/_guards.py", line 357, in create
    return self.create_fn(builder, self)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1959, in SHAPE_ENV
    python_code_parts, verbose_code_parts = _get_code_parts(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py", line 1942, in _get_code_parts
    return output_graph.shape_env.produce_guards_verbose(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 5409, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.

Suggested fixes:
  d1 = 4
  dy = dx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 557, in &lt;module&gt;
    ep = export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 756, in _export_to_torch_ir
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of d1 = L['x'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - Not all values of d1 = L['y'].size()[1] in the specified range are valid because d1 was inferred to be a constant (4).
  - The values of dy = L['y'].size()[0] and dx = L['x'].size()[0] must always be equal.

Suggested fixes:
  d1 = 4
  dy = dx
</pre></div>
</div>
<p>The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards.</p>
<p>Lastly, there’s couple nice-to-knows about the options for specification:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> is a good option for static behavior:
- <code class="docutils literal notranslate"><span class="pre">dynamic_shapes=None</span></code> (default) exports with the entire model being static.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at a dimension-level specializes that dimension, though this is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>.</p></li>
<li><p>specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification.</p></li>
</ul>
<p>These options are combined in the inputs &amp; dynamic shapes spec below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="mi">16</span><span class="p">,</span>
    <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"tensor_0"</span><span class="p">:</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch-export-dynamic_shapes sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.dynamic_shapes.Dim" title="torch.export.dynamic_shapes.Dim"><span class="n">Dim</span></a><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">"tensor_1"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"int_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">"bool_val"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="data-dependent-errors">
<h2><a class="toc-backref" href="#id11">Data-dependent errors</a><a class="headerlink" href="#data-dependent-errors" title="Permalink to this heading">¶</a></h2>
<p>While trying to export models, you have may have encountered errors like “Could not guard on data-dependent expression”, or Could not extract specialized integer from data-dependent expression”.
These errors exist because <code class="docutils literal notranslate"><span class="pre">torch.export()</span></code> compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties
(e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be
unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out,
complaining that the value is not available.</p>
<p>Data-dependent values appear in many places, and common sources are calls like <code class="docutils literal notranslate"><span class="pre">item()</span></code>, <code class="docutils literal notranslate"><span class="pre">tolist()</span></code>, or <code class="docutils literal notranslate"><span class="pre">torch.unbind()</span></code> that extract scalar values from tensors.
How are these values represented in the exported program? In the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a>
section, we talked about allocating symbols to represent dynamic input dimensions.
The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are “unbacked” symbols,
in contrast to the “backed” symbols allocated for input dimensions. The <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">“backed/unbacked”</a>
nomenclature refers to the presence/absence of a “hint” for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed.</p>
<p>In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties.
For data-dependent values, the symbols are taken from FakeTensor “data” during tracing, and so the compiler doesn’t know the actual values (hints) that these symbols would take on.</p>
<p>Let’s see how these show up in exported programs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.673000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [14/0] create_env
I0701 17:27:08.677000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.678000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u0]
I0701 17:27:08.680000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u1 [-int_oo, int_oo] b = y.tolist()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.681000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u1]
I0701 17:27:08.682000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [14/0] create_unbacked_symint u2 [-int_oo, int_oo] b = y.tolist()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.683000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [14/0] compute_unbacked_bindings [u2]
I0701 17:27:08.686000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [14/0] produce_guards
V0701 17:27:08.687000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.687000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].size()[0] 2 None
V0701 17:27:08.687000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].stride()[0] 1 None
V0701 17:27:08.687000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [14/0] track_symint L['y'].storage_offset() 0 None
I0701 17:27:08.693000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u3 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.694000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u4 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.700000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u5 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.700000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u5]
I0701 17:27:08.701000 31114 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u5 = u0 (rename_unbacked_to) VR[-int_oo, int_oo]
I0701 17:27:08.702000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u6 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.703000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u6]
I0701 17:27:08.703000 31114 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u6 = u1 (rename_unbacked_to) VR[-int_oo, int_oo]
I0701 17:27:08.705000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u7 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.705000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u7]
I0701 17:27:08.706000 31114 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u7 = u2 (rename_unbacked_to) VR[-int_oo, int_oo]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "i64[2]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist()
            select: "i64[]" = torch.ops.aten.select.int(y, 0, 0)
            item_1: "Sym(u1)" = torch.ops.aten.item.default(select);  select = None
            select_1: "i64[]" = torch.ops.aten.select.int(y, 0, 1);  y = None
            item_2: "Sym(u2)" = torch.ops.aten.item.default(select_1);  select_1 = None
            return (item_1, item_2, item)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item_1'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item_2'), target=None), OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=SymIntArgument(name='item'), target=None)])
Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo], u3: VR[-int_oo, int_oo], u4: VR[-int_oo, int_oo], u5: VR[-int_oo, int_oo], u6: VR[-int_oo, int_oo], u7: VR[-int_oo, int_oo]}
</pre></div>
</div>
<p>The result is that 3 unbacked symbols (notice they’re prefixed with “u”, instead of the usual “s” for input shape/backed symbols) are allocated and returned:
1 for the <code class="docutils literal notranslate"><span class="pre">item()</span></code> call, and 1 for each of the elements of <code class="docutils literal notranslate"><span class="pre">y</span></code> with the <code class="docutils literal notranslate"><span class="pre">tolist()</span></code> call.
Note from the range constraints field that these take on ranges of <code class="docutils literal notranslate"><span class="pre">[-int_oo,</span> <span class="pre">int_oo]</span></code>, not the default <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">int_oo]</span></code> range allocated to input shape symbols,
since we have no information on what these values are - they don’t represent sizes, so don’t necessarily have positive values.</p>
<div class="section" id="guards-torch-check">
<h3><a class="toc-backref" href="#id12">Guards, torch._check()</a><a class="headerlink" href="#guards-torch-check" title="Permalink to this heading">¶</a></h3>
<p>But the case above is easy to export, because the concrete values of these symbols aren’t used in any compiler decision-making; all that’s relevant is that the return values are unbacked symbols.
The data-dependent errors highlighted in this section are cases like the following, where <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#control-flow-static-vs-dynamic">data-dependent guards</a> are encountered:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Here we actually need the “hint”, or the concrete value of <code class="docutils literal notranslate"><span class="pre">a</span></code> for the compiler to decide whether to trace <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> or <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code> as the output.
Because we trace with FakeTensors, we don’t know what <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span></code> actually evaluates to, and export errors out with “Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span> <span class="pre">(unhinted)</span></code>”.</p>
<p>So how do we export this toy model? Unlike <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, export requires full graph compilation, and we can’t just graph break on this. Here are some basic options:</p>
<ol class="arabic simple">
<li><p>Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using <code class="docutils literal notranslate"><span class="pre">torch.compiler.is_compiling()</span></code> to guard what’s traced at compile-time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code>: we could rewrite the control-flow code to use <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> so we don’t specialize on a branch.</p></li>
</ol>
<p>While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> is not a comprehensive system for handling data-dependent errors.
As we will see, there are data-dependent errors that do not involve control-flow.</p>
<p>The generally recommended approach is to start with <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols.
While a <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true,
are stored as symbol properties (provided it’s smart enough to infer those properties). So even if unbacked symbols don’t have hints, if we’re able to communicate properties that are generally true for these symbols via
<code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls, we can potentially bypass data-dependent guards without rewriting the offending model code.</p>
<p>For example in the model above, inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&gt;=</span> <span class="pre">10)</span></code> would tell the compiler that <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> can always be returned, and <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">4)</span></code> tells it to return <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code>.
See what happens when we re-export this model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="o">*</span> <span class="mi">5</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.715000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [15/0] create_env
I0701 17:27:08.719000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [15/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.720000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [15/0] compute_unbacked_bindings [u0]
I0701 17:27:08.722000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [15/0] runtime_assert u0 &gt;= 10 [guard added] torch._check(a &gt;= 10)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:673 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 10"
V0701 17:27:08.723000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [15/0] _update_var_to_range u0 = VR[10, int_oo] (update)
I0701 17:27:08.728000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [15/0] runtime_assert u0 &lt;= 60 [guard added] torch._check(a &lt;= 60)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:674 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt;= 60"
V0701 17:27:08.729000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [15/0] _update_var_to_range u0 = VR[10, 60] (update)
V0701 17:27:08.734000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [15/0] eval False == True [statically known]
V0701 17:27:08.737000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [15/0] runtime_assert u0 &gt;= 10 == True [statically known]
V0701 17:27:08.738000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [15/0] runtime_assert u0 &lt;= 60 == True [statically known]
I0701 17:27:08.740000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [15/0] produce_guards
V0701 17:27:08.741000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.741000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].size()[0] 4 None
V0701 17:27:08.741000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].stride()[0] 1 None
V0701 17:27:08.742000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [15/0] track_symint L['y'].storage_offset() 0 None
I0701 17:27:08.753000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.753000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u1]
V0701 17:27:08.754000 31114 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u1 = VR[10, 60] (update)
I0701 17:27:08.754000 31114 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u1 = u0 (rename_unbacked_to) VR[10, 60]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[4]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_1: "Sym(u0 &gt;= 10)" = item &gt;= 10
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, "Runtime assertion failed for expression u0 &gt;= 10 on node 'ge_1'");  ge_1 = _assert_scalar_default = None
            le_1: "Sym(u0 &lt;= 60)" = item &lt;= 60;  item = None
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u0 &lt;= 60 on node 'le_1'");  le_1 = _assert_scalar_default_1 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2
            add: "f32[4]" = torch.ops.aten.add.Tensor(y, 2);  y = None
            return (add,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add'), target=None)])
Range constraints: {u0: VR[10, 60], u1: VR[10, 60]}
</pre></div>
</div>
<p>Export succeeds, and note from the range constraints field that <code class="docutils literal notranslate"><span class="pre">u0</span></code> takes on a range of <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">60]</span></code>.</p>
<p>So what information do <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true:</p>
<ol class="arabic simple">
<li><p>Equality with non-data-dependent expressions: <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls that communicate equalities like <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">s0</span> <span class="pre">+</span> <span class="pre">4</span></code> or <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p>Range refinement: calls that provide lower or upper bounds for symbols, like the above.</p></li>
<li><p>Some basic reasoning around more complicated expressions: inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&lt;</span> <span class="pre">4)</span></code> will typically tell the compiler that <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;=</span> <span class="pre">4</span></code> is false. Checks on complex expressions like <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">-</span> <span class="pre">3</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">&lt;=</span> <span class="pre">10)</span></code> will typically get you past identical guards.</p></li>
</ol>
<p>As mentioned previously, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls have applicability outside of data-dependent control flow. For example, here’s a model where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion
prevails while manual specialization &amp; <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> do not:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.767000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [16/0] create_env
I0701 17:27:08.771000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [16/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:701 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.771000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [16/0] compute_unbacked_bindings [u0]
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0] Data dependent variable 'u0' allocated at:
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/bin/sphinx-build", line 8, in &lt;module&gt;
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     sys.exit(main())
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 313, in main
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make_main(argv)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 195, in make_main
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make_mode.run_make_mode(argv[1:])
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 160, in run_make_mode
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return make.run_generic_build(args[0])
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 148, in run_generic_build
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return build_main(args + opts)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 276, in build_main
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 262, in __init__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._init_builder()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 335, in _init_builder
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.events.emit('builder-inited')
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx/events.py", line 94, in emit
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     results.append(listener.handler(self.app, *args))
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 491, in generate_gallery_rst
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ) = generate_dir_rst(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 431, in generate_dir_rst
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     intro, title, cost = generate_file_rst(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/conf.py", line 79, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     p.start()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._popen = self._Popen(self)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _default_context.get_context().Process._Popen(process_obj)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return Popen(process_obj)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._launch(process_obj)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     code = process_obj._bootstrap(parent_sentinel=child_r)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.run()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self._target(*self._args, **self._kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/conf.py", line 67, in call_fn
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     result = func(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1027, in generate_file_rst
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     output_blocks, time_elapsed = execute_script(script_blocks,
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 945, in execute_script
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     output_blocks.append(execute_code_block(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 810, in execute_code_block
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     is_last_expr, mem_max = _exec_and_get_memory(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 676, in _exec_and_get_memory
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     mem_max, _ = gallery_conf['call_memory'](
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 223, in call_memory
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return 0., func()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 600, in __call__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     exec(self.code, self.fake_main.__dict__)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 709, in &lt;module&gt;
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     export(Foo(), inps)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _export(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = _export_for_training(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ep = fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     export_artifact = export_func(  # type: ignore[operator]
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     gm_torch_level = _export_to_torch_ir(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     gm_torch_level, _ = torch._dynamo.export(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     result_traced = opt_f(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._call_impl(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return forward_call(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._call_impl(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return forward_call(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._torchdynamo_orig_callable(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _compile(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     guarded_code = compile_inner(code, one_graph, hooks, transform)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return function(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _compile_inner(code, one_graph, hooks, transform)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 797, in _compile_inner
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     out_code = transform_code_object(code, transform)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1422, in transform_code_object
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     transformations(instructions, code_options)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 257, in _fn
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 715, in transform
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     tracer.run()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 3500, in run
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     super().run()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     while self.step():
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.dispatch_table[inst.opcode](self, inst)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 819, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return inner_fn(self, inst)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2168, in CALL_FUNCTION
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.call_function(fn, args, {})
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1170, in call_function
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/misc.py", line 903, in call_function
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self.obj.call_method(tx, self.name, args, kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/tensor.py", line 632, in call_method
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return wrap_fx_proxy(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2302, in wrap_fx_proxy
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2368, in wrap_fx_proxy_cls
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return _wrap_fx_proxy(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 2464, in _wrap_fx_proxy
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3127, in get_fake_value
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     ret_val = wrap_fake_exception(
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 2641, in wrap_fake_exception
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3128, in &lt;lambda&gt;
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 3295, in run_node
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return getattr(args[0], node.target)(*args[1:], **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return fn(*args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1282, in __torch_dispatch__
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self.dispatch(func, types, args, kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1823, in dispatch
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return self._cached_dispatch_impl(func, types, args, kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1393, in _cached_dispatch_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     output = self._dispatch_impl(func, types, args, kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2397, in _dispatch_impl
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     op_impl_out = op_impl(self, func, *args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 160, in dispatch_to_op_implementations_dict
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 422, in local_scalar_dense
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     r = fake_mode.shape_env.create_unbacked_symint()
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]     return retlog(fn(*args, **kwargs))
V0701 17:27:08.774000 31114 torch/fx/experimental/symbolic_shapes.py:5984] [16/0]
W0701 17:27:08.784000 31114 torch/fx/experimental/symbolic_shapes.py:6679] [16/0] failed during evaluate_expr(-u0 &gt; 60, hint=None, size_oblivious=True, forcing_spec=False
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] failed while running evaluate_expr(*(-u0 &gt; 60, None, False, True), **{})
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] Traceback (most recent call last):
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]     return retlog(fn(*args, **kwargs))
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]     return self._evaluate_expr(
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]     raise self._make_data_dependent_error(
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] For more information, run with TORCH_LOGS="dynamic"
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] User Stack (most recent call last):
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]   (snipped, see stack below for prefix)
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]     return y[a]
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0]
E0701 17:27:08.784000 31114 torch/fx/experimental/recording.py:299] [16/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] failed while attempting to run meta for aten.select.int
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] Traceback (most recent call last):
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2427, in _dispatch_impl
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     r = func(*args, **kwargs)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 756, in __call__
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self._op(*args, **kwargs)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py", line 5278, in meta_select
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     guard_size_oblivious(-index &gt; size) or guard_size_oblivious(index &gt;= size)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 408, in guard_size_oblivious
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return expr.node.guard_size_oblivious("", 0)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 588, in guard_size_oblivious
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     r = self.evaluate(size_oblivious=True)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 510, in evaluate
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self.shape_env.evaluate_sym_node(self, size_oblivious)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6655, in evaluate_sym_node
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self.evaluate_expr(
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return retlog(fn(*args, **kwargs))
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return self._evaluate_expr(
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     raise self._make_data_dependent_error(
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] For more information, run with TORCH_LOGS="dynamic"
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] User Stack (most recent call last):
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   (snipped, see stack below for prefix)
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]     return y[a]
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0]
E0701 17:27:08.786000 31114 torch/_subclasses/fake_tensor.py:2431] [16/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 709, in &lt;module&gt;
    export(Foo(), inps)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1344, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 739, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 1677, in inner
    result_traced = opt_f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
    raise e.with_traceback(None) from None
torch._dynamo.exc.UserError: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)

Caused by: return y[a]  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:702 in forward (_meta_registrations.py:5278 in meta_select)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

User Stack (most recent call last):
  (snipped, see stack below for prefix)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
    return y[a]

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#constrain-as-size-example

from user code:
   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 702, in forward
    return y[a]

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
</pre></div>
</div>
<p>Here is a scenario where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion is required simply to prevent an operation from failing. The export call will fail with
“Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">-u0</span> <span class="pre">&gt;</span> <span class="pre">60</span></code>”, implying that the compiler doesn’t know if this is a valid indexing operation -
if the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> is out-of-bounds for <code class="docutils literal notranslate"><span class="pre">y</span></code> or not. Here, manual specialization is too prohibitive, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> has no place.
Instead, informing the compiler of <code class="docutils literal notranslate"><span class="pre">u0</span></code>’s range is sufficient:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">y</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.800000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [17/0] create_env
I0701 17:27:08.805000 31114 torch/fx/experimental/symbolic_shapes.py:4276] [17/0] create_unbacked_symint u0 [-int_oo, int_oo] a = x.item()  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.806000 31114 torch/fx/experimental/symbolic_shapes.py:1130] [17/0] compute_unbacked_bindings [u0]
I0701 17:27:08.808000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [17/0] runtime_assert u0 &gt;= 0 [guard added] torch._check(a &gt;= 0)  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:722 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0701 17:27:08.808000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [17/0] _update_var_to_range u0 = VR[0, int_oo] (update)
I0701 17:27:08.813000 31114 torch/fx/experimental/symbolic_shapes.py:6630] [17/0] runtime_assert u0 &lt; 60 [guard added] torch._check(a &lt; y.shape[0])  # ar/lib/workspace/intermediate_source/torch_export_tutorial.py:723 in forward (_dynamo/utils.py:3284 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &lt; 60"
V0701 17:27:08.814000 31114 torch/fx/experimental/symbolic_shapes.py:6071] [17/0] _update_var_to_range u0 = VR[0, 59] (update)
V0701 17:27:08.817000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval size_oblivious(-u0 &gt; 60) == False [statically known]
V0701 17:27:08.817000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval size_oblivious(u0 &gt;= 60) == False [statically known]
V0701 17:27:08.818000 31114 torch/fx/experimental/symbolic_shapes.py:6787] [17/0] eval False == True [statically known]
V0701 17:27:08.821000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &gt;= 0 == True [statically known]
V0701 17:27:08.822000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &lt;= 59 == True [statically known]
V0701 17:27:08.823000 31114 torch/fx/experimental/symbolic_shapes.py:7018] [17/0] runtime_assert u0 &lt; 60 == True [statically known]
I0701 17:27:08.826000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [17/0] produce_guards
V0701 17:27:08.826000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:08.826000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].size()[0] 60 None
V0701 17:27:08.827000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].stride()[0] 1 None
V0701 17:27:08.827000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [17/0] track_symint L['y'].storage_offset() 0 None
I0701 17:27:08.839000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.840000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u1]
V0701 17:27:08.840000 31114 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u1 = VR[0, 59] (update)
I0701 17:27:08.841000 31114 torch/fx/experimental/symbolic_shapes.py:6234] set_replacement u1 = u0 (rename_unbacked_to) VR[0, 59]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None
            ge_1: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge_1'");  ge_1 = _assert_scalar_default = None
            le_1: "Sym(u0 &lt;= 59)" = item &lt;= 59
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u0 &lt;= 59 on node 'le_1'");  le_1 = _assert_scalar_default_1 = None

             #
            lt_1: "Sym(u0 &lt; 60)" = item &lt; 60
            _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, "Runtime assertion failed for expression u0 &lt; 60 on node 'lt_1'");  lt_1 = _assert_scalar_default_2 = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a]
            select: "f32[]" = torch.ops.aten.select.int(y, 0, item);  y = item = None
            return (select,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='select'), target=None)])
Range constraints: {u0: VR[0, 59], u1: VR[0, 59]}
</pre></div>
</div>
</div>
<div class="section" id="specialized-values">
<h3><a class="toc-backref" href="#id13">Specialized values</a><a class="headerlink" href="#specialized-values" title="Permalink to this heading">¶</a></h3>
<p>Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value
while tracing. This looks something like “Could not extract specialized integer from data-dependent expression”, and is analogous to
the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise
with evaluating concrete boolean values.</p>
<p>This error typically occurs when there is an explicit or implicit <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on a data-dependent expression. For example, this list comprehension
has a <cite>range()</cite> call that implicitly does an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on the size of the list:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" title="torch.cat"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a><span class="p">([</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="k">for</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.856000 31114 torch/fx/experimental/symbolic_shapes.py:3334] create_env
I0701 17:27:08.861000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.862000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u0]
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984] Data dependent variable 'u0' allocated at:
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/bin/sphinx-build", line 8, in &lt;module&gt;
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     sys.exit(main())
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 313, in main
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return make_main(argv)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 195, in make_main
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return make_mode.run_make_mode(argv[1:])
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 160, in run_make_mode
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return make.run_generic_build(args[0])
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py", line 148, in run_generic_build
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return build_main(args + opts)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py", line 276, in build_main
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 262, in __init__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self._init_builder()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/application.py", line 335, in _init_builder
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self.events.emit('builder-inited')
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx/events.py", line 94, in emit
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     results.append(listener.handler(self.app, *args))
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 491, in generate_gallery_rst
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ) = generate_dir_rst(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 431, in generate_dir_rst
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     intro, title, cost = generate_file_rst(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/conf.py", line 79, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     p.start()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self._popen = self._Popen(self)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return _default_context.get_context().Process._Popen(process_obj)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/context.py", line 281, in _Popen
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return Popen(process_obj)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self._launch(process_obj)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 71, in _launch
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     code = process_obj._bootstrap(parent_sentinel=child_r)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self.run()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     self._target(*self._args, **self._kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/conf.py", line 67, in call_fn
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     result = func(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 1027, in generate_file_rst
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     output_blocks, time_elapsed = execute_script(script_blocks,
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 945, in execute_script
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     output_blocks.append(execute_code_block(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 810, in execute_code_block
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     is_last_expr, mem_max = _exec_and_get_memory(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 676, in _exec_and_get_memory
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     mem_max, _ = gallery_conf['call_memory'](
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py", line 223, in call_memory
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return 0., func()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py", line 600, in __call__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     exec(self.code, self.fake_main.__dict__)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     export(Foo(), inps, strict=False)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return _export(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ep = fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ep = _export_for_training(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ep = fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     export_artifact = export_func(  # type: ignore[operator]
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1910, in _non_strict_export
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     aten_export_artifact = _to_aten_func(  # type: ignore[operator]
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1696, in _export_to_aten_ir_make_fx
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     gm, graph_signature = transform(_make_fx_helper)(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1840, in _aot_export_non_strict
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1616, in _make_fx_helper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     gm = make_fx(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2240, in wrapped
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return make_fx_tracer.trace(f, *args)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2178, in trace
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self._trace_inner(f, *args)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2149, in _trace_inner
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     t = dispatch_trace(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 51, in inner
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return disable_fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1174, in dispatch_trace
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1738, in trace
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     res = super().trace(root, concrete_args)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 838, in trace
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     (self.create_arg(fn(*args)),),
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1229, in wrapped
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     out = f(*tensors)  # type:ignore[call-arg]
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "&lt;string&gt;", line 1, in &lt;lambda&gt;
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1520, in wrapped_fn
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return tuple(flat_fn(*args))
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 184, in flat_fn
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     tree_out = fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 903, in functional_call
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     out = mod(*args[params_len:], **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self.call_module(mod, forward, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return Tracer.call_module(self, m, forward, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ret_val = forward(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return _orig_module_call(mod, *args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self._call_impl(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return forward_call(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1824, in forward
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     tree_out = mod(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self.call_module(mod, forward, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return Tracer.call_module(self, m, forward, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     ret_val = forward(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return _orig_module_call(mod, *args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self._call_impl(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return forward_call(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 747, in forward
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     a = x.item()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1277, in __torch_function__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1324, in __torch_function__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_export/non_strict_utils.py", line 683, in __torch_function__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return func(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 875, in handler
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return torch._library.utils.handle_dispatch_mode(
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_library/utils.py", line 296, in handle_dispatch_mode
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1379, in __torch_dispatch__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return proxy_call(self, func, self.pre_dispatch, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 914, in proxy_call
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     out = func(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 756, in __call__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self._op(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 27, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return fn(*args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1282, in __torch_dispatch__
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self.dispatch(func, types, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1823, in dispatch
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return self._cached_dispatch_impl(func, types, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1393, in _cached_dispatch_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     output = self._dispatch_impl(func, types, args, kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 2397, in _dispatch_impl
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     op_impl_out = op_impl(self, func, *args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 160, in dispatch_to_op_implementations_dict
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py", line 422, in local_scalar_dense
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     r = fake_mode.shape_env.create_unbacked_symint()
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]     return retlog(fn(*args, **kwargs))
V0701 17:27:08.863000 31114 torch/fx/experimental/symbolic_shapes.py:5984]
W0701 17:27:08.873000 31114 torch/fx/experimental/symbolic_shapes.py:6679] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] failed while running evaluate_expr(*(u0, None, False, False), **{})
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] Traceback (most recent call last):
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]     return retlog(fn(*args, **kwargs))
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]     return self._evaluate_expr(
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]     raise self._make_data_dependent_error(
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward)
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] For more information, run with TORCH_LOGS="dynamic"
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299]
E0701 17:27:08.873000 31114 torch/fx/experimental/recording.py:299] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1




def forward(self, arg0_1: "i64[]", arg1_1: "f32[60]"):
     # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: "Sym(u0)" = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None

Traceback (most recent call last):
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 756, in &lt;module&gt;
    export(Foo(), inps, strict=False)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py", line 360, in export
    return _export(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 2112, in _export
    ep = _export_for_training(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1092, in wrapper
    raise e
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1065, in wrapper
    ep = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py", line 121, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1975, in _export_for_training
    export_artifact = export_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1910, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1696, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1840, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1616, in _make_fx_helper
    gm = make_fx(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2240, in wrapped
    return make_fx_tracer.trace(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2178, in trace
    return self._trace_inner(f, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 2149, in _trace_inner
    t = dispatch_trace(
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1174, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1738, in trace
    res = super().trace(root, concrete_args)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 838, in trace
    (self.create_arg(fn(*args)),),
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1229, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File "&lt;string&gt;", line 1, in &lt;lambda&gt;
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1520, in wrapped_fn
    return tuple(flat_fn(*args))
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 903, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/export/_trace.py", line 1824, in forward
    tree_out = mod(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 813, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py", line 1808, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 531, in call_module
    ret_val = forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py", line 806, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/var/lib/workspace/intermediate_source/torch_export_tutorial.py", line 748, in forward
    b = torch.cat([y for y in range(a)], dim=0)
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 431, in __index__
    return self.node.int_()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 466, in int_
    return self.guard_int("", 0)  # NB: uses Python backtrace
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 516, in guard_int
    r = self.evaluate()
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/sym_node.py", line 510, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6655, in evaluate_sym_node
    return self.evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/recording.py", line 263, in wrapper
    return retlog(fn(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6671, in evaluate_expr
    return self._evaluate_expr(
  File "/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 6894, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)

Caused by: (ar/lib/workspace/intermediate_source/torch_export_tutorial.py:748 in forward)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
</pre></div>
</div>
<p>For these errors, some basic options you have are:</p>
<ol class="arabic simple">
<li><p>Avoid unnecessary <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast calls, in this case the <code class="docutils literal notranslate"><span class="pre">int(a)</span></code> in the return statement.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls; unfortunately all you may be able to do in this case is specialize (with <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">60)</span></code>).</p></li>
<li><p>Rewrite the offending code at a higher level. For example, the list comprehension is semantically a <code class="docutils literal notranslate"><span class="pre">repeat()</span></code> op, which doesn’t involve an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast. The following rewrite avoids data-dependent errors:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" title="torch.tensor"><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span></a><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">Foo</span></a><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.883000 31114 torch/fx/experimental/symbolic_shapes.py:3334] create_env
I0701 17:27:08.888000 31114 torch/fx/experimental/symbolic_shapes.py:4276] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:422 in local_scalar_dense)
I0701 17:27:08.889000 31114 torch/fx/experimental/symbolic_shapes.py:1130] compute_unbacked_bindings [u0]
I0701 17:27:08.893000 31114 torch/fx/experimental/symbolic_shapes.py:6630] runtime_assert u0 &gt;= 0 [guard added] (_refs/__init__.py:4796 in new_empty), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED="u0 &gt;= 0"
V0701 17:27:08.894000 31114 torch/fx/experimental/symbolic_shapes.py:6071] _update_var_to_range u0 = VR[0, int_oo] (update)
V0701 17:27:08.896000 31114 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Eq(u0, 0)) == False [statically known]
V0701 17:27:08.899000 31114 torch/fx/experimental/symbolic_shapes.py:6787] eval size_oblivious(Eq(u0, 1)) == False [statically known]
V0701 17:27:08.899000 31114 torch/fx/experimental/symbolic_shapes.py:7018] runtime_assert True == True [statically known]
I0701 17:27:08.903000 31114 torch/fx/experimental/symbolic_shapes.py:4734] produce_guards
V0701 17:27:08.903000 31114 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][0].storage_offset() 0 None
V0701 17:27:08.903000 31114 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].size()[0] 60 None
V0701 17:27:08.903000 31114 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].stride()[0] 1 None
V0701 17:27:08.904000 31114 torch/fx/experimental/symbolic_shapes.py:4954] track_symint L['args'][0][1].storage_offset() 0 None
V0701 17:27:08.905000 31114 torch/fx/experimental/symbolic_shapes.py:7018] runtime_assert u0 &gt;= 0 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "i64[]", y: "f32[60]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            item: "Sym(u0)" = torch.ops.aten.item.default(x);  x = None

             #
            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            ge: "Sym(u0 &gt;= 0)" = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, "Runtime assertion failed for expression u0 &gt;= 0 on node 'ge'");  ge = _assert_scalar_default = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1)
            unsqueeze: "f32[1, 60]" = torch.ops.aten.unsqueeze.default(y, 0);  y = None
            repeat: "f32[u0, 60]" = torch.ops.aten.repeat.default(unsqueeze, [item, 1]);  unsqueeze = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a
            add: "f32[u0, 60]" = torch.ops.aten.add.Tensor(repeat, item);  repeat = item = None
            return (add,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='add'), target=None)])
Range constraints: {u0: VR[0, int_oo]}
</pre></div>
</div>
<p>Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: <code class="docutils literal notranslate"><span class="pre">torch._check_is_size()</span></code>, <code class="docutils literal notranslate"><span class="pre">guard_size_oblivious()</span></code>, or real-tensor tracing, as starters.
For more in-depth guides, please refer to the <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html">Export Programming Model</a>,
or <a class="reference external" href="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs">Dealing with GuardOnDataDependentSymNode errors</a>.</p>
</div>
</div>
<div class="section" id="custom-ops">
<h2><a class="toc-backref" href="#id14">Custom Ops</a><a class="headerlink" href="#custom-ops" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> can export PyTorch programs with custom operators. Please
refer to <a class="reference external" href="https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html">this page</a>
on how to author a custom operator in either C++ or Python.</p>
<p>The following is an example of registering a custom operator in python to be
used by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. The important thing to note is that the custom op
must have a <a class="reference external" href="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290">FakeTensor kernel</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">"my_custom_library::custom_op"</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_op called!"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>

<span class="nd">@custom_op</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op_meta</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
    <span class="c1"># Returns an empty tensor with the same shape as the expected output</span>
    <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<p>Here is an example of exporting a program with the custom op.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomOpExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="torch.sin"><span class="n">torch</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">my_custom_library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="torch.cos"><span class="n">torch</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">export</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">CustomOpExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">exported_custom_op_example</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">exported_custom_op_example</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.920000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [18/0] create_env
I0701 17:27:08.929000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [18/0] produce_guards
V0701 17:27:08.929000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].size()[0] 3 None
V0701 17:27:08.930000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].size()[1] 3 None
V0701 17:27:08.930000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].stride()[0] 3 None
V0701 17:27:08.930000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].stride()[1] 1 None
V0701 17:27:08.930000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [18/0] track_symint L['x'].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]"):
             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x)
            sin: "f32[3, 3]" = torch.ops.aten.sin.default(x);  x = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x)
            custom_op: "f32[3, 3]" = torch.ops.my_custom_library.custom_op.default(sin);  sin = None

             # File: /var/lib/workspace/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x)
            cos: "f32[3, 3]" = torch.ops.aten.cos.default(custom_op);  custom_op = None
            return (cos,)

Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=&lt;InputKind.USER_INPUT: 1&gt;, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=&lt;OutputKind.USER_OUTPUT: 1&gt;, arg=TensorArgument(name='cos'), target=None)])
Range constraints: {}

custom_op called!
tensor([[1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 0.5830]])
</pre></div>
</div>
<p>Note that in the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, the custom operator is included in the graph.</p>
</div>
<div class="section" id="ir-decompositions">
<h2><a class="toc-backref" href="#id15">IR/Decompositions</a><a class="headerlink" href="#ir-decompositions" title="Permalink to this heading">¶</a></h2>
<p>The graph produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> returns a graph containing only
<a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen operators</a>, which are the
basic unit of computation in PyTorch. As there are over 3000 ATen operators,
export provides a way to narrow down the operator set used in the graph based
on certain characteristics, creating different IRs.</p>
<p>By default, export produces the most generic IR which contains all ATen
operators, including both functional and non-functional operators. A functional
operator is one that does not contain any mutations or aliasing of the inputs.
You can find a list of all ATen operators
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">here</a>
and you can inspect if an operator is functional by checking
<code class="docutils literal notranslate"><span class="pre">op._schema.is_mutable</span></code>, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add_</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>False
True
</pre></div>
</div>
<p>This generic IR can be used to train in eager PyTorch Autograd. This IR can be
more explicitly reached through the API <code class="docutils literal notranslate"><span class="pre">torch.export.export_for_training</span></code>,
which was introduced in PyTorch 2.5, but calling <code class="docutils literal notranslate"><span class="pre">torch.export.export</span></code>
should produce the same graph as of PyTorch 2.6.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DecompExample</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span></a><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,)</span>

<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_training</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">DecompExample</span></a><span class="p">(),</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_training</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:08.953000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [19/0] create_env
I0701 17:27:08.984000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [19/0] produce_guards
V0701 17:27:08.985000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[0] 1 None
V0701 17:27:08.985000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[1] 1 None
V0701 17:27:08.985000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[2] 3 None
V0701 17:27:08.985000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].size()[3] 3 None
V0701 17:27:08.986000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[0] 9 None
V0701 17:27:08.986000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[1] 9 None
V0701 17:27:08.986000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[2] 3 None
V0701 17:27:08.986000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].stride()[3] 1 None
V0701 17:27:08.987000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [19/0] track_symint L['x'].storage_offset() 0 None
graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {})
    return (batch_norm,)
</pre></div>
</div>
<p>We can then lower this exported program to an operator set which only contains
functional ATen operators through the API <code class="docutils literal notranslate"><span class="pre">run_decompositions</span></code>, which
decomposes the ATen operators into the ones specified in the decomposition
table, and functionalizes the graph. By specifying an empty set, we’re only
performing functionalization, and does not do any additional decompositions.
This results in an IR which contains ~2000 operators (instead of the 3000
operators above), and is ideal for inference cases.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_inference</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep_for_inference</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>As we can see, the previously mutable operator,
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add_.default</span></code> has now been replaced with
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.default</span></code>, a l operator.</p>
<p>We can also further lower this exported program to an operator set which only
contains the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir">Core ATen Operator Set</a>,
which is a collection of only ~180 operators. This IR is optimal for backends
who do not want to reimplement all ATen operators.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">default_decompositions</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a> <span class="o">=</span> <span class="n">default_decompositions</span><span class="p">()</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">core_aten_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">core_aten_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">core_aten_ep</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>We now see that <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> has been decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>. This is because <code class="docutils literal notranslate"><span class="pre">convolution</span></code>
is a more “core” operator, as operations like <code class="docutils literal notranslate"><span class="pre">conv1d</span></code> and <code class="docutils literal notranslate"><span class="pre">conv2d</span></code> can be
implemented using the same op.</p>
<p>We can also specify our own decomposition behaviors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module" href="https://docs.pytorch.org/docs/stable/export.html#module-torch.export" title="torch.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span></a><span class="o">.</span><span class="n">default_decompositions</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_awesome_custom_conv2d_function</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dilation</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">groups</span><span class="p">)</span>

<a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">conv2d</span><span class="o">.</span><span class="n">default</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_awesome_custom_conv2d_function</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">my_ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.run_decompositions" title="torch.export.ExportedProgram.run_decompositions"><span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export-decomp_utils sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.decomp_utils.CustomDecompTable" title="torch.export.decomp_utils.CustomDecompTable"><span class="n">my_decomp_table</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">my_ep</span></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>Notice that instead of <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> being decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>, it is now decomposed into
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mul.Tensor</span></code>,
which matches our custom decomposition rule.</p>
</div>
<div class="section" id="exportdb">
<h2><a class="toc-backref" href="#id16">ExportDB</a><a class="headerlink" href="#exportdb" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will only ever export a single computation graph from a PyTorch program. Because of this requirement,
there will be Python or PyTorch features that are not compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, which will require users to
rewrite parts of their model code. We have seen examples of this earlier in the tutorial – for example, rewriting
if-statements using <code class="docutils literal notranslate"><span class="pre">cond</span></code>.</p>
<p><a class="reference external" href="https://pytorch.org/docs/main/generated/exportdb/index.html">ExportDB</a> is the standard reference that documents
supported and unsupported Python/PyTorch features for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. It is essentially a list a program samples, each
of which represents the usage of one particular Python/PyTorch feature and its interaction with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.
Examples are also tagged by category so that they can be more easily searched.</p>
<p>For example, let’s use ExportDB to get a better understanding of how the predicate works in the <code class="docutils literal notranslate"><span class="pre">cond</span></code> operator.
We can look at the example called <code class="docutils literal notranslate"><span class="pre">cond_predicate</span></code>, which has a <code class="docutils literal notranslate"><span class="pre">torch.cond</span></code> tag. The example code looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cond_predicate</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    The conditional statement (aka predicate) passed to ``cond()`` must be one of the following:</span>
<span class="sd">    - ``torch.Tensor`` with a single element</span>
<span class="sd">    - boolean expression</span>
<span class="sd">    NOTE: If the `pred` is test on a dim with batch size &lt; 2, it will be specialized.</span>
<span class="sd">    """</span>
    <span class="n">pred</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="torch.Size"><span class="n">x</span><span class="o">.</span><span class="n">shape</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="k">lambda</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="p">:</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">y</span></a><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="p">[</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">])</span>
</pre></div>
</div>
<p>More generally, ExportDB can be used as a reference when one of the following occurs:</p>
<ol class="arabic simple">
<li><p>Before attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, you know ahead of time that your model uses some tricky Python/PyTorch features
and you want to know if <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> covers that feature.</p></li>
<li><p>When attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, there is a failure and it’s unclear how to work around it.</p></li>
</ol>
<p>ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach
out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
</div>
<div class="section" id="running-the-exported-program">
<h2><a class="toc-backref" href="#id17">Running the Exported Program</a><a class="headerlink" href="#running-the-exported-program" title="Permalink to this heading">¶</a></h2>
<p>As <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is only a graph capturing mechanism, calling the artifact
produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> eagerly will be equivalent to running the eager
module. To optimize the execution of the Exported Program, we can pass this
exported artifact to backends such as Inductor through <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AOTInductor</a>,
or <a class="reference external" href="https://pytorch.org/TensorRT/dynamo/dynamo_export.html">TensorRT</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">M</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">):</span>
        <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a><span class="p">)</span>
        <span class="k">return</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">x</span></a>

<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><span class="n">M</span></a><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">,))</span>

<span class="c1"># Run it eagerly</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">()(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>

<span class="c1"># Run it with torch.compile</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-method" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram.module" title="torch.export.ExportedProgram.module"><span class="n">ep</span><span class="o">.</span><span class="n">module</span></a><span class="p">(),</span> <span class="n">backend</span><span class="o">=</span><span class="s2">"inductor"</span><span class="p">)(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I0701 17:27:09.929000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [20/0] create_env
I0701 17:27:09.943000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [20/0] produce_guards
V0701 17:27:09.944000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].size()[0] 2 None
V0701 17:27:09.944000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].size()[1] 3 None
V0701 17:27:09.944000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].stride()[0] 3 None
V0701 17:27:09.944000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].stride()[1] 1 None
V0701 17:27:09.945000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [20/0] track_symint L['x'].storage_offset() 0 None
tensor([[ 0.0603,  1.3543, -0.8964],
        [-0.4105,  0.2331,  0.5954]], device='cuda:0',
       grad_fn=&lt;AddmmBackward0&gt;)
I0701 17:27:11.058000 31114 torch/fx/experimental/symbolic_shapes.py:3334] [21/0] create_env
/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:236: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.

I0701 17:27:11.947000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [21/0] produce_guards
I0701 17:27:11.956000 31114 torch/fx/experimental/symbolic_shapes.py:4734] [21/0] produce_guards
V0701 17:27:11.956000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].size()[0] 2 None
V0701 17:27:11.957000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].size()[1] 3 None
V0701 17:27:11.957000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].stride()[0] 3 None
V0701 17:27:11.957000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].stride()[1] 1 None
V0701 17:27:11.957000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['x'].storage_offset() 0 None
V0701 17:27:11.958000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 3 None
V0701 17:27:11.958000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 3 None
V0701 17:27:11.958000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 3 None
V0701 17:27:11.958000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None
V0701 17:27:11.958000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None
V0701 17:27:11.959000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 3 None
V0701 17:27:11.959000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None
V0701 17:27:11.959000 31114 torch/fx/experimental/symbolic_shapes.py:4954] [21/0] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None
V0701 17:27:11.960000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].size()[0] == 2
V0701 17:27:11.960000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].size()[1] == 3
V0701 17:27:11.960000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].stride()[0] == 3
V0701 17:27:11.960000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].stride()[1] == 1
V0701 17:27:11.961000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['x'].storage_offset() == 0
V0701 17:27:11.961000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 3
V0701 17:27:11.961000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 3
V0701 17:27:11.961000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 3
V0701 17:27:11.962000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1
V0701 17:27:11.962000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0
V0701 17:27:11.962000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 3
V0701 17:27:11.962000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1
V0701 17:27:11.963000 31114 torch/fx/experimental/symbolic_shapes.py:5156] [21/0] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0
tensor([[ 0.0603,  1.3543, -0.8964],
        [-0.4105,  0.2331,  0.5954]], device='cuda:0',
       grad_fn=&lt;CompiledFunctionBackward&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._inductor</span>

<span class="c1"># Note: these APIs are subject to change</span>
<span class="c1"># Compile the exported program to a PT2 archive using ``AOTInductor``</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt2_path</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_compile_and_package</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><span class="n">ep</span></a><span class="p">)</span>

<span class="c1"># Load and run the .so file in Python.</span>
<span class="c1"># To load and run it in a C++ environment, see:</span>
<span class="c1"># https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</span>
<span class="n">aoti_compiled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_load_package</span><span class="p">(</span><span class="n">pt2_path</span><span class="p">)</span>
<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">res</span></a> <span class="o">=</span> <span class="n">aoti_compiled</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor"><span class="n">inp</span></a><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="conclusion">
<h2><a class="toc-backref" href="#id18">Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>We introduced <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, the new PyTorch 2.X way to export single computation
graphs from PyTorch programs. In particular, we demonstrate several code modifications
and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  4.518 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-torch-export-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b865be3c401c1b4fbdb03f49916ac8e8/torch_export_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torch_export_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fb8083290582c4f473d970913a4186c4/torch_export_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torch_export_tutorial.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</article>
</div>
<footer>
<hr class="rating-hr hr-top"/>
<div class="rating-container">
<div class="rating-prompt">Rate this Tutorial</div>
<div class="stars-outer">
<i class="far fa-star" data-behavior="tutorial-rating" data-count="1" title="1 Star"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="2" title="2 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="3" title="3 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="4" title="4 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="5" title="5 Stars"></i>
</div>
</div>
<hr class="rating-hr hr-bottom"/>
<div role="contentinfo">
<p>
        © Copyright 2024, PyTorch.

    </p>
</div>
<div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
</footer>
</div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> This tutorial describes a prototype feature. Prototype features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">torch.export Tutorial</a><ul>
<li><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li><a class="reference internal" href="#graph-breaks">Graph Breaks</a></li>
<li><a class="reference internal" href="#non-strict-export">Non-Strict Export</a></li>
<li><a class="reference internal" href="#control-flow-ops">Control Flow Ops</a></li>
<li><a class="reference internal" href="#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a><ul>
<li><a class="reference internal" href="#basic-concepts-symbols-and-guards">Basic concepts: symbols and guards</a></li>
<li><a class="reference internal" href="#specialization">0/1 specialization</a></li>
<li><a class="reference internal" href="#named-dims">Named Dims</a></li>
<li><a class="reference internal" href="#constraint-violations-suggested-fixes">Constraint violations, suggested fixes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-dependent-errors">Data-dependent errors</a><ul>
<li><a class="reference internal" href="#guards-torch-check">Guards, torch._check()</a></li>
<li><a class="reference internal" href="#specialized-values">Specialized values</a></li>
</ul>
</li>
<li><a class="reference internal" href="#custom-ops">Custom Ops</a></li>
<li><a class="reference internal" href="#ir-decompositions">IR/Decompositions</a></li>
<li><a class="reference internal" href="#exportdb">ExportDB</a></li>
<li><a class="reference internal" href="#running-the-exported-program">Running the Exported Program</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/sphinx_highlight.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/katex.min.js"></script>
<script src="../_static/auto-render.min.js"></script>
<script src="../_static/katex_autorenderer.js"></script>
<script src="../_static/design-tabs.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script>

// Helper function to make it easier to call dataLayer.push()
function gtag(){window.dataLayer.push(arguments);}

//add microsoft link

if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }

    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }

  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    fbq('trackCustom', "Tutorial Rating", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      rating: $(this).attr("data-count")
    });
    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count"),
      'customEvent:Rating': $(this).attr("data-count") // send to GA custom dimension customEvent:Rating.
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }


</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView
  &amp;noscript=1" width="1"/>
</noscript>
<script type="text/javascript">
  var collapsedSections = ['PyTorch Recipes', 'Learning PyTorch', 'Image and Video', 'Audio', 'Text', 'Backends', 'Reinforcement Learning', 'Deploying PyTorch Models in Production', 'Profiling PyTorch', 'Code Transforms with FX', 'Frontend APIs', 'Extending PyTorch', 'Model Optimization', 'Parallel and Distributed Training', 'Edge with ExecuTorch', 'Recommendation Systems', 'Multimodality'];
</script>
<img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1"/>
<!-- Begin Footer -->
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4 text-center">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4 text-center">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4 text-center">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="footer-logo-wrapper">
<a class="footer-logo" href="https://pytorch.org/"></a>
</div>
<div class="footer-links-wrapper">
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://pytorch.org/get-started">Get Started</a></li>
<li><a href="https://pytorch.org/features">Features</a></li>
<li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
<li><a href="https://pytorch.org/blog/">Blog</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
<li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
<li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
<li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">Stay up to date</li>
<li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">PyTorch Podcasts</li>
<li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
<li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
<li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
</ul>
</div>
</div>
<div class="privacy-policy">
<ul>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
<li class="privacy-policy-links">|</li>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
</ul>
</div>
<div class="copyright">
<p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/images/pytorch-x.svg"/>
</div>
</div>
<!-- End Footer -->
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="container">
<div class="mobile-main-menu-header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#"></a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li class="resources-mobile-menu-title">
<a>Learn</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Ecosystem</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/ecosystem">Tools</a>
</li>
<li>
<a href="https://pytorch.org/#community-module">Community</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Edge</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/edge">About PyTorch Edge</a>
</li>
<li>
<a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
</li>
<li>
<a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Docs</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>Blog &amp; News</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/blog/">PyTorch Blog</a>
</li>
<li>
<a href="https://pytorch.org/community-blog">Community Blog</a>
</li>
<li>
<a href="https://pytorch.org/videos">Videos</a>
</li>
<li>
<a href="https://pytorch.org/community-stories">Community Stories</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://pytorch.org/newsletter">Newsletter</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
<a>About</a>
</li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/governing-board">Governing Board</a>
</li>
<li>
<a href="https://pytorch.org/credits">Cloud Credit Program</a>
</li>
<li>
<a href="https://pytorch.org/tac">Technical Advisory Council</a>
</li>
<li>
<a href="https://pytorch.org/staff">Staff</a>
</li>
<li>
<a href="https://pytorch.org/contact-us">Contact Us</a>
</li>
</ul>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>