{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Python Operators {#python-custom-ops-tutorial}\n",
    "=======================\n",
    "\n",
    "<div style=\"width: 45%; float: left; padding: 20px;\"><h2> What you will learn</h2><ul><li>How to integrate custom operators written in Python with PyTorch</li><li>How to test custom operators using <code>torch.library.opcheck</code></li></ul></div><div style=\"width: 45%; float: right; padding: 20px;\"><h2> Prerequisites</h2><ul><li>PyTorch 2.4 or later</li></ul></div>\n",
    "\n",
    "PyTorch offers a large library of operators that work on Tensors (e.g.\n",
    "`torch.add`, `torch.sum`, etc). However, you might wish to use a new\n",
    "customized operator with PyTorch, perhaps written by a third-party\n",
    "library. This tutorial shows how to wrap Python functions so that they\n",
    "behave like PyTorch native operators. Reasons why you may wish to create\n",
    "a custom operator in PyTorch include:\n",
    "\n",
    "-   Treating an arbitrary Python function as an opaque callable with\n",
    "    respect to `torch.compile` (that is, prevent `torch.compile` from\n",
    "    tracing into the function).\n",
    "-   Adding training support to an arbitrary Python function\n",
    "\n",
    "Use `torch.library.custom_op`{.interpreted-text role=\"func\"} to create\n",
    "Python custom operators. Use the C++ `TORCH_LIBRARY` APIs to create C++\n",
    "custom operators (these work in Python-less environments). See the\n",
    "[Custom Operators Landing\n",
    "Page](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html)\n",
    "for more details.\n",
    "\n",
    "Please note that if your operation can be expressed as a composition of\n",
    "existing PyTorch operators, then there is usually no need to use the\n",
    "custom operator API \\-- everything (for example `torch.compile`,\n",
    "training support) should just work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Wrapping PIL\\'s crop into a custom operator\n",
    "\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Let\\'s\n",
    "say that we are using PIL\\'s `crop` operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "import PIL\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop(pic, box):\n",
    "    img = to_pil_image(pic.cpu())\n",
    "    cropped_img = img.crop(box)\n",
    "    return pil_to_tensor(cropped_img).to(pic.device) / 255.\n",
    "\n",
    "def display(img):\n",
    "    plt.imshow(img.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "img = torch.ones(3, 64, 64)\n",
    "img *= torch.linspace(0, 1, steps=64) * torch.linspace(0, 1, steps=64).unsqueeze(-1)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cropped_img = crop(img, (10, 10, 50, 50))\n",
    "display(cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crop` is not handled effectively out-of-the-box by `torch.compile`:\n",
    "`torch.compile` induces a [\\\"graph\n",
    "break\\\"](https://pytorch.org/docs/stable/torch.compiler_faq.html#graph-breaks)\n",
    "on functions it is unable to handle and graph breaks are bad for\n",
    "performance. The following code demonstrates this by raising an error\n",
    "(`torch.compile` with `fullgraph=True` raises an error if a graph break\n",
    "occurs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f(img):\n",
    "    return crop(img, (10, 10, 50, 50))\n",
    "\n",
    "# The following raises an error. Uncomment the line to see it.\n",
    "# cropped_img = f(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to black-box `crop` for use with `torch.compile`, we need to do\n",
    "two things:\n",
    "\n",
    "1.  wrap the function into a PyTorch custom operator.\n",
    "2.  add a \\\"`FakeTensor` kernel\\\" (aka \\\"meta kernel\\\") to the operator.\n",
    "    Given some `FakeTensors` inputs (dummy Tensors that don\\'t have\n",
    "    storage), this function should return dummy Tensors of your choice\n",
    "    with the correct Tensor metadata (shape/strides/`dtype`/device).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "# Use torch.library.custom_op to define a new custom operator.\n",
    "# If your operator mutates any input Tensors, their names must be specified\n",
    "# in the ``mutates_args`` argument.\n",
    "@torch.library.custom_op(\"mylib::crop\", mutates_args=())\n",
    "def crop(pic: torch.Tensor, box: Sequence[int]) -> torch.Tensor:\n",
    "    img = to_pil_image(pic.cpu())\n",
    "    cropped_img = img.crop(box)\n",
    "    return (pil_to_tensor(cropped_img) / 255.).to(pic.device, pic.dtype)\n",
    "\n",
    "# Use register_fake to add a ``FakeTensor`` kernel for the operator\n",
    "@crop.register_fake\n",
    "def _(pic, box):\n",
    "    channels = pic.shape[0]\n",
    "    x0, y0, x1, y1 = box\n",
    "    result = pic.new_empty(y1 - y0, x1 - x0, channels).permute(2, 0, 1)\n",
    "    # The result should have the same metadata (shape/strides/``dtype``/device)\n",
    "    # as running the ``crop`` function above.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, `crop` now works without graph breaks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f(img):\n",
    "    return crop(img, (10, 10, 50, 50))\n",
    "\n",
    "cropped_img = f(img)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding training support for crop\n",
    "================================\n",
    "\n",
    "Use `torch.library.register_autograd` to add training support for an\n",
    "operator. Prefer this over directly using `torch.autograd.Function`;\n",
    "some compositions of `autograd.Function` with PyTorch operator\n",
    "registration APIs can lead to (and has led to) silent incorrectness when\n",
    "composed with `torch.compile`.\n",
    "\n",
    "If you don\\'t need training support, there is no need to use\n",
    "`torch.library.register_autograd`. If you end up training with a\n",
    "`custom_op` that doesn\\'t have an autograd registration, we\\'ll raise an\n",
    "error message.\n",
    "\n",
    "The gradient formula for `crop` is essentially `PIL.paste` (we\\'ll leave\n",
    "the derivation as an exercise to the reader). Let\\'s first wrap `paste`\n",
    "into a custom operator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"mylib::paste\", mutates_args=())\n",
    "def paste(im1: torch.Tensor, im2: torch.Tensor, coord: Sequence[int]) -> torch.Tensor:\n",
    "    assert im1.device == im2.device\n",
    "    assert im1.dtype == im2.dtype\n",
    "    im1_pil = to_pil_image(im1.cpu())\n",
    "    im2_pil = to_pil_image(im2.cpu())\n",
    "    PIL.Image.Image.paste(im1_pil, im2_pil, coord)\n",
    "    return (pil_to_tensor(im1_pil) / 255.).to(im1.device, im1.dtype)\n",
    "\n",
    "@paste.register_fake\n",
    "def _(im1, im2, coord):\n",
    "    assert im1.device == im2.device\n",
    "    assert im1.dtype == im2.dtype\n",
    "    return torch.empty_like(im1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let\\'s use `register_autograd` to specify the gradient formula\n",
    "for `crop`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward(ctx, grad_output):\n",
    "    grad_input = grad_output.new_zeros(ctx.pic_shape)\n",
    "    grad_input = paste(grad_input, grad_output, ctx.coords)\n",
    "    return grad_input, None\n",
    "\n",
    "def setup_context(ctx, inputs, output):\n",
    "    pic, box = inputs\n",
    "    ctx.coords = box[:2]\n",
    "    ctx.pic_shape = pic.shape\n",
    "\n",
    "crop.register_autograd(backward, setup_context=setup_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the backward must be a composition of PyTorch-understood\n",
    "operators, which is why we wrapped paste into a custom operator instead\n",
    "of directly using PIL\\'s paste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = img.requires_grad_()\n",
    "result = crop(img, (10, 10, 50, 50))\n",
    "result.sum().backward()\n",
    "display(img.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct gradient, with 1s (white) in the cropped region and\n",
    "0s (black) in the unused region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Python Custom operators\n",
    "===============================\n",
    "\n",
    "Use `torch.library.opcheck` to test that the custom operator was\n",
    "registered correctly. This does not test that the gradients are\n",
    "mathematically correct; please write separate tests for that (either\n",
    "manual ones or `torch.autograd.gradcheck`).\n",
    "\n",
    "To use `opcheck`, pass it a set of example inputs to test against. If\n",
    "your operator supports training, then the examples should include\n",
    "Tensors that require grad. If your operator supports multiple devices,\n",
    "then the examples should include Tensors from each device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    [torch.randn(3, 64, 64), [0, 0, 10, 10]],\n",
    "    [torch.randn(3, 91, 91, requires_grad=True), [10, 0, 20, 10]],\n",
    "    [torch.randn(3, 60, 60, dtype=torch.double), [3, 4, 32, 20]],\n",
    "    [torch.randn(3, 512, 512, requires_grad=True, dtype=torch.double), [3, 4, 32, 45]],\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    torch.library.opcheck(crop, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutable Python Custom operators\n",
    "===============================\n",
    "\n",
    "You can also wrap a Python function that mutates its inputs into a\n",
    "custom operator. Functions that mutate inputs are common because that is\n",
    "how many low-level kernels are written; for example, a kernel that\n",
    "computes `sin` may take in the input and an output tensor and write\n",
    "`input.sin()` to the output tensor.\n",
    "\n",
    "We\\'ll use `numpy.sin` to demonstrate an example of a mutable Python\n",
    "custom operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@torch.library.custom_op(\"mylib::numpy_sin\", mutates_args={\"output\"}, device_types=\"cpu\")\n",
    "def numpy_sin(input: torch.Tensor, output: torch.Tensor) -> None:\n",
    "    assert input.device == output.device\n",
    "    assert input.device.type == \"cpu\"\n",
    "    input_np = input.numpy()\n",
    "    output_np = output.numpy()\n",
    "    np.sin(input_np, out=output_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the operator doesn\\'t return anything, there is no need to\n",
    "register a `FakeTensor` kernel (meta kernel) to get it to work with\n",
    "`torch.compile`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f(x):\n",
    "    out = torch.empty(3)\n",
    "    numpy_sin(x, out)\n",
    "    return out\n",
    "\n",
    "x = torch.randn(3)\n",
    "y = f(x)\n",
    "assert torch.allclose(y, x.sin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here\\'s an `opcheck` run telling us that we did indeed register the\n",
    "operator correctly. `opcheck` would error out if we forgot to add the\n",
    "output to `mutates_args`, for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_inputs = [\n",
    "    [torch.randn(3), torch.empty(3)],\n",
    "    [torch.randn(0, 3), torch.empty(0, 3)],\n",
    "    [torch.randn(1, 2, 3, 4, dtype=torch.double), torch.empty(1, 2, 3, 4, dtype=torch.double)],\n",
    "]\n",
    "\n",
    "for example in example_inputs:\n",
    "    torch.library.opcheck(numpy_sin, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we learned how to use `torch.library.custom_op` to\n",
    "create a custom operator in Python that works with PyTorch subsystems\n",
    "such as `torch.compile` and autograd.\n",
    "\n",
    "This tutorial provides a basic introduction to custom operators. For\n",
    "more detailed information, see:\n",
    "\n",
    "-   [the torch.library\n",
    "    documentation](https://pytorch.org/docs/stable/library.html)\n",
    "-   [the Custom Operators\n",
    "    Manual](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-custom-operators-manual)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
