{
  "nbformat": 4,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "version": "3.5.2",
      "nbconvert_exporter": "python"
    }
  },
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nnn package\n==========\n\nWe\u2019ve redesigned the nn package, so that it\u2019s fully integrated with\nautograd. Let's review the changes.\n\n**Replace containers with autograd:**\n\n    You no longer have to use Containers like ``ConcatTable``, or modules like\n    ``CAddTable``, or use and debug with nngraph. We will seamlessly use\n    autograd to define our neural networks. For example,\n\n    * ``output = nn.CAddTable():forward({input1, input2})`` simply becomes\n      ``output = input1 + input2``\n    * ``output = nn.MulConstant(0.5):forward(input)`` simply becomes\n      ``output = input * 0.5``\n\n**State is no longer held in the module, but in the network graph:**\n\n    Using recurrent networks should be simpler because of this reason. If\n    you want to create a recurrent network, simply use the same Linear layer\n    multiple times, without having to think about sharing weights.\n\n    .. figure:: /_static/img/torch-nn-vs-pytorch-nn.png\n       :alt: torch-nn-vs-pytorch-nn\n\n       torch-nn-vs-pytorch-nn\n\n**Simplified debugging:**\n\n    Debugging is intuitive using Python\u2019s pdb debugger, and **the debugger\n    and stack traces stop at exactly where an error occurred.** What you see\n    is what you get.\n\nExample 1: ConvNet\n------------------\n\nLet\u2019s see how to create a small ConvNet.\n\nAll of your networks are derived from the base class ``nn.Module``:\n\n-  In the constructor, you declare all the layers you want to use.\n-  In the forward function, you define how your model is going to be\n   run, from input to output\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MNISTConvNet(nn.Module):\n\n    def __init__(self):\n        # this is the place where you instantiate all your modules\n        # you can later access them using the same names you've given them in\n        # here\n        super(MNISTConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, 5)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(10, 20, 5)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    # it's the forward function that defines the network structure\n    # we're accepting only a single input in here, but if you want,\n    # feel free to use more\n    def forward(self, input):\n        x = self.pool1(F.relu(self.conv1(input)))\n        x = self.pool2(F.relu(self.conv2(x)))\n\n        # in your model definition you can go full crazy and use arbitrary\n        # python code to define your model structure\n        # all these are perfectly legal, and will be handled correctly\n        # by autograd:\n        # if x.gt(0) > x.numel() / 2:\n        #      ...\n        #\n        # you can even do a loop and reuse the same module inside it\n        # modules no longer hold ephemeral state, so you can use them\n        # multiple times during your forward pass\n        # while x.norm(2) < 10:\n        #    x = self.conv1(x)\n\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the defined ConvNet now.\nYou create an instance of the class first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "net = MNISTConvNet()\nprint(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches The entire ``torch.nn``\n    package only supports inputs that are a mini-batch of samples, and not\n    a single sample.\n\n    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n    ``nSamples x nChannels x Height x Width``.\n\n    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n    a fake batch dimension.</p></div>\n\nCreate a mini-batch containing a single sample of random data and send the\nsample through the ConvNet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "input = Variable(torch.randn(1, 1, 28, 28))\nout = net(input)\nprint(out.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a dummy target label and compute error using a loss function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "target = Variable(torch.LongTensor([3]))\nloss_fn = nn.CrossEntropyLoss()  # LogSoftmax + ClassNLL Loss\nerr = loss_fn(out, target)\nerr.backward()\n\nprint(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of the ConvNet ``out`` is a ``Variable``. We compute the loss\nusing that, and that results in ``err`` which is also a ``Variable``.\nCalling ``.backward`` on ``err`` hence will propagate gradients all the\nway through the ConvNet to it\u2019s weights\n\nLet's access individual layer weights and gradients:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "print(net.conv1.weight.grad.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "print(net.conv1.weight.data.norm())  # norm of the weight\nprint(net.conv1.weight.grad.data.norm())  # norm of the gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Forward and Backward Function Hooks\n-----------------------------------\n\nWe\u2019ve inspected the weights and the gradients. But how about inspecting\n/ modifying the output and grad\\_output of a layer?\n\nWe introduce **hooks** for this purpose.\n\nYou can register a function on a ``Module`` or a ``Variable``.\nThe hook can be a forward hook or a backward hook.\nThe forward hook will be executed when a forward call is executed.\nThe backward hook will be executed in the backward phase.\nLet\u2019s look at an example.\n\nWe register a forward hook on conv2 and print some information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def printnorm(self, input, output):\n    # input is a tuple of packed inputs\n    # output is a Variable. output.data is the Tensor we are interested\n    print('Inside ' + self.__class__.__name__ + ' forward')\n    print('')\n    print('input: ', type(input))\n    print('input[0]: ', type(input[0]))\n    print('output: ', type(output))\n    print('')\n    print('input size:', input[0].size())\n    print('output size:', output.data.size())\n    print('output norm:', output.data.norm())\n\n\nnet.conv2.register_forward_hook(printnorm)\n\nout = net(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We register a backward hook on conv2 and print some information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def printgradnorm(self, grad_input, grad_output):\n    print('Inside ' + self.__class__.__name__ + ' backward')\n    print('Inside class:' + self.__class__.__name__)\n    print('')\n    print('grad_input: ', type(grad_input))\n    print('grad_input[0]: ', type(grad_input[0]))\n    print('grad_output: ', type(grad_output))\n    print('grad_output[0]: ', type(grad_output[0]))\n    print('')\n    print('grad_input size:', grad_input[0].size())\n    print('grad_output size:', grad_output[0].size())\n    print('grad_input norm:', grad_input[0].data.norm())\n\n\nnet.conv2.register_backward_hook(printgradnorm)\n\nout = net(input)\nerr = loss_fn(out, target)\nerr.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A full and working MNIST example is located here\nhttps://github.com/pytorch/examples/tree/master/mnist\n\nExample 2: Recurrent Net\n------------------------\n\nNext, let\u2019s look at building recurrent nets with PyTorch.\n\nSince the state of the network is held in the graph and not in the\nlayers, you can simply create an nn.Linear and reuse it over and over\nagain for the recurrence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "class RNN(nn.Module):\n\n    # you can also accept arguments in your model constructor\n    def __init__(self, data_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n        input_size = data_size + hidden_size\n\n        self.i2h = nn.Linear(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n\n    def forward(self, data, last_hidden):\n        input = torch.cat((data, last_hidden), 1)\n        hidden = self.i2h(input)\n        output = self.h2o(hidden)\n        return hidden, output\n\n\nrnn = RNN(50, 20, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more complete Language Modeling example using LSTMs and Penn Tree-bank\nis located\n`here <https://github.com/pytorch/examples/tree/master/word\\_language\\_model>`_\n\nPyTorch by default has seamless CuDNN integration for ConvNets and\nRecurrent Nets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "loss_fn = nn.MSELoss()\n\nbatch_size = 10\nTIMESTEPS = 5\n\n# Create some fake data\nbatch = Variable(torch.randn(batch_size, 50))\nhidden = Variable(torch.zeros(batch_size, 20))\ntarget = Variable(torch.zeros(batch_size, 10))\n\nloss = 0\nfor t in range(TIMESTEPS):\n    # yes! you can reuse the same network several times,\n    # sum up the losses, and call backward!\n    hidden, output = rnn(batch, hidden)\n    loss += loss_fn(output, target)\nloss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}