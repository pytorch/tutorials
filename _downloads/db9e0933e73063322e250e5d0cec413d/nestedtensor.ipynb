{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Nested Tensors\n\nNested tensor is very similar to regular tensor, except for the shape:\n\n* for a regular tensor, each dimension has a size\n\n* for a nested tensor, not all dimensions have regular sizes; some of them are jagged\n\nNested tensors are a natural solution for representing sequential data within various domains:\n\n* in NLP, sentences can have variable lengths, so a batch of sentences forms a nested tensor\n\n* in CV, images can have variable shapes, so a batch of images forms a nested tensor\n\nIn this tutorial, we will demonstrate basic usage of nested tensors and motivate their usefulness\nfor operating on sequential data of varying lengths with a real-world example.\n\nThe nested tensor operations used here have not been released yet.\nYou will have to install the latest nightly to run this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn.functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nested Tensor Initialization\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the Python frontend, a nested tensor can be created from a list of tensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nt = torch.nested_tensor([torch.randn((2, 6)), torch.randn((3, 6))], device=device)\nprint(nt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By padding every underlying tensor to the same shape,\na nested tensor can be converted to a regular tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pt = nt.to_padded_tensor(0.0)\nprint(pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For practical reasons, conceptually we implement nested tensor\nas a batch of tensors with different shapes,\ni.e. dimension 0 is assumed to be the batch dimension.\nIndexing dimension 0 gives back the underlying tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"0th underlying tensor:\", nt[0], sep='\\n')\nprint(\"last column of 1st underlying tensor:\", nt[1, :, -1], sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Slicing in dimension 0 has not been supported yet.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nested Tensor Operations\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As each operation must be explicitly implemented for nested tensors,\noperation coverage for nested tensors is currently narrower than that of regular tensors.\nFor now, only basic operations such as index, dropout, softmax, transpose, reshape, linear, bmm are covered.\nHowever, coverage is being expanded rapidly.\nIf you need certain operations, please file an [issue](https://github.com/pytorch/pytorch)_\nto help us prioritize coverage.\n\n**reshape**\n\nThe reshape op is for changing the shape of a tensor.\nIts full semantics for regular tensors can be found\n[here](https://pytorch.org/docs/stable/generated/torch.reshape.html)_.\nFor regular tensors, when specifying the new shape,\na single dimension may be -1, in which case it is inferred\nfrom the remaining dimensions and the number of elements.\n\nThe semantics for nested tensors are similar, except that -1 no longer infers.\nInstead, it inherits the old size (here 2 for ``nt[0]`` and 3 for ``nt[1]``).\n-1 is the only legal size to specify for a jagged dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nt1 = nt.reshape(2, -1, 2, 3)\nprint(nt1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**transpose**\n\nThe transpose op is for swapping two dimensions of a tensor.\nIts full semantics can be found\n[here](https://pytorch.org/docs/stable/generated/torch.transpose.html)_.\nNote that nested tensor dimension 0 is special;\nit is assumed to be the batch dimension,\nso transposes involving nested tensor dimension 0 are forbidden.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nt2 = nt1.transpose(1, 2)\nprint(nt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**others**\n\nOther operations have the same semantics as for regular tensors.\nApplying the operation on a nested tensor is equivalent to\napplying the operation to the underlying tensor components,\nwith the result being a nested tensor as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nt_mm = torch.nested_tensor([torch.randn((2, 3, 4)), torch.randn((2, 3, 5))], device=device)\nnt3 = torch.matmul(nt2, nt_mm)\nprint(\"matmul:\", nt3, sep='\\n')\n\nnt4 = F.dropout(nt3, 0.1)\nprint(\"dropout:\", nt4, sep='\\n')\n\nnt5 = F.softmax(nt4, -1)\nprint(\"softmax:\", nt5, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Nested Tensor\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the age before nested tensor, one has to manually pad each data tensor\nto the same shape to form a batch as a regular tensor.\nFor example, we have 2 sentences and a vocabulary, then pad with 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sentences = [[\"goodbye\", \"padding\"],\n             [\"embrace\", \"nested\", \"tensor\"]]\nvocabulary = {\"goodbye\" : 1.0, \"padding\" : 2.0,\n              \"embrace\" : 3.0, \"nested\" : 4.0, \"tensor\" : 5.0}\npadded_sentences = torch.tensor([[1.0, 2.0, 0.0],\n                                 [3.0, 4.0, 5.0]])\nnested_sentences = torch.nested_tensor([torch.tensor([1.0, 2.0]),\n                                        torch.tensor([3.0, 4.0, 5.0])])\nprint(padded_sentences)\nprint(nested_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, padding introduces inefficiency.\nFurther, padding with zeros does not correctly treat entries as padding for every operation,\ne.g. in softmax one has to pad with -inf rather than 0 to ignore specific entries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "padded_sentences_for_softmax = torch.tensor([[1.0, 2.0, float(\"-inf\")],\n                                             [3.0, 4.0, 5.0]])\nprint(F.softmax(padded_sentences_for_softmax, -1))\nprint(F.softmax(nested_sentences, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us take a look at a practical example: the multi-head attention component\nutilized in [Transformers](https://arxiv.org/pdf/1706.03762.pdf)_.\nThe nested tensor version is straightforward.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n\n\"\"\"\nArgs:\n    query: query of shape (N, L_t, E_q)\n    key: key of shape (N, L_s, E_k)\n    value: value of shape (N, L_s, E_v)\n    nheads: number of heads in multi-head attention\n    W_q: Weight for query input projection of shape (E_total, E_q)\n    W_k: Weight for key input projection of shape (E_total, E_k)\n    W_v: Weight for value input projection of shape (E_total, E_v)\n    W_out: Weight for output projection of shape (E_out, E_total)\n    b_q (optional): Bias for query input projection of shape E_total. Default: None\n    b_k (optional): Bias for key input projection of shape E_total. Default: None\n    b_v (optional): Bias for value input projection of shape E_total. Default: None\n    b_out (optional): Bias for output projection of shape E_out. Default: None\n    dropout_p: dropout probability. Default: 0.0\n    where:\n        N is the batch size\n        L_t is the target sequence length (jagged)\n        L_s is the source sequence length (jagged)\n        E_q is the embedding size for query\n        E_k is the embedding size for key\n        E_v is the embedding size for value\n        E_total is the embedding size for all heads combined\n        E_out is the output embedding size\nReturns:\n    attn_output: Output of shape (N, L_t, E_out)\n\"\"\"\ndef mha_nested(query, key, value, nheads,\nW_q, W_k, W_v, W_out,\nb_q=None, b_k=None, b_v=None, b_out=None,\ndropout_p=0.0):\n    N = query.size(0)\n    E_total = W_q.size(0)\n    assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n    E_head = E_total // nheads\n\n    # apply input projection\n    # (N, L_t, E_q) -> (N, L_t, E_total)\n    query = F.linear(query, W_q, b_q)\n    # (N, L_s, E_k) -> (N, L_s, E_total)\n    key = F.linear(key, W_k, b_k)\n    # (N, L_s, E_v) -> (N, L_s, E_total)\n    value = F.linear(value, W_v, b_v)\n\n    # reshape query, key, value to separate by head\n    # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n    query = query.reshape(-1, -1, nheads, E_head).transpose(1, 2)\n    # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n    key = key.reshape(-1, -1, nheads, E_head).transpose(1, 2)\n    # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n    value = value.reshape(-1, -1, nheads, E_head).transpose(1, 2)\n\n    # query matmul key^T\n    # (N, nheads, L_t, E_head) x (N, nheads, L_s, E_head)^T -> (N, nheads, L_t, L_s)\n    keyT = key.transpose(-1, -2)\n    attn_weights = torch.matmul(query, keyT)\n\n    # scale down\n    attn_weights = attn_weights * (1.0 / math.sqrt(E_head))\n\n    # softmax\n    attn_weights = F.softmax(attn_weights, dim=-1)\n\n    # dropout\n    if dropout_p > 0.0:\n        attn_weights = F.dropout(attn_weights, p=dropout_p)\n\n    # attention_weights matmul value\n    # (N, nheads, L_t, L_s) x (N, nheads, L_s, E_head) -> (N, nheads, L_t, E_head)\n    attn_output = torch.matmul(attn_weights, value)\n\n    # merge heads\n    # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n    attn_output = attn_output.transpose(1, 2).reshape(N, -1, E_total)\n\n    # apply output projection\n    # (N, L_t, E_total) -> (N, L_t, E_out)\n    attn_output = F.linear(attn_output, W_out, b_out)\n\n    return attn_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 0-padded tensor version additionally requires masks\nfor more complicated treatments at padded entries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\nArgs:\n    query: query of shape (N, L_t, E_q)\n    key: key of shape (N, L_s, E_k)\n    value: value of shape (N, L_s, E_v)\n    nheads: number of heads in multi-head attention\n    attn_mask_q: boolean mask indicating locations that should not take part in attention for query, shape (N, L_t)\n    attn_mask_kv: boolean mask indicating locations that should not take part in attention for key and value, shape (N, L_s)\n    W_q: Weight for query input projection of shape (E_total, E_q)\n    W_k: Weight for key input projection of shape (E_total, E_k)\n    W_v: Weight for value input projection of shape (E_total, E_v)\n    W_out: Weight for output projection of shape (E_out, E_total)\n    b_q (optional): Bias for query input projection of shape E_total. Default: None\n    b_k (optional): Bias for key input projection of shape E_total. Default: None\n    b_v (optional): Bias for value input projection of shape E_total. Default: None\n    b_out (optional): Bias for output projection of shape E_out. Default: None\n    dropout_p: dropout probability. Default: 0.0\n    where:\n        N is the batch size\n        L_t is the target sequence length (padded)\n        L_s is the source sequence length (padded)\n        E_q is the embedding size for query\n        E_k is the embedding size for key\n        E_v is the embedding size for value\n        E_total is the embedding size for all heads combined\n        E_out is the output embedding size\nReturns:\n    attn_output: Output of shape (N, L_t, E_out)\n\"\"\"\ndef mha_padded(query, key, value, nheads,\nattn_mask_q, attn_mask_kv,\nW_q, W_k, W_v, W_out,\nb_q=None, b_k=None, b_v=None, b_out=None,\ndropout_p=0.0):\n    N = query.size(0)\n    L_t = query.size(1)\n    L_s = key.size(1)\n    E_total = W_q.size(0)\n    assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n    E_head = E_total // nheads\n\n    # apply input projection\n    # (N, L_t, E_q) -> (N, L_t, E_total)\n    query = F.linear(query, W_q, b_q)\n    # (N, L_s, E_k) -> (N, L_s, E_total)\n    key = F.linear(key, W_k, b_k)\n    # (N, L_s, E_v) -> (N, L_s, E_total)\n    value = F.linear(value, W_v, b_v)\n\n    # padding-specific step: remove bias from padded entries\n    # in the specific multihead-attention formula it is not necessary to remove these bias\n    # because the -inf padding later on in softmax step can take care of it\n    # but to be general here we demonstrate the bias removal\n    for i in range(N):\n        for j in range(L_t):\n            if attn_mask_q[i, j]:\n                query[i, j, :] = 0.0\n        for j in range(L_s):\n            if attn_mask_kv[i, j]:\n                key[i, j, :] = 0.0\n                value[i, j, :] = 0.0\n\n    # reshape query, key, value to separate by head\n    # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head) -> (N * nheads, L_t, E_head)\n    query = query.reshape(N, -1, nheads, E_head).transpose(1, 2).reshape(N * nheads, -1, E_head)\n    # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head) -> (N * nheads, L_s, E_head)\n    key = key.reshape(N, -1, nheads, E_head).transpose(1, 2).reshape(N * nheads, -1, E_head)\n    # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head) -> (N * nheads, L_s, E_head)\n    value = value.reshape(N, -1, nheads, E_head).transpose(1, 2).reshape(N * nheads, -1, E_head)\n\n    # query bmm key^T\n    # (N * nheads, L_t, E_head) x (N * nheads, L_s, E_head)^T -> (N * nheads, L_t, L_s)\n    keyT = key.transpose(-1, -2)\n    # padding-specific step: add -inf mask for padding in softmax\n    attn_mask = query.new_zeros((N, nheads, L_t, L_s))\n    for i in range(N):\n        for j in range(L_t):\n            for k in range(L_s):\n                if attn_mask_q[i, j] or attn_mask_kv[i, k]:\n                    attn_mask[i, :, j, k] = float(\"-inf\")\n    attn_mask = attn_mask.reshape((N * nheads, L_t, L_s))\n    attn_weights = torch.baddbmm(attn_mask, query, keyT)\n    # if no padding, it could have been as simple as\n    #     attn_weights = torch.bmm(query, keyT)\n\n    # scale down\n    attn_weights = attn_weights * (1.0 / math.sqrt(E_head))\n\n    # softmax\n    attn_weights = F.softmax(attn_weights, dim=-1).nan_to_num_(0.0)\n\n    # dropout\n    if dropout_p > 0.0:\n        attn_weights = F.dropout(attn_weights, p=dropout_p)\n\n    # attention_weights bmm value\n    # (N * nheads, L_t, L_s) x (N * nheads, L_s, E_head) -> (N * nheads, L_t, E_head)\n    attn_output = attn_weights.bmm(value)\n\n    # merge heads\n    # (N * nheads, L_t, E_head) -> (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n    attn_output = attn_output.reshape(N, nheads, -1, E_head).transpose(1, 2).reshape(N, -1, E_total)\n\n    # apply output projection\n    # (N, L_t, E_total) -> (N, L_t, E_out)\n    attn_output = F.linear(attn_output, W_out, b_out)\n\n    # padding-specific step: remove output projection bias from padded entries\n    for i in range(N):\n        for j in range(L_t):\n            if attn_mask_q[i, j]:\n                attn_output[i, j, :] = 0.0\n\n    return attn_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "set hyperparameters following [the Transformer paper](https://arxiv.org/pdf/1706.03762.pdf)_\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 512\nE_q, E_k, E_v, E_total, E_out = 512, 512, 512, 512, 512\nnheads = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "except for dropout probability: set to 0 for correctness check\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dropout_p = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us generate some realistic fake data from Zipf's law.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndef zipf_sentence_lengths(alpha: float, batch_size: int) -> np.ndarray:\n    # generate fake corpus by unigram Zipf distribution\n    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n    sentence_lengths = np.empty(batch_size, dtype=int)\n    for ibatch in range(batch_size):\n        sentence_lengths[ibatch] = 1\n        word = np.random.zipf(alpha)\n        while word != 3 and word != 386 and word != 858:\n            sentence_lengths[ibatch] += 1\n            word = np.random.zipf(alpha)\n    return sentence_lengths\n\nalpha = 1.2\n\nsentence_lengths = zipf_sentence_lengths(alpha, N)\nL_t = np.max(sentence_lengths)\nL_s = L_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create inputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create parameters\nW_q, b_q = torch.randn((E_total, E_q), device=device), torch.randn(E_total, device=device)\nW_k, b_k = torch.randn((E_total, E_k), device=device), torch.randn(E_total, device=device)\nW_v, b_v = torch.randn((E_total, E_v), device=device), torch.randn(E_total, device=device)\nW_out, b_out = torch.randn((E_out, E_total), device=device), torch.randn(E_out, device=device)\n\n# create nested input\nqueries = []\nkeys    = []\nvalues  = []\nfor i in range(N):\n    l = sentence_lengths[i]\n    s = l\n    queries.append(torch.randn((l, E_q), device=device))\n    keys   .append(torch.randn((s, E_k), device=device))\n    values .append(torch.randn((s, E_v), device=device))\nquery = torch.nested_tensor(queries)\nkey   = torch.nested_tensor(keys   )\nvalue = torch.nested_tensor(values )\n\n# pad input\npadded_query = query.to_padded_tensor(0.0, (N, L_t, E_q))\npadded_key   = key  .to_padded_tensor(0.0, (N, L_s, E_k))\npadded_value = value.to_padded_tensor(0.0, (N, L_s, E_v))\n\n# create attention masks\nattn_mask_q = torch.zeros((N, L_t), dtype=torch.bool)\nattn_mask_kv = torch.zeros((N, L_s), dtype=torch.bool)\nfor i in range(N):\n    for j in range(L_t):\n        if padded_query[i, j, :].abs().max().item() == 0.0:\n            attn_mask_q[i, j] = True\n    for j in range(L_s):\n        if padded_key[i, j, :].abs().max().item() == 0.0:\n            attn_mask_kv[i, j] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "check correctness and performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\nt0 = timeit.default_timer()\nout_nested = mha_nested(\n    query, key, value, nheads,\n    W_q, W_k, W_v, W_out,\n    b_q=b_q, b_k=b_k, b_v=b_v, b_out=b_out,\n    dropout_p=dropout_p)\n\nt1 = timeit.default_timer()\nout_padded = mha_padded(\n    padded_query, padded_key, padded_value, nheads,\n    attn_mask_q, attn_mask_kv,\n    W_q, W_k, W_v, W_out,\n    b_q=b_q, b_k=b_k, b_v=b_v, b_out=b_out,\n    dropout_p=dropout_p)\nt2 = timeit.default_timer()\n\nprint(\"nested and padded calculations differ by\", (out_nested.to_padded_tensor(0.0, (N, L_t, E_out)) - out_padded).abs().max().item())\nprint(\"nested tensor multi-head attention takes\", t1 - t0, \"seconds\")\nprint(\"padded tensor multi-head attention takes\", t2 - t1, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The nested tensor version avoids wasted computation on padding,\nso in sequential CPU execution it is faster than padded tensor version as expected.\nOptimization for multi-threaded environment is underway.\n\nFor now, performant kernels are provided for specific use cases, e.g.\nself-attention evaluation by multi-head attention formula.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# embeddings are assumed to be the same\nE = E_total\nmha_lib = torch.nn.MultiheadAttention(E, nheads, batch_first=True, device=device)\nmha_lib.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "extract parameters for correctness check\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mha_lib.in_proj_weight.requires_grad_(False)\nmha_lib.in_proj_bias.requires_grad_(False)\nmha_lib.out_proj.weight.requires_grad_(False)\nmha_lib.out_proj.bias.requires_grad_(False)\nW_q, b_q = mha_lib.in_proj_weight[: E, :], mha_lib.in_proj_bias[: E]\nW_k, b_k = mha_lib.in_proj_weight[E : 2 * E, :], mha_lib.in_proj_bias[E : 2 * E]\nW_v, b_v = mha_lib.in_proj_weight[2 * E :, :], mha_lib.in_proj_bias[2 * E :]\nW_out, b_out = mha_lib.out_proj.weight, mha_lib.out_proj.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "check correctness and performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = timeit.default_timer()\nout_lib, out_lib_weights = mha_lib(query, query, query)\n\nt1 = timeit.default_timer()\nout_nested = mha_nested(\n    query, query, query, nheads,\n    W_q, W_k, W_v, W_out,\n    b_q=b_q, b_k=b_k, b_v=b_v, b_out=b_out,\n    dropout_p=dropout_p)\n\nt2 = timeit.default_timer()\npadded_out = mha_padded(\n    padded_query, padded_query, padded_query, nheads,\n    attn_mask_q, attn_mask_q,\n    W_q, W_k, W_v, W_out,\n    b_q=b_q, b_k=b_k, b_v=b_v, b_out=b_out,\n    dropout_p=dropout_p)\nt3 = timeit.default_timer()\n\nprint(\"nested general and library calculations differ by\", (out_nested.to_padded_tensor(0.0) - out_lib.to_padded_tensor(0.0)).abs().max().item())\nprint(\"nested library multi-head attention takes\", t1 - t0, \"seconds\")\nprint(\"nested general multi-head attention takes\", t2 - t1, \"seconds\")\nprint(\"padded tensor multi-head attention takes\", t3 - t2, \"seconds\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}