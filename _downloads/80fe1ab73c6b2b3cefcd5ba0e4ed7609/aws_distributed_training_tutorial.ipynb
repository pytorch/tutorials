{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n(advanced) PyTorch 1.0 Distributed Trainer with Amazon AWS\n=============================================================\n\n**Author**: `Nathan Inkawhich <https://github.com/inkawhich>`_\n\n**Edited by**: `Teng Li <https://github.com/teng-li>`_\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial we will show how to setup, code, and run a PyTorch 1.0\ndistributed trainer across two multi-gpu Amazon AWS nodes. We will start\nwith describing the AWS setup, then the PyTorch environment\nconfiguration, and finally the code for the distributed trainer.\nHopefully you will find that there is actually very little code change\nrequired to extend your current training code to a distributed\napplication, and most of the work is in the one-time environment setup.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Amazon AWS Setup\n----------------\n\nIn this tutorial we will run distributed training across two multi-gpu\nnodes. In this section we will first cover how to create the nodes, then\nhow to setup the security group so the nodes can communicate with\neachother.\n\nCreating the Nodes\n~~~~~~~~~~~~~~~~~~\n\nIn Amazon AWS, there are seven steps to creating an instance. To get\nstarted, login and select **Launch Instance**.\n\n**Step 1: Choose an Amazon Machine Image (AMI)** - Here we will select\nthe ``Deep Learning AMI (Ubuntu) Version 14.0``. As described, this\ninstance comes with many of the most popular deep learning frameworks\ninstalled and is preconfigured with CUDA, cuDNN, and NCCL. It is a very\ngood starting point for this tutorial.\n\n**Step 2: Choose an Instance Type** - Now, select the GPU compute unit\ncalled ``p2.8xlarge``. Notice, each of these instances has a different\ncost but this instance provides 8 NVIDIA Tesla K80 GPUs per node, and\nprovides a good architecture for multi-gpu distributed training.\n\n**Step 3: Configure Instance Details** - The only setting to change here\nis increasing the *Number of instances* to 2. All other configurations\nmay be left at default.\n\n**Step 4: Add Storage** - Notice, by default these nodes do not come\nwith a lot of storage (only 75 GB). For this tutorial, since we are only\nusing the STL-10 dataset, this is plenty of storage. But, if you want to\ntrain on a larger dataset such as ImageNet, you will have to add much\nmore storage just to fit the dataset and any trained models you wish to\nsave.\n\n**Step 5: Add Tags** - Nothing to be done here, just move on.\n\n**Step 6: Configure Security Group** - This is a critical step in the\nconfiguration process. By default two nodes in the same security group\nwould not be able to communicate in the distributed training setting.\nHere, we want to create a **new** security group for the two nodes to be\nin. However, we cannot finish configuring in this step. For now, just\nremember your new security group name (e.g. launch-wizard-12) then move\non to Step 7.\n\n**Step 7: Review Instance Launch** - Here, review the instance then\nlaunch it. By default, this will automatically start initializing the\ntwo instances. You can monitor the initialization progress from the\ndashboard.\n\nConfigure Security Group\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nRecall that we were not able to properly configure the security group\nwhen creating the instances. Once you have launched the instance, select\nthe *Network & Security > Security Groups* tab in the EC2 dashboard.\nThis will bring up a list of security groups you have access to. Select\nthe new security group you created in Step 6 (i.e. launch-wizard-12),\nwhich will bring up tabs called *Description, Inbound, Outbound, and\nTags*. First, select the *Inbound* tab and *Edit* to add a rule to allow\n\"All Traffic\" from \"Sources\" in the launch-wizard-12 security group.\nThen select the *Outbound* tab and do the exact same thing. Now, we have\neffectively allowed all Inbound and Outbound traffic of all types\nbetween nodes in the launch-wizard-12 security group.\n\nNecessary Information\n~~~~~~~~~~~~~~~~~~~~~\n\nBefore continuing, we must find and remember the IP addresses of both\nnodes. In the EC2 dashboard find your running instances. For both\ninstances, write down the *IPv4 Public IP* and the *Private IPs*. For\nthe remainder of the document, we will refer to these as the\n**node0-publicIP**, **node0-privateIP**, **node1-publicIP**, and\n**node1-privateIP**. The public IPs are the addresses we will use to SSH\nin, and the private IPs will be used for inter-node communication.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environment Setup\n-----------------\n\nThe next critical step is the setup of each node. Unfortunately, we\ncannot configure both nodes at the same time, so this process must be\ndone on each node separately. However, this is a one time setup, so once\nyou have the nodes configured properly you will not have to reconfigure\nfor future distributed training projects.\n\nThe first step, once logged onto the node, is to create a new conda\nenvironment with python 3.6 and numpy. Once created activate the\nenvironment.\n\n::\n\n    $ conda create -n nightly_pt python=3.6 numpy\n    $ source activate nightly_pt\n\nNext, we will install a nightly build of Cuda 9.0 enabled PyTorch with\npip in the conda environment.\n\n::\n\n    $ pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html\n\nWe must also install torchvision so we can use the torchvision model and\ndataset. At this time, we must build torchvision from source as the pip\ninstallation will by default install an old version of PyTorch on top of\nthe nightly build we just installed.\n\n::\n\n    $ cd\n    $ git clone https://github.com/pytorch/vision.git\n    $ cd vision\n    $ python setup.py install\n\nAnd finally, **VERY IMPORTANT** step is to set the network interface\nname for the NCCL socket. This is set with the environment variable\n``NCCL_SOCKET_IFNAME``. To get the correct name, run the ``ifconfig``\ncommand on the node and look at the interface name that corresponds to\nthe node's *privateIP* (e.g. ens3). Then set the environment variable as\n\n::\n\n    $ export NCCL_SOCKET_IFNAME=ens3\n\nRemember, do this on both nodes. You may also consider adding the\nNCCL\\_SOCKET\\_IFNAME setting to your *.bashrc*. An important observation\nis that we did not setup a shared filesystem between the nodes.\nTherefore, each node will have to have a copy of the code and a copy of\nthe datasets. For more information about setting up a shared network\nfilesystem between nodes, see\n`here <https://aws.amazon.com/blogs/aws/amazon-elastic-file-system-shared-file-storage-for-amazon-ec2/>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distributed Training Code\n-------------------------\n\nWith the instances running and the environments setup we can now get\ninto the training code. Most of the code here has been taken from the\n`PyTorch ImageNet\nExample <https://github.com/pytorch/examples/tree/master/imagenet>`__\nwhich also supports distributed training. This code provides a good\nstarting point for a custom trainer as it has much of the boilerplate\ntraining loop, validation loop, and accuracy tracking functionality.\nHowever, you will notice that the argument parsing and other\nnon-essential functions have been stripped out for simplicity.\n\nIn this example we will use\n`torchvision.models.resnet18 <https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18>`__\nmodel and will train it on the\n`torchvision.datasets.STL10 <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.STL10>`__\ndataset. To accomodate for the dimensionality mismatch of STL-10 with\nResnet18, we will resize each image to 224x224 with a transform. Notice,\nthe choice of model and dataset are orthogonal to the distributed\ntraining code, you may use any dataset and model you wish and the\nprocess is the same. Lets get started by first handling the imports and\ntalking about some helper functions. Then we will define the train and\ntest functions, which have been largely taken from the ImageNet Example.\nAt the end, we will build the main part of the code which handles the\ndistributed training setup. And finally, we will discuss how to actually\nrun the code.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n~~~~~~~\n\nThe important distributed training specific imports here are\n`torch.nn.parallel <https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__,\n`torch.distributed <https://pytorch.org/docs/stable/distributed.html>`__,\n`torch.utils.data.distributed <https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler>`__,\nand\n`torch.multiprocessing <https://pytorch.org/docs/stable/multiprocessing.html>`__.\nIt is also important to set the multiprocessing start method to *spawn*\nor *forkserver* (only supported in Python 3),\nas the default is *fork* which may cause deadlocks when using multiple\nworker processes for dataloading.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport sys\nimport torch\n\nif __name__ == '__main__':\n    torch.multiprocessing.set_start_method('spawn')\n\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.distributed as dist\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom torch.multiprocessing import Pool, Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions\n~~~~~~~~~~~~~~~~\n\nWe must also define some helper functions and classes that will make\ntraining easier. The ``AverageMeter`` class tracks training statistics\nlike accuracy and iteration count. The ``accuracy`` function computes\nand returns the top-k accuracy of the model so we can track learning\nprogress. Both are provided for training convenience but neither are\ndistributed training specific.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Functions\n~~~~~~~~~~~~~~~\n\nTo simplify the main loop, it is best to separate a training epoch step\ninto a function called ``train``. This function trains the input model\nfor one epoch of the *train\\_loader*. The only distributed training\nartifact in this function is setting the\n`non\\_blocking <https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers>`__\nattributes of the data and label tensors to ``True`` before the forward\npass. This allows asynchronous GPU copies of the data meaning transfers\ncan be overlapped with computation. This function also outputs training\nstatistics along the way so we can track progress throughout the epoch.\n\nThe other function to define here is ``adjust_learning_rate``, which\ndecays the initial learning rate at a fixed schedule. This is another\nboilerplate trainer function that is useful to train accurate models.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        # Create non_blocking tensors for distributed training\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradients in a backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Call step of optimizer to update model params\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 10 == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n\ndef adjust_learning_rate(initial_lr, optimizer, epoch):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    lr = initial_lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation Function\n~~~~~~~~~~~~~~~~~~~\n\nTo track generalization performance and simplify the main loop further\nwe can also extract the validation step into a function called\n``validate``. This function runs a full validation step of the input\nmodel on the input validation dataloader and returns the top-1 accuracy\nof the model on the validation set. Again, you will notice the only\ndistributed training feature here is setting ``non_blocking=True`` for\nthe training data and labels before they are passed to the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion):\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (input, target) in enumerate(val_loader):\n\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 100 == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses,\n                       top1=top1, top5=top5))\n\n        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n              .format(top1=top1, top5=top5))\n\n    return top1.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inputs\n~~~~~~\n\nWith the helper functions out of the way, now we have reached the\ninteresting part. Here is where we will define the inputs for the run.\nSome of the inputs are standard model training inputs such as batch size\nand number of training epochs, and some are specific to our distributed\ntraining task. The required inputs are:\n\n-  **batch\\_size** - batch size for *each* process in the distributed\n   training group. Total batch size across distributed model is\n   batch\\_size\\*world\\_size\n\n-  **workers** - number of worker processes used with the dataloaders in\n   each process\n\n-  **num\\_epochs** - total number of epochs to train for\n\n-  **starting\\_lr** - starting learning rate for training\n\n-  **world\\_size** - number of processes in the distributed training\n   environment\n\n-  **dist\\_backend** - backend to use for distributed training\n   communication (i.e. NCCL, Gloo, MPI, etc.). In this tutorial, since\n   we are using several multi-gpu nodes, NCCL is suggested.\n\n-  **dist\\_url** - URL to specify the initialization method of the\n   process group. This may contain the IP address and port of the rank0\n   process or be a non-existant file on a shared file system. Here,\n   since we do not have a shared file system this will incorporate the\n   **node0-privateIP** and the port on node0 to use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Collect Inputs...\")\n\n# Batch Size for training and testing\nbatch_size = 32\n\n# Number of additional worker processes for dataloading\nworkers = 2\n\n# Number of epochs to train for\nnum_epochs = 2\n\n# Starting Learning Rate\nstarting_lr = 0.1\n\n# Number of distributed processes\nworld_size = 4\n\n# Distributed backend type\ndist_backend = 'nccl'\n\n# Url used to setup distributed training\ndist_url = \"tcp://172.31.22.234:23456\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize process group\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nOne of the most important parts of distributed training in PyTorch is to\nproperly setup the process group, which is the **first** step in\ninitializing the ``torch.distributed`` package. To do this, we will use\nthe ``torch.distributed.init_process_group`` function which takes\nseveral inputs. First, a *backend* input which specifies the backend to\nuse (i.e. NCCL, Gloo, MPI, etc.). An *init\\_method* input which is\neither a url containing the address and port of the rank0 machine or a\npath to a non-existant file on the shared file system. Note, to use the\nfile init\\_method, all machines must have access to the file, similarly\nfor the url method, all machines must be able to communicate on the\nnetwork so make sure to configure any firewalls and network settings to\naccomodate. The *init\\_process\\_group* function also takes *rank* and\n*world\\_size* arguments which specify the rank of this process when run\nand the number of processes in the collective, respectively.\nThe *init\\_method* input can also be \"env://\". In this case, the address\nand port of the rank0 machine will be read from the following two\nenvironment variables respectively: MASTER_ADDR, MASTER_PORT.  If *rank*\nand *world\\_size* arguments are not specified in the *init\\_process\\_group*\nfunction, they both can be read from the following two environment\nvariables respectively as well: RANK, WORLD_SIZE.\n\nAnother important step, especially when each node has multiple gpus is\nto set the *local\\_rank* of this process. For example, if you have two\nnodes, each with 8 GPUs and you wish to train with all of them then\n$world\\_size=16$ and each node will have a process with local rank\n0-7. This local\\_rank is used to set the device (i.e. which GPU to use)\nfor the process and later used to set the device when creating a\ndistributed data parallel model. It is also recommended to use NCCL\nbackend in this hypothetical environment as NCCL is preferred for\nmulti-gpu nodes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Initialize Process Group...\")\n# Initialize Process Group\n# v1 - init with url\ndist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[1]), world_size=world_size)\n# v2 - init with file\n# dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size)\n# v3 - init with environment variables\n# dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=int(sys.argv[1]), world_size=world_size)\n\n\n# Establish Local Rank and set device on this node\nlocal_rank = int(sys.argv[2])\ndp_device_ids = [local_rank]\ntorch.cuda.set_device(local_rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize Model\n~~~~~~~~~~~~~~~~\n\nThe next major step is to initialize the model to be trained. Here, we\nwill use a resnet18 model from ``torchvision.models`` but any model may\nbe used. First, we initialize the model and place it in GPU memory.\nNext, we make the model ``DistributedDataParallel``, which handles the\ndistribution of the data to and from the model and is critical for\ndistributed training. The ``DistributedDataParallel`` module also\nhandles the averaging of gradients across the world, so we do not have\nto explicitly average the gradients in the training step.\n\nIt is important to note that this is a blocking function, meaning\nprogram execution will wait at this function until *world\\_size*\nprocesses have joined the process group. Also, notice we pass our device\nids list as a parameter which contains the local rank (i.e. GPU) we are\nusing. Finally, we specify the loss function and optimizer to train the\nmodel with.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Initialize Model...\")\n# Construct Model\nmodel = models.resnet18(pretrained=False).cuda()\n# Make model DistributedDataParallel\nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank)\n\n# define loss function (criterion) and optimizer\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = torch.optim.SGD(model.parameters(), starting_lr, momentum=0.9, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize Dataloaders\n~~~~~~~~~~~~~~~~~~~~~~\n\nThe last step in preparation for the training is to specify which\ndataset to use. Here we use the `STL-10\ndataset <https://cs.stanford.edu/~acoates/stl10/>`__ from\n`torchvision.datasets.STL10 <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.STL10>`__.\nThe STL10 dataset is a 10 class dataset of 96x96px color images. For use\nwith our model, we resize the images to 224x224px in the transform. One\ndistributed training specific item in this section is the use of the\n``DistributedSampler`` for the training set, which is designed to be\nused in conjunction with ``DistributedDataParallel`` models. This object\nhandles the partitioning of the dataset across the distributed\nenvironment so that not all models are training on the same subset of\ndata, which would be counterproductive. Finally, we create the\n``DataLoader``'s which are responsible for feeding the data to the\nprocesses.\n\nThe STL-10 dataset will automatically download on the nodes if they are\nnot present. If you wish to use your own dataset you should download the\ndata, write your own dataset handler, and construct a dataloader for\nyour dataset here.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Initialize Dataloaders...\")\n# Define the transform for the data. Notice, we must resize to 224x224 with this dataset and model.\ntransform = transforms.Compose(\n    [transforms.Resize(224),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Initialize Datasets. STL10 will automatically download if not present\ntrainset = datasets.STL10(root='./data', split='train', download=True, transform=transform)\nvalset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n\n# Create DistributedSampler to handle distributing the dataset across nodes when training\n# This can only be called after torch.distributed.init_process_group is called\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n\n# Create the Dataloaders to feed data to the training and validation steps\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(train_sampler is None), num_workers=workers, pin_memory=False, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop\n~~~~~~~~~~~~~\n\nThe last step is to define the training loop. We have already done most\nof the work for setting up the distributed training so this is not\ndistributed training specific. The only detail is setting the current\nepoch count in the ``DistributedSampler``, as the sampler shuffles the\ndata going to each process deterministically based on epoch. After\nupdating the sampler, the loop runs a full training epoch, runs a full\nvalidation step then prints the performance of the current model against\nthe best performing model so far. After training for num\\_epochs, the\nloop exits and the tutorial is complete. Notice, since this is an\nexercise we are not saving models but one may wish to keep track of the\nbest performing model then save it at the end of training (see\n`here <https://github.com/pytorch/examples/blob/master/imagenet/main.py#L184>`__).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_prec1 = 0\n\nfor epoch in range(num_epochs):\n    # Set epoch count for DistributedSampler\n    train_sampler.set_epoch(epoch)\n\n    # Adjust learning rate according to schedule\n    adjust_learning_rate(starting_lr, optimizer, epoch)\n\n    # train for one epoch\n    print(\"\\nBegin Training Epoch {}\".format(epoch+1))\n    train(train_loader, model, criterion, optimizer, epoch)\n\n    # evaluate on validation set\n    print(\"Begin Validation @ Epoch {}\".format(epoch+1))\n    prec1 = validate(val_loader, model, criterion)\n\n    # remember best prec@1 and save checkpoint if desired\n    # is_best = prec1 > best_prec1\n    best_prec1 = max(prec1, best_prec1)\n\n    print(\"Epoch Summary: \")\n    print(\"\\tEpoch Accuracy: {}\".format(prec1))\n    print(\"\\tBest Accuracy: {}\".format(best_prec1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the Code\n----------------\n\nUnlike most of the other PyTorch tutorials, this code may not be run\ndirectly out of this notebook. To run, download the .py version of this\nfile (or convert it using\n`this <https://gist.github.com/chsasank/7218ca16f8d022e02a9c0deb94a310fe>`__)\nand upload a copy to both nodes. The astute reader would have noticed\nthat we hardcoded the **node0-privateIP** and $world\\_size=4$ but\ninput the *rank* and *local\\_rank* inputs as arg[1] and arg[2] command\nline arguments, respectively. Once uploaded, open two ssh terminals into\neach node.\n\n-  On the first terminal for node0, run ``$ python main.py 0 0``\n\n-  On the second terminal for node0 run ``$ python main.py 1 1``\n\n-  On the first terminal for node1, run ``$ python main.py 2 0``\n\n-  On the second terminal for node1 run ``$ python main.py 3 1``\n\nThe programs will start and wait after printing \"Initialize Model...\"\nfor all four processes to join the process group. Notice the first\nargument is not repeated as this is the unique global rank of the\nprocess. The second argument is repeated as that is the local rank of\nthe process running on the node. If you run ``nvidia-smi`` on each node,\nyou will see two processes on each node, one running on GPU0 and one on\nGPU1.\n\nWe have now completed the distributed training example! Hopefully you\ncan see how you would use this tutorial to help train your own models on\nyour own datasets, even if you are not using the exact same distributed\nenvrionment. If you are using AWS, don't forget to **SHUT DOWN YOUR\nNODES** if you are not using them or you may find an uncomfortably large\nbill at the end of the month.\n\n**Where to go next**\n\n-  Check out the `launcher\n   utility <https://pytorch.org/docs/stable/distributed.html#launch-utility>`__\n   for a different way of kicking off the run\n\n-  Check out the `torch.multiprocessing.spawn\n   utility <https://pytorch.org/docs/master/multiprocessing.html#spawning-subprocesses>`__\n   for another easy way of kicking off multiple distributed processes.\n   `PyTorch ImageNet Example <https://github.com/pytorch/examples/tree/master/imagenet>`__\n   has it implemented and can demonstrate how to use it.\n\n-  If possible, setup a NFS so you only need one copy of the dataset\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}