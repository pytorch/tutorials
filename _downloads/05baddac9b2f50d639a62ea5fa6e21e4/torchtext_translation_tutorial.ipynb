{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLanguage Translation with TorchText\n===================================\n\nThis tutorial shows how to use several convenience classes of ``torchtext`` to preprocess\ndata from a well-known dataset containing sentences in both English and German and use it to\ntrain a sequence-to-sequence model with attention that can translate German sentences\ninto English.\n\nIt is based off of\n`this tutorial <https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\nfrom PyTorch community member `Ben Trevett <https://github.com/bentrevett>`__\nand was created by `Seth Weidman <https://github.com/SethHWeidman/>`__ with Ben's permission.\n\nBy the end of this tutorial, you will be able to:\n\n- Preprocess sentences into a commonly-used format for NLP modeling using the following ``torchtext`` convenience classes:\n    - `TranslationDataset <https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset>`__\n    - `Field <https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field>`__\n    - `BucketIterator <https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator>`__\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Field` and `TranslationDataset`\n----------------\n``torchtext`` has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. One key class is a\n`Field <https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64>`__,\nwhich specifies the way each sentence should be preprocessed, and another is the\n`TranslationDataset` ; ``torchtext``\nhas several such datasets; in this tutorial we'll use the\n`Multi30k dataset <https://github.com/multi30k/dataset>`__, which contains about\n30,000 sentences (averaging about 13 words in length) in both English and German.\n\nNote: the tokenization in this tutorial requires `Spacy <https://spacy.io>`__\nWe use Spacy because it provides strong support for tokenization in languages\nother than English. ``torchtext`` provides a ``basic_english`` tokenizer\nand supports other tokenizers for English (e.g.\n`Moses <https://bitbucket.org/luismsgomes/mosestokenizer/src/default/>`__)\nbut for language translation - where multiple languages are required -\nSpacy is your best bet.\n\nTo run this tutorial, first install ``spacy`` using ``pip`` or ``conda``.\nNext, download the raw data for the English and German Spacy tokenizers:\n\n::\n\n   python -m spacy download en\n   python -m spacy download de\n\nWith Spacy installed, the following code will tokenize each of the sentences\nin the ``TranslationDataset`` based on the tokenizer defined in the ``Field``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import Multi30k\nfrom torchtext.data import Field, BucketIterator\n\nSRC = Field(tokenize = \"spacy\",\n            tokenizer_language=\"de\",\n            init_token = '<sos>',\n            eos_token = '<eos>',\n            lower = True)\n\nTRG = Field(tokenize = \"spacy\",\n            tokenizer_language=\"en\",\n            init_token = '<sos>',\n            eos_token = '<eos>',\n            lower = True)\n\ntrain_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n                                                    fields = (SRC, TRG))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've defined ``train_data``, we can see an extremely useful\nfeature of ``torchtext``'s ``Field``: the ``build_vocab`` method\nnow allows us to create the vocabulary associated with each language\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\nTRG.build_vocab(train_data, min_freq = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once these lines of code have been run, ``SRC.vocab.stoi`` will  be a\ndictionary with the tokens in the vocabulary as keys and their\ncorresponding indices as values; ``SRC.vocab.itos`` will be the same\ndictionary with the keys and values swapped. We won't make extensive\nuse of this fact in this tutorial, but this will likely be useful in\nother NLP tasks you'll encounter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``BucketIterator``\n----------------\nThe last ``torchtext`` specific feature we'll use is the ``BucketIterator``,\nwhich is easy to use since it takes a ``TranslationDataset`` as its\nfirst argument. Specifically, as the docs say:\nDefines an iterator that batches examples of similar lengths together.\nMinimizes amount of padding needed while producing freshly shuffled\nbatches for each new epoch. See pool for the bucketing procedure used.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 128\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size = BATCH_SIZE,\n    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These iterators can be called just like ``DataLoader``s; below, in\nthe ``train`` and ``evaluate`` functions, they are called simply with:\n\n::\n\n   for i, batch in enumerate(iterator):\n\nEach ``batch`` then has ``src`` and ``trg`` attributes:\n\n::\n\n   src = batch.src\n   trg = batch.trg\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining our ``nn.Module`` and ``Optimizer``\n----------------\nThat's mostly it from a ``torchtext`` perspecive: with the dataset built\nand the iterator defined, the rest of this tutorial simply defines our\nmodel as an ``nn.Module``, along with an ``Optimizer``, and then trains it.\n\nOur model specifically, follows the architecture described\n`here <https://arxiv.org/abs/1409.0473>`__ (you can find a\nsignificantly more commented version\n`here <https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__).\n\nNote: this model is just an example model that can be used for language\ntranslation; we choose it because it is a standard model for the task,\nnot because it is the recommended model to use for translation. As you're\nlikely aware, state-of-the-art models are currently based on Transformers;\nyou can see PyTorch's capabilities for implementing Transformer layers\n`here <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__; and\nin particular, the \"attention\" used in the model below is different from\nthe multi-headed self-attention present in a transformer model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nfrom typing import Tuple\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: float):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n\n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n\n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,\n                src: Tensor) -> Tuple[Tensor]:\n\n        embedded = self.dropout(self.embedding(src))\n\n        outputs, hidden = self.rnn(embedded)\n\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n\n        return outputs, hidden\n\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 attn_dim: int):\n        super().__init__()\n\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n\n        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n\n        self.attn = nn.Linear(self.attn_in, attn_dim)\n\n    def forward(self,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tensor:\n\n        src_len = encoder_outputs.shape[0]\n\n        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        energy = torch.tanh(self.attn(torch.cat((\n            repeated_decoder_hidden,\n            encoder_outputs),\n            dim = 2)))\n\n        attention = torch.sum(energy, dim=2)\n\n        return F.softmax(attention, dim=1)\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 output_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: int,\n                 attention: nn.Module):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n\n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n\n        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n\n    def _weighted_encoder_rep(self,\n                              decoder_hidden: Tensor,\n                              encoder_outputs: Tensor) -> Tensor:\n\n        a = self.attention(decoder_hidden, encoder_outputs)\n\n        a = a.unsqueeze(1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n\n        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n\n        return weighted_encoder_rep\n\n\n    def forward(self,\n                input: Tensor,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tuple[Tensor]:\n\n        input = input.unsqueeze(0)\n\n        embedded = self.dropout(self.embedding(input))\n\n        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n                                                          encoder_outputs)\n\n        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n\n        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n\n        output = self.out(torch.cat((output,\n                                     weighted_encoder_rep,\n                                     embedded), dim = 1))\n\n        return output, decoder_hidden.squeeze(0)\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder: nn.Module,\n                 decoder: nn.Module,\n                 device: torch.device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                teacher_forcing_ratio: float = 0.5) -> Tensor:\n\n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n\n        # first input to the decoder is the <sos> token\n        output = trg[0,:]\n\n        for t in range(1, max_len):\n            output, hidden = self.decoder(output, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            output = (trg[t] if teacher_force else top1)\n\n        return outputs\n\n\nINPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\n# ENC_EMB_DIM = 256\n# DEC_EMB_DIM = 256\n# ENC_HID_DIM = 512\n# DEC_HID_DIM = 512\n# ATTN_DIM = 64\n# ENC_DROPOUT = 0.5\n# DEC_DROPOUT = 0.5\n\nENC_EMB_DIM = 32\nDEC_EMB_DIM = 32\nENC_HID_DIM = 64\nDEC_HID_DIM = 64\nATTN_DIM = 8\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n\n\ndef init_weights(m: nn.Module):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\n\nmodel.apply(init_weights)\n\noptimizer = optim.Adam(model.parameters())\n\n\ndef count_parameters(model: nn.Module):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: when scoring the performance of a language translation model in\nparticular, we have to tell the ``nn.CrossEntropyLoss`` function to\nignore the indices where the target is simply padding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can train and evaluate this model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nimport time\n\n\ndef train(model: nn.Module,\n          iterator: BucketIterator,\n          optimizer: optim.Optimizer,\n          criterion: nn.Module,\n          clip: float):\n\n    model.train()\n\n    epoch_loss = 0\n\n    for _, batch in enumerate(iterator):\n\n        src = batch.src\n        trg = batch.trg\n\n        optimizer.zero_grad()\n\n        output = model(src, trg)\n\n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n\n        loss = criterion(output, trg)\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model: nn.Module,\n             iterator: BucketIterator,\n             criterion: nn.Module):\n\n    model.eval()\n\n    epoch_loss = 0\n\n    with torch.no_grad():\n\n        for _, batch in enumerate(iterator):\n\n            src = batch.src\n            trg = batch.trg\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef epoch_time(start_time: int,\n               end_time: int):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\ntest_loss = evaluate(model, test_iterator, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next steps\n--------------\n\n- Check out the rest of Ben Trevett's tutorials using ``torchtext``\n  `here <https://github.com/bentrevett/>`__\n- Stay tuned for a tutorial using other ``torchtext`` features along\n  with ``nn.Transformer`` for language modeling via next word prediction!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}