{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using Ray Tune\n",
    "====================================\n",
    "\n",
    "**Author:** [Ricardo Decal](https://github.com/crypdick)\n",
    "\n",
    "This tutorial shows how to integrate Ray Tune into your PyTorch training\n",
    "workflow to perform scalable and efficient hyperparameter tuning.\n",
    "\n",
    "<div style=\"width: 45%; float: left; padding: 20px;\"><h2> What you will learn</h2><ul><li>How to modify a PyTorch training loop for Ray Tune</li><li>How to scale a hyperparameter sweep to multiple nodes and GPUs without code changes</li><li>How to define a hyperparameter search space and run a sweep with <code>tune.Tuner</code></li><li>How to use an early-stopping scheduler (ASHA) and report metrics/checkpoints</li><li>How to use checkpointing to resume training and load the best model</li></ul></div><div style=\"width: 45%; float: right; padding: 20px;\"><h2> Prerequisites</h2><ul><li>PyTorch v2.9+ and <code>torchvision</code></li><li>Ray Tune (<code>ray[tune]</code>) v2.52.1+</li><li>GPU(s) are optional, but recommended for faster training</li></ul></div>\n",
    "\n",
    "[Ray](https://docs.ray.io/en/latest/index.html), a project of the\n",
    "PyTorch Foundation, is an open source unified framework for scaling AI\n",
    "and Python applications. It helps run distributed jobs by handling the\n",
    "complexity of distributed computing. [Ray\n",
    "Tune](https://docs.ray.io/en/latest/tune/index.html) is a library built\n",
    "on Ray for hyperparameter tuning that enables you to scale a\n",
    "hyperparameter sweep from your machine to a large cluster with no code\n",
    "changes.\n",
    "\n",
    "This tutorial adapts the [PyTorch tutorial for training a CIFAR10\n",
    "classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "to run multi-GPU hyperparameter sweeps with Ray Tune.\n",
    "\n",
    "Setup\n",
    "-----\n",
    "\n",
    "To run this tutorial, install the following dependencies:\n",
    "\n",
    "``` {.bash}\n",
    "pip install \"ray[tune]\" torchvision\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start with the imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# New: imports for Ray Tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading\n",
    "============\n",
    "\n",
    "Wrap the data loaders in a constructor function. In this tutorial, a\n",
    "global data directory is passed to the function to enable reusing the\n",
    "dataset across different trials. In a cluster environment, you can use\n",
    "shared storage, such as network file systems, to prevent each node from\n",
    "downloading the data separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    # Mean and standard deviation of the CIFAR10 training subset.\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.2022, 0.19932, 0.20086))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "==================\n",
    "\n",
    "This tutorial searches for the best sizes for the fully connected layers\n",
    "and the learning rate. To enable this, the `Net` class exposes the layer\n",
    "sizes `l1` and `l2` as configurable parameters that Ray Tune can search\n",
    "over:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the search space\n",
    "=======================\n",
    "\n",
    "Next, define the hyperparameters to tune and how Ray Tune samples them.\n",
    "Ray Tune offers a variety of [search space\n",
    "distributions](https://docs.ray.io/en/latest/tune/api/search_space.html)\n",
    "to suit different parameter types: `loguniform`, `uniform`, `choice`,\n",
    "`randint`, `grid`, and more. You can also express complex dependencies\n",
    "between parameters with [conditional search\n",
    "spaces](https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html#how-to-use-custom-and-conditional-search-spaces-in-tune)\n",
    "or sample from arbitrary functions.\n",
    "\n",
    "Here is the search space for this tutorial:\n",
    "\n",
    "``` {.python}\n",
    "config = {\n",
    "    \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "    \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "}\n",
    "```\n",
    "\n",
    "The `tune.choice()` accepts a list of values that are uniformly sampled\n",
    "from. In this example, the `l1` and `l2` parameter values are powers of\n",
    "2 between 1 and 256, and the learning rate samples on a log scale\n",
    "between 0.0001 and 0.1. Sampling on a log scale enables exploration\n",
    "across a range of magnitudes on a relative scale, rather than an\n",
    "absolute scale.\n",
    "\n",
    "Training function\n",
    "=================\n",
    "\n",
    "Ray Tune requires a training function that accepts a configuration\n",
    "dictionary and runs the main training loop. As Ray Tune runs different\n",
    "trials, it updates the configuration dictionary for each trial.\n",
    "\n",
    "Here is the full training function, followed by explanations of the key\n",
    "Ray Tune integration points:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    net = net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    # Load checkpoint if resuming training\n",
    "    checkpoint = tune.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\"\n",
    "            checkpoint_state = torch.load(checkpoint_path)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, _testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Save checkpoint and report metrics\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\"\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            tune.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key integration points\n",
    "======================\n",
    "\n",
    "Using hyperparameters from the configuration dictionary\n",
    "-------------------------------------------------------\n",
    "\n",
    "Ray Tune updates the `config` dictionary with the hyperparameters for\n",
    "each trial. In this example, the model architecture and optimizer\n",
    "receive the hyperparameters from the `config` dictionary:\n",
    "\n",
    "``` {.python}\n",
    "net = Net(config[\"l1\"], config[\"l2\"])\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "```\n",
    "\n",
    "Reporting metrics and saving checkpoints\n",
    "----------------------------------------\n",
    "\n",
    "The most important integration is communicating with Ray Tune. Ray Tune\n",
    "uses the validation metrics to determine the best hyperparameter\n",
    "configuration and to stop underperforming trials early, saving\n",
    "resources.\n",
    "\n",
    "Checkpointing enables you to later load the trained models, resume\n",
    "hyperparameter searches, and provides fault tolerance. It's also\n",
    "required for some Ray Tune schedulers like [Population Based\n",
    "Training](https://docs.ray.io/en/latest/tune/examples/pbt_guide.html)\n",
    "that pause and resume trials during the search.\n",
    "\n",
    "This code from the training function loads model and optimizer state at\n",
    "the start if a checkpoint exists:\n",
    "\n",
    "``` {.python}\n",
    "checkpoint = tune.get_checkpoint()\n",
    "if checkpoint:\n",
    "    with checkpoint.as_directory() as checkpoint_dir:\n",
    "        checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\"\n",
    "        checkpoint_state = torch.load(checkpoint_path)\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "```\n",
    "\n",
    "At the end of each epoch, save a checkpoint and report the validation\n",
    "metrics:\n",
    "\n",
    "``` {.python}\n",
    "checkpoint_data = {\n",
    "    \"epoch\": epoch,\n",
    "    \"net_state_dict\": net.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}\n",
    "with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "    checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\"\n",
    "    torch.save(checkpoint_data, checkpoint_path)\n",
    "\n",
    "    checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "    tune.report(\n",
    "        {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "```\n",
    "\n",
    "Ray Tune checkpointing supports local file systems, cloud storage, and\n",
    "distributed file systems. For more information, see the [Ray Tune\n",
    "storage\n",
    "documentation](https://docs.ray.io/en/latest/tune/tutorials/tune-storage.html).\n",
    "\n",
    "Multi-GPU support\n",
    "-----------------\n",
    "\n",
    "Image classification models can be greatly accelerated by using GPUs.\n",
    "The training function supports multi-GPU training by wrapping the model\n",
    "in `nn.DataParallel`:\n",
    "\n",
    "``` {.python}\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "```\n",
    "\n",
    "This training function supports training on CPUs, a single GPU, multiple\n",
    "GPUs, or multiple nodes without code changes. Ray Tune automatically\n",
    "distributes the trials across the nodes according to the available\n",
    "resources. Ray Tune also supports [fractional\n",
    "GPUs](https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html#fractional-accelerators)\n",
    "so that one GPU can be shared among multiple trials, provided that the\n",
    "models, optimizers, and data batches fit into the GPU memory.\n",
    "\n",
    "Validation split\n",
    "----------------\n",
    "\n",
    "The original CIFAR10 dataset only has train and test subsets. This is\n",
    "sufficient for training a single model, however for hyperparameter\n",
    "tuning a validation subset is required. The training function creates a\n",
    "validation subset by reserving 20% of the training subset. The test\n",
    "subset is used to evaluate the best model's generalization error after\n",
    "the search completes.\n",
    "\n",
    "### Evaluation function\n",
    "\n",
    "After finding the optimal hyperparameters, test the model on a held-out\n",
    "test set to estimate the generalization error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\", data_dir=None):\n",
    "    _trainset, testset = load_data(data_dir)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            image_batch, labels = data\n",
    "            image_batch, labels = image_batch.to(device), labels.to(device)\n",
    "            outputs = net(image_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure and run Ray Tune\n",
    "==========================\n",
    "\n",
    "With the training and evaluation functions defined, configure Ray Tune\n",
    "to run the hyperparameter search.\n",
    "\n",
    "Scheduler for early stopping\n",
    "----------------------------\n",
    "\n",
    "Ray Tune provides schedulers to improve the efficiency of the\n",
    "hyperparameter search by detecting underperforming trials and stopping\n",
    "them early. The `ASHAScheduler` uses the Asynchronous Successive Halving\n",
    "Algorithm (ASHA) to aggressively terminate low-performing trials:\n",
    "\n",
    "``` {.python}\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "```\n",
    "\n",
    "Ray Tune also provides [advanced search\n",
    "algorithms](https://docs.ray.io/en/latest/tune/api/suggestion.html) to\n",
    "smartly pick the next set of hyperparameters based on previous results,\n",
    "instead of relying only on random or grid search. Examples include\n",
    "[Optuna](https://docs.ray.io/en/latest/tune/api/suggestion.html#optuna)\n",
    "and\n",
    "[BayesOpt](https://docs.ray.io/en/latest/tune/api/suggestion.html#bayesopt).\n",
    "\n",
    "Resource allocation\n",
    "-------------------\n",
    "\n",
    "Tell Ray Tune what resources to allocate for each trial by passing a\n",
    "`resources` dictionary to `tune.with_resources`:\n",
    "\n",
    "``` {.python}\n",
    "tune.with_resources(\n",
    "    partial(train_cifar, data_dir=data_dir),\n",
    "    resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial}\n",
    ")\n",
    "```\n",
    "\n",
    "Ray Tune automatically manages the placement of these trials and ensures\n",
    "that the trials run in isolation, so you don't need to manually assign\n",
    "GPUs to processes.\n",
    "\n",
    "For example, if you are running this experiment on a cluster of 20\n",
    "machines, each with 8 GPUs, you can set `gpus_per_trial = 0.5` to\n",
    "schedule two concurrent trials per GPU. This configuration runs 320\n",
    "trials in parallel across the cluster.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>To run this tutorial without GPUs, set <code>gpus_per_trial=0</code>and expect significantly longer runtimes.To avoid long runtimes during development, start with a small numberof trials and epochs.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Creating the Tuner\n",
    "------------------\n",
    "\n",
    "The Ray Tune API is modular and composable. Pass your configuration to\n",
    "the `tune.Tuner` class to create a tuner object, then run `tuner.fit()`\n",
    "to start training:\n",
    "\n",
    "``` {.python}\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial}\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        scheduler=scheduler,\n",
    "        num_samples=num_trials,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "results = tuner.fit()\n",
    "```\n",
    "\n",
    "After training completes, retrieve the best performing trial, load its\n",
    "checkpoint, and evaluate on the test set.\n",
    "\n",
    "Putting it all together\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(num_trials=10, max_num_epochs=10, gpus_per_trial=0, cpus_per_trial=2):\n",
    "    print(\"Starting hyperparameter tuning.\")\n",
    "    ray.init(include_dashboard=False)\n",
    "    \n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)  # Pre-download the dataset\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "        \"device\": device,\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            partial(train_cifar, data_dir=data_dir),\n",
    "            resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_trials,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "    print(f\"Best trial config: {best_result.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_result.metrics['accuracy']}\")\n",
    "\n",
    "    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n",
    "    best_trained_model = best_trained_model.to(device)\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "\n",
    "    best_checkpoint = best_result.checkpoint\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        checkpoint_path = Path(checkpoint_dir) / \"checkpoint.pt\"\n",
    "        best_checkpoint_data = torch.load(checkpoint_path)\n",
    "\n",
    "        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "        test_acc = test_accuracy(best_trained_model, device, data_dir)\n",
    "        print(f\"Best trial test set accuracy: {test_acc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the number of trials, epochs, and GPUs per trial here:\n",
    "    main(num_trials=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "=======\n",
    "\n",
    "Your Ray Tune trial summary output looks something like this. The text\n",
    "table summarizes the validation performance of the trials and highlights\n",
    "the best hyperparameter configuration:\n",
    "\n",
    "``` {.bash}\n",
    "Number of trials: 10/10 (10 TERMINATED)\n",
    "+-----+--------------+------+------+-------------+--------+---------+------------+\n",
    "| ... |   batch_size |   l1 |   l2 |          lr |   iter |    loss |   accuracy |\n",
    "|-----+--------------+------+------+-------------+--------+---------+------------|\n",
    "| ... |            2 |    1 |  256 | 0.000668163 |      1 | 2.31479 |     0.0977 |\n",
    "| ... |            4 |   64 |    8 | 0.0331514   |      1 | 2.31605 |     0.0983 |\n",
    "| ... |            4 |    2 |    1 | 0.000150295 |      1 | 2.30755 |     0.1023 |\n",
    "| ... |           16 |   32 |   32 | 0.0128248   |     10 | 1.66912 |     0.4391 |\n",
    "| ... |            4 |    8 |  128 | 0.00464561  |      2 | 1.7316  |     0.3463 |\n",
    "| ... |            8 |  256 |    8 | 0.00031556  |      1 | 2.19409 |     0.1736 |\n",
    "| ... |            4 |   16 |  256 | 0.00574329  |      2 | 1.85679 |     0.3368 |\n",
    "| ... |            8 |    2 |    2 | 0.00325652  |      1 | 2.30272 |     0.0984 |\n",
    "| ... |            2 |    2 |    2 | 0.000342987 |      2 | 1.76044 |     0.292  |\n",
    "| ... |            4 |   64 |   32 | 0.003734    |      8 | 1.53101 |     0.4761 |\n",
    "+-----+--------------+------+------+-------------+--------+---------+------------+\n",
    "\n",
    "Best trial config: {'l1': 64, 'l2': 32, 'lr': 0.0037339984519545164, 'batch_size': 4}\n",
    "Best trial final validation loss: 1.5310075663924216\n",
    "Best trial final validation accuracy: 0.4761\n",
    "Best trial test set accuracy: 0.4737\n",
    "```\n",
    "\n",
    "Most trials stopped early to conserve resources. The best performing\n",
    "trial achieved a validation accuracy of approximately 47%, which the\n",
    "test set confirms.\n",
    "\n",
    "Observability\n",
    "=============\n",
    "\n",
    "Monitoring is critical when running large-scale experiments. Ray\n",
    "provides a\n",
    "[dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)\n",
    "that lets you view the status of your trials, check cluster resource\n",
    "use, and inspect logs in real time.\n",
    "\n",
    "For debugging, Ray also offers [distributed debugging\n",
    "tools](https://docs.ray.io/en/latest/ray-observability/index.html) that\n",
    "let you attach a debugger to running trials across the cluster.\n",
    "\n",
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, you learned how to tune the hyperparameters of a\n",
    "PyTorch model using Ray Tune. You saw how to integrate Ray Tune into\n",
    "your PyTorch training loop, define a search space for your\n",
    "hyperparameters, use an efficient scheduler like `ASHAScheduler` to\n",
    "terminate low-performing trials early, save checkpoints and report\n",
    "metrics to Ray Tune, and run the hyperparameter search and analyze the\n",
    "results.\n",
    "\n",
    "Ray Tune makes it straightforward to scale your experiments from a\n",
    "single machine to a large cluster, helping you find the best model\n",
    "configuration efficiently.\n",
    "\n",
    "Further reading\n",
    "===============\n",
    "\n",
    "-   [Ray Tune\n",
    "    documentation](https://docs.ray.io/en/latest/tune/index.html)\n",
    "-   [Ray Tune\n",
    "    examples](https://docs.ray.io/en/latest/tune/examples/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
