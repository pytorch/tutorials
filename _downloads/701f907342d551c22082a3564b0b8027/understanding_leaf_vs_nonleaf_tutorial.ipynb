{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding requires\\_grad, retain\\_grad, Leaf, and Non-leaf Tensors\n",
    "======================================================================\n",
    "\n",
    "**Author:** [Justin Silver](https://github.com/j-silv)\n",
    "\n",
    "This tutorial explains the subtleties of `requires_grad`, `retain_grad`,\n",
    "leaf, and non-leaf tensors using a simple example.\n",
    "\n",
    "Before starting, make sure you understand [tensors and how to manipulate\n",
    "them](https://docs.pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html).\n",
    "A basic knowledge of [how autograd\n",
    "works](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "would also be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "=====\n",
    "\n",
    "First, make sure [PyTorch is\n",
    "installed](https://pytorch.org/get-started/locally/) and then import the\n",
    "necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a simple network to focus on the gradients. This\n",
    "will be an affine layer, followed by a ReLU activation, and ending with\n",
    "a MSE loss between prediction and label tensors.\n",
    "\n",
    "$$\\mathbf{y}_{\\text{pred}} = \\text{ReLU}(\\mathbf{x} \\mathbf{W} + \\mathbf{b})$$\n",
    "\n",
    "$$L = \\text{MSE}(\\mathbf{y}_{\\text{pred}}, \\mathbf{y})$$\n",
    "\n",
    "Note that the `requires_grad=True` is necessary for the parameters (`W`\n",
    "and `b`) so that PyTorch tracks operations involving those tensors.\n",
    "We'll discuss more about this in a future [section](#requires-grad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensor setup\n",
    "x = torch.ones(1, 3)                      # input with shape: (1, 3)\n",
    "W = torch.ones(3, 2, requires_grad=True)  # weights with shape: (3, 2)\n",
    "b = torch.ones(1, 2, requires_grad=True)  # bias with shape: (1, 2)\n",
    "y = torch.ones(1, 2)                      # output with shape: (1, 2)\n",
    "\n",
    "# forward pass\n",
    "z = (x @ W) + b                           # pre-activation with shape: (1, 2)\n",
    "y_pred = F.relu(z)                        # activation with shape: (1, 2)\n",
    "loss = F.mse_loss(y_pred, y)              # scalar loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaf vs. non-leaf tensors\n",
    "=========================\n",
    "\n",
    "After running the forward pass, PyTorch autograd has built up a [dynamic\n",
    "computational\n",
    "graph](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph)\n",
    "which is shown below. This is a [Directed Acyclic Graph\n",
    "(DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) which keeps\n",
    "a record of input tensors (leaf nodes), all subsequent operations on\n",
    "those tensors, and the intermediate/output tensors (non-leaf nodes). The\n",
    "graph is used to compute gradients for each tensor starting from the\n",
    "graph roots (outputs) to the leaves (inputs) using the [chain\n",
    "rule](https://en.wikipedia.org/wiki/Chain_rule) from calculus:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{f}_k\\bigl(\\mathbf{f}_{k-1}(\\dots \\mathbf{f}_1(\\mathbf{x}) \\dots)\\bigr)$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
    "\\frac{\\partial \\mathbf{f}_k}{\\partial \\mathbf{f}_{k-1}} \\cdot\n",
    "\\frac{\\partial \\mathbf{f}_{k-1}}{\\partial \\mathbf{f}_{k-2}} \\cdot\n",
    "\\cdots \\cdot\n",
    "\\frac{\\partial \\mathbf{f}_1}{\\partial \\mathbf{x}}$$\n",
    "\n",
    "![Computational graph after forward\n",
    "pass](https://pytorch.org/tutorials/_static/img/understanding_leaf_vs_nonleaf/comp-graph-1.png)\n",
    "\n",
    "PyTorch considers a node to be a *leaf* if it is not the result of a\n",
    "tensor operation with at least one input having `requires_grad=True`\n",
    "(e.g. `x`, `W`, `b`, and `y`), and everything else to be *non-leaf*\n",
    "(e.g. `z`, `y_pred`, and `loss`). You can verify this programmatically\n",
    "by probing the `is_leaf` attribute of the tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints True because new tensors are leafs by convention\n",
    "print(f\"{x.is_leaf=}\")\n",
    "\n",
    "# prints False because tensor is the result of an operation with at\n",
    "# least one input having requires_grad=True\n",
    "print(f\"{z.is_leaf=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction between leaf and non-leaf determines whether the\n",
    "tensor's gradient will be stored in the `grad` property after the\n",
    "backward pass, and thus be usable for [gradient\n",
    "descent](https://en.wikipedia.org/wiki/Gradient_descent). We'll cover\n",
    "this some more in the [following section](#retain-grad).\n",
    "\n",
    "Let's now investigate how PyTorch calculates and stores gradients for\n",
    "the tensors in its computational graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad`\n",
    "===============\n",
    "\n",
    "To build the computational graph which can be used for gradient\n",
    "calculation, we need to pass in the `requires_grad=True` parameter to a\n",
    "tensor constructor. By default, the value is `False`, and thus PyTorch\n",
    "does not track gradients on any created tensors. To verify this, try not\n",
    "setting `requires_grad`, re-run the forward pass, and then run\n",
    "backpropagation. You will see:\n",
    "\n",
    "    >>> loss.backward()\n",
    "    RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "This error means that autograd can't backpropagate to any leaf tensors\n",
    "because `loss` is not tracking gradients. If you need to change the\n",
    "property, you can call `requires_grad_()` on the tensor (notice the \\_\n",
    "suffix).\n",
    "\n",
    "We can sanity check which nodes require gradient calculation, just like\n",
    "we did above with the `is_leaf` attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"{x.requires_grad=}\") # prints False because requires_grad=False by default\n",
    "print(f\"{W.requires_grad=}\") # prints True because we set requires_grad=True in constructor\n",
    "print(f\"{z.requires_grad=}\") # prints True because tensor is a non-leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to remember that a non-leaf tensor has `requires_grad=True`\n",
    "by definition, since backpropagation would fail otherwise. If the tensor\n",
    "is a leaf, then it will only have `requires_grad=True` if it was\n",
    "specifically set by the user. Another way to phrase this is that if at\n",
    "least one of the inputs to a tensor requires the gradient, then it will\n",
    "require the gradient as well.\n",
    "\n",
    "There are two exceptions to this rule:\n",
    "\n",
    "1.  Any `nn.Module` that has `nn.Parameter` will have\n",
    "    `requires_grad=True` for its parameters (see\n",
    "    [here](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models))\n",
    "2.  Locally disabling gradient computation with context managers (see\n",
    "    [here](https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disabling-gradient-computation))\n",
    "\n",
    "In summary, `requires_grad` tells autograd which tensors need to have\n",
    "their gradients calculated for backpropagation to work. This is\n",
    "different from which tensors have their `grad` field populated, which is\n",
    "the topic of the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retain_grad`\n",
    "=============\n",
    "\n",
    "To actually perform optimization (e.g. SGD, Adam, etc.), we need to run\n",
    "the backward pass so that we can extract the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `backward()` populates the `grad` field of all leaf tensors\n",
    "which had `requires_grad=True`. The `grad` is the gradient of the loss\n",
    "with respect to the tensor we are probing. Before running `backward()`,\n",
    "this attribute is set to `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"{W.grad=}\")\n",
    "print(f\"{b.grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering about the other tensors in our network. Let's\n",
    "check the remaining leaf nodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints all None because requires_grad=False\n",
    "print(f\"{x.grad=}\")\n",
    "print(f\"{y.grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients for these tensors haven't been populated because we did\n",
    "not explicitly tell PyTorch to calculate their gradient\n",
    "(`requires_grad=False`).\n",
    "\n",
    "Let's now look at an intermediate non-leaf node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"{z.grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch returns `None` for the gradient and also warns us that a\n",
    "non-leaf node's `grad` attribute is being accessed. Although autograd\n",
    "has to calculate intermediate gradients for backpropagation to work, it\n",
    "assumes you don't need to access the values afterwards. To change this\n",
    "behavior, we can use the `retain_grad()` function on a tensor. This\n",
    "tells the autograd engine to populate that tensor's `grad` after calling\n",
    "`backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have to re-run the forward pass\n",
    "z = (x @ W) + b\n",
    "y_pred = F.relu(z)\n",
    "loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "# tell PyTorch to store the gradients after backward()\n",
    "z.retain_grad()\n",
    "y_pred.retain_grad()\n",
    "loss.retain_grad()\n",
    "\n",
    "# have to zero out gradients otherwise they would accumulate\n",
    "W.grad = None\n",
    "b.grad = None\n",
    "\n",
    "# backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# print gradients for all tensors that have requires_grad=True\n",
    "print(f\"{W.grad=}\")\n",
    "print(f\"{b.grad=}\")\n",
    "print(f\"{z.grad=}\")\n",
    "print(f\"{y_pred.grad=}\")\n",
    "print(f\"{loss.grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result for `W.grad` as before. Also note that because\n",
    "the loss is scalar, the gradient of the loss with respect to itself is\n",
    "simply `1.0`.\n",
    "\n",
    "If we look at the state of the computational graph now, we see that the\n",
    "`retains_grad` attribute has changed for the intermediate tensors. By\n",
    "convention, this attribute will print `False` for any leaf node, even if\n",
    "it requires its gradient.\n",
    "\n",
    "![Computational graph after backward\n",
    "pass](https://pytorch.org/tutorials/_static/img/understanding_leaf_vs_nonleaf/comp-graph-2.png)\n",
    "\n",
    "If you call `retain_grad()` on a non-leaf node, it results in a no-op.\n",
    "If we call `retain_grad()` on a node that has `requires_grad=False`,\n",
    "PyTorch actually throws an error, since it can't store the gradient if\n",
    "it is never calculated.\n",
    "\n",
    "    >>> x.retain_grad()\n",
    "    RuntimeError: can't retain_grad on Tensor that has requires_grad=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary table\n",
    "=============\n",
    "\n",
    "Using `retain_grad()` and `retains_grad` only make sense for non-leaf\n",
    "nodes, since the `grad` attribute will already be populated for leaf\n",
    "tensors that have `requires_grad=True`. By default, these non-leaf nodes\n",
    "do not retain (store) their gradient after backpropagation. We can\n",
    "change that by rerunning the forward pass, telling PyTorch to store the\n",
    "gradients, and then performing backpropagation.\n",
    "\n",
    "The following table can be used as a reference which summarizes the\n",
    "above discussions. The following scenarios are the only ones that are\n",
    "valid for PyTorch tensors.\n",
    "\n",
    "  ----------------------------------------------------------------------------------------\n",
    "  `is_leaf`   `requires_grad`   `retains_grad`   `require_grad()`        `retain_grad()`\n",
    "  ----------- ----------------- ---------------- ----------------------- -----------------\n",
    "  `True`      `False`           `False`          sets `requires_grad` to no-op\n",
    "                                                 `True` or `False`       \n",
    "\n",
    "  `True`      `True`            `False`          sets `requires_grad` to no-op\n",
    "                                                 `True` or `False`       \n",
    "\n",
    "  `False`     `True`            `False`          no-op                   sets\n",
    "                                                                         `retains_grad` to\n",
    "                                                                         `True`\n",
    "\n",
    "  `False`     `True`            `True`           no-op                   no-op\n",
    "  ----------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we covered when and how PyTorch computes gradients for\n",
    "leaf and non-leaf tensors. By using `retain_grad`, we can access the\n",
    "gradients of intermediate tensors within autograd's computational graph.\n",
    "\n",
    "If you would like to learn more about how PyTorch's autograd system\n",
    "works, please visit the [references](#references) below. If you have any\n",
    "feedback for this tutorial (improvements, typo fixes, etc.) then please\n",
    "use the [PyTorch Forums](https://discuss.pytorch.org/) and/or the [issue\n",
    "tracker](https://github.com/pytorch/tutorials/issues) to reach out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "==========\n",
    "\n",
    "-   [A Gentle Introduction to\n",
    "    torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "-   [Automatic Differentiation with\n",
    "    torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial)\n",
    "-   [Autograd\n",
    "    mechanics](https://docs.pytorch.org/docs/stable/notes/autograd.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
