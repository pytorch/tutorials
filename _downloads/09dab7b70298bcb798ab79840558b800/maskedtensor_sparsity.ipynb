{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# (Prototype) MaskedTensor Sparsity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before working on this tutorial, please make sure to review our\n`MaskedTensor Overview tutorial <https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`.\n\n## Introduction\n\nSparsity has been an area of rapid growth and importance within PyTorch; if any sparsity terms are confusing below,\nplease refer to the [sparsity tutorial](https://pytorch.org/docs/stable/sparse.html)_ for additional details.\n\nSparse storage formats have been proven to be powerful in a variety of ways. As a primer, the first use case\nmost practitioners think about is when the majority of elements are equal to zero (a high degree of sparsity),\nbut even in cases of lower sparsity, certain formats (e.g. BSR) can take advantage of substructures within a matrix.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>At the moment, MaskedTensor supports COO and CSR tensors with plans to support additional formats\n    (such as BSR and CSC) in the future. If you have any requests for additional formats,\n    please file a feature request [here](https://github.com/pytorch/pytorch/issues)_!</p></div>\n\n## Principles\n\nWhen creating a :class:`MaskedTensor` with sparse tensors, there are a few principles that must be observed:\n\n1. ``data`` and ``mask`` must have the same storage format, whether that's :attr:`torch.strided`, :attr:`torch.sparse_coo`, or :attr:`torch.sparse_csr`\n2. ``data`` and ``mask`` must have the same size, indicated by :func:`size()`\n\n\n## Sparse COO tensors\n\nIn accordance with Principle #1, a sparse COO MaskedTensor is created by passing in two sparse COO tensors,\nwhich can be initialized by any of its constructors, for example :func:`torch.sparse_coo_tensor`.\n\nAs a recap of [sparse COO tensors](https://pytorch.org/docs/stable/sparse.html#sparse-coo-tensors)_, the COO format\nstands for \"coordinate format\", where the specified elements are stored as tuples of their indices and the\ncorresponding values. That is, the following are provided:\n\n* ``indices``: array of size ``(ndim, nse)`` and dtype ``torch.int64``\n* ``values``: array of size `(nse,)` with any integer or floating point dtype\n\nwhere ``ndim`` is the dimensionality of the tensor and ``nse`` is the number of specified elements.\n\nFor both sparse COO and CSR tensors, you can construct a :class:`MaskedTensor` by doing either:\n\n1. ``masked_tensor(sparse_tensor_data, sparse_tensor_mask)``\n2. ``dense_masked_tensor.to_sparse_coo()`` or ``dense_masked_tensor.to_sparse_csr()``\n\nThe second method is easier to illustrate so we've shown that below, but for more on the first and the nuances behind\nthe approach, please read the `Sparse COO Appendix <sparse-coo-appendix>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.masked import masked_tensor\nimport warnings\n\n# Disable prototype warnings and such\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n\nvalues = torch.tensor([[0, 0, 3], [4, 0, 5]])\nmask = torch.tensor([[False, False, True], [False, False, True]])\nmt = masked_tensor(values, mask)\nsparse_coo_mt = mt.to_sparse_coo()\n\nprint(\"mt:\\n\", mt)\nprint(\"mt (sparse coo):\\n\", sparse_coo_mt)\nprint(\"mt data (sparse coo):\\n\", sparse_coo_mt.get_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparse CSR tensors\n\nSimilarly, :class:`MaskedTensor` also supports the\n[CSR (Compressed Sparse Row)](https://pytorch.org/docs/stable/sparse.html#sparse-csr-tensor)_\nsparse tensor format. Instead of storing the tuples of the indices like sparse COO tensors, sparse CSR tensors\naim to decrease the memory requirements by storing compressed row indices.\nIn particular, a CSR sparse tensor consists of three 1-D tensors:\n\n* ``crow_indices``: array of compressed row indices with size ``(size[0] + 1,)``. This array indicates which row\n  a given entry in values lives in. The last element is the number of specified elements,\n  while `crow_indices[i+1] - crow_indices[i]` indicates the number of specified elements in row i.\n* ``col_indices``: array of size ``(nnz,)``. Indicates the column indices for each value.\n* ``values``: array of size ``(nnz,)``. Contains the values of the CSR tensor.\n\nOf note, both sparse COO and CSR tensors are in a [beta](https://pytorch.org/docs/stable/index.html)_ state.\n\nBy way of example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt_sparse_csr = mt.to_sparse_csr()\n\nprint(\"mt (sparse csr):\\n\", mt_sparse_csr)\nprint(\"mt data (sparse csr):\\n\", mt_sparse_csr.get_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supported Operations\n\n### Unary\nAll [unary operators](https://pytorch.org/docs/master/masked.html#unary-operators)_ are supported, e.g.:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt.sin()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary\n[Binary operators](https://pytorch.org/docs/master/masked.html#unary-operators)_ are also supported, but the\ninput masks from the two masked tensors must match. For more information on why this decision was made, please\nfind our [MaskedTensor: Advanced Semantics tutorial](https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html)_.\n\nPlease find an example below:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "i = [[0, 1, 1],\n     [2, 0, 2]]\nv1 = [3, 4, 5]\nv2 = [20, 30, 40]\nm = torch.tensor([True, False, True])\n\ns1 = torch.sparse_coo_tensor(i, v1, (2, 3))\ns2 = torch.sparse_coo_tensor(i, v2, (2, 3))\nmask = torch.sparse_coo_tensor(i, m, (2, 3))\n\nmt1 = masked_tensor(s1, mask)\nmt2 = masked_tensor(s2, mask)\n\nprint(\"mt1:\\n\", mt1)\nprint(\"mt2:\\n\", mt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"torch.div(mt2, mt1):\\n\", torch.div(mt2, mt1))\nprint(\"torch.mul(mt1, mt2):\\n\", torch.mul(mt1, mt2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reductions\nFinally, [reductions](https://pytorch.org/docs/master/masked.html#reductions)_ are supported:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mt.sum():\\n\", mt.sum())\nprint(\"mt.sum(dim=1):\\n\", mt.sum(dim=1))\nprint(\"mt.amin():\\n\", mt.amin())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MaskedTensor Helper Methods\nFor convenience, :class:`MaskedTensor` has a number of methods to help convert between the different layouts\nand identify the current layout:\n\nSetup:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "v = [[3, 0, 0],\n     [0, 4, 5]]\nm = [[True, False, False],\n     [False, True, True]]\n\nmt = masked_tensor(torch.tensor(v), torch.tensor(m))\nmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":meth:`MaskedTensor.to_sparse_coo()` / :meth:`MaskedTensor.to_sparse_csr()` / :meth:`MaskedTensor.to_dense()`\nto help convert between the different layouts.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt_sparse_coo = mt.to_sparse_coo()\nmt_sparse_csr = mt.to_sparse_csr()\nmt_dense = mt_sparse_coo.to_dense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":meth:`MaskedTensor.is_sparse()` -- this will check if the :class:`MaskedTensor`'s layout\nmatches any of the supported sparse layouts (currently COO and CSR).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mt_dense.is_sparse: \", mt_dense.is_sparse())\nprint(\"mt_sparse_coo.is_sparse: \", mt_sparse_coo.is_sparse())\nprint(\"mt_sparse_csr.is_sparse: \", mt_sparse_csr.is_sparse())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":meth:`MaskedTensor.is_sparse_coo()`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mt_dense.is_sparse_coo: \", mt_dense.is_sparse_coo())\nprint(\"mt_sparse_coo.is_sparse_coo: \", mt_sparse_coo.is_sparse_coo())\nprint(\"mt_sparse_csr.is_sparse_coo: \", mt_sparse_csr.is_sparse_coo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":meth:`MaskedTensor.is_sparse_csr()`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mt_dense.is_sparse_csr: \", mt_dense.is_sparse_csr())\nprint(\"mt_sparse_coo.is_sparse_csr: \", mt_sparse_coo.is_sparse_csr())\nprint(\"mt_sparse_csr.is_sparse_csr: \", mt_sparse_csr.is_sparse_csr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix\n\n\n### Sparse COO Construction\n\nRecall in our `original example <sparse-coo-tensors>`, we created a :class:`MaskedTensor`\nand then converted it to a sparse COO MaskedTensor with :meth:`MaskedTensor.to_sparse_coo`.\n\nAlternatively, we can also construct a sparse COO MaskedTensor directly by passing in two sparse COO tensors:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = torch.tensor([[0, 0, 3], [4, 0, 5]]).to_sparse()\nmask = torch.tensor([[False, False, True], [False, False, True]]).to_sparse()\nmt = masked_tensor(values, mask)\n\nprint(\"values:\\n\", values)\nprint(\"mask:\\n\", mask)\nprint(\"mt:\\n\", mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of using :meth:`torch.Tensor.to_sparse`, we can also create the sparse COO tensors directly,\nwhich brings us to a warning:\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>When using a function like :meth:`MaskedTensor.to_sparse_coo` (analogous to :meth:`Tensor.to_sparse`),\n  if the user does not specify the indices like in the above example,\n  then the 0 values will be \"unspecified\" by default.</p></div>\n\nBelow, we explicitly specify the 0's:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "i = [[0, 1, 1],\n     [2, 0, 2]]\nv = [3, 4, 5]\nm = torch.tensor([True, False, True])\nvalues = torch.sparse_coo_tensor(i, v, (2, 3))\nmask = torch.sparse_coo_tensor(i, m, (2, 3))\nmt2 = masked_tensor(values, mask)\n\nprint(\"values:\\n\", values)\nprint(\"mask:\\n\", mask)\nprint(\"mt2:\\n\", mt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that ``mt`` and ``mt2`` look identical on the surface, and in the vast majority of operations, will yield the same\nresult. But this brings us to a detail on the implementation:\n\n``data`` and ``mask`` -- only for sparse MaskedTensors -- can have a different number of elements (:func:`nnz`)\n**at creation**, but the indices of ``mask`` must then be a subset of the indices of ``data``. In this case,\n``data`` will assume the shape of ``mask`` by ``data = data.sparse_mask(mask)``; in other words, any of the elements\nin ``data`` that are not ``True`` in ``mask`` (that is, not specified) will be thrown away.\n\nTherefore, under the hood, the data looks slightly different; ``mt2`` has the \"4\" value masked out and ``mt``\nis completely without it. Their underlying data has different shapes,\nwhich would make operations like ``mt + mt2`` invalid.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mt data:\\n\", mt.get_data())\nprint(\"mt2 data:\\n\", mt2.get_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n### Sparse CSR Construction\n\nWe can also construct a sparse CSR MaskedTensor using sparse CSR tensors,\nand like the example above, this results in a similar treatment under the hood.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crow_indices = torch.tensor([0, 2, 4])\ncol_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([1, 2, 3, 4])\nmask_values = torch.tensor([True, False, False, True])\n\ncsr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\nmask = torch.sparse_csr_tensor(crow_indices, col_indices, mask_values, dtype=torch.bool)\nmt = masked_tensor(csr, mask)\n\nprint(\"mt:\\n\", mt)\nprint(\"mt data:\\n\", mt.get_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nIn this tutorial, we have introduced how to use :class:`MaskedTensor` with sparse COO and CSR formats and\ndiscussed some of the subtleties under the hood in case users decide to access the underlying data structures\ndirectly. Sparse storage formats and masked semantics indeed have strong synergies, so much so that they are\nsometimes used as proxies for each other (as we will see in the next tutorial). In the future, we certainly plan\nto invest and continue developing in this direction.\n\n## Further Reading\n\nTo continue learning more, you can find our\n[Efficiently writing \"sparse\" semantics for Adagrad with MaskedTensor tutorial](https://pytorch.org/tutorials/prototype/maskedtensor_adagrad.html)_\nto see an example of how MaskedTensor can simplify existing workflows with native masking semantics.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}