{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum: Writing your environment and transforms with TorchRL\n",
    "==============================================================\n",
    "\n",
    "**Author**: [Vincent Moens](https://github.com/vmoens)\n",
    "\n",
    "Creating an environment (a simulator or an interface to a physical\n",
    "control system) is an integrative part of reinforcement learning and\n",
    "control engineering.\n",
    "\n",
    "TorchRL provides a set of tools to do this in multiple contexts. This\n",
    "tutorial demonstrates how to use PyTorch and TorchRL code a pendulum\n",
    "simulator from the ground up. It is freely inspired by the Pendulum-v1\n",
    "implementation from [OpenAI-Gym/Farama-Gymnasium control\n",
    "library](https://github.com/Farama-Foundation/Gymnasium).\n",
    "\n",
    "![Simple\n",
    "Pendulum](https://pytorch.org/tutorials/_static/img/pendulum.gif){.align-center}\n",
    "\n",
    "Key learnings:\n",
    "\n",
    "-   How to design an environment in TorchRL:\n",
    "    -   Writing specs (input, observation and reward);\n",
    "    -   Implementing behavior: seeding, reset and step.\n",
    "-   Transforming your environment inputs and outputs, and writing your\n",
    "    own transforms;\n",
    "-   How to use `~tensordict.TensorDict`{.interpreted-text role=\"class\"}\n",
    "    to carry arbitrary data structures through the `codebase`.\n",
    "\n",
    "    In the process, we will touch three crucial components of TorchRL:\n",
    "\n",
    "-   [environments](https://pytorch.org/rl/reference/envs.html)\n",
    "-   [transforms](https://pytorch.org/rl/reference/envs.html#transforms)\n",
    "-   [models (policy and value\n",
    "    function)](https://pytorch.org/rl/reference/modules.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a sense of what can be achieved with TorchRL\\'s environments, we\n",
    "will be designing a *stateless* environment. While stateful environments\n",
    "keep track of the latest physical state encountered and rely on this to\n",
    "simulate the state-to-state transition, stateless environments expect\n",
    "the current state to be provided to them at each step, along with the\n",
    "action undertaken. TorchRL supports both types of environments, but\n",
    "stateless environments are more generic and hence cover a broader range\n",
    "of features of the environment API in TorchRL.\n",
    "\n",
    "Modeling stateless environments gives users full control over the input\n",
    "and outputs of the simulator: one can reset an experiment at any stage\n",
    "or actively modify the dynamics from the outside. However, it assumes\n",
    "that we have some control over a task, which may not always be the case:\n",
    "solving a problem where we cannot control the current state is more\n",
    "challenging but has a much wider set of applications.\n",
    "\n",
    "Another advantage of stateless environments is that they can enable\n",
    "batched execution of transition simulations. If the backend and the\n",
    "implementation allow it, an algebraic operation can be executed\n",
    "seamlessly on scalars, vectors, or tensors. This tutorial gives such\n",
    "examples.\n",
    "\n",
    "This tutorial will be structured as follows:\n",
    "\n",
    "-   We will first get acquainted with the environment properties: its\n",
    "    shape (`batch_size`), its methods (mainly\n",
    "    `~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"},\n",
    "    `~torchrl.envs.EnvBase.reset`{.interpreted-text role=\"meth\"} and\n",
    "    `~torchrl.envs.EnvBase.set_seed`{.interpreted-text role=\"meth\"}) and\n",
    "    finally its specs.\n",
    "-   After having coded our simulator, we will demonstrate how it can be\n",
    "    used during training with transforms.\n",
    "-   We will explore new avenues that follow from the TorchRL\\'s API,\n",
    "    including: the possibility of transforming inputs, the vectorized\n",
    "    execution of the simulation and the possibility of backpropagation\n",
    "    through the simulation graph.\n",
    "-   Finally, we will train a simple policy to solve the system we\n",
    "    implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four things you must take care of when designing a new\n",
    "environment class:\n",
    "\n",
    "-   `EnvBase._reset`{.interpreted-text role=\"meth\"}, which codes for the\n",
    "    resetting of the simulator at a (potentially random) initial state;\n",
    "-   `EnvBase._step`{.interpreted-text role=\"meth\"} which codes for the\n",
    "    state transition dynamic;\n",
    "-   `EnvBase._set_seed`{.interpreted-text role=\"meth\"}\\` which\n",
    "    implements the seeding mechanism;\n",
    "-   the environment specs.\n",
    "\n",
    "Let us first describe the problem at hand: we would like to model a\n",
    "simple pendulum over which we can control the torque applied on its\n",
    "fixed point. Our goal is to place the pendulum in upward position\n",
    "(angular position at 0 by convention) and having it standing still in\n",
    "that position. To design our dynamic system, we need to define two\n",
    "equations: the motion equation following an action (the torque applied)\n",
    "and the reward equation that will constitute our objective function.\n",
    "\n",
    "For the motion equation, we will update the angular velocity following:\n",
    "\n",
    "$$\\dot{\\theta}_{t+1} = \\dot{\\theta}_t + (3 * g / (2 * L) * \\sin(\\theta_t) + 3 / (m * L^2) * u) * dt$$\n",
    "\n",
    "where $\\dot{\\theta}$ is the angular velocity in rad/sec, $g$ is the\n",
    "gravitational force, $L$ is the pendulum length, $m$ is its mass,\n",
    "$\\theta$ is its angular position and $u$ is the torque. The angular\n",
    "position is then updated according to\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\dot{\\theta}_{t+1} dt$$\n",
    "\n",
    "We define our reward as\n",
    "\n",
    "$$r = -(\\theta^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * u^2)$$\n",
    "\n",
    "which will be maximized when the angle is close to 0 (pendulum in upward\n",
    "position), the angular velocity is close to 0 (no motion) and the torque\n",
    "is 0 too.\n",
    "\n",
    "Coding the effect of an action: `~torchrl.envs.EnvBase._step`{.interpreted-text role=\"func\"}\n",
    "============================================================================================\n",
    "\n",
    "The step method is the first thing to consider, as it will encode the\n",
    "simulation that is of interest to us. In TorchRL, the\n",
    "`~torchrl.envs.EnvBase`{.interpreted-text role=\"class\"} class has a\n",
    "`EnvBase.step`{.interpreted-text role=\"meth\"} method that receives a\n",
    "`tensordict.TensorDict`{.interpreted-text role=\"class\"} instance with an\n",
    "`\"action\"` entry indicating what action is to be taken.\n",
    "\n",
    "To facilitate the reading and writing from that `tensordict` and to make\n",
    "sure that the keys are consistent with what\\'s expected from the\n",
    "library, the simulation part has been delegated to a private abstract\n",
    "method `_step`{.interpreted-text role=\"meth\"} which reads input data\n",
    "from a `tensordict`, and writes a *new* `tensordict` with the output\n",
    "data.\n",
    "\n",
    "The `_step`{.interpreted-text role=\"func\"} method should do the\n",
    "following:\n",
    "\n",
    "> 1.  Read the input keys (such as `\"action\"`) and execute the\n",
    ">     simulation based on these;\n",
    "> 2.  Retrieve observations, done state and reward;\n",
    "> 3.  Write the set of observation values along with the reward and done\n",
    ">     state at the corresponding entries in a new\n",
    ">     `TensorDict`{.interpreted-text role=\"class\"}.\n",
    "\n",
    "Next, the `~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"}\n",
    "method will merge the output of\n",
    "`~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"} in the input\n",
    "`tensordict` to enforce input/output consistency.\n",
    "\n",
    "Typically, for stateful environments, this will look like this:\n",
    "\n",
    "``` {.sourceCode .}\n",
    ">>> policy(env.reset())\n",
    ">>> print(tensordict)\n",
    "TensorDict(\n",
    "    fields={\n",
    "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "    batch_size=torch.Size([]),\n",
    "    device=cpu,\n",
    "    is_shared=False)\n",
    ">>> env.step(tensordict)\n",
    ">>> print(tensordict)\n",
    "TensorDict(\n",
    "    fields={\n",
    "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "        next: TensorDict(\n",
    "            fields={\n",
    "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "                observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "            batch_size=torch.Size([]),\n",
    "            device=cpu,\n",
    "            is_shared=False),\n",
    "        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "    batch_size=torch.Size([]),\n",
    "    device=cpu,\n",
    "    is_shared=False)\n",
    "```\n",
    "\n",
    "Notice that the root `tensordict` has not changed, the only modification\n",
    "is the appearance of a new `\"next\"` entry that contains the new\n",
    "information.\n",
    "\n",
    "In the Pendulum example, our `_step`{.interpreted-text role=\"meth\"}\n",
    "method will read the relevant entries from the input `tensordict` and\n",
    "compute the position and velocity of the pendulum after the force\n",
    "encoded by the `\"action\"` key has been applied onto it. We compute the\n",
    "new angular position of the pendulum `\"new_th\"` as the result of the\n",
    "previous position `\"th\"` plus the new velocity `\"new_thdot\"` over a time\n",
    "interval `dt`.\n",
    "\n",
    "Since our goal is to turn the pendulum up and maintain it still in that\n",
    "position, our `cost` (negative reward) function is lower for positions\n",
    "close to the target and low speeds. Indeed, we want to discourage\n",
    "positions that are far from being \\\"upward\\\" and/or speeds that are far\n",
    "from 0.\n",
    "\n",
    "In our example, `EnvBase._step`{.interpreted-text role=\"meth\"} is\n",
    "encoded as a static method since our environment is stateless. In\n",
    "stateful settings, the `self` argument is needed as the state needs to\n",
    "be read from the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n",
    "\n",
    "    g_force = tensordict[\"params\", \"g\"]\n",
    "    mass = tensordict[\"params\", \"m\"]\n",
    "    length = tensordict[\"params\", \"l\"]\n",
    "    dt = tensordict[\"params\", \"dt\"]\n",
    "    u = tensordict[\"action\"].squeeze(-1)\n",
    "    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "    new_thdot = (\n",
    "        thdot\n",
    "        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n",
    "    )\n",
    "    new_thdot = new_thdot.clamp(\n",
    "        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n",
    "    )\n",
    "    new_th = th + new_thdot * dt\n",
    "    reward = -costs.view(*tensordict.shape, 1)\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": new_th,\n",
    "            \"thdot\": new_thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting the simulator: `~torchrl.envs.EnvBase._reset`{.interpreted-text role=\"func\"}\n",
    "======================================================================================\n",
    "\n",
    "The second method we need to care about is the\n",
    "`~torchrl.envs.EnvBase._reset`{.interpreted-text role=\"meth\"} method.\n",
    "Like `~torchrl.envs.EnvBase._step`{.interpreted-text role=\"meth\"}, it\n",
    "should write the observation entries and possibly a done state in the\n",
    "`tensordict` it outputs (if the done state is omitted, it will be filled\n",
    "as `False` by the parent method\n",
    "`~torchrl.envs.EnvBase.reset`{.interpreted-text role=\"meth\"}). In some\n",
    "contexts, it is required that the `_reset` method receives a command\n",
    "from the function that called it (for example, in multi-agent settings\n",
    "we may want to indicate which agents need to be reset). This is why the\n",
    "`~torchrl.envs.EnvBase._reset`{.interpreted-text role=\"meth\"} method\n",
    "also expects a `tensordict` as input, albeit it may perfectly be empty\n",
    "or `None`.\n",
    "\n",
    "The parent `EnvBase.reset`{.interpreted-text role=\"meth\"} does some\n",
    "simple checks like the `EnvBase.step`{.interpreted-text role=\"meth\"}\n",
    "does, such as making sure that a `\"done\"` state is returned in the\n",
    "output `tensordict` and that the shapes match what is expected from the\n",
    "specs.\n",
    "\n",
    "For us, the only important thing to consider is whether\n",
    "`EnvBase._reset`{.interpreted-text role=\"meth\"} contains all the\n",
    "expected observations. Once more, since we are working with a stateless\n",
    "environment, we pass the configuration of the pendulum in a nested\n",
    "`tensordict` named `\"params\"`.\n",
    "\n",
    "In this example, we do not pass a done state as this is not mandatory\n",
    "for `_reset`{.interpreted-text role=\"meth\"} and our environment is\n",
    "non-terminating, so we always expect it to be `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "    high_th = torch.tensor(DEFAULT_X, device=self.device)\n",
    "    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n",
    "    low_th = -high_th\n",
    "    low_thdot = -high_thdot\n",
    "\n",
    "    # for non batch-locked environments, the input ``tensordict`` shape dictates the number\n",
    "    # of simulators run simultaneously. In other contexts, the initial\n",
    "    # random state's shape will depend upon the environment batch-size instead.\n",
    "    th = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_th - low_th)\n",
    "        + low_th\n",
    "    )\n",
    "    thdot = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_thdot - low_thdot)\n",
    "        + low_thdot\n",
    "    )\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": th,\n",
    "            \"thdot\": thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment metadata: `env.*_spec`\n",
    "==================================\n",
    "\n",
    "The specs define the input and output domain of the environment. It is\n",
    "important that the specs accurately define the tensors that will be\n",
    "received at runtime, as they are often used to carry information about\n",
    "environments in multiprocessing and distributed settings. They can also\n",
    "be used to instantiate lazily defined neural networks and test scripts\n",
    "without actually querying the environment (which can be costly with\n",
    "real-world physical systems for instance).\n",
    "\n",
    "There are four specs that we must code in our environment:\n",
    "\n",
    "-   `EnvBase.observation_spec`{.interpreted-text role=\"obj\"}: This will\n",
    "    be a `~torchrl.data.CompositeSpec`{.interpreted-text role=\"class\"}\n",
    "    instance where each key is an observation (a\n",
    "    `CompositeSpec`{.interpreted-text role=\"class\"} can be viewed as a\n",
    "    dictionary of specs).\n",
    "-   `EnvBase.action_spec`{.interpreted-text role=\"obj\"}: It can be any\n",
    "    type of spec, but it is required that it corresponds to the\n",
    "    `\"action\"` entry in the input `tensordict`;\n",
    "-   `EnvBase.reward_spec`{.interpreted-text role=\"obj\"}: provides\n",
    "    information about the reward space;\n",
    "-   `EnvBase.done_spec`{.interpreted-text role=\"obj\"}: provides\n",
    "    information about the space of the done flag.\n",
    "\n",
    "TorchRL specs are organized in two general containers: `input_spec`\n",
    "which contains the specs of the information that the step function reads\n",
    "(divided between `action_spec` containing the action and `state_spec`\n",
    "containing all the rest), and `output_spec` which encodes the specs that\n",
    "the step outputs (`observation_spec`, `reward_spec` and `done_spec`). In\n",
    "general, you should not interact directly with `output_spec` and\n",
    "`input_spec` but only with their content: `observation_spec`,\n",
    "`reward_spec`, `done_spec`, `action_spec` and `state_spec`. The reason\n",
    "if that the specs are organized in a non-trivial way within\n",
    "`output_spec` and `input_spec` and neither of these should be directly\n",
    "modified.\n",
    "\n",
    "In other words, the `observation_spec` and related properties are\n",
    "convenient shortcuts to the content of the output and input spec\n",
    "containers.\n",
    "\n",
    "TorchRL offers multiple `~torchrl.data.TensorSpec`{.interpreted-text\n",
    "role=\"class\"}\n",
    "[subclasses](https://pytorch.org/rl/reference/data.html#tensorspec) to\n",
    "encode the environment\\'s input and output characteristics.\n",
    "\n",
    "Specs shape\n",
    "-----------\n",
    "\n",
    "The environment specs leading dimensions must match the environment\n",
    "batch-size. This is done to enforce that every component of an\n",
    "environment (including its transforms) have an accurate representation\n",
    "of the expected input and output shapes. This is something that should\n",
    "be accurately coded in stateful settings.\n",
    "\n",
    "For non batch-locked environments, such as the one in our example (see\n",
    "below), this is irrelevant as the environment batch size will most\n",
    "likely be empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    # Under the hood, this will populate self.output_spec[\"observation\"]\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        th=BoundedTensorSpec(\n",
    "            low=-torch.pi,\n",
    "            high=torch.pi,\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        thdot=BoundedTensorSpec(\n",
    "            low=-td_params[\"params\", \"max_speed\"],\n",
    "            high=td_params[\"params\", \"max_speed\"],\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        # we need to add the ``params`` to the observation specs, as we want\n",
    "        # to pass it at each step during a rollout\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "    # since the environment is stateless, we expect the previous output as input.\n",
    "    # For this, ``EnvBase`` expects some state_spec to be available\n",
    "    self.state_spec = self.observation_spec.clone()\n",
    "    # action-spec will be automatically wrapped in input_spec when\n",
    "    # `self.action_spec = spec` will be called supported\n",
    "    self.action_spec = BoundedTensorSpec(\n",
    "        low=-td_params[\"params\", \"max_torque\"],\n",
    "        high=td_params[\"params\", \"max_torque\"],\n",
    "        shape=(1,),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducible experiments: seeding\n",
    "=================================\n",
    "\n",
    "Seeding an environment is a common operation when initializing an\n",
    "experiment. The only goal of `EnvBase._set_seed`{.interpreted-text\n",
    "role=\"func\"} is to set the seed of the contained simulator. If possible,\n",
    "this operation should not call `reset()` or interact with the\n",
    "environment execution. The parent `EnvBase.set_seed`{.interpreted-text\n",
    "role=\"func\"} method incorporates a mechanism that allows seeding\n",
    "multiple environments with a different pseudo-random and reproducible\n",
    "seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping things together: the `~torchrl.envs.EnvBase`{.interpreted-text role=\"class\"} class\n",
    "===========================================================================================\n",
    "\n",
    "We can finally put together the pieces and design our environment class.\n",
    "The specs initialization needs to be performed during the environment\n",
    "construction, so we must take care of calling the\n",
    "`_make_spec`{.interpreted-text role=\"func\"} method within\n",
    "`PendulumEnv.__init__`{.interpreted-text role=\"func\"}.\n",
    "\n",
    "We add a static method `PendulumEnv.gen_params`{.interpreted-text\n",
    "role=\"meth\"} which deterministically generates a set of hyperparameters\n",
    "to be used during execution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"max_speed\": 8,\n",
    "                    \"max_torque\": 2.0,\n",
    "                    \"dt\": 0.05,\n",
    "                    \"g\": g,\n",
    "                    \"m\": 1.0,\n",
    "                    \"l\": 1.0,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the environment as non-`batch_locked` by turning the\n",
    "`homonymous` attribute to `False`. This means that we will **not**\n",
    "enforce the input `tensordict` to have a `batch-size` that matches the\n",
    "one of the environment.\n",
    "\n",
    "The following code will just put together the pieces we have coded\n",
    "above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PendulumEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our environment\n",
    "=======================\n",
    "\n",
    "TorchRL provides a simple function\n",
    "`~torchrl.envs.utils.check_env_specs`{.interpreted-text role=\"func\"} to\n",
    "check that a (transformed) environment has an input/output structure\n",
    "that matches the one dictated by its specs. Let us try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at our specs to have a visual representation of the\n",
    "environment signature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a couple of commands too to check that the output\n",
    "structure matches what is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the `env.rand_step`{.interpreted-text role=\"func\"} to\n",
    "generate an action randomly from the `action_spec` domain. A\n",
    "`tensordict` containing the hyperparameters and the current state\n",
    "**must** be passed since our environment is stateless. In stateful\n",
    "contexts, `env.rand_step()` works perfectly too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming an environment\n",
    "===========================\n",
    "\n",
    "Writing environment transforms for stateless simulators is slightly more\n",
    "complicated than for stateful ones: transforming an output entry that\n",
    "needs to be read at the following iteration requires to apply the\n",
    "inverse transform before calling `meth.step`{.interpreted-text\n",
    "role=\"func\"} at the next step. This is an ideal scenario to showcase all\n",
    "the features of TorchRL\\'s transforms!\n",
    "\n",
    "For instance, in the following transformed environment we `unsqueeze`\n",
    "the entries `[\"th\", \"thdot\"]` to be able to stack them along the last\n",
    "dimension. We also pass them as `in_keys_inv` to squeeze them back to\n",
    "their original shape once they are passed as input in the next\n",
    "iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    # ``Unsqueeze`` the observations that we will concatenate\n",
    "    UnsqueezeTransform(\n",
    "        unsqueeze_dim=-1,\n",
    "        in_keys=[\"th\", \"thdot\"],\n",
    "        in_keys_inv=[\"th\", \"thdot\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing custom transforms\n",
    "=========================\n",
    "\n",
    "TorchRL\\'s transforms may not cover all the operations one wants to\n",
    "execute after an environment has been executed. Writing a transform does\n",
    "not require much effort. As for the environment design, there are two\n",
    "steps in writing a transform:\n",
    "\n",
    "-   Getting the dynamics right (forward and inverse);\n",
    "-   Adapting the environment specs.\n",
    "\n",
    "A transform can be used in two settings: on its own, it can be used as a\n",
    "`~torch.nn.Module`{.interpreted-text role=\"class\"}. It can also be used\n",
    "appended to a\n",
    "`~torchrl.envs.transforms.TransformedEnv`{.interpreted-text\n",
    "role=\"class\"}. The structure of the class allows to customize the\n",
    "behavior in the different contexts.\n",
    "\n",
    "A `~torchrl.envs.transforms.Transform`{.interpreted-text role=\"class\"}\n",
    "skeleton can be summarized as follows:\n",
    "\n",
    "``` {.sourceCode .}\n",
    "class Transform(nn.Module):\n",
    "    def forward(self, tensordict):\n",
    "        ...\n",
    "    def _apply_transform(self, tensordict):\n",
    "        ...\n",
    "    def _step(self, tensordict):\n",
    "        ...\n",
    "    def _call(self, tensordict):\n",
    "        ...\n",
    "    def inv(self, tensordict):\n",
    "        ...\n",
    "    def _inv_apply_transform(self, tensordict):\n",
    "        ...\n",
    "```\n",
    "\n",
    "There are three entry points (`forward`{.interpreted-text role=\"func\"},\n",
    "`_step`{.interpreted-text role=\"func\"} and `inv`{.interpreted-text\n",
    "role=\"func\"}) which all receive\n",
    "`tensordict.TensorDict`{.interpreted-text role=\"class\"} instances. The\n",
    "first two will eventually go through the keys indicated by\n",
    "`~tochrl.envs.transforms.Transform.in_keys`{.interpreted-text\n",
    "role=\"obj\"} and call\n",
    "`~torchrl.envs.transforms.Transform._apply_transform`{.interpreted-text\n",
    "role=\"meth\"} to each of these. The results will be written in the\n",
    "entries pointed by `Transform.out_keys`{.interpreted-text role=\"obj\"} if\n",
    "provided (if not the `in_keys` will be updated with the transformed\n",
    "values). If inverse transforms need to be executed, a similar data flow\n",
    "will be executed but with the `Transform.inv`{.interpreted-text\n",
    "role=\"func\"} and `Transform._inv_apply_transform`{.interpreted-text\n",
    "role=\"func\"} methods and across the `in_keys_inv` and `out_keys_inv`\n",
    "list of keys. The following figure summarized this flow for environments\n",
    "and replay buffers.\n",
    "\n",
    "> Transform API\n",
    "\n",
    "In some cases, a transform will not work on a subset of keys in a\n",
    "unitary manner, but will execute some operation on the parent\n",
    "environment or work with the entire input `tensordict`. In those cases,\n",
    "the `_call`{.interpreted-text role=\"func\"} and\n",
    "`forward`{.interpreted-text role=\"func\"} methods should be re-written,\n",
    "and the `_apply_transform`{.interpreted-text role=\"func\"} method can be\n",
    "skipped.\n",
    "\n",
    "Let us code new transforms that will compute the `sine` and `cosine`\n",
    "values of the position angle, as these values are more useful to us to\n",
    "learn a policy than the raw angle value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SinTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.sin()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "class CosTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.cos()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "t_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\n",
    "t_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\n",
    "env.append_transform(t_sin)\n",
    "env.append_transform(t_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenates the observations onto an \\\"observation\\\" entry.\n",
    "`del_keys=False` ensures that we keep these values for the next\n",
    "iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_transform = CatTensors(\n",
    "    in_keys=[\"sin\", \"cos\", \"thdot\"], dim=-1, out_key=\"observation\", del_keys=False\n",
    ")\n",
    "env.append_transform(cat_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, let us check that our environment specs match what is\n",
    "received:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a rollout\n",
    "===================\n",
    "\n",
    "Executing a rollout is a succession of simple steps:\n",
    "\n",
    "-   reset the environment\n",
    "-   while some condition is not met:\n",
    "    -   compute an action given a policy\n",
    "    -   execute a step given this action\n",
    "    -   collect the data\n",
    "    -   make a `MDP` step\n",
    "-   gather the data and return\n",
    "\n",
    "These operations have been conveniently wrapped in the\n",
    "`~torchrl.envs.EnvBase.rollout`{.interpreted-text role=\"meth\"} method,\n",
    "from which we provide a simplified version here below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_rollout(steps=100):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching computations\n",
    "=====================\n",
    "\n",
    "The last unexplored end of our tutorial is the ability that we have to\n",
    "batch computations in TorchRL. Because our environment does not make any\n",
    "assumptions regarding the input data shape, we can seamlessly execute it\n",
    "over batches of data. Even better: for non-batch-locked environments\n",
    "such as our Pendulum, we can change the batch size on the fly without\n",
    "recreating the environment. To do this, we just generate parameters with\n",
    "the desired shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10  # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(\"reset (batch size of 10)\", td)\n",
    "td = env.rand_step(td)\n",
    "print(\"rand step (batch size of 10)\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a rollout with a batch of data requires us to reset the\n",
    "environment out of the rollout function, since we need to define the\n",
    "batch\\_size dynamically and this is not supported by\n",
    "`~torchrl.envs.EnvBase.rollout`{.interpreted-text role=\"meth\"}:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rollout = env.rollout(\n",
    "    3,\n",
    "    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n",
    "    tensordict=env.reset(env.gen_params(batch_size=[batch_size])),\n",
    ")\n",
    "print(\"rollout of len 3 (batch size of 10):\", rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a simple policy\n",
    "========================\n",
    "\n",
    "In this example, we will train a simple policy using the reward as a\n",
    "differentiable objective, such as a negative loss. We will take\n",
    "advantage of the fact that our dynamic system is fully differentiable to\n",
    "backpropagate through the trajectory return and adjust the weights of\n",
    "our policy to maximize this value directly. Of course, in many settings\n",
    "many of the assumptions we make do not hold, such as differentiable\n",
    "system and full access to the underlying mechanics.\n",
    "\n",
    "Still, this is a very simple example that showcases how a training loop\n",
    "can be coded with a custom environment in TorchRL.\n",
    "\n",
    "Let us first write the policy network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "=============\n",
    "\n",
    "We will successively:\n",
    "\n",
    "-   generate a trajectory\n",
    "-   sum the rewards\n",
    "-   backpropagate through the graph defined by these operations\n",
    "-   clip the gradient norm and make an optimization step\n",
    "-   repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to\n",
    "0 which demonstrates that the pendulum is upward and still as desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we have learned how to code a stateless environment\n",
    "from scratch. We touched the subjects of:\n",
    "\n",
    "-   The four essential components that need to be taken care of when\n",
    "    coding an environment (`step`, `reset`, seeding and building specs).\n",
    "    We saw how these methods and classes interact with the\n",
    "    `~tensordict.TensorDict`{.interpreted-text role=\"class\"} class;\n",
    "-   How to test that an environment is properly coded using\n",
    "    `~torchrl.envs.utils.check_env_specs`{.interpreted-text\n",
    "    role=\"func\"};\n",
    "-   How to append transforms in the context of stateless environments\n",
    "    and how to write custom transformations;\n",
    "-   How to train a policy on a fully differentiable simulator.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
