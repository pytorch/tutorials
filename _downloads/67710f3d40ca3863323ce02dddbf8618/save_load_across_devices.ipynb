{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Saving and loading models across devices in PyTorch\n\nThere may be instances where you want to save and load your neural\nnetworks across different devices.\n\n## Introduction\n\nSaving and loading models across devices is relatively straightforward\nusing PyTorch. In this recipe, we will experiment with saving and\nloading models across CPUs and GPUs.\n\n## Setup\n\nIn order for every code block to run properly in this recipe, you must\nfirst change the runtime to \u201cGPU\u201d or higher. Once you do, we need to\ninstall ``torch`` if it isn\u2019t already available.\n\n::\n\n   pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steps\n\n1. Import all necessary libraries for loading our data\n2. Define and initialize the neural network\n3. Save on a GPU, load on a CPU\n4. Save on a GPU, load on a GPU\n5. Save on a CPU, load on a GPU\n6. Saving and loading ``DataParallel`` models\n\n### 1. Import necessary libraries for loading our data\n\nFor this recipe, we will use ``torch`` and its subsidiaries ``torch.nn``\nand ``torch.optim``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Define and initialize the neural network\n\nFor sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Save on GPU, Load on CPU\n\nWhen loading a model on a CPU that was trained with a GPU, pass\n``torch.device('cpu')`` to the ``map_location`` argument in the\n``torch.load()`` function.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Specify a path to save to\nPATH = \"model.pt\"\n\n# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device('cpu')\nmodel = Net()\nmodel.load_state_dict(torch.load(PATH, map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the storages underlying the tensors are dynamically\nremapped to the CPU device using the ``map_location`` argument.\n\n### 4. Save on GPU, Load on GPU\n\nWhen loading a model on a GPU that was trained and saved on GPU, simply\nconvert the initialized model to a CUDA optimized model using\n``model.to(torch.device('cuda'))``.\n\nBe sure to use the ``.to(torch.device('cuda'))`` function on all model\ninputs to prepare the data for the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device(\"cuda\")\nmodel = Net()\nmodel.load_state_dict(torch.load(PATH))\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that calling ``my_tensor.to(device)`` returns a new copy of\n``my_tensor`` on GPU. It does NOT overwrite ``my_tensor``. Therefore,\nremember to manually overwrite tensors:\n``my_tensor = my_tensor.to(torch.device('cuda'))``.\n\n### 5. Save on CPU, Load on GPU\n\nWhen loading a model on a GPU that was trained and saved on CPU, set the\n``map_location`` argument in the ``torch.load()`` function to\n``cuda:device_id``. This loads the model to a given GPU device.\n\nBe sure to call ``model.to(torch.device('cuda'))`` to convert the\nmodel\u2019s parameter tensors to CUDA tensors.\n\nFinally, also be sure to use the ``.to(torch.device('cuda'))`` function\non all model inputs to prepare the data for the CUDA optimized model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device(\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Saving ``torch.nn.DataParallel`` Models\n\n``torch.nn.DataParallel`` is a model wrapper that enables parallel GPU\nutilization.\n\nTo save a ``DataParallel`` model generically, save the\n``model.module.state_dict()``. This way, you have the flexibility to\nload the model any way you want to any device you want.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save\ntorch.save(net.module.state_dict(), PATH)\n\n# Load to whatever device you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! You have successfully saved and loaded models across\ndevices in PyTorch.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}