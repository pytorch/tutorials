{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n(prototype) Graph Mode Post Training Static Quantization in PyTorch\n=========================================================\n\n**Author**: `Jerry Zhang <https://github.com/jerryzh168>`_\n\nThis tutorial introduces the steps to do post training static quantization in graph mode. \nThe advantage of graph mode quantization is that as long as the model can be scripted or traced, \nwe can perform quantization fully automatically on the model. \nRight now we can do post training static and post training dynamic quantization \nand quantization aware training support will come later. \nWe have a separate tutorial for `Graph Mode Post Training Dynamic Quantization <https://pytorch.org/tutorials/prototype_source/graph_mode_dynamic_bert_tutorial.html>`_.\n\ntldr; The graph mode API looks like the following:\n\n.. code:: python\n\n    import torch\n    from torch.quantization import get_default_qconfig, quantize_jit\n    \n    ts_model = torch.jit.script(float_model.eval()) # or torch.jit.trace(float_model, input)\n    qconfig = get_default_qconfig('fbgemm')\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n    quantized_model = quantize_jit(\n        ts_model, # TorchScript model\n        {'': qconfig}, # qconfig dict\n        calibrate, # calibration function\n        [data_loader_test]) # positional arguments to calibration function, typically some sample dataset\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Motivation of Graph Mode Quantization\n---------------------\nCurrently PyTorch only has eager mode quantization: `Static Quantization with Eager Mode in PyTorch <https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.\n\nWe can see there are multiple manual steps involved in the process, including:\n\n- Explicitly quantize and dequantize activations, this is time consuming when floating point and quantized operations are mixed in a model.\n- Explicitly fuse modules, this requires manually identifying the sequence of convolutions, batch norms and relus and other fusion patterns.\n- Special handling is needed for pytorch tensor operations (like add, concat etc.)\n- Functionals did not have first class support (functional.conv2d and functional.linear would not get quantized)\n\nMost of these required modifications comes from the underlying limitations of eager mode quantization. Eager mode works in module level since it can not inspect the code that is actually run (in the forward function), quantization is achieved by module swapping, and we don\u2019t know how the modules are used in forward function in eager mode, so it requires users to insert QuantStub and DeQuantStub manually to mark the points they want to quantize or dequantize. \nIn graph mode, we can inspect the actual code that\u2019s been executed in forward function (e.g. aten function calls) and quantization is achieved by module and graph manipulations. Since graph mode has full visibility of the code that is run, our tool is able to automatically figure out things like which modules to fuse and where to insert observer calls, quantize/dequantize functions etc., we are able to automate the whole quantization process.\n\nAdvantages of graph mode quantization are:\n\n- Simple quantization flow, minimal manual steps\n- Unlocks the possibility of doing higher level optimizations like automatic precision selection\n\nLimitations of graph mode quantization is that quantization is configurable only at the level of module and the set of operators that are quantized is not configurable by user currently.\n\n2. Define Helper Functions and Prepare Dataset\n---------------------\nWe\u2019ll start by doing the necessary imports, defining some helper functions and prepare the data. \nThese steps are identitcal to `Static Quantization with Eager Mode in PyTorch <https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.    \n\nDownload dataset:\n\n.. code::\n\n    wget https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip\n\nand unzip to `data` folder.\nDownload the `torchvision resnet18 model <https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L12>`_ and rename it to\n``data/resnet18_pretrained_float.pth``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport os\nimport time\nimport sys\nimport torch.quantization\n\n# # Setup warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.quantization'\n)\n\n# Specify random seed for repeatable results\n_ = torch.manual_seed(191009)\n\n\nfrom torchvision.models.resnet import resnet18\nfrom torch.quantization import get_default_qconfig, quantize_jit\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n    print('')\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = resnet18(pretrained=False)\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    if isinstance(model, torch.jit.RecursiveScriptModule):\n        torch.jit.save(model, \"temp.p\")\n    else:\n        torch.jit.save(torch.jit.script(model), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\ndef prepare_data_loaders(data_path):\n\n    traindir = os.path.join(data_path, 'train')\n    valdir = os.path.join(data_path, 'val')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    dataset = torchvision.datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    dataset_test = torchvision.datasets.ImageFolder(\n        valdir,\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test\n\ndata_path = 'data/imagenet_1k'\nsaved_model_dir = 'data/'\nfloat_model_file = 'resnet18_pretrained_float.pth'\n\ntrain_batch_size = 30\neval_batch_size = 30\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\nfloat_model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Script/Trace the model\n--------------------------\nThe input for graph mode quantization is a TorchScript model, so we'll need to either script or trace the model first.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ts_model = torch.jit.script(float_model).eval() # ts_model = torch.jit.trace(float_model, input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Specify how to quantize the model with ``qconfig_dict``\n-------------------------\n\n.. code:: python\n\n  qconfig_dict = {'' : default_qconfig}\n\nWe use the same ``qconfig`` used in eager mode quantization, ``qconfig`` is just a named tuple of the observers for ``activation`` and ``weight``. `qconfig_dict` is a dictionary with names of sub modules as key and qconfig for that module as value, empty key means the qconfig will be applied to whole model unless it\u2019s overwritten by more specific configurations, the qconfig for each module is either found in the dictionary or fallback to the qconfig of parent module.\n\nRight now ``qconfig_dict`` is the only way to configure how the model is quantized, and it is done in the granularity of module, that is, we only support one type of ``qconfig`` for each ``torch.nn.Module``, for example, if we have:\n\n.. code:: python\n\n  qconfig = {\n        '' : qconfig_global,\n       'sub' : qconfig_sub,\n        'sub.fc' : qconfig_fc,\n       'sub.conv': None\n  }\n\nModule ``sub.fc`` will be configured with ``qconfig_fc``, and all other child modules in ``sub`` will be configured with ``qconfig_sub`` and ``sub.conv`` will not be quantized. All other modules in the model will be quantized with ``qconfig_global``\nUtility functions related to ``qconfig`` can be found in https://github.com/pytorch/pytorch/blob/master/torch/quantization/qconfig.py.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qconfig = get_default_qconfig('fbgemm')\nqconfig_dict = {'': qconfig}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Define Calibration Function\n-------------------------\n\n.. code:: python\n\n  def calibrate(model, sample_data, ...):\n      model(sample_data, ...)\n\n\nCalibration function is run after the observers are inserted in the model. \nThe purpose for calibration is to run through some sample examples that is representative of the workload \n(for example a sample of the training data set) so that the observers in the model are able to observe\nthe statistics of the Tensors and we can later use this information to calculate quantization parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Quantize\n---------------------\n\n.. code:: python\n\n    quantized_model = quantize_jit(\n        ts_model, # TorchScript model\n        {'': qconfig}, # qconfig dict\n        calibrate, # calibration function\n        [data_loader_test], # positional arguments to calibration function, typically some sample dataset\n        inplace=False, # whether to modify the model inplace or not\n        debug=True) # whether to prduce a debug friendly model or not\n\nThere are three things we do in ``quantize_jit``:\n\n1. ``prepare_jit`` folds BatchNorm modules into previous Conv2d modules, and insert observers in appropriate places in the Torchscript model.\n2. Run calibrate function on the provided sample dataset.\n3. ``convert_jit`` takes a calibrated model and produces a quantized model.\n\nIf ``debug`` is False (default option), ``convert_jit`` will:\n\n- Calculate quantization parameters using the observers in the model\n- Ifnsert quantization ops like ``aten::quantize_per_tensor`` and ``aten::dequantize`` to the model, and remove the observer modules after that.\n- Replace floating point ops with quantized ops\n- Freeze the model (remove constant attributes and make them as Constant node in the graph).\n- Fold the quantize and prepack ops like ``quantized::conv2d_prepack`` into an attribute, so we don't need to quantize and prepack the weight everytime we run the model.\n\nIf ``debug`` is set to ``True``:\n\n- We can still access the attributes of the quantized model the same way as the original floating point model, e.g. ``model.conv1.weight`` (might be harder if you use a module list or sequential)\n- The arithmetic operations all occur in floating point with the numerics being identical to the final quantized model, allowing for debugging.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_model = quantize_jit(\n    ts_model,\n    {'': qconfig},\n    calibrate,\n    [data_loader_test])\n\nprint(quantized_model.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see ``aten::conv2d`` is changed to ``quantized::conv2d`` and the floating point weight has been quantized \nand packed into an attribute (``quantized._jit_pass_packed_weight_30``), so we don't need to quantize/pack in runtime.\nAlso we can't access the weight attributes anymore after the debug option since they are frozen.\n\n7. Evaluation\n--------------\nWe can now print the size and accuracy of the quantized model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Size of model before quantization')\nprint_size_of_model(ts_model)\nprint('Size of model after quantization')\nprint_size_of_model(quantized_model)\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test)\nprint('[before serilaization] Evaluation accuracy on test dataset: %2.2f, %2.2f'%(top1.avg, top5.avg))\n\ngraph_mode_model_file = 'resnet18_graph_mode_quantized.pth'\ntorch.jit.save(quantized_model, saved_model_dir + graph_mode_model_file)\nquantized_model = torch.jit.load(saved_model_dir + graph_mode_model_file)\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test)\nprint('[after serialization/deserialization] Evaluation accuracy on test dataset: %2.2f, %2.2f'%(top1.avg, top5.avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to get better accuracy or performance,  try changing the `qconfig_dict`. \nWe plan to add support for graph mode in the Numerical Suite so that you can \neasily determine the sensitivity towards quantization of different modules in a model: `PyTorch Numeric Suite Tutorial <https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html>`_\n\n8. Debugging Quantized Model\n---------------------------\nWe can also use debug option:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_debug_model = quantize_jit(\n    ts_model,\n    {'': qconfig},\n    calibrate,\n    [data_loader_test],\n    debug=True)\n\ntop1, top5 = evaluate(quantized_debug_model, criterion, data_loader_test)\nprint('[debug=True] quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f'%(top1.avg, top5.avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the accuracy of the debug version is close to, but not exactly the same as the non-debug \nversion as the debug version uses floating point ops to emulate quantized ops and the numerics match \nis approximate. We are working on making this even more exact.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(quantized_debug_model.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there is no ``quantized::conv2d`` in the model, but the numerically equivalent pattern \nof ``aten::dequnatize - aten::conv2d - aten::quantize_per_tensor``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print_size_of_model(quantized_debug_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Size of the debug model is the close to the floating point model because all the weights are \nin float and not yet quantized and frozen, this allows people to inspect the weight. \nYou may access the weight attributes directly in the torchscript model, except for batch norm as\nit is fused into the preceding convolutions. We will also develop graph mode ``Numeric Suite`` \nto allow easier inspection of weights in the future. Accessing the weight in the debug model is \nthe same as accessing the weight in a TorchScript model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_first_conv_weight(model):\n    return model.conv1.weight\nw1 = get_first_conv_weight(ts_model)\nw2 = get_first_conv_weight(quantized_debug_model)\nprint('first conv weight for input model:', str(w1)[:200])\nprint('first conv weight for quantized model:', str(w2)[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The weights are different because we fold the weights of BatchNorm to the previous conv before we quantize the model.\nMore instructions on how to debug TorchScript model can be found `here <https://pytorch.org/docs/stable/jit.html#debugging>`_.\n\n\nAs we can see, this is not as straightforward as eager mode, that's why we also plan to support graph mode ``Numeric Suite``,\nand it will probably be the primary tool people use to debug numerical issues.\n\n9. Comparison with Baseline Float Model and Eager Mode Quantization\n---------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scripted_float_model_file = 'resnet18_scripted.pth'\n\nprint('Size of baseline model')\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test)\nprint('Baseline Float Model Evaluation accuracy: %2.2f, %2.2f'%(top1.avg, top5.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we compare the model quantized with graph mode quantization with the model \nquantized in eager mode. Graph mode and eager mode produce very similar quantized models, \nso the expectation is that the accuracy and speedup are similar as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Size of graph mode quantized model')\nprint_size_of_model(quantized_model)\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test)\nprint('graph mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f'%(top1.avg, top5.avg))\n\nfrom torchvision.models.quantization.resnet import resnet18\neager_quantized_model = resnet18(pretrained=True, quantize=True).eval()\nprint('Size of eager mode quantized model')\neager_quantized_model = torch.jit.script(eager_quantized_model)\nprint_size_of_model(eager_quantized_model)\ntop1, top5 = evaluate(eager_quantized_model, criterion, data_loader_test)\nprint('eager mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f'%(top1.avg, top5.avg))\neager_mode_model_file = 'resnet18_eager_mode_quantized.pth'\ntorch.jit.save(eager_quantized_model, saved_model_dir + eager_mode_model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model size and accuracy of graph mode and eager mode quantized model are pretty similar.\n\nRunning the model in AIBench (with single threading) gives the following result:\n\n.. code::\n\n  Scripted Float Model:\n  Self CPU time total: 418.472ms\n\n  Scripted Eager Mode Quantized Model:\n  Self CPU time total: 177.768ms\n\n  Graph Mode Quantized Model:\n  Self CPU time total: 157.256ms\n\nAs we can see for resnet18 both graph mode and eager mode quantized model get similar speed up over the floating point model,\nwhich is around 2-3x faster than the floating point model. But the actual speedup over floating point model may vary \ndepending on model, device, build, input batch sizes, threading etc.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}