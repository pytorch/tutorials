{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(beta) Running the compiled optimizer with an LR Scheduler\n",
    "==========================================================\n",
    "\n",
    "**Author:** [Michael Lazos](https://github.com/mlazos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is a key algorithm for training any deep learning model.\n",
    "In this example, we will show how to pair the optimizer, which has been\n",
    "compiled using `torch.compile`, with the LR schedulers to accelerate\n",
    "training convergence.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>This tutorial requires PyTorch 2.3.0 or later.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup\n",
    "===========\n",
    "\n",
    "For this example, we\\'ll use a simple sequence of linear layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create simple model\n",
    "model = torch.nn.Sequential(\n",
    "    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n",
    ")\n",
    "input = torch.rand(1024, device=\"cuda\")\n",
    "\n",
    "# run forward pass\n",
    "output = model(input)\n",
    "\n",
    "# run backward to populate the grads for our optimizer below\n",
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up and running the compiled optimizer with LR Scheduler\n",
    "===============================================================\n",
    "\n",
    "In this section, we\\'ll use the Adam optimizer with LinearLR Scheduler\n",
    "and create a helper function to wrap the `step()` call for each of them\n",
    "in `torch.compile()`.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p><code>torch.compile</code> is only supported on CUDA devices that have a compute capability of 7.0 or higher.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exit cleanly if we are on a device that doesn't support ``torch.compile``\n",
    "if torch.cuda.get_device_capability() < (7, 0):\n",
    "    print(\"Exiting because torch.compile is not supported on this device.\")\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "# !!! IMPORTANT !!! Wrap the lr in a Tensor if we are pairing the\n",
    "# the optimizer with an LR Scheduler.\n",
    "# Without this, torch.compile will recompile as the value of the LR\n",
    "# changes.\n",
    "opt = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01))\n",
    "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "    print(opt.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension: What happens with a non-tensor LR?\n",
    "=============================================\n",
    "\n",
    "For the curious, we will show how to peek into what happens with\n",
    "`torch.compile` when we don\\'t wrap the LR in a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No longer wrap the LR in a tensor here\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "# Setup logging to view recompiles\n",
    "torch._logging.set_logs(recompiles=True)\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "# We will now recompile on each iteration\n",
    "# as the value of the lr is mutated.\n",
    "for _ in range(5):\n",
    "    fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example, we can see that we recompile the optimizer a few\n",
    "times due to the guard failure on the `lr` in `param_groups[0]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial we showed how to pair the optimizer compiled with\n",
    "`torch.compile` with an LR Scheduler to accelerate training convergence.\n",
    "We used a model consisting of a simple sequence of linear layers with\n",
    "the Adam optimizer paired with a LinearLR scheduler to demonstrate the\n",
    "LR changing across iterations.\n",
    "\n",
    "See also:\n",
    "\n",
    "-   [Compiled optimizer\n",
    "    tutorial](https://pytorch.org/tutorials/recipes/compiling_optimizer.html) -\n",
    "    an intro into the compiled optimizer.\n",
    "-   [Compiling the optimizer with\n",
    "    PT2](https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669) -\n",
    "    deeper technical details on the compiled optimizer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
