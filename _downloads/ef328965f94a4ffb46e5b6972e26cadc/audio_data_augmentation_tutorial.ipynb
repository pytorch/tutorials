{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAudio Data Augmentation\n=================\n\n``torchaudio`` provides a variety of ways to augment audio data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# When running this tutorial in Google Colab, install the required packages\n# with the following.\n# !pip install torchaudio\n\nimport torch\nimport torchaudio\nimport torchaudio.functional as F\n\nprint(torch.__version__)\nprint(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing data and utility functions (skip this section)\n--------------------------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Prepare data and utility functions. {display-mode: \"form\"}\n#@markdown\n#@markdown You do not need to look into this cell.\n#@markdown Just execute once and you are good to go.\n#@markdown\n#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), which is licensed under Creative Commos BY 4.0.\n\n#-------------------------------------------------------------------------------\n# Preparation of data and helper functions.\n#-------------------------------------------------------------------------------\n\nimport math\nimport os\nimport requests\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, display\n\n\n_SAMPLE_DIR = \"_sample_data\"\n\nSAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\nSAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n\nSAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\nSAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n\nSAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\nSAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n\nSAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\nSAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n\nos.makedirs(_SAMPLE_DIR, exist_ok=True)\n\ndef _fetch_data():\n  uri = [\n    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n  ]\n  for url, path in uri:\n    with open(path, 'wb') as file_:\n      file_.write(requests.get(url).content)\n\n_fetch_data()\n\ndef _get_sample(path, resample=None):\n  effects = [\n    [\"remix\", \"1\"]\n  ]\n  if resample:\n    effects.extend([\n      [\"lowpass\", f\"{resample // 2}\"],\n      [\"rate\", f'{resample}'],\n    ])\n  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n\ndef get_sample(*, resample=None):\n  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n\ndef get_speech_sample(*, resample=None):\n  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].plot(time_axis, waveform[c], linewidth=1)\n    axes[c].grid(True)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n    if ylim:\n      axes[c].set_ylim(ylim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef print_stats(waveform, sample_rate=None, src=None):\n  if src:\n    print(\"-\" * 10)\n    print(\"Source:\", src)\n    print(\"-\" * 10)\n  if sample_rate:\n    print(\"Sample Rate:\", sample_rate)\n  print(\"Shape:\", tuple(waveform.shape))\n  print(\"Dtype:\", waveform.dtype)\n  print(f\" - Max:     {waveform.max().item():6.3f}\")\n  print(f\" - Min:     {waveform.min().item():6.3f}\")\n  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n  print()\n  print(waveform)\n  print()\n\ndef plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].specgram(waveform[c], Fs=sample_rate)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef play_audio(waveform, sample_rate):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  if num_channels == 1:\n    display(Audio(waveform[0], rate=sample_rate))\n  elif num_channels == 2:\n    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n  else:\n    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n\ndef get_rir_sample(*, resample=None, processed=False):\n  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n  if not processed:\n    return rir_raw, sample_rate\n  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n  rir = rir / torch.norm(rir, p=2)\n  rir = torch.flip(rir, [1])\n  return rir, sample_rate\n\ndef get_noise_sample(*, resample=None):\n  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying effects and filtering\n------------------------------\n\n``torchaudio.sox_effects`` allows for directly applying filters similar to\nthose available in ``sox`` to Tensor objects and file object audio sources.\n\nThere are two functions for this:\n\n-  ``torchaudio.sox_effects.apply_effects_tensor`` for applying effects\n   to Tensor.\n-  ``torchaudio.sox_effects.apply_effects_file`` for applying effects to\n   other audio sources.\n\nBoth functions accept effect definitions in the form\n``List[List[str]]``.\nThis is mostly consistent with how ``sox`` command works, but one caveat is\nthat ``sox`` adds some effects automatically, whereas ``torchaudio``\u2019s\nimplementation does not.\n\nFor the list of available effects, please refer to `the sox\ndocumentation <http://sox.sourceforge.net/sox.html>`__.\n\n**Tip** If you need to load and resample your audio data on the fly,\nthen you can use ``torchaudio.sox_effects.apply_effects_file`` with\neffect ``\"rate\"``.\n\n**Note** ``apply_effects_file`` accepts a file-like object or path-like\nobject. Similar to ``torchaudio.load``, when the audio format cannot be\ninferred from either the file extension or header, you can provide\nargument ``format`` to specify the format of the audio source.\n\n**Note** This process is not differentiable.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the data\nwaveform1, sample_rate1 = get_sample(resample=16000)\n\n# Define effects\neffects = [\n  [\"lowpass\", \"-1\", \"300\"], # apply single-pole lowpass filter\n  [\"speed\", \"0.8\"],  # reduce the speed\n                     # This only changes sample rate, so it is necessary to\n                     # add `rate` effect with original sample rate after this.\n  [\"rate\", f\"{sample_rate1}\"],\n  [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n]\n\n# Apply effects\nwaveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(\n    waveform1, sample_rate1, effects)\n\nplot_waveform(waveform1, sample_rate1, title=\"Original\", xlim=(-.1, 3.2))\nplot_waveform(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(-.1, 3.2))\nprint_stats(waveform1, sample_rate=sample_rate1, src=\"Original\")\nprint_stats(waveform2, sample_rate=sample_rate2, src=\"Effects Applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the number of frames and number of channels are different from\nthose of the original after the effects are applied. Let\u2019s listen to the\naudio. Doesn\u2019t it sound more dramatic?\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_specgram(waveform1, sample_rate1, title=\"Original\", xlim=(0, 3.04))\nplay_audio(waveform1, sample_rate1)\nplot_specgram(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(0, 3.04))\nplay_audio(waveform2, sample_rate2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulating room reverberation\n----------------------------\n\n`Convolution\nreverb <https://en.wikipedia.org/wiki/Convolution_reverb>`__ is a\ntechnique that's used to make clean audio sound as though it has been\nproduced in a different environment.\n\nUsing Room Impulse Response (RIR), for instance, we can make clean speech\nsound as though it has been uttered in a conference room.\n\nFor this process, we need RIR data. The following data are from the VOiCES\ndataset, but you can record your own \u2014 just turn on your microphone\nand clap your hands.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_rate = 8000\n\nrir_raw, _ = get_rir_sample(resample=sample_rate)\n\nplot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\", ylim=None)\nplot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\nplay_audio(rir_raw, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to clean up the RIR. We extract the main impulse, normalize\nthe signal power, then flip along the time axis.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\nrir = rir / torch.norm(rir, p=2)\nrir = torch.flip(rir, [1])\n\nprint_stats(rir)\nplot_waveform(rir, sample_rate, title=\"Room Impulse Response\", ylim=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we convolve the speech signal with the RIR filter.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "speech, _ = get_speech_sample(resample=sample_rate)\n\nspeech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\naugmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n\nplot_waveform(speech, sample_rate, title=\"Original\", ylim=None)\nplot_waveform(augmented, sample_rate, title=\"RIR Applied\", ylim=None)\n\nplot_specgram(speech, sample_rate, title=\"Original\")\nplay_audio(speech, sample_rate)\n\nplot_specgram(augmented, sample_rate, title=\"RIR Applied\")\nplay_audio(augmented, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding background noise\n-----------------------\n\nTo add background noise to audio data, you can simply add a noise Tensor to\nthe Tensor representing the audio data. A common method to adjust the\nintensity of noise is changing the Signal-to-Noise Ratio (SNR).\n[`wikipedia <https://en.wikipedia.org/wiki/Signal-to-noise_ratio>`__]\n\n\\begin{align}\\mathrm{SNR} = \\frac{P_\\mathrm{signal}}{P_\\mathrm{noise}}\\end{align}\n\n\\begin{align}{\\mathrm  {SNR_{{dB}}}}=10\\log _{{10}}\\left({\\mathrm  {SNR}}\\right)\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_rate = 8000\nspeech, _ = get_speech_sample(resample=sample_rate)\nnoise, _ = get_noise_sample(resample=sample_rate)\nnoise = noise[:, :speech.shape[1]]\n\nplot_waveform(noise, sample_rate, title=\"Background noise\")\nplot_specgram(noise, sample_rate, title=\"Background noise\")\nplay_audio(noise, sample_rate)\n\nspeech_power = speech.norm(p=2)\nnoise_power = noise.norm(p=2)\n\nfor snr_db in [20, 10, 3]:\n  snr = math.exp(snr_db / 10)\n  scale = snr * noise_power / speech_power\n  noisy_speech = (scale * speech + noise) / 2\n\n  plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n  plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n  play_audio(noisy_speech, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying codec to Tensor object\n-------------------------------\n\n``torchaudio.functional.apply_codec`` can apply codecs to a Tensor object.\n\n**Note** This process is not differentiable.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample(resample=8000)\n\nplot_specgram(waveform, sample_rate, title=\"Original\")\nplay_audio(waveform, sample_rate)\n\nconfigs = [\n    ({\"format\": \"wav\", \"encoding\": 'ULAW', \"bits_per_sample\": 8}, \"8 bit mu-law\"),\n    ({\"format\": \"gsm\"}, \"GSM-FR\"),\n    ({\"format\": \"mp3\", \"compression\": -9}, \"MP3\"),\n    ({\"format\": \"vorbis\", \"compression\": -1}, \"Vorbis\"),\n]\nfor param, title in configs:\n  augmented = F.apply_codec(waveform, sample_rate, **param)\n  plot_specgram(augmented, sample_rate, title=title)\n  play_audio(augmented, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulating a phone recoding\n---------------------------\n\nCombining the previous techniques, we can simulate audio that sounds\nlike a person talking over a phone in a echoey room with people talking\nin the background.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\nspeech, _ = get_speech_sample(resample=sample_rate)\n\nplot_specgram(speech, sample_rate, title=\"Original\")\nplay_audio(speech, sample_rate)\n\n# Apply RIR\nrir, _ = get_rir_sample(resample=sample_rate, processed=True)\nspeech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\nspeech = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n\nplot_specgram(speech, sample_rate, title=\"RIR Applied\")\nplay_audio(speech, sample_rate)\n\n# Add background noise\n# Because the noise is recorded in the actual environment, we consider that\n# the noise contains the acoustic feature of the environment. Therefore, we add\n# the noise after RIR application.\nnoise, _ = get_noise_sample(resample=sample_rate)\nnoise = noise[:, :speech.shape[1]]\n\nsnr_db = 8\nscale = math.exp(snr_db / 10) * noise.norm(p=2) / speech.norm(p=2)\nspeech = (scale * speech + noise) / 2\n\nplot_specgram(speech, sample_rate, title=\"BG noise added\")\nplay_audio(speech, sample_rate)\n\n# Apply filtering and change sample rate\nspeech, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n  speech,\n  sample_rate,\n  effects=[\n      [\"lowpass\", \"4000\"],\n      [\"compand\", \"0.02,0.05\", \"-60,-60,-30,-10,-20,-8,-5,-8,-2,-8\", \"-8\", \"-7\", \"0.05\"],\n      [\"rate\", \"8000\"],\n  ],\n)\n\nplot_specgram(speech, sample_rate, title=\"Filtered\")\nplay_audio(speech, sample_rate)\n\n# Apply telephony codec\nspeech = F.apply_codec(speech, sample_rate, format=\"gsm\")\n\nplot_specgram(speech, sample_rate, title=\"GSM Codec Applied\")\nplay_audio(speech, sample_rate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}