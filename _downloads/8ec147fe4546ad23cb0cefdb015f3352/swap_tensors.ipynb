{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension points in `nn.Module` for `load_state_dict` and tensor subclasses\n",
    "===========================================================================\n",
    "\n",
    "**Author:** [Mikayla Gawarecki](https://github.com/mikaylagawarecki)\n",
    "\n",
    "This recipe introduces a new utility function `torch.utils.swap_tensors`\n",
    "as well as two new extension points where it has been integrated in\n",
    "`nn.Module`:\n",
    "\n",
    "-   `nn.Module.to()` and related methods\n",
    "-   `nn.Module.load_state_dict()`\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>This recipe requires PyTorch 2.3.0 or later.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.utils.swap_tensors`\n",
    "==========================\n",
    "\n",
    "`torch.utils.swap_tensors` (hereafter referred to as `swap_tensors`) is\n",
    "a utility function that takes in two Python tensors and swaps them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "t1 = torch.arange(2)\n",
    "t2 = torch.arange(3)\n",
    "print(f\"Before swapping, t1: {t1}, t2: {t2}\")\n",
    "torch.utils.swap_tensors(t1, t2)\n",
    "print(f\"After swapping, t1: {t1}, t2: {t2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, `swap_tensors` swaps the Python `__class__`,\n",
    "`__dict__` and `__slots__` of the two tensors, as well as their\n",
    "associated `at::Tensor`.\n",
    "\n",
    "Application to `nn.Module`\n",
    "==========================\n",
    "\n",
    "This utility is pertinent to `nn.Module` when a Python object outside of\n",
    "the module holds a reference to parameters of the module. If an\n",
    "`nn.Module` modifies any of its parameters out of place, the object\n",
    "holding references to the parameters will not see the change. A classic\n",
    "example of this is the optimizer, which holds a reference to the\n",
    "parameters of the `nn.Module`. This leads to a silent correctness issue\n",
    "where the `optimizer.step()` will run without error but the weights of\n",
    "the `nn.Module` will not be updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = torch.nn.Linear(1, 2, bias=False)\n",
    "optimizer = torch.optim.SGD(mod.parameters())\n",
    "print(f\"weight in mod: {mod.weight}\")\n",
    "print(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")\n",
    "mod.weight = torch.nn.Parameter(2 * mod.weight)\n",
    "print(f\"weight in mod: {mod.weight}\")\n",
    "print(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module.to()` and related methods\n",
    "====================================\n",
    "\n",
    "This includes methods that change the device of the module (such as\n",
    "`nn.Module.cpu()`), methods that change the `dtype` of the module (such\n",
    "as `nn.Module.float()`) as well as methods that allow the module to be\n",
    "materialized (such as `nn.Module.to_empty()`).\n",
    "\n",
    "At first glance, it might be non-intuitive that these methods are able\n",
    "to modify the parameters of the module in-place. The existing approach\n",
    "has been to use a nasty hack dating back from the first days of PyTorch.\n",
    "\n",
    "Notably, the existing approach does not work in these cases:\n",
    "\n",
    "-   when using `__torch_dispatch__` subclasses\n",
    "-   when `param` and `new_param` do not have the same Python `type()`\n",
    "-   For tensors with special C++ representations (such as sparse tensors\n",
    "    and `XLA` tensors)\n",
    "\n",
    "In the following part of this recipe, we will define a toy\n",
    "`__torch_dispatch__` subclass `MyQuantizedLinearWeight` that represents\n",
    "quantized linear weights. This subclass will be used for illustration\n",
    "purposes throughout the rest of the tutorial. For brevity, we omit most\n",
    "of the `__torch_dispatch__` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aten = torch.ops.aten\n",
    "\n",
    "class MyQuantizedLinearWeight(torch.Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, elem, scale):\n",
    "        return torch.Tensor._make_wrapper_subclass(\n",
    "            cls,\n",
    "            elem.shape,\n",
    "            dtype=elem.dtype,\n",
    "            layout=elem.layout,\n",
    "            device=elem.device,\n",
    "            strides=elem.stride(),\n",
    "            storage_offset=elem.storage_offset())\n",
    "\n",
    "    def __init__(self, elem: torch.Tensor, scale: float):\n",
    "        self.elem = elem\n",
    "        self.scale = scale\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyQuantizedLinearWeight({self.elem}, scale={self.scale})\"\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args, kwargs):\n",
    "        if func in (aten.detach.default, aten._to_copy.default):\n",
    "            new_elem = func(args[0].elem, *args[1:], **kwargs)\n",
    "            return cls(new_elem, args[0].scale)\n",
    "        # Implementations for certain ops would be added to ``OP_TABLE``.\n",
    "        # We omit this for brevity.\n",
    "        OP_TABLE = dict()\n",
    "        if func in OP_TABLE:\n",
    "          return OP_TABLE[func](func, args, kwargs)\n",
    "        raise NotImplementedError(f\"Unsupported function {func}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create an `nn.Linear` layer of `dtype` `torch.float32` where the\n",
    "weight is a `MyQuantizedLinearWeight` and try to convert it to\n",
    "`torch.bfloat16`. Observe that the weight\\'s `dtype` changes as\n",
    "expected. However, the `dtype` of the subclass\\' payload (`elem`) does\n",
    "not change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = nn.Linear(3, 5, dtype=torch.float32)\n",
    "m.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\n",
    "print(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\n",
    "m.bfloat16()\n",
    "print(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\n",
    "print(f\"m.weight.dtype: {m.weight.dtype}\")\n",
    "print(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\n",
    "print(f\"m.bias.dtype: {m.bias.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we introduce a global config\n",
    "`torch.__future__.set_swap_module_params_on_conversion` that will use\n",
    "`swap_tensors` to swap the parameters of the module while preserving\n",
    "references in place of `.data` setting. When this config is set,\n",
    "`swap_tensors` will be used during the conversion, which ensures that\n",
    "the `dtype` of the payload is properly converted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.__future__.set_swap_module_params_on_conversion(True)\n",
    "m = nn.Linear(3, 5, dtype=torch.float32)\n",
    "m.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\n",
    "print(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\n",
    "m.bfloat16()\n",
    "print(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\n",
    "print(f\"m.weight.dtype: {m.weight.dtype}\")\n",
    "print(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\n",
    "print(f\"m.bias.dtype: {m.bias.dtype}\")\n",
    "torch.__future__.set_swap_module_params_on_conversion(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module.load_state_dict()`\n",
    "=============================\n",
    "\n",
    "Depending on the value of the `assign` keyword argument passed to\n",
    "`load_state_dict()`, there are two ways to load the `state_dict`:\n",
    "\n",
    "-   `assign=False`: preserves the properties of `module.param` and only\n",
    "    takes the values from `state_dict['param_name']`\n",
    "-   `assign=True`: preserves the properties and values of\n",
    "    `state_dict['param_name']`.\n",
    "\n",
    "Previously, these were implemented with in-place `copy_` and\n",
    "`__setattr__` respectively. With the existing implementation, each\n",
    "approach had its own limitations \\-- `assign=False` imposes the\n",
    "constraint that the type of the parameter in the `state_dict` must be\n",
    "the same as the type of the parameter in the module while `assign=True`\n",
    "imposes the constraint that anything that holds references to the\n",
    "module\\'s parameters must be initialized after\n",
    "`nn.Module.load_state_dict()`.\n",
    "\n",
    "Now, we address both constraints by adding a `swap_tensors` path to\n",
    "`load_state_dict()` and introducing a new extension point\n",
    "`torch.Tensor.module_load(self, other, assign=False)`. When the\n",
    "`swap_tensors` path is enabled via the `__future__` mentioned above, we\n",
    "can use a `__torch_function__` handler for `module_load` to apply a\n",
    "custom transformation to the value in the `state_dict`. The result of\n",
    "this transformation will be swapped with the parameter in the module.\n",
    "\n",
    "In the following example, we will use the `MyQuantizedLinearWeight`\n",
    "subclass defined above to illustrate how we can use these features to\n",
    "apply a custom quantization scheme to the weights of a linear layer when\n",
    "loading the `state_dict`.\n",
    "\n",
    "Recall that the `__torch_function__` handler for `module_load` will be\n",
    "invoked if either `self` or `other` (in this case `param` or\n",
    "`state_dict[param_key]`) are `MyQuantizedLinearWeight` subclasses.\n",
    "\n",
    "Assume that we expect the `state_dict` to contain plain tensors and the\n",
    "module to contain `MyQuantizedLinearWeight` parameters where we want the\n",
    "tensors in the `state_dict` to be transformed into the subclass. Then we\n",
    "can define a `__torch_function__` handler for `torch.Tensor.module_load`\n",
    "as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@classmethod\n",
    "def custom_torch_function(cls, func, types, args=(), kwargs=None):\n",
    "    kwargs = {} if kwargs is None else kwargs\n",
    "\n",
    "    if func is torch.Tensor.module_load:\n",
    "        dest, src = args[0], args[1]\n",
    "        assert type(dest) == cls and type(src) == torch.Tensor\n",
    "        return MyQuantizedLinearWeight(src, dest.scale)\n",
    "    else:\n",
    "        with torch._C.DisableTorchFunctionSubclass():\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "MyQuantizedLinearWeight.__torch_function__ = custom_torch_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us create a skeleton of a model on the meta device to avoid\n",
    "materializing storages. We convert all weights in the modules to\n",
    "`MyQuantizedLinearWeight` subclasses while leaving biases intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fn(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        requires_grad = m.weight.requires_grad\n",
    "        m.weight = torch.nn.Parameter(\n",
    "                    MyQuantizedLinearWeight(m.weight, 0.5), requires_grad=requires_grad\n",
    "                   )\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    m = nn.Linear(3, 5)\n",
    "    m.apply(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the `state_dict`. Observe that we use `assign=True`\n",
    "because for biases, we want to preserve the properties of the tensor in\n",
    "the `state_dict` (for example, we do not want the bias to be on the\n",
    "`meta` device after loading).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.__future__.set_swap_module_params_on_conversion(True)\n",
    "print(f\"Before: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\n",
    "print(f\"m.state_dict() before load_state_dict():\\n {m.state_dict()}\")\n",
    "state_dict = nn.Linear(3, 5).state_dict()\n",
    "print(f\"state_dict:\\n {state_dict}\")\n",
    "m.load_state_dict(state_dict, assign=True)\n",
    "print(f\"After: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\n",
    "print(f\"m.state_dict() after load_state_dict():\\n {m.state_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a toy example of how we can use the new extension point in\n",
    "`nn.Module.load_state_dict()`. One can also imagine alternate scenarios\n",
    "such as when we have tensor subclasses in the `state_dict` and plain\n",
    "`nn.Parameters`/ tensors in the module or when both are tensor\n",
    "subclasses. Based on the use case, we can define the\n",
    "`__torch_function__` handler for `module_load` to apply the transforms\n",
    "as needed.\n",
    "\n",
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this recipe, we learned about `swap_tensors`, the importance of\n",
    "preserving references for parameters in `nn.Module` as well as how to\n",
    "use the two new extension points that are gated by\n",
    "`torch.__future__.set_swap_module_params_on_conversion`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
