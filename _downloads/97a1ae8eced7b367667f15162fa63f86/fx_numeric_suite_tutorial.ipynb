{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# PyTorch FX Numeric Suite Core APIs Tutorial\n\n## Introduction\n\nQuantization is good when it works, but it is difficult to know what is wrong\nwhen it does not satisfy the accuracy we expect. Debugging the accuracy issue\nof quantization is not easy and time-consuming.\n\nOne important step of debugging is to measure the statistics of the float model\nand its corresponding quantized model to know where they differ most.\nWe built a suite of numeric tools called PyTorch FX Numeric Suite Core APIs in\nPyTorch quantization to enable the measurement of the statistics between\nquantized module and float module to support quantization debugging efforts.\nEven for the quantized model with good accuracy, PyTorch FX Numeric Suite Core\nAPIs can still be used as the profiling tool to better understand the\nquantization error within the model and provide the guidance for further\noptimization.\n\nPyTorch FX Numeric Suite Core APIs currently supports models quantized through\nboth static quantization and dynamic quantization with unified APIs.\n\nIn this tutorial we will use MobileNetV2 as an example to show how to use\nPyTorch FX Numeric Suite Core APIs to measure the statistics between static\nquantized model and float model.\n\n### Setup\nWe\u2019ll start by doing the necessary imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Imports and util functions\n\nimport copy\nimport torch\nimport torchvision\nimport torch.quantization\nimport torch.ao.ns._numeric_suite_fx as ns\nimport torch.quantization.quantize_fx as quantize_fx\n\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\n\ntorch.manual_seed(0)\nplt.style.use('seaborn-whitegrid')\n\n\n# a simple line graph\ndef plot(xdata, ydata, xlabel, ylabel, title):\n    _ = plt.figure(figsize=(10, 5), dpi=100)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    ax = plt.axes()\n    ax.plot(xdata, ydata)\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load the pretrained float MobileNetV2 model, and quantize it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create float model\nmobilenetv2_float = torchvision.models.quantization.mobilenet_v2(\n    pretrained=True, quantize=False).eval()\n\n# create quantized model\nqconfig_dict = {\n    '': torch.quantization.get_default_qconfig('x86'), # The old 'fbgemm' is still available but 'x86' is the recommended default.\n    # adjust the qconfig to make the results more interesting to explore\n    'module_name': [\n        # turn off quantization for the first couple of layers\n        ('features.0', None),\n        ('features.1', None),\n        # use MinMaxObserver for `features.17`, this should lead to worse\n        # weight SQNR\n        ('features.17', torch.quantization.default_qconfig),\n    ]\n}\n# Note: quantization APIs are inplace, so we save a copy of the float model for\n# later comparison to the quantized model. This is done throughout the\n# tutorial.\ndatum = torch.randn(1, 3, 224, 224)\nmobilenetv2_prepared = quantize_fx.prepare_fx(\n    copy.deepcopy(mobilenetv2_float), qconfig_dict, (datum,))\nmobilenetv2_prepared(datum)\n# Note: there is a long standing issue that we cannot copy.deepcopy a\n# quantized model. Since quantization APIs are inplace and we need to use\n# different copies of the quantized model throughout this tutorial, we call\n# `convert_fx` on a copy, so we have access to the original `prepared_model`\n# later. This is done throughout the tutorial.\nmobilenetv2_quantized = quantize_fx.convert_fx(\n    copy.deepcopy(mobilenetv2_prepared))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Compare the weights of float and quantized models\nThe first analysis we can do is comparing the weights of the fp32 model and\nthe int8 model by calculating the SQNR between each pair of weights.\n\nThe `extract_weights` API can be used to extract weights from linear,\nconvolution and LSTM layers. It works for dynamic quantization as well as\nPTQ/QAT.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Note: when comparing weights in models with Conv-BN for PTQ, we need to\n# compare weights after Conv-BN fusion for a proper comparison.  Because of\n# this, we use `prepared_model` instead of `float_model` when comparing\n# weights.\n\n# Extract conv and linear weights from corresponding parts of two models, and\n# save them in `wt_compare_dict`.\nmobilenetv2_wt_compare_dict = ns.extract_weights(\n    'fp32',  # string name for model A\n    mobilenetv2_prepared,  # model A\n    'int8',  # string name for model B\n    mobilenetv2_quantized,  # model B\n)\n\n# calculate SQNR between each pair of weights\nns.extend_logger_results_with_comparison(\n    mobilenetv2_wt_compare_dict,  # results object to modify inplace\n    'fp32',  # string name of model A (from previous step)\n    'int8',  # string name of model B (from previous step)\n    torch.ao.ns.fx.utils.compute_sqnr,  # tensor comparison function\n    'sqnr',  # the name to use to store the results under\n)\n\n# massage the data into a format easy to graph and print\nmobilenetv2_wt_to_print = []\nfor idx, (layer_name, v) in enumerate(mobilenetv2_wt_compare_dict.items()):\n    mobilenetv2_wt_to_print.append([\n        idx,\n        layer_name,\n        v['weight']['int8'][0]['prev_node_target_type'],\n        v['weight']['int8'][0]['values'][0].shape,\n        v['weight']['int8'][0]['sqnr'][0],\n    ])\n\n# plot the SQNR between fp32 and int8 weights for each layer\nplot(\n    [x[0] for x in mobilenetv2_wt_to_print],\n    [x[4] for x in mobilenetv2_wt_to_print],\n    'idx',\n    'sqnr',\n    'weights, idx to sqnr'\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also print out the SQNR, so we can inspect the layer name and type:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tabulate(\n    mobilenetv2_wt_to_print,\n    headers=['idx', 'layer_name', 'type', 'shape', 'sqnr']\n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Compare activations API\nThe second tool allows for comparison of activations between float and\nquantized models at corresponding locations for the same input.\n\n.. figure:: /_static/img/compare_output.png\n\nThe `add_loggers`/`extract_logger_info` API can be used to to extract\nactivations from any layer with a `torch.Tensor` return type. It works for\ndynamic quantization as well as PTQ/QAT.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compare unshadowed activations\n\n# Create a new copy of the quantized model, because we cannot `copy.deepcopy`\n# a quantized model.\nmobilenetv2_quantized = quantize_fx.convert_fx(\n    copy.deepcopy(mobilenetv2_prepared))\nmobilenetv2_float_ns, mobilenetv2_quantized_ns = ns.add_loggers(\n    'fp32',  # string name for model A\n    copy.deepcopy(mobilenetv2_prepared),  # model A\n    'int8',  # string name for model B\n    mobilenetv2_quantized,  # model B\n    ns.OutputLogger,  # logger class to use\n)\n\n# feed data through network to capture intermediate activations\nmobilenetv2_float_ns(datum)\nmobilenetv2_quantized_ns(datum)\n\n# extract intermediate activations\nmobilenetv2_act_compare_dict = ns.extract_logger_info(\n    mobilenetv2_float_ns,  # model A, with loggers (from previous step)\n    mobilenetv2_quantized_ns,  # model B, with loggers (from previous step)\n    ns.OutputLogger,  # logger class to extract data from\n    'int8',  # string name of model to use for layer names for the output\n)\n\n# add SQNR comparison\nns.extend_logger_results_with_comparison(\n    mobilenetv2_act_compare_dict,  # results object to modify inplace\n    'fp32',  # string name of model A (from previous step)\n    'int8',  # string name of model B (from previous step)\n    torch.ao.ns.fx.utils.compute_sqnr,  # tensor comparison function\n    'sqnr',  # the name to use to store the results under\n)\n\n# massage the data into a format easy to graph and print\nmobilenet_v2_act_to_print = []\nfor idx, (layer_name, v) in enumerate(mobilenetv2_act_compare_dict.items()):\n    mobilenet_v2_act_to_print.append([\n        idx,\n        layer_name,\n        v['node_output']['int8'][0]['prev_node_target_type'],\n        v['node_output']['int8'][0]['values'][0].shape,\n        v['node_output']['int8'][0]['sqnr'][0]])\n\n# plot the SQNR between fp32 and int8 activations for each layer\nplot(\n    [x[0] for x in mobilenet_v2_act_to_print],\n    [x[4] for x in mobilenet_v2_act_to_print],\n    'idx',\n    'sqnr',\n    'unshadowed activations, idx to sqnr',\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also print out the SQNR, so we can inspect the layer name and type:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tabulate(\n    mobilenet_v2_act_to_print,\n    headers=['idx', 'layer_name', 'type', 'shape', 'sqnr']\n))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}