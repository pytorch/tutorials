{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAutomatic Mixed Precision\n*************************\n**Author**: `Michael Carilli <https://github.com/mcarilli>`_\n\n`torch.cuda.amp <https://pytorch.org/docs/stable/amp.html>`_ provides convenience methods for mixed precision,\nwhere some operations use the ``torch.float32`` (``float``) datatype and other operations\nuse ``torch.float16`` (``half``). Some ops, like linear layers and convolutions,\nare much faster in ``float16``. Other ops, like reductions, often require the dynamic\nrange of ``float32``.  Mixed precision tries to match each op to its appropriate datatype,\nwhich can reduce your network's runtime and memory footprint.\n\nOrdinarily, \"automatic mixed precision training\" uses `torch.cuda.amp.autocast <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast>`_ and\n`torch.cuda.amp.GradScaler <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_ together.\n\nThis recipe measures the performance of a simple network in default precision,\nthen walks through adding ``autocast`` and ``GradScaler`` to run the same network in\nmixed precision with improved performance.\n\nYou may download and run this recipe as a standalone Python script.\nThe only requirements are Pytorch 1.6+ and a CUDA-capable GPU.\n\nMixed precision primarily benefits Tensor Core-enabled architectures (Volta, Turing, Ampere).\nThis recipe should show significant (2-3X) speedup on those architectures.\nOn earlier architectures (Kepler, Maxwell, Pascal), you may observe a modest speedup.\nRun ``nvidia-smi`` to display your GPU's architecture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple network\n----------------\nThe following sequence of linear layers and ReLUs should show a speedup with mixed precision.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append(torch.nn.Linear(in_size, in_size))\n        layers.append(torch.nn.ReLU())\n    layers.append(torch.nn.Linear(in_size, out_size))\n    return torch.nn.Sequential(*tuple(layers)).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``batch_size``, ``in_size``, ``out_size``, and ``num_layers`` are chosen to be large enough to saturate the GPU with work.\nTypically, mixed precision provides the greatest speedup when the GPU is saturated.\nSmall networks may be CPU bound, in which case mixed precision won't improve performance.\nSizes are also chosen such that linear layers' participating dimensions are multiples of 8,\nto permit Tensor Core usage on Tensor Core-capable GPUs (see `Troubleshooting<troubleshooting>` below).\n\nExercise: Vary participating sizes and see how the mixed precision speedup changes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' dtype when enabling mixed precision.\ndata = [torch.randn(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\ntargets = [torch.randn(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n\nloss_fn = torch.nn.MSELoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Default Precision\n-----------------\nWithout ``torch.cuda.amp``, the following simple network executes all ops in default precision (``torch.float32``):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Default precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding autocast\n---------------\nInstances of `torch.cuda.amp.autocast <https://pytorch.org/docs/stable/amp.html#autocasting>`_\nserve as context managers that allow regions of your script to run in mixed precision.\n\nIn these regions, CUDA ops run in a dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee the `Autocast Op Reference <https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_\nfor details on what precision autocast chooses for each op, and under what circumstances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with torch.cuda.amp.autocast():\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding GradScaler\n-----------------\n`Gradient scaling <https://pytorch.org/docs/stable/amp.html#gradient-scaling>`_\nhelps prevent gradients with small magnitudes from flushing to zero\n(\"underflowing\") when training with mixed precision.\n\n`torch.cuda.amp.GradScaler <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_\nperforms the steps of gradient scaling conveniently.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Constructs scaler once, at the beginning of the convergence run, using default args.\n# If your network fails to converge with default GradScaler args, please file an issue.\n# The same GradScaler instance should be used for the entire convergence run.\n# If you perform multiple convergence runs in the same script, each run should use\n# a dedicated fresh GradScaler instance.  GradScaler instances are lightweight.\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with torch.cuda.amp.autocast():\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(opt)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n        opt.zero_grad() # set_to_none=True here can modestly improve performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All together: \"Automatic Mixed Precision\"\n------------------------------------------\n(The following also demonstrates ``enabled``, an optional convenience argument to ``autocast`` and ``GradScaler``.\nIf False, ``autocast`` and ``GradScaler``\\ 's calls become no-ops.\nThis allows switching between default precision and mixed precision without if/else statements.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspecting/modifying gradients (e.g., clipping)\n--------------------------------------------------------\nAll gradients produced by ``scaler.scale(loss).backward()`` are scaled.  If you wish to modify or inspect\nthe parameters' ``.grad`` attributes between ``backward()`` and ``scaler.step(optimizer)``, you should\nunscale them first using `scaler.unscale_(optimizer) <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with torch.cuda.amp.autocast():\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving/Resuming\n----------------\nTo save/resume Amp-enabled runs with bitwise accuracy, use\n`scaler.state_dict <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict>`_ and\n`scaler.load_state_dict <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict>`_.\n\nWhen saving, save the scaler state dict alongside the usual model and optimizer state dicts.\nDo this either at the beginning of an iteration before any forward passes, or at the end of\nan iteration after ``scaler.update()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# Write checkpoint as desired, e.g.,\n# torch.save(checkpoint, \"filename\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When resuming, load the scaler state dict alongside the model and optimizer state dicts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read checkpoint as desired, e.g.,\n# dev = torch.cuda.current_device()\n# checkpoint = torch.load(\"filename\",\n#                         map_location = lambda storage, loc: storage.cuda(dev))\nnet.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If a checkpoint was created from a run *without* Amp, and you want to resume training *with* Amp,\nload model and optimizer states from the checkpoint as usual.  The checkpoint won't contain a saved scaler state, so\nuse a fresh instance of ``GradScaler``.\n\nIf a checkpoint was created from a run *with* Amp and you want to resume training *without* Amp,\nload model and optimizer states from the checkpoint as usual, and ignore the saved scaler state.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference/Evaluation\n--------------------\n``autocast`` may be used by itself to wrap inference or evaluation forward passes. ``GradScaler`` is not necessary.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAdvanced topics\n---------------\nSee the `Automatic Mixed Precision Examples <https://pytorch.org/docs/stable/notes/amp_examples.html>`_ for advanced use cases including:\n\n* Gradient accumulation\n* Gradient penalty/double backward\n* Networks with multiple models, optimizers, or losses\n* Multiple GPUs (``torch.nn.DataParallel`` or ``torch.nn.parallel.DistributedDataParallel``)\n* Custom autograd functions (subclasses of ``torch.autograd.Function``)\n\nIf you perform multiple convergence runs in the same script, each run should use\na dedicated fresh GradScaler instance.  GradScaler instances are lightweight.\n\nIf you're registering a custom C++ op with the dispatcher, see the\n`autocast section <https://pytorch.org/tutorials/advanced/dispatcher.html#autocast>`_\nof the dispatcher tutorial.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTroubleshooting\n---------------\nSpeedup with Amp is minor\n~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Your network may fail to saturate the GPU(s) with work, and is therefore CPU bound. Amp's effect on GPU performance\n   won't matter.\n\n   * A rough rule of thumb to saturate the GPU is to increase batch and/or network size(s)\n     as much as you can without running OOM.\n   * Try to avoid excessive CPU-GPU synchronization (``.item()`` calls, or printing values from CUDA tensors).\n   * Try to avoid sequences of many small CUDA ops (coalesce these into a few large CUDA ops if you can).\n2. Your network may be GPU compute bound (lots of matmuls/convolutions) but your GPU does not have Tensor Cores.\n   In this case a reduced speedup is expected.\n3. Matmul dimensions are not Tensor Core-friendly.  Make sure matmuls' participating sizes are multiples of 8.\n   (For NLP models with encoders/decoders, this can be subtle.  Also, convolutions used to have similar size constraints\n   for Tensor Core use, but for CuDNN versions 7.3 and later, no such constraints exist.  See\n   `here <https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841>`_ for guidance.)\n\nLoss is inf/NaN\n~~~~~~~~~~~~~~~\nFirst, check if your network fits an `advanced use case<advanced-topics>`.\nSee also `Prefer binary_cross_entropy_with_logits over binary_cross_entropy <https://pytorch.org/docs/stable/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy>`_.\n\nIf you're confident your Amp usage is correct, you may need to file an issue, but before doing so, it's helpful to gather the following information:\n\n1. Disable ``autocast`` or ``GradScaler`` individually (by passing ``enabled=False`` to their constructor) and see if infs/NaNs persist.\n2. If you suspect part of your network (e.g., a complicated loss function) overflows , run that forward region in ``float32``\n   and see if infs/NaNs persist.\n   `The autocast docstring <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast>`_'s last code snippet\n   shows forcing a subregion to run in ``float32`` (by locally disabling autocast and casting the subregion's inputs).\n\nType mismatch error (may manifest as CUDNN_STATUS_BAD_PARAM)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nAutocast tries to cover all ops that benefit from or require casting.\n`Ops that receive explicit coverage <https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_\nare chosen based on numerical properties, but also on experience.\nIf you see a type mismatch error in an autocast-enabled forward region or a backward pass following that region,\nit's possible autocast missed an op.\n\nPlease file an issue with the error backtrace.  ``export TORCH_SHOW_CPP_STACKTRACES=1`` before running your script to provide\nfine-grained information on which backend op is failing.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}