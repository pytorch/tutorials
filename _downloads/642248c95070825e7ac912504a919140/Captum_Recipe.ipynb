{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model Interpretability using Captum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum helps you understand how the data features impact your model\npredictions or neuron activations, shedding light on how your model\noperates.\n\nUsing Captum, you can apply a wide range of state-of-the-art feature\nattribution algorithms such as \\ ``Guided GradCam``\\  and\n\\ ``Integrated Gradients``\\  in a unified way.\n\nIn this recipe you will learn how to use Captum to: \n\n- Attribute the predictions of an image classifier to their corresponding image features. \n- Visualize the attribution results.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure Captum is installed in your active Python environment. Captum\nis available both on GitHub, as a ``pip`` package, or as a ``conda``\npackage. For detailed instructions, consult the installation guide at\nhttps://captum.ai/\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a model, we use a built-in image classifier in PyTorch. Captum can\nreveal which parts of a sample image support certain predictions made by\nthe model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = torchvision.models.resnet18(pretrained=True).eval()\n\nresponse = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n])\n\nnormalize = transforms.Compose([\n    transforms.ToTensor(),               # converts the image to a tensor with values between 0 and 1\n    transforms.Normalize(                # normalize to follow 0-centered imagenet pixel RGB distribution\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n    )\n])\ninput_img = normalize(center_crop(img)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Attribution\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Among the top-3 predictions of the models are classes 208 and 283 which\ncorrespond to dog and cat.\n\nLet us attribute each of these predictions to the corresponding part of\nthe input, using Captum\u2019s \\ ``Occlusion``\\  algorithm.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from captum.attr import Occlusion \n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\ntarget=208,                       # Labrador index in ImageNet \nsliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\nbaselines = 0                     # values to occlude the image with. 0 corresponds to gray\n\nattribution_dog = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=baselines)\n\n\ntarget=283,                       # Persian cat index in ImageNet \nattribution_cat = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides ``Occlusion``, Captum features many algorithms such as\n\\ ``Integrated Gradients``\\ , \\ ``Deconvolution``\\ ,\n\\ ``GuidedBackprop``\\ , \\ ``Guided GradCam``\\ , \\ ``DeepLift``\\ , and\n\\ ``GradientShap``\\ . All of these algorithms are subclasses of\n``Attribution`` which expects your model as a callable ``forward_func``\nupon initialization and has an ``attribute(...)`` method which returns\nthe attribution result in a unified format.\n\nLet us visualize the computed attribution results in case of images.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Results\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\u2019s \\ ``visualization``\\  utility provides out-of-the-box methods\nto visualize attribution results both for pictorial and for textual\ninputs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom captum.attr import visualization as viz\n\n# Convert the compute attribution tensor into an image-like numpy array\nattribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n# positive attribution indicates that the presence of the area increases the prediction score\n# negative attribution indicates distractor areas whose absence increases the score\n\n_ = viz.visualize_image_attr_multiple(attribution_dog,\n                                      np.array(center_crop(img)),\n                                      vis_types,\n                                      vis_signs,\n                                      [\"attribution for dog\", \"image\"],\n                                      show_colorbar = True\n                                     )\n\n\nattribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n\n_ = viz.visualize_image_attr_multiple(attribution_cat,\n                                      np.array(center_crop(img)),\n                                      [\"heat_map\", \"original_image\"],  \n                                      [\"all\", \"all\"], # positive/negative attribution or all\n                                      [\"attribution for cat\", \"image\"],\n                                      show_colorbar = True\n                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your data is textual, ``visualization.visualize_text()`` offers a\ndedicated view to explore attribution on top of the input text. Find out\nmore at http://captum.ai/tutorials/IMDB_TorchText_Interpret\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Notes\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum can handle most model types in PyTorch across modalities\nincluding vision, text, and more. With Captum you can: \\* Attribute a\nspecific output to the model input as illustrated above. \\* Attribute a\nspecific output to a hidden-layer neuron (see Captum API reference). \\*\nAttribute a hidden-layer neuron response to the model input (see Captum\nAPI reference).\n\nFor complete API of the supported methods and a list of tutorials,\nconsult our website http://captum.ai\n\nAnother useful post by Gilbert Tanner:\nhttps://gilberttanner.com/blog/interpreting-pytorch-models-with-captum\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}