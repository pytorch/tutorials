{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# (Prototype) MaskedTensor Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial is designed to serve as a starting point for using MaskedTensors\nand discuss its masking semantics.\n\nMaskedTensor serves as an extension to :class:`torch.Tensor` that provides the user with the ability to:\n\n* use any masked semantics (for example, variable length tensors, nan* operators, etc.)\n* differentiation between 0 and NaN gradients\n* various sparse applications (see tutorial below)\n\nFor a more detailed introduction on what MaskedTensors are, please find the\n[torch.masked documentation](https://pytorch.org/docs/master/masked.html)_.\n\n## Using MaskedTensor\n\nIn this section we discuss how to use MaskedTensor including how to construct, access, the data\nand mask, as well as indexing and slicing.\n\n### Preparation\n\nWe'll begin by doing the necessary setup for the tutorial:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.masked import masked_tensor, as_masked_tensor\nimport warnings\n\n# Disable prototype warnings and such\nwarnings.filterwarnings(action='ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Construction\n\nThere are a few different ways to construct a MaskedTensor:\n\n* The first way is to directly invoke the MaskedTensor class\n* The second (and our recommended way) is to use :func:`masked.masked_tensor` and :func:`masked.as_masked_tensor`\n  factory functions, which are analogous to :func:`torch.tensor` and :func:`torch.as_tensor`\n\nThroughout this tutorial, we will be assuming the import line: `from torch.masked import masked_tensor`.\n\n### Accessing the data and mask\n\nThe underlying fields in a MaskedTensor can be accessed through:\n\n* the :meth:`MaskedTensor.get_data` function\n* the :meth:`MaskedTensor.get_mask` function. Recall that ``True`` indicates \"specified\" or \"valid\"\n  while ``False`` indicates \"unspecified\" or \"invalid\".\n\nIn general, the underlying data that is returned may not be valid in the unspecified entries, so we recommend that\nwhen users require a Tensor without any masked entries, that they use :meth:`MaskedTensor.to_tensor` (as shown above) to\nreturn a Tensor with filled values.\n\n### Indexing and slicing\n\n:class:`MaskedTensor` is a Tensor subclass, which means that it inherits the same semantics for indexing and slicing\nas :class:`torch.Tensor`. Below are some examples of common indexing and slicing patterns:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.arange(24).reshape(2, 3, 4)\nmask = data % 2 == 0\n\nprint(\"data:\\n\", data)\nprint(\"mask:\\n\", mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# float is used for cleaner visualization when being printed\nmt = masked_tensor(data.float(), mask)\n\nprint(\"mt[0]:\\n\", mt[0])\nprint(\"mt[:, :, 2:4]:\\n\", mt[:, :, 2:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why is MaskedTensor useful?\n\nBecause of :class:`MaskedTensor`'s treatment of specified and unspecified values as a first-class citizen\ninstead of an afterthought (with filled values, nans, etc.), it is able to solve for several of the shortcomings\nthat regular Tensors are unable to; indeed, :class:`MaskedTensor` was born in a large part due to these recurring issues.\n\nBelow, we will discuss some of the most common issues that are still unresolved in PyTorch today\nand illustrate how :class:`MaskedTensor` can solve these problems.\n\n### Distinguishing between 0 and NaN gradient\n\nOne issue that :class:`torch.Tensor` runs into is the inability to distinguish between gradients that are\nundefined (NaN) vs. gradients that are actually 0. Because PyTorch does not have a way of marking a value\nas specified/valid vs. unspecified/invalid, it is forced to rely on NaN or 0 (depending on the use case), leading\nto unreliable semantics since many operations aren't meant to handle NaN values properly. What is even more confusing\nis that sometimes depending on the order of operations, the gradient could vary (for example, depending on how early\nin the chain of operations a NaN value manifests).\n\n:class:`MaskedTensor` is the perfect solution for this!\n\n#### torch.where\n\nIn [Issue 10729](https://github.com/pytorch/pytorch/issues/10729)_, we notice a case where the order of operations\ncan matter when using :func:`torch.where` because we have trouble differentiating between if the 0 is a real 0\nor one from undefined gradients. Therefore, we remain consistent and mask out the results:\n\nCurrent result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([-10., -5, 0, 5, 10, 50, 60, 70, 80, 90, 100], requires_grad=True, dtype=torch.float)\ny = torch.where(x < 0, torch.exp(x), torch.ones_like(x))\ny.sum().backward()\nx.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`MaskedTensor` result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([-10., -5, 0, 5, 10, 50, 60, 70, 80, 90, 100])\nmask = x < 0\nmx = masked_tensor(x, mask, requires_grad=True)\nmy = masked_tensor(torch.ones_like(x), ~mask, requires_grad=True)\ny = torch.where(mask, torch.exp(mx), my)\ny.sum().backward()\nmx.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient here is only provided to the selected subset. Effectively, this changes the gradient of `where`\nto mask out elements instead of setting them to zero.\n\n#### Another torch.where\n\n[Issue 52248](https://github.com/pytorch/pytorch/issues/52248)_ is another example.\n\nCurrent result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.randn((), requires_grad=True)\nb = torch.tensor(False)\nc = torch.ones(())\nprint(\"torch.where(b, a/0, c):\\n\", torch.where(b, a/0, c))\nprint(\"torch.autograd.grad(torch.where(b, a/0, c), a):\\n\", torch.autograd.grad(torch.where(b, a/0, c), a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`MaskedTensor` result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = masked_tensor(torch.randn(()), torch.tensor(True), requires_grad=True)\nb = torch.tensor(False)\nc = torch.ones(())\nprint(\"torch.where(b, a/0, c):\\n\", torch.where(b, a/0, c))\nprint(\"torch.autograd.grad(torch.where(b, a/0, c), a):\\n\", torch.autograd.grad(torch.where(b, a/0, c), a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This issue is similar (and even links to the next issue below) in that it expresses frustration with\nunexpected behavior because of the inability to differentiate \"no gradient\" vs \"zero gradient\",\nwhich in turn makes working with other ops difficult to reason about.\n\n#### When using mask, x/0 yields NaN grad\n\nIn [Issue 4132](https://github.com/pytorch/pytorch/issues/4132)_, the user proposes that\n`x.grad` should be `[0, 1]` instead of the `[nan, 1]`,\nwhereas :class:`MaskedTensor` makes this very clear by masking out the gradient altogether.\n\nCurrent result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\ny = x/div # => y is [inf, 1]\nmask = (div != 0)  # => mask is [0, 1]\ny[mask].backward()\nx.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`MaskedTensor` result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\ny = x/div # => y is [inf, 1]\nmask = (div != 0) # => mask is [0, 1]\nloss = as_masked_tensor(y, mask)\nloss.sum().backward()\nx.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### :func:`torch.nansum` and :func:`torch.nanmean`\n\nIn [Issue 67180](https://github.com/pytorch/pytorch/issues/67180)_,\nthe gradient isn't calculate properly (a longstanding issue), whereas :class:`MaskedTensor` handles it correctly.\n\nCurrent result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1., 2., float('nan')])\nb = torch.tensor(1.0, requires_grad=True)\nc = a * b\nc1 = torch.nansum(c)\nbgrad1, = torch.autograd.grad(c1, b, retain_graph=True)\nbgrad1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`MaskedTensor` result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1., 2., float('nan')])\nb = torch.tensor(1.0, requires_grad=True)\nmt = masked_tensor(a, ~torch.isnan(a))\nc = mt * b\nc1 = torch.sum(c)\nbgrad1, = torch.autograd.grad(c1, b, retain_graph=True)\nbgrad1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Safe Softmax\n\nSafe softmax is another great example of [an issue](https://github.com/pytorch/pytorch/issues/55056)_\nthat arises frequently. In a nutshell, if there is an entire batch that is \"masked out\"\nor consists entirely of padding (which, in the softmax case, translates to being set `-inf`),\nthen this will result in NaNs, which can lead to training divergence.\n\nLuckily, :class:`MaskedTensor` has solved this issue. Consider this setup:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.randn(3, 3)\nmask = torch.tensor([[True, False, False], [True, False, True], [False, False, False]])\nx = data.masked_fill(~mask, float('-inf'))\nmt = masked_tensor(data, mask)\nprint(\"x:\\n\", x)\nprint(\"mt:\\n\", mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, we want to calculate the softmax along `dim=0`. Note that the second column is \"unsafe\" (i.e. entirely\nmasked out), so when the softmax is calculated, the result will yield `0/0 = nan` since `exp(-inf) = 0`.\nHowever, what we would really like is for the gradients to be masked out since they are unspecified and would be\ninvalid for training.\n\nPyTorch result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x.softmax(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`MaskedTensor` result:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt.softmax(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing missing torch.nan* operators\n\nIn [Issue 61474](https://github.com/pytorch/pytorch/issues/61474)_,\nthere is a request to add additional operators to cover the various `torch.nan*` applications,\nsuch as ``torch.nanmax``, ``torch.nanmin``, etc.\n\nIn general, these problems lend themselves more naturally to masked semantics, so instead of introducing additional\noperators, we propose using :class:`MaskedTensor` instead.\nSince [nanmean has already landed](https://github.com/pytorch/pytorch/issues/21987)_,\nwe can use it as a comparison point:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.arange(16).float()\ny = x * x.fmod(4)\nz = y.masked_fill(y == 0, float('nan'))  # we want to get the mean of y when ignoring the zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"y:\\n\", y)\n# z is just y with the zeros replaced with nan's\nprint(\"z:\\n\", z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"y.mean():\\n\", y.mean())\nprint(\"z.nanmean():\\n\", z.nanmean())\n# MaskedTensor successfully ignores the 0's\nprint(\"torch.mean(masked_tensor(y, y != 0)):\\n\", torch.mean(masked_tensor(y, y != 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above example, we've constructed a `y` and would like to calculate the mean of the series while ignoring\nthe zeros. `torch.nanmean` can be used to do this, but we don't have implementations for the rest of the\n`torch.nan*` operations. :class:`MaskedTensor` solves this issue by being able to use the base operation,\nand we already have support for the other operations listed in the issue. For example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.argmin(masked_tensor(y, y != 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indeed, the index of the minimum argument when ignoring the 0's is the 1 in index 1.\n\n:class:`MaskedTensor` can also support reductions when the data is fully masked out, which is equivalent\nto the case above when the data Tensor is completely ``nan``. ``nanmean`` would return ``nan``\n(an ambiguous return value), while MaskedTensor would more accurately indicate a masked out result.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.empty(16).fill_(float('nan'))\nprint(\"x:\\n\", x)\nprint(\"torch.nanmean(x):\\n\", torch.nanmean(x))\nprint(\"torch.nanmean via maskedtensor:\\n\", torch.mean(masked_tensor(x, ~torch.isnan(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a similar problem to safe softmax where `0/0 = nan` when what we really want is an undefined value.\n\n## Conclusion\n\nIn this tutorial, we've introduced what MaskedTensors are, demonstrated how to use them, and motivated their\nvalue through a series of examples and issues that they've helped resolve.\n\n## Further Reading\n\nTo continue learning more, you can find our\n[MaskedTensor Sparsity tutorial](https://pytorch.org/tutorials/prototype/maskedtensor_sparsity.html)_\nto see how MaskedTensor enables sparsity and the different storage formats we currently support.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}