{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n(experimental) Static Quantization with Eager Mode in PyTorch\n=========================================================\n\n**Author**: `Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_\n\n**Edited by**: `Seth Weidman <https://github.com/SethHWeidman/>`_\n\nThis tutorial shows how to do post-training static quantization, as well as illustrating\ntwo more advanced techniques - per-channel quantization and quantization-aware training -\nto further improve the model's accuracy. Note that quantization is currently only supported\nfor CPUs, so we will not be utilizing GPUs / CUDA in this tutorial.\n\nBy the end of this tutorial, you will see how quantization in PyTorch can result in\nsignificant decreases in model size while increasing speed. Furthermore, you'll see how\nto easily apply some advanced quantization techniques shown\n`here <https://arxiv.org/abs/1806.08342>`_ so that your quantized models take much less\nof an accuracy hit than they would otherwise.\n\nWarning: we use a lot of boilerplate code from other PyTorch repos to, for example,\ndefine the ``MobileNetV2`` model archtecture, define data loaders, and so on. We of course\nencourage you to read it; but if you want to get to the quantization features, feel free\nto skip to the \"4. Post-training static quantization\" section.\n\nWe'll start by doing the necessary imports:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport os\nimport time\nimport sys\nimport torch.quantization\n\n# # Setup warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Model architecture\n---------------------\n\nWe first define the MobileNetV2 model architecture, with several notable modifications\nto enable quantization:\n\n- Replacing addition with ``nn.quantized.FloatFunctional``\n- Insert ``QuantStub`` and ``DeQuantStub`` at the beginning and end of the network.\n- Replace ReLU6 with ReLU\n\nNote: this code is taken from\n`here <https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes, momentum=0.1),\n            # Replace with ReLU\n            nn.ReLU(inplace=False)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup, momentum=0.1),\n        ])\n        self.conv = nn.Sequential(*layers)\n        # Replace torch.add with floatfunctional\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return self.skip_add.add(x, self.conv(x))\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        \"\"\"\n        MobileNet V2 main class\n\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n\n        x = self.quant(x)\n\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        x = self.dequant(x)\n        return x\n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n    # This operation does not change the numerics\n    def fuse_model(self):\n        for m in self.modules():\n            if type(m) == ConvBNReLU:\n                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual:\n                for idx in range(len(m.conv)):\n                    if type(m.conv[idx]) == nn.Conv2d:\n                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Helper functions\n-------------------\n\nWe next define several helper functions to help with model evaluation. These mostly come from\n`here <https://github.com/pytorch/examples/blob/master/imagenet/main.py>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt >= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = MobileNetV2()\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Define dataset and data loaders\n----------------------------------\n\nAs our last major setup step, we define our dataloaders for our training and testing set.\n\nImageNet Data\n^^^^^^^^^^^^^\n\nThe specific dataset we've created for this tutorial contains just 1000 images from the ImageNet data, one from\neach class (this dataset, at just over 250 MB, is small enough that it can be downloaded\nrelatively easily). The URL for this custom dataset is:\n\n.. code::\n\n    https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip\n\nTo download this data locally using Python, you could use:\n\n.. code:: python\n\n    import requests\n\n    url = 'https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip`\n    filename = '~/Downloads/imagenet_1k_data.zip'\n\n    r = requests.get(url)\n\n    with open(filename, 'wb') as f:\n        f.write(r.content)\n\nFor this tutorial to run, we download this data and move it to the right place using\n`these lines <https://github.com/pytorch/tutorials/blob/master/Makefile#L97-L98>`_\nfrom the `Makefile <https://github.com/pytorch/tutorials/blob/master/Makefile>`_.\n\nTo run the code in this tutorial using the entire ImageNet dataset, on the other hand, you could download\nthe data using ``torchvision`` following\n`here <https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet>`_. For example,\nto download the training set and apply some standard transformations to it, you could use:\n\n.. code:: python\n\n    import torchvision\n    import torchvision.transforms as transforms\n\n    imagenet_dataset = torchvision.datasets.ImageNet(\n        '~/.data/imagenet',\n        split='train',\n        download=True,\n        transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\nWith the data downloaded, we show functions below that define dataloaders we'll use to read\nin this data. These functions mostly come from\n`here <https://github.com/pytorch/vision/blob/master/references/detection/train.py>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_data_loaders(data_path):\n\n    traindir = os.path.join(data_path, 'train')\n    valdir = os.path.join(data_path, 'val')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    dataset = torchvision.datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    dataset_test = torchvision.datasets.ImageFolder(\n        valdir,\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll load in the pre-trained MobileNetV2 model. We provide the URL to download the data from in ``torchvision``\n`here <https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py#L9>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = 'data/imagenet_1k'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 30\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\nwhile also improving numerical accuracy. While this can be used with any model, this is\nespecially common with quantized models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally to get a \"baseline\" accuracy, let's see the accuracy of our un-quantized model\nwith fused modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_eval_batches = 10\n\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see 78% accuracy on 300 images, a solid baseline for ImageNet,\nespecially considering our model is just 14.0 MB.\n\nThis will be our baseline to compare to. Next, let's try different quantization methods\n\n4. Post-training static quantization\n------------------------------------\n\nPost-training static quantization involves not just converting the weights from float to int,\nas in dynamic quantization, but also performing the additional step of first feeding batches\nof data through the network and computing the resulting distributions of the different activations\n(specifically, this is done by inserting `observer` modules at different points that record this\ndata). These distributions are then used to determine how the specifically the different activations\nshould be quantized at inference time (a simple technique would be to simply divide the entire range\nof activations into 256 levels, but we support more sophisticated methods as well). Importantly,\nthis additional step allows us to pass quantized values between operations instead of converting these\nvalues to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_calibration_batches = 10\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')\nmyModel.eval()\n\n# Fuse Conv, bn and relu\nmyModel.fuse_model()\n\n# Specify quantization configuration\n# Start with simple min/max range estimation and per-tensor quantization of weights\nmyModel.qconfig = torch.quantization.default_qconfig\nprint(myModel.qconfig)\ntorch.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first\nprint('Post Training Quantization Prepare: Inserting Observers')\nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n\n# Calibrate with the training set\nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\nprint('Post Training Quantization: Calibration done')\n\n# Convert to quantized model\ntorch.quantization.convert(myModel, inplace=True)\nprint('Post Training Quantization: Convert done')\nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n\nprint(\"Size of model after quantization\")\nprint_size_of_model(myModel)\n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this quantized model, we see a significantly lower accuracy of just ~62% on these same 300\nimages. Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost a 4x\ndecrease.\n\nIn addition, we can significantly improve on the accuracy simply by using a different\nquantization configuration. We repeat the same exercise with the recommended configuration for\nquantizing for x86 architectures. This configuration does the following:\n\n- Quantizes weights on a per-channel basis\n- Uses a histogram observer that collects a histogram of activations and then picks\n  quantization parameters in an optimal manner.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\nper_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing just this quantization configuration method resulted in an increase\nof the accuracy to over 76%! Still, this is 1-2% worse than the baseline of 78% achieved above.\nSo lets try quantization aware training.\n\n5. Quantization-aware training\n------------------------------\n\nQuantization-aware training (QAT) is the quantization method that typically results in the highest accuracy.\nWith QAT, all weights and activations are \u201cfake quantized\u201d during both the forward and backward passes of\ntraining: that is, float values are rounded to mimic int8 values, but all computations are still done with\nfloating point numbers. Thus, all the weight adjustments during training are made while \u201caware\u201d of the fact\nthat the model will ultimately be quantized; after quantizing, therefore, this method will usually yield\nhigher accuracy than either dynamic quantization or post-training static quantization.\n\nThe overall workflow for actually performing QAT is very similar to before:\n\n- We can use the same model as before: there is no additional preparation needed for quantization-aware\n  training.\n- We need to use a ``qconfig`` specifying what kind of fake-quantization is to be inserted after weights\n  and activations, instead of specifying observers\n\nWe first define a training function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    model.train()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt >= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fuse modules as before\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\nqat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, ``prepare_qat`` performs the \"fake quantization\", preparing the model for quantization-aware\ntraining\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training a quantized model with high accuracy requires accurate modeling of numerics at\ninference. For quantization aware training, therefore, we modify the training loop by:\n\n- Switch batch norm to use running mean and variance towards the end of training to better\n  match inference numerics.\n- We also freeze the quantizer parameters (scale and zero-point) and fine tune the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_train_batches = 20\n\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch > 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.quantization.disable_observer)\n    if nepoch > 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we just perform quantization-aware training for a small number of epochs. Nevertheless,\nquantization-aware training yields an accuracy of over 71% on the entire imagenet dataset,\nwhich is close to the floating point accuracy of 71.9%.\n\nMore on quantization-aware training:\n\n- QAT is a super-set of post training quant techniques that allows for more debugging.\n  For example, we can analyze if the accuracy of the model is limited by weight or activation\n  quantization.\n- We can also simulate the accuracy of a quantized model in floating point since\n  we are using fake-quantization to model the numerics of actual quantized arithmetic.\n- We can mimic post training quantization easily too.\n\nSpeedup from quantization\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFinally, let's confirm something we alluded to above: do our quantized models actually perform inference\nfaster? Let's test:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_benchmark(model_file, img_loader):\n    elapsed = 0\n    model = torch.jit.load(model_file)\n    model.eval()\n    num_batches = 5\n    # Run the scripted model on a few batches of images\n    for i, (images, target) in enumerate(img_loader):\n        if i < num_batches:\n            start = time.time()\n            output = model(images)\n            end = time.time()\n            elapsed = elapsed + (end-start)\n        else:\n            break\n    num_images = images.size()[0] * num_batches\n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n    return elapsed\n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running this locally on a MacBook pro yielded 61 ms for the regular model, and\njust 20 ms for the quantized model, illustrating the typical 2-4x speedup\nwe see for quantized models compared to floating point ones.\n\nConclusion\n----------\n\nIn this tutorial, we showed two quantization methods - post-training static quantization,\nand quantization-aware training - describing what they do \"under the hood\" and how to use\nthem in PyTorch.\n\nThanks for reading! As always, we welcome any feedback, so please create an issue\n`here <https://github.com/pytorch/pytorch/issues>`_ if you have any.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}