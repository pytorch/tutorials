{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nText classification with the torchtext library\n==================================\n\nIn this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n\n   - Access to the raw data as an iterator\n   - Build data processing pipeline to convert the raw text strings into ``torch.Tensor`` that can be used to train the model\n   - Shuffle and iterate the data with `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Access to the raw dataset iterators\n-----------------------------------\n\nThe torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the ``AG_NEWS`` dataset iterators yield the raw data as a tuple of label and text.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchtext.datasets import AG_NEWS\ntrain_iter = AG_NEWS(split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::\n\n    next(train_iter)\n    >>> (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - \n    Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green \n    again.\")\n\n    next(train_iter)\n    >>> (3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private \n    investment firm Carlyle Group,\\\\which has a reputation for making well-timed \n    and occasionally\\\\controversial plays in the defense industry, has quietly \n    placed\\\\its bets on another part of the market.')\n\n    next(train_iter)\n    >>> (3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring \n    crude prices plus worries\\\\about the economy and the outlook for earnings are \n    expected to\\\\hang over the stock market next week during the depth of \n    the\\\\summer doldrums.\")\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare data processing pipelines\n---------------------------------\n\nWe have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer. Those are the basic data processing building blocks for raw text string.\n\nHere is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Users can have a customized vocab by setting up arguments in the constructor of the Vocab class. For example, the minimum frequency ``min_freq`` for the tokens to be included.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\n\ntokenizer = get_tokenizer('basic_english')\ntrain_iter = AG_NEWS(split='train')\ncounter = Counter()\nfor (label, line) in train_iter:\n    counter.update(tokenizer(line))\nvocab = Vocab(counter, min_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vocabulary block converts a list of tokens into integers.\n\n::\n\n    [vocab[token] for token in ['here', 'is', 'an', 'example']]\n    >>> [476, 22, 31, 5298]\n\nPrepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\nlabel_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,\n\n::\n\n    text_pipeline('here is the an example')\n    >>> [475, 21, 2, 30, 5286]\n    label_pipeline('10')\n    >>> 9\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data batch and iterator \n--------------------------------\n\n`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\nis recommended for PyTorch users (a tutorial is `here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\nIt works with a map-style dataset that implements the ``getitem()`` and ``len()`` protocols, and represents a map from indices/keys to data samples. It also works with an iterable datasets with the shuffle argumnent of ``False``.\n\nBefore sending to the model, ``collate_fn`` function works on a batch of samples generated from ``DataLoader``. The input to ``collate_fn`` is a batch of data with the batch size in ``DataLoader``, and ``collate_fn`` processes them according to the data processing pipelines declared previouly. Pay attention here and make sure that ``collate_fn`` is declared as a top level def. This ensures that the function is available in each worker.\n\nIn this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of ``nn.EmbeddingBag``. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of indidividual text entries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for (_label, _text) in batch:\n         label_list.append(label_pipeline(_label))\n         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n         text_list.append(processed_text)\n         offsets.append(processed_text.size(0))\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    return label_list.to(device), text_list.to(device), offsets.to(device)    \n\ntrain_iter = AG_NEWS(split='train')\ndataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nThe model is composed of the `nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__ layer plus a linear layer for the classification purpose. ``nn.EmbeddingBag`` with the default mode of \"mean\" computes the mean value of a \u201cbag\u201d of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n\nAdditionally, since ``nn.EmbeddingBag`` accumulates the average across\nthe embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\nperformance and memory efficiency to process a sequence of tensors.\n\n![](../_static/img/text_sentiment_ngrams_model.png)\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n\nclass TextClassificationModel(nn.Module):\n\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an instance\n--------------------\n\nThe ``AG_NEWS`` dataset has four labels and therefore the number of classes is four.\n\n::\n\n   1 : World\n   2 : Sports\n   3 : Business\n   4 : Sci/Tec\n\nWe build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_iter = AG_NEWS(split='train')\nnum_class = len(set([label for (label, text) in train_iter]))\nvocab_size = len(vocab)\nemsize = 64\nmodel = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define functions to train the model and evaluate results.\n---------------------------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\ndef train(dataloader):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (label, text, offsets) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predited_label = model(text, offsets)\n        loss = criterion(predited_label, label)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n        optimizer.step()\n        total_acc += (predited_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        if idx % log_interval == 0 and idx > 0:\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches '\n                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n                                              total_acc/total_count))\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\ndef evaluate(dataloader):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (label, text, offsets) in enumerate(dataloader):\n            predited_label = model(text, offsets)\n            loss = criterion(predited_label, label)\n            total_acc += (predited_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the dataset and run the model\n-----------------------------------\n\nSince the original ``AG_NEWS`` has no valid dataset, we split the training\ndataset into train/valid sets with a split ratio of 0.95 (train) and\n0.05 (valid). Here we use\n`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\nfunction in PyTorch core library.\n\n`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\ncriterion combines ``nn.LogSoftmax()`` and ``nn.NLLLoss()`` in a single class.\nIt is useful when training a classification problem with C classes.\n`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.\n`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\nis used here to adjust the learning rate through epochs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n# Hyperparameters\nEPOCHS = 10 # epoch\nLR = 5  # learning rate\nBATCH_SIZE = 64 # batch size for training\n  \ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\ntotal_accu = None\ntrain_iter, test_iter = AG_NEWS()\ntrain_dataset = list(train_iter)\ntest_dataset = list(test_iter)\nnum_train = int(len(train_dataset) * 0.95)\nsplit_train_, split_valid_ = \\\n    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n\ntrain_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=collate_batch)\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train(train_dataloader)\n    accu_val = evaluate(valid_dataloader)\n    if total_accu is not None and total_accu > accu_val:\n      scheduler.step()\n    else:\n       total_accu = accu_val\n    print('-' * 59)\n    print('| end of epoch {:3d} | time: {:5.2f}s | '\n          'valid accuracy {:8.3f} '.format(epoch,\n                                           time.time() - epoch_start_time,\n                                           accu_val))\n    print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the model on GPU with the following printout:\n\n::\n\n       | epoch   1 |   500/ 1782 batches | accuracy    0.684\n       | epoch   1 |  1000/ 1782 batches | accuracy    0.852\n       | epoch   1 |  1500/ 1782 batches | accuracy    0.877\n       -----------------------------------------------------------\n       | end of epoch   1 | time:  8.33s | valid accuracy    0.867\n       -----------------------------------------------------------\n       | epoch   2 |   500/ 1782 batches | accuracy    0.895\n       | epoch   2 |  1000/ 1782 batches | accuracy    0.900\n       | epoch   2 |  1500/ 1782 batches | accuracy    0.903\n       -----------------------------------------------------------\n       | end of epoch   2 | time:  8.18s | valid accuracy    0.890\n       -----------------------------------------------------------\n       | epoch   3 |   500/ 1782 batches | accuracy    0.914\n       | epoch   3 |  1000/ 1782 batches | accuracy    0.914\n       | epoch   3 |  1500/ 1782 batches | accuracy    0.916\n       -----------------------------------------------------------\n       | end of epoch   3 | time:  8.20s | valid accuracy    0.897\n       -----------------------------------------------------------\n       | epoch   4 |   500/ 1782 batches | accuracy    0.926\n       | epoch   4 |  1000/ 1782 batches | accuracy    0.924\n       | epoch   4 |  1500/ 1782 batches | accuracy    0.921\n       -----------------------------------------------------------\n       | end of epoch   4 | time:  8.18s | valid accuracy    0.895\n       -----------------------------------------------------------\n       | epoch   5 |   500/ 1782 batches | accuracy    0.938\n       | epoch   5 |  1000/ 1782 batches | accuracy    0.935\n       | epoch   5 |  1500/ 1782 batches | accuracy    0.937\n       -----------------------------------------------------------\n       | end of epoch   5 | time:  8.16s | valid accuracy    0.902\n       -----------------------------------------------------------\n       | epoch   6 |   500/ 1782 batches | accuracy    0.939\n       | epoch   6 |  1000/ 1782 batches | accuracy    0.939\n       | epoch   6 |  1500/ 1782 batches | accuracy    0.938\n       -----------------------------------------------------------\n       | end of epoch   6 | time:  8.16s | valid accuracy    0.906\n       -----------------------------------------------------------\n       | epoch   7 |   500/ 1782 batches | accuracy    0.941\n       | epoch   7 |  1000/ 1782 batches | accuracy    0.939\n       | epoch   7 |  1500/ 1782 batches | accuracy    0.939\n       -----------------------------------------------------------\n       | end of epoch   7 | time:  8.19s | valid accuracy    0.903\n       -----------------------------------------------------------\n       | epoch   8 |   500/ 1782 batches | accuracy    0.942\n       | epoch   8 |  1000/ 1782 batches | accuracy    0.941\n       | epoch   8 |  1500/ 1782 batches | accuracy    0.942\n       -----------------------------------------------------------\n       | end of epoch   8 | time:  8.16s | valid accuracy    0.904\n       -----------------------------------------------------------\n       | epoch   9 |   500/ 1782 batches | accuracy    0.942\n       | epoch   9 |  1000/ 1782 batches | accuracy    0.941\n       | epoch   9 |  1500/ 1782 batches | accuracy    0.942\n       -----------------------------------------------------------\n         end of epoch   9 | time:  8.16s | valid accuracy    0.904\n       -----------------------------------------------------------\n       | epoch  10 |   500/ 1782 batches | accuracy    0.940\n       | epoch  10 |  1000/ 1782 batches | accuracy    0.942\n       | epoch  10 |  1500/ 1782 batches | accuracy    0.942\n       -----------------------------------------------------------\n       | end of epoch  10 | time:  8.15s | valid accuracy    0.904\n       -----------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model with test dataset\n------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the results of the test dataset\u2026\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Checking the results of test dataset.')\naccu_test = evaluate(test_dataloader)\nprint('test accuracy {:8.3f}'.format(accu_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::\n\n       test accuracy    0.906\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on a random news\n---------------------\n\nUse the best model so far and test a golf news.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\ndef predict(text, text_pipeline):\n    with torch.no_grad():\n        text = torch.tensor(text_pipeline(text))\n        output = model(text, torch.tensor([0]))\n        return output.argmax(1).item() + 1\n\nex_text_str = \"MEMPHIS, Tenn. \u2013 Four days ago, Jon Rahm was \\\n    enduring the season\u2019s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday\u2019s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he\u2019d never played the \\\n    front nine at TPC Southwind.\"\n\nmodel = model.to(\"cpu\")\n\nprint(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::\n\n       This is a Sports news\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}