{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reinforcement Learning (PPO) with TorchRL Tutorial\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\nThis tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` to train a parametric policy\nnetwork to solve the Inverted Pendulum task from the [OpenAI-Gym/Farama-Gymnasium\ncontrol library](https://github.com/Farama-Foundation/Gymnasium)_.\n\n.. figure:: /_static/img/invpendulum.gif\n   :alt: Inverted pendulum\n\n   Inverted pendulum\n\nKey learnings:\n\n- How to create an environment in TorchRL, transform its outputs, and collect data from this env;\n- How to make your classes talk to each other using :class:`tensordict.TensorDict`;\n- The basics of building your training loop with TorchRL:\n\n  - How to compute the advantage signal for policy gradient methods;\n  - How to create a stochastic policy using a probabilistic neural network;\n  - How to create a dynamic replay buffer and sample from it without repetition.\n\nWe will cover six crucial components of TorchRL:\n\n* [environments](https://pytorch.org/rl/reference/envs.html)_\n* [transforms](https://pytorch.org/rl/reference/envs.html#transforms)_\n* [models (policy and value function)](https://pytorch.org/rl/reference/modules.html)_\n* [loss modules](https://pytorch.org/rl/reference/objectives.html)_\n* [data collectors](https://pytorch.org/rl/reference/collectors.html)_\n* [replay buffers](https://pytorch.org/rl/reference/data.html#replay-buffers)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this in Google Colab, make sure you install the following dependencies:\n\n```bash\n!pip3 install torchrl\n!pip3 install gym[mujoco]\n!pip3 install tqdm\n```\nProximal Policy Optimization (PPO) is a policy-gradient algorithm where a\nbatch of data is being collected and directly consumed to train the policy to maximise\nthe expected return given some proximality constraints. You can think of it\nas a sophisticated version of [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf),\nthe foundational policy-optimization algorithm. For more information, see the\n[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) paper.\n\nPPO is usually regarded as a fast and efficient method for online, on-policy\nreinforcement algorithm. TorchRL provides a loss-module that does all the work\nfor you, so that you can rely on this implementation and focus on solving your\nproblem rather than re-inventing the wheel every time you want to train a policy.\n\nFor completeness, here is a brief overview of what the loss computes, even though\nthis is taken care of by our :class:`ClipPPOLoss` module\u2014the algorithm works as follows:\n1. we will sample a batch of data by playing the\npolicy in the environment for a given number of steps.\n2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using\na clipped version of the REINFORCE loss.\n3. The clipping will put a pessimistic bound on our loss: lower return estimates will\nbe favored compared to higher ones.\nThe precise formula of the loss is:\n\n\\begin{align}L(s,a,\\theta_k,\\theta) = \\min\\left(\n    \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n    g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n    \\right),\\end{align}\n\nThere are two components in that loss: in the first part of the minimum operator,\nwe simply compute an importance-weighted version of the REINFORCE loss (for example, a\nREINFORCE loss that we have corrected for the fact that the current policy\nconfiguration lags the one that was used for the data collection).\nThe second part of that minimum operator is a similar loss where we have clipped\nthe ratios when they exceeded or were below a given pair of thresholds.\n\nThis loss ensures that whether the advantage is positive or negative, policy\nupdates that would produce significant shifts from the previous configuration\nare being discouraged.\n\nThis tutorial is structured as follows:\n\n1. First, we will define a set of hyperparameters we will be using for training.\n\n2. Next, we will focus on creating our environment, or simulator, using TorchRL's\n   wrappers and transforms.\n\n3. Next, we will design the policy network and the value model,\n   which is indispensable to the loss function. These modules will be used\n   to configure our loss module.\n\n4. Next, we will create the replay buffer and data loader.\n\n5. Finally, we will run our training loop and analyze the results.\n\nThroughout this tutorial, we'll be using the :mod:`tensordict` library.\n:class:`tensordict.TensorDict` is the lingua franca of TorchRL: it helps us abstract\nwhat a module reads and writes and care less about the specific data\ndescription and more about the algorithm itself.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom tensordict.nn import TensorDictModule\nfrom tensordict.nn.distributions import NormalParamExtractor\nfrom torch import nn\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data.replay_buffers import ReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\nfrom torchrl.data.replay_buffers.storages import LazyTensorStorage\nfrom torchrl.envs import (\n    Compose,\n    DoubleToFloat,\n    ObservationNorm,\n    StepCounter,\n    TransformedEnv,\n)\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.utils import check_env_specs, set_exploration_mode\nfrom torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\nfrom torchrl.objectives import ClipPPOLoss\nfrom torchrl.objectives.value import GAE\nfrom tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Hyperparameters\n\nWe set the hyperparameters for our algorithm. Depending on the resources\navailable, one may choose to execute the policy on GPU or on another\ndevice.\nThe ``frame_skip`` will control how for how many frames is a single\naction being executed. The rest of the arguments that count frames\nmust be corrected for this value (since one environment step will\nactually return ``frame_skip`` frames).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cpu\" if not torch.has_cuda else \"cuda:0\"\nnum_cells = 256  # number of cells in each layer\nlr = 3e-4\nmax_grad_norm = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collection parameters\n\nWhen collecting data, we will be able to choose how big each batch will be\nby defining a ``frames_per_batch`` parameter. We will also define how many\nframes (such as the number of interactions with the simulator) we will allow ourselves to\nuse. In general, the goal of an RL algorithm is to learn to solve the task\nas fast as it can in terms of environment interactions: the lower the ``total_frames``\nthe better.\nWe also define a ``frame_skip``: in some contexts, repeating the same action\nmultiple times over the course of a trajectory may be beneficial as it makes\nthe behavior more consistent and less erratic. However, \"skipping\"\ntoo many frames will hamper training by reducing the reactivity of the actor\nto observation changes.\n\nWhen using ``frame_skip`` it is good practice to\ncorrect the other frame counts by the number of frames we are grouping\ntogether. If we configure a total count of X frames for training but\nuse a ``frame_skip`` of Y, we will be actually collecting XY frames in total\nwhich exceeds our predefined budget.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frame_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO parameters\n\nAt each data collection (or batch collection) we will run the optimization\nover a certain number of *epochs*, each time consuming the entire data we just\nacquired in a nested training loop. Here, the ``sub_batch_size`` is different from the\n``frames_per_batch`` here above: recall that we are working with a \"batch of data\"\ncoming from our collector, which size is defined by ``frames_per_batch``, and that\nwe will further split in smaller sub-batches during the inner training loop.\nThe size of these sub-batches is controlled by ``sub_batch_size``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\nnum_epochs = 10  # optimisation steps per batch of data collected\nclip_epsilon = (\n    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n)\ngamma = 0.99\nlmbda = 0.95\nentropy_eps = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define an environment\n\nIn RL, an *environment* is usually the way we refer to a simulator or a\ncontrol system. Various libraries provide simulation environments for reinforcement\nlearning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and\nmany others.\nAs a generalistic library, TorchRL's goal is to provide an interchangeable interface\nto a large panel of RL simulators, allowing you to easily swap one environment\nwith another. For example, creating a wrapped gym environment can be achieved with few characters:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device, frame_skip=frame_skip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are a few things to notice in this code: first, we created\nthe environment by calling the ``GymEnv`` wrapper. If extra keyword arguments\nare passed, they will be transmitted to the ``gym.make`` method, hence covering\nthe most common env construction commands.\nAlternatively, one could also directly create a gym environment using ``gym.make(env_name, **kwargs)``\nand wrap it in a `GymWrapper` class.\n\nAlso the ``device`` argument: for gym, this only controls the device where\ninput action and observered states will be stored, but the execution will always\nbe done on CPU. The reason for this is simply that gym does not support on-device\nexecution, unless specified otherwise. For other libraries, we have control over\nthe execution device and, as much as we can, we try to stay consistent in terms of\nstoring and execution backends.\n\n### Transforms\n\nWe will append some transforms to our environments to prepare the data for\nthe policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different\napproach, more similar to other pytorch domain libraries, through the use of transforms.\nTo add transforms to an environment, one should simply wrap it in a :class:`TransformedEnv`\ninstance, and append the sequence of transforms to it. The transformed env will inherit\nthe device and meta-data of the wrapped env, and transform these depending on the sequence\nof transforms it contains.\n\n### Normalization\n\nThe first to encode is a normalization transform.\nAs a rule of thumbs, it is preferable to have data that loosely\nmatch a unit Gaussian distribution: to obtain this, we will\nrun a certain number of random steps in the environment and compute\nthe summary statistics of these observations.\n\nWe'll append two other transforms: the :class:`DoubleToFloat` transform will\nconvert double entries to single-precision numbers, ready to be read by the\npolicy. The :class:`StepCounter` transform will be used to count the steps before\nthe environment is terminated. We will use this measure as a supplementary measure\nof performance.\n\nAs we will see later, many of the TorchRL's classes rely on :class:`tensordict.TensorDict`\nto communicate. You could think of it as a python dictionary with some extra\ntensor features. In practice, this means that many modules we will be working\nwith need to be told what key to read (``in_keys``) and what key to write\n(``out_keys``) in the tensordict they will receive. Usually, if ``out_keys``\nis omitted, it is assumed that the ``in_keys`` entries will be updated\nin-place. For our transforms, the only entry we are interested in is referred\nto as ``\"observation\"`` and our transform layers will be told to modify this\nentry and this entry only:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n    base_env,\n    Compose(\n        # normalize observations\n        ObservationNorm(in_keys=[\"observation\"]),\n        DoubleToFloat(in_keys=[\"observation\"]),\n        StepCounter(),\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you may have noticed, we have created a normalization layer but we did not\nset its normalization parameters. To do this, :class:`ObservationNorm` can\nautomatically gather the summary statistics of our environment:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`ObservationNorm` transform has now been populated with a\nlocation and a scale that will be used to normalize the data.\n\nLet us do a little sanity check for the shape of our summary stats:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An environment is not only defined by its simulator and transforms, but also\nby a series of metadata that describe what can be expected during its\nexecution.\nFor efficiency purposes, TorchRL is quite stringent when it comes to\nenvironment specs, but you can easily check that your environment specs are\nadequate.\nIn our example, the :class:`GymWrapper` and :class:`GymEnv` that inherits\nfrom it already take care of setting the proper specs for your env so\nyou should not have to care about this.\n\nNevertheless, let's see a concrete example using our transformed\nenvironment by looking at its specs.\nThere are three specs to look at: ``observation_spec`` which defines what\nis to be expected when executing an action in the environment,\n``reward_spec`` which indicates the reward domain and finally the\n``input_spec`` (which contains the ``action_spec``) and which represents\neverything an environment requires to execute a single step.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"observation_spec:\", env.observation_spec)\nprint(\"reward_spec:\", env.reward_spec)\nprint(\"input_spec:\", env.input_spec)\nprint(\"action_spec (as defined by input_spec):\", env.action_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the :func:`check_env_specs` function runs a small rollout and compares its output against the environemnt\nspecs. If no error is raised, we can be confident that the specs are properly defined:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For fun, let's see what a simple random rollout looks like. You can\ncall `env.rollout(n_steps)` and get an overview of what the environment inputs\nand outputs look like. Actions will automatically be drawn from the action spec\ndomain, so you don't need to care about designing a random sampler.\n\nTypically, at each step, an RL environment receives an\naction as input, and outputs an observation, a reward and a done state. The\nobservation may be composite, meaning that it could be composed of more than one\ntensor. This is not a problem for TorchRL, since the whole set of observations\nis automatically packed in the output :class:`tensordict.TensorDict`. After executing a rollout\n(ie a sequence of environment steps and random action generations) over a given\nnumber of steps, we will retrieve a :class:`tensordict.TensorDict` instance with a shape\nthat matches this trajectory length:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = env.rollout(3)\nprint(\"rollout of three steps:\", rollout)\nprint(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our rollout data has a shape of ``torch.Size([3])`, which matches the number of steps\nwe ran it for. The ``\"next\"`` entry points to the data coming after the current step.\nIn most cases, the ``\"next\"\"`` data at time `t` matches the data at ``t+1``, but this\nmay not be the case if we are using some specific transformations (e.g. mutli-step).\n\n## Policy\n\nPPO utilizes a stochastic policy to handle exploration. This means that our\nneural network will have to output the parameters of a distribution, rather\nthan a single value corresponding to the action taken.\n\nAs the data is continuous, we use a Tanh-Normal distribution to respect the\naction space boundaries. TorchRL provides such distribution, and the only\nthing we need to care about is to build a neural network that outputs the\nright number of parameters for the policy to work with (a location, or mean,\nand a scale):\n\n\\begin{align}f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})\\end{align}\n\nThe only extra-difficulty that is brought up here is to split our output in two\nequal parts and map the second to a scrictly positive space.\n\nWe design the policy in three steps:\n\n1. Define a neural network ``D_obs`` -> ``2 * D_action``. Indeed, our ``loc`` (mu) and ``scale`` (sigma) both have dimension ``D_action``;\n\n2. Append a :class:`NormalParamExtractor` to extract a location and a scale (ie splits the input in two equal parts\n  and applies a positive transformation to the scale parameter);\n\n3. Create a probabilistic :class:`TensorDictModule` that can create this distribution and sample from it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "actor_net = nn.Sequential(\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n    NormalParamExtractor(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To enable the policy to \"talk\" with the environment through the tensordict\ndata carrier, we wrap the ``nn.Module`` in a :class:`TensorDictModule`. This\nclass will simply ready the ``in_keys`` it is provided with and write the\noutputs in-place at the registered ``out_keys``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_module = TensorDictModule(\n    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now need to build a distribution out of the location and scale of our\nnormal distribution. To do so, we instruct the :class:`ProbabilisticActor`\nclass to build a :class:`TanhNormal` out of the location and scale\nparameters. We also provide the minimum and maximum values of this\ndistribution, which we gather from the environment specs.\n\nThe name of the ``in_keys`` (and hence the name of the ``out_keys`` from\nthe :class:`TensorDictModule` above) cannot be set to any value one may\nlike, as the :class:`TanhNormal` distribution constructor will expect the\n``loc`` and ``scale`` keyword arguments. That being said,\n:class:`ProbabilisticActor` also accepts ``Dict[str, str]`` typed ``in_keys``\nwhere the key-value pair indicates what ``in_key`` string should be used for\nevery keyword argument that is to be used.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_module = ProbabilisticActor(\n    module=policy_module,\n    spec=env.action_spec,\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=TanhNormal,\n    distribution_kwargs={\n        \"min\": env.action_spec.space.minimum,\n        \"max\": env.action_spec.space.maximum,\n    },\n    return_log_prob=True,\n    # we'll need the log-prob for the numerator of the importance weights\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Value network\n\nThe value network is a crucial component of the PPO algorithm, even though it\nwon't be used at inference time. This module will read the observations and\nreturn an estimation of the discounted return for the following trajectory.\nThis allows us to amortize learning by relying on the some utility estimation\nthat is learnt on-the-fly during training. Our value network share the same\nstructure as the policy, but for simplicity we assign it its own set of\nparameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "value_net = nn.Sequential(\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(num_cells, device=device),\n    nn.Tanh(),\n    nn.LazyLinear(1, device=device),\n)\n\nvalue_module = ValueOperator(\n    module=value_net,\n    in_keys=[\"observation\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's try our policy and value modules. As we said earlier, the usage of\n:class:`TensorDictModule` makes it possible to directly read the output\nof the environment to run these modules, as they know what information to read\nand where to write it:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Running policy:\", policy_module(env.reset()))\nprint(\"Running value:\", value_module(env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data collector\n\nTorchRL provides a set of :class:`DataCollector` classes. Briefly, these\nclasses execute three operations: reset an environment, compute an action\ngiven the latest observation, execute a step in the environment, and repeat\nthe last two steps until the environment reaches a stop signal (or ``\"done\"``\nstate).\n\nThey allow you to control how many frames to collect at each iteration\n(through the ``frames_per_batch`` parameter),\nwhen to reset the environment (through the ``max_frames_per_traj`` argument),\non which ``device`` the policy should be executed, etc. They are also\ndesigned to work efficiently with batched and multiprocessed environments.\n\nThe simplest data collector is the :class:`SyncDataCollector`: it is an\niterator that you can use to get batches of data of a given length, and\nthat will stop once a total number of frames (``total_frames``) have been\ncollected.\nOther data collectors (``MultiSyncDataCollector`` and\n``MultiaSyncDataCollector``) will execute the same operations in synchronous\nand asynchronous manner over a set of multiprocessed workers.\n\nAs for the policy and environment before, the data collector will return\n:class:`tensordict.TensorDict` instances with a total number of elements that will\nmatch ``frames_per_batch``. Using :class:`tensordict.TensorDict` to pass data to the\ntraining loop allows you to write dataloading pipelines\nthat are 100% oblivious to the actual specificities of the rollout content.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collector = SyncDataCollector(\n    env,\n    policy_module,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    split_trajs=False,\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay buffer\n\nReplay buffers are a common building piece of off-policy RL algorithms.\nIn on-policy contexts, a replay buffer is refilled every time a batch of\ndata is collected, and its data is repeatedly consumed for a certain number\nof epochs.\n\nTorchRL's replay buffers are built using a common container\n:class:`ReplayBuffer` which takes as argument the components of the buffer:\na storage, a writer, a sampler and possibly some transforms. Only the\nstorage (which indicates the replay buffer capacity) is mandatory. We\nalso specify a sampler without repetition to avoid sampling multiple times\nthe same item in one epoch.\nUsing a replay buffer for PPO is not mandatory and we could simply\nsample the sub-batches from the collected batch, but using these classes\nmake it easy for us to build the inner training loop in a reproducible way.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer(\n    storage=LazyTensorStorage(frames_per_batch),\n    sampler=SamplerWithoutReplacement(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss function\n\nThe PPO loss can be directly imported from torchrl for convenience using the\n:class:`ClipPPOLoss` class. This is the easiest way of utilizing PPO:\nit hides away the mathematical operations of PPO and the control flow that\ngoes with it.\n\nPPO requires some \"advantage estimation\" to be computed. In short, an advantage\nis a value that reflects an expectancy over the return value while dealing with\nthe bias / variance tradeoff.\nTo compute the advantage, one just needs to (1) build the advantage module, which\nutilizes our value operator, and (2) pass each batch of data through it before each\nepoch.\nThe GAE module will update the input tensordict with new ``\"advantage\"`` and\n``\"value_target\"`` entries.\nThe ``\"value_target\"`` is a gradient-free tensor that represents the empirical\nvalue that the value network should represent with the input observation.\nBoth of these will be used by :class:`ClipPPOLoss` to\nreturn the policy and value losses.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "advantage_module = GAE(\n    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n)\n\nloss_module = ClipPPOLoss(\n    actor=policy_module,\n    critic=value_module,\n    advantage_key=\"advantage\",\n    clip_epsilon=clip_epsilon,\n    entropy_bonus=bool(entropy_eps),\n    entropy_coef=entropy_eps,\n    # these keys match by default but we set this for completeness\n    value_target_key=advantage_module.value_target_key,\n    critic_coef=1.0,\n    gamma=0.99,\n    loss_critic_type=\"smooth_l1\",\n)\n\noptim = torch.optim.Adam(loss_module.parameters(), lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optim, total_frames // frames_per_batch, 0.0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\nWe now have all the pieces needed to code our training loop.\nThe steps include:\n\n* Collect data\n\n  * Compute advantage\n\n    * Loop over the collected to compute loss values\n    * Back propagate\n    * Optimize\n    * Repeat\n\n  * Repeat\n\n* Repeat\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logs = defaultdict(list)\npbar = tqdm(total=total_frames * frame_skip)\neval_str = \"\"\n\n# We iterate over the collector until it reaches the total number of frames it was\n# designed to collect:\nfor i, tensordict_data in enumerate(collector):\n    # we now have a batch of data to work with. Let's learn something from it.\n    for _ in range(num_epochs):\n        # We'll need an \"advantage\" signal to make PPO work.\n        # We re-compute it at each epoch as its value depends on the value\n        # network which is updated in the inner loop.\n        advantage_module(tensordict_data)\n        data_view = tensordict_data.reshape(-1)\n        replay_buffer.extend(data_view.cpu())\n        for _ in range(frames_per_batch // sub_batch_size):\n            subdata = replay_buffer.sample(sub_batch_size)\n            loss_vals = loss_module(subdata.to(device))\n            loss_value = (\n                loss_vals[\"loss_objective\"]\n                + loss_vals[\"loss_critic\"]\n                + loss_vals[\"loss_entropy\"]\n            )\n\n            # Optimization: backward, grad clipping and optim step\n            loss_value.backward()\n            # this is not strictly mandatory but it's good practice to keep\n            # your gradient norm bounded\n            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n            optim.step()\n            optim.zero_grad()\n\n    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n    pbar.update(tensordict_data.numel() * frame_skip)\n    cum_reward_str = (\n        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n    )\n    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n    if i % 10 == 0:\n        # We evaluate the policy once every 10 batches of data.\n        # Evaluation is rather simple: execute the policy without exploration\n        # (take the expected value of the action distribution) for a given\n        # number of steps (1000, which is our env horizon).\n        # The ``rollout`` method of the env can take a policy as argument:\n        # it will then execute this policy at each step.\n        with set_exploration_mode(\"mean\"), torch.no_grad():\n            # execute a rollout with the trained policy\n            eval_rollout = env.rollout(1000, policy_module)\n            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n            logs[\"eval reward (sum)\"].append(\n                eval_rollout[\"next\", \"reward\"].sum().item()\n            )\n            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n            eval_str = (\n                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n                f\"eval step-count: {logs['eval step_count'][-1]}\"\n            )\n            del eval_rollout\n    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n\n    # We're also using a learning rate scheduler. Like the gradient clipping,\n    # this is a nice-to-have but nothing necessary for PPO to work.\n    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n\nBefore the 1M step cap is reached, the algorithm should have reached a max\nstep count of 1000 steps, which is the maximum number of steps before the\ntrajectory is truncated.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and next steps\n\nIn this tutorial, we have learned:\n\n1. How to create and customize an environment with :py:mod:`torchrl`;\n2. How to write a model and a loss function;\n3. How to set up a typical training loop.\n\nIf you want to experiment with this tutorial a bit more, you can apply the following modifications:\n\n* From an efficiency perspective,\n  we could run several simulations in parallel to speed up data collection.\n  Check :class:`torchrl.envs.ParallelEnv` for further information.\n\n* From a logging perspective, one could add a :class:`torchrl.record.VideoRecorder` transform to\n  the environment after asking for rendering to get a visual rendering of the\n  inverted pendulum in action. Check :py:mod:`torchrl.record` to\n  know more.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}