{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAudio Classifier Tutorial\n=========================\n**Author**: `Winston Herring <https://github.com/winston6>`_\n\nThis tutorial will show you how to correctly format an audio dataset and\nthen train/test an audio classifier network on the dataset. First, let\u2019s\nimport the common torch packages as well as ``torchaudio``, ``pandas``,\nand ``numpy``. ``torchaudio`` is available `here <https://github.com/pytorch/audio>`_\nand can be installed by following the\ninstructions on the website.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport pandas as pd\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s check if a CUDA GPU is available and select our device. Running\nthe network on a GPU will greatly decrease the training/testing runtime.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the Dataset\n---------------------\n\nWe will use the UrbanSound8K dataset to train our network. It is\navailable for free `here <https://urbansounddataset.weebly.com/>`_ and contains\n10 audio classes with over 8000 audio samples! Once you have downloaded\nthe compressed dataset, extract it to your current working directory.\nFirst, we will look at the csv file that provides information about the\nindividual sound files. ``pandas`` allows us to open the csv file and\nuse ``.iloc()`` to access the data within it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csvData = pd.read_csv('./UrbanSound8K/metadata/UrbanSound8K.csv')\nprint(csvData.iloc[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 10 audio classes in the UrbanSound8K dataset are air_conditioner,\ncar_horn, children_playing, dog_bark, drilling, enginge_idling,\ngun_shot, jackhammer, siren, and street_music. Let\u2019s play a couple files\nand see what they sound like. The first file is street music and the\nsecond is an air conditioner.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\nipd.Audio('./UrbanSound8K/audio/fold1/108041-9-0-5.wav')\n\nipd.Audio('./UrbanSound8K/audio/fold5/100852-0-0-19.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Formatting the Data\n-------------------\n\nNow that we know the format of the csv file entries, we can construct\nour dataset. We will create a rapper class for our dataset using\n``torch.utils.data.Dataset`` that will handle loading the files and\nperforming some formatting steps. The UrbanSound8K dataset is separated\ninto 10 folders. We will use the data from 9 of these folders to train\nour network and then use the 10th folder to test the network. The rapper\nclass will store the file names, labels, and folder numbers of the audio\nfiles in the inputted folder list when initialized. The actual loading\nand formatting steps will happen in the access function ``__getitem__``.\n\nIn ``__getitem__``, we use ``torchaudio.load()`` to convert the wav\nfiles to tensors. ``torchaudio.load()`` returns a tuple containing the\nnewly created tensor along with the sampling frequency of the audio file\n(44.1kHz for UrbanSound8K). The dataset uses two channels for audio so\nwe will use ``torchaudio.transforms.DownmixMono()`` to convert the audio\ndata to one channel. Next, we need to format the audio data. The network\nwe will make takes an input size of 32,000, while most of the audio\nfiles have well over 100,000 samples. The UrbanSound8K audio is sampled\nat 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By\ndownsampling the audio to aproximately 8kHz, we can represent 4 seconds\nwith the 32,000 samples. This downsampling is achieved by taking every\nfifth sample of the original audio tensor. Not every audio tensor is\nlong enough to handle the downsampling so these tensors will need to be\npadded with zeros. The minimum length that won\u2019t require padding is\n160,000 samples.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class UrbanSoundDataset(Dataset):\n#rapper for the UrbanSound8K dataset\n    # Argument List\n    #  path to the UrbanSound8K csv file\n    #  path to the UrbanSound8K audio files\n    #  list of folders to use in the dataset\n    \n    def __init__(self, csv_path, file_path, folderList):\n        csvData = pd.read_csv(csv_path)\n        #initialize lists to hold file names, labels, and folder numbers\n        self.file_names = []\n        self.labels = []\n        self.folders = []\n        #loop through the csv entries and only add entries from folders in the folder list\n        for i in range(0,len(csvData)):\n            if csvData.iloc[i, 5] in folderList:\n                self.file_names.append(csvData.iloc[i, 0])\n                self.labels.append(csvData.iloc[i, 6])\n                self.folders.append(csvData.iloc[i, 5])\n                \n        self.file_path = file_path\n        self.mixer = torchaudio.transforms.DownmixMono() #UrbanSound8K uses two channels, this will convert them to one\n        self.folderList = folderList\n        \n    def __getitem__(self, index):\n        #format the file path and load the file\n        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n        sound = torchaudio.load(path, out = None, normalization = True)\n        #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n        soundData = self.mixer(sound[0])\n        #downsample the audio to ~8kHz\n        tempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\n        if soundData.numel() < 160000:\n            tempData[:soundData.numel()] = soundData[:]\n        else:\n            tempData[:] = soundData[:160000]\n        \n        soundData = tempData\n        soundFormatted = torch.zeros([32000, 1])\n        soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n        soundFormatted = soundFormatted.permute(1, 0)\n        return soundFormatted, self.labels[index]\n    \n    def __len__(self):\n        return len(self.file_names)\n\n    \ncsv_path = './UrbanSound8K/metadata/UrbanSound8K.csv'\nfile_path = './UrbanSound8K/audio/'\n\ntrain_set = UrbanSoundDataset(csv_path, file_path, range(1,10))\ntest_set = UrbanSoundDataset(csv_path, file_path, [10])\nprint(\"Train set size: \" + str(len(train_set)))\nprint(\"Test set size: \" + str(len(test_set)))\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the Network\n------------------\n\nFor this tutorial we will use a convolutional neural network to process\nthe raw audio data. Usually more advanced transforms are applied to the\naudio data, however CNNs can be used to accurately process the raw data.\nThe specific architecture is modeled after the M5 network architecture\ndescribed in https://arxiv.org/pdf/1610.00087.pdf. An important aspect\nof models processing raw audio data is the receptive field of their\nfirst layer\u2019s filters. Our model\u2019s first filter is length 80 so when\nprocessing audio sampled at 8kHz the receptive field is around 10ms.\nThis size is similar to speech processing applications that often use\nreceptive fields ranging from 20ms to 40ms.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(4)\n        self.conv2 = nn.Conv1d(128, 128, 3)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(4)\n        self.conv3 = nn.Conv1d(128, 256, 3)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(4)\n        self.conv4 = nn.Conv1d(256, 512, 3)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.pool4 = nn.MaxPool1d(4)\n        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n        self.fc1 = nn.Linear(512, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = F.relu(self.bn3(x))\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = F.relu(self.bn4(x))\n        x = self.pool4(x)\n        x = self.avgPool(x)\n        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n        x = self.fc1(x)\n        return F.log_softmax(x, dim = 2)\n\nmodel = Net()\nmodel.to(device)\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same optimization technique used in the paper, an Adam\noptimizer with weight decay set to 0.0001. At first, we will train with\na learning rate of 0.01, but we will use a ``scheduler`` to decrease it\nto 0.001 during training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and Testing the Network\n--------------------------------\n\nNow let\u2019s define a training function that will feed our training data\ninto the model and perform the backward pass and optimization steps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(model, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        target = target.to(device)\n        data = data.requires_grad_() #set requires_grad to True for training\n        output = model(data)\n        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0: #print training stats\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a training function, we need to make one for testing\nthe networks accuracy. We will set the model to ``eval()`` mode and then\nrun inference on the test dataset. Calling ``eval()`` sets the training\nvariable in all modules in the network to false. Certain layers like\nbatch normalization and dropout layers behave differently during\ntraining so this step is crucial for getting correct results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(model, epoch):\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        data = data.to(device)\n        target = target.to(device)\n        output = model(data)\n        output = output.permute(1, 0, 2)\n        pred = output.max(2)[1] # get the index of the max log-probability\n        correct += pred.eq(target).cpu().sum().item()\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can train and test the network. We will train the network\nfor ten epochs then reduce the learn rate and train for ten more epochs.\nThe network will be tested after each epoch to see how the accuracy\nvaries during the training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_interval = 20\nfor epoch in range(1, 41):\n    if epoch == 31:\n        print(\"First round of training complete. Setting learn rate to 0.001.\")\n    scheduler.step()\n    train(model, epoch)\n    test(model, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nIf trained on 9 folders, the network should be more than 50% accurate by\nthe end of the training process. Training on less folders will result in\na lower overall accuracy but may be necessary if long runtimes are a\nproblem. Greater accuracies can be achieved using deeper CNNs at the\nexpense of a larger memory footprint.\n\nFor more advanced audio applications, such as speech recognition,\nrecurrent neural networks (RNNs) are commonly used. There are also other\ndata preprocessing methods, such as finding the mel frequency cepstral\ncoefficients (MFCC), that can reduce the size of the dataset.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}