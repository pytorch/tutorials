{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DebugMode: Recording Dispatched Operations and Numerical Debugging\n",
    "=================================================================\n",
    "\n",
    "**Authors:** Pian Pawakapan, Shangdi Yu\n",
    "\n",
    "<div style=\"width: 45%; float: left; padding: 20px;\"><h2> What you will learn</h2><ul><li>How to capture dispatched ops for eager and <code>torch.compile</code> runs</li><li>How to use tensor hashes and stack traces in DebugMode to pinpoint numerical divergence</li></ul></div><div style=\"width: 45%; float: right; padding: 20px;\"><h2> Prerequisites</h2><ul><li>PyTorch 2.10 or later</li></ul></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "========\n",
    "\n",
    "`DebugMode` (`torch.utils._debug_mode.DebugMode`{.interpreted-text\n",
    "role=\"class\"}) is a `TorchDispatchMode` that intercepts PyTorch runtime\n",
    "calls and emits a hierarchical log of operations. It is particularly\n",
    "useful when you need to understand *what* actually runs, both in eager\n",
    "mode and under `torch.compile` or when you need to pinpoint numerical\n",
    "divergence between two runs.\n",
    "\n",
    "Key capabilities:\n",
    "\n",
    "-   **Runtime logging** -- Records dispatched operations and\n",
    "    TorchInductor compiled Triton kernels.\n",
    "-   **Tensor hashing** -- Attaches deterministic hashes to\n",
    "    inputs/outputs to enable diffing runs to locate numerical\n",
    "    divergences.\n",
    "-   **Dispatch hooks** -- Allows registration of custom hooks to\n",
    "    annotate calls\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>This recipe describes a prototype feature. Prototype features are typicallyat an early stage for feedback and testing and are subject to change.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick start\n",
    "===========\n",
    "\n",
    "The snippet below captures a small eager workload and prints the debug\n",
    "string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch._inductor.decomposition import decomps_to_exclude\n",
    "import torch\n",
    "from torch.utils._debug_mode import DebugMode\n",
    "\n",
    "def run_once():\n",
    "    x = torch.randn(8, 8)\n",
    "    y = torch.randn(8, 8)\n",
    "    return torch.mm(torch.relu(x), y)\n",
    "\n",
    "with DebugMode() as debug_mode:\n",
    "    out = run_once()\n",
    "\n",
    "print(\"DebugMode output:\")\n",
    "print(debug_mode.debug_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting more metadata \\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n",
    "\n",
    "For most investigations, you\\'ll want to enable stack traces, tensor\n",
    "IDs, and tensor hashing. These features provide metadata to correlate\n",
    "operations back to model code.\n",
    "\n",
    "`DebugMode.log_tensor_hashes` decorates the log with hashes for every\n",
    "call. The `hash_tensor` hash function uses `torch.hash_tensor`, which\n",
    "returns 0 for tensors whose elements are all the same. The `norm` hash\n",
    "function uses `norm` with `p=1`. With both these functions, especially\n",
    "`norm`, tensor closeness in numerics is related to hash closeness, so\n",
    "it\\'s rather interpretable. The default `hash_fn` is `norm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with (\n",
    "    DebugMode(\n",
    "        # record_stack_trace is only supported for eager in pytorch 2.10\n",
    "        record_stack_trace=True,\n",
    "        record_ids=True,\n",
    "    ) as debug_mode,\n",
    "    DebugMode.log_tensor_hashes(\n",
    "        hash_fn=[\"norm\"], # this is the default\n",
    "        hash_inputs=True,\n",
    "    ),\n",
    "):\n",
    "    result = run_once()\n",
    "\n",
    "print(\"DebugMode output with more metadata:\")\n",
    "print(\n",
    "    debug_mode.debug_string(show_stack_trace=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line follows `op(args) -> outputs`. When `record_ids` is enabled,\n",
    "tensors are suffixed with `$<id>` and DTensors are labeled `dt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Triton kernels\n",
    "==================\n",
    "\n",
    "Though Triton kernels are not dispatched, DebugMode has custom logic\n",
    "that logs their inputs and outputs.\n",
    "\n",
    "Inductor-generated Triton kernels show up with a `[triton]` prefix.\n",
    "Pre/post hash annotations report buffer hashes around each kernel call,\n",
    "which is helpful when isolating incorrect kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.mm(torch.relu(x), x.T)\n",
    "\n",
    "x = torch.randn(3, 3, device=\"cuda\")\n",
    "\n",
    "with (\n",
    "    DebugMode(record_output=True) as debug_mode,\n",
    "    DebugMode.log_tensor_hashes(\n",
    "        hash_inputs=True,\n",
    "    )\n",
    "):\n",
    "    a = torch.compile(f)(x)\n",
    "\n",
    "print(\"Triton in DebugMode logs:\")\n",
    "print(debug_mode.debug_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical debugging with tensor hashes\n",
    "======================================\n",
    "\n",
    "If you have numerical divergence between modes, you can use DebugMode to\n",
    "find where the numerical divergence originates. In the example below,\n",
    "you can see that all tensor hashes are the same for eager mode and\n",
    "compiled mode. If any hash is different, then that\\'s where the\n",
    "numerical divergence is coming from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(model, data, *, compile_with=None):\n",
    "    if compile_with is not None:\n",
    "        model = torch.compile(model, backend=compile_with)\n",
    "    with DebugMode(record_output=True) as dm, DebugMode.log_tensor_hashes(\n",
    "        hash_inputs=True,\n",
    "    ):\n",
    "        dm_out = model(*data)\n",
    "    return dm, dm_out\n",
    "\n",
    "class Toy(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.relu(x).mm(x.T)\n",
    "\n",
    "inputs = (torch.randn(4, 4),)\n",
    "dm_eager, _ = run_model(Toy(), inputs)\n",
    "dm_compiled, _ = run_model(Toy(), inputs, compile_with=\"aot_eager\")\n",
    "\n",
    "print(\"Eager mode:\")\n",
    "print(dm_eager.debug_string())\n",
    "print(\"Compiled aot_eager mode:\")\n",
    "print(dm_compiled.debug_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let\\'s look at an example where the tensor hashes are different. I\n",
    "intentionally wrote a wrong decomposition that decomposes cosine to sin.\n",
    "This will cause numerical divergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from torch._dynamo.backends.debugging import get_nop_func\n",
    "\n",
    "def wrong_decomp(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "decomp_table = {}\n",
    "decomp_table[torch.ops.aten.cos.default] = wrong_decomp\n",
    "\n",
    "backend = aot_autograd(\n",
    "    fw_compiler=get_nop_func(),\n",
    "    bw_compiler=get_nop_func(),\n",
    "    decompositions=decomp_table\n",
    ")\n",
    "\n",
    "def f(x):\n",
    "    y = x.relu()\n",
    "    z = torch.cos(x)\n",
    "    return y + z\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "with DebugMode(record_output=True) as dm_eager, DebugMode.log_tensor_hashes(\n",
    "    hash_inputs=True,\n",
    "):\n",
    "    f(x)\n",
    "\n",
    "with DebugMode(record_output=True) as dm_compiled, DebugMode.log_tensor_hashes(\n",
    "    hash_inputs=True,\n",
    "):\n",
    "    torch.compile(f, backend=backend)(x)\n",
    "\n",
    "print(\"Eager:\")\n",
    "print(dm_eager.debug_string(show_stack_trace=True))\n",
    "print()\n",
    "print(\"Compiled with wrong decomposition:\")\n",
    "print(dm_compiled.debug_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the eager log, we have `aten::cos`, but in the compiled log, we have\n",
    "`aten::sin`. Moreover, the output hash is different between eager and\n",
    "compiled mode. Diffing the two logs would show that the first numerical\n",
    "divergence shows up in the `aten::cos` call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dispatch hooks\n",
    "=====================\n",
    "\n",
    "Hooks allow you to annotate each call with custom metadata such as GPU\n",
    "memory usage. `log_hook` returns a mapping that is rendered inline with\n",
    "the debug string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MB = 1024 * 1024.0\n",
    "\n",
    "def memory_hook(func, types, args, kwargs, result):\n",
    "    mem = torch.cuda.memory_allocated() / MB if torch.cuda.is_available() else 0.0\n",
    "    peak = torch.cuda.max_memory_allocated() / MB if torch.cuda.is_available() else 0.0\n",
    "    torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
    "    return {\"mem\": f\"{mem:.3f} MB\", \"peak\": f\"{peak:.3f} MB\"}\n",
    "\n",
    "with (\n",
    "    DebugMode() as dm,\n",
    "    DebugMode.dispatch_hooks(log_hook=memory_hook),\n",
    "):\n",
    "    run_once()\n",
    "\n",
    "print(\"DebugMode output with memory usage:\")\n",
    "print(dm.debug_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module boundaries\n",
    "=================\n",
    "\n",
    "`record_nn_module=True` inserts `[nn.Mod]` markers that show which\n",
    "module executed each set of operations. As of PyTorch 2.10 it only works\n",
    "in eager mode, but support for compiled modes is under development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Foo(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.l1 = torch.nn.Linear(4, 4)\n",
    "            self.l2 = torch.nn.Linear(4, 4)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.l2(self.l1(x))\n",
    "\n",
    "class Bar(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.abc = Foo()\n",
    "        self.xyz = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz(self.abc(x))\n",
    "\n",
    "mod = Bar()\n",
    "inp = torch.randn(4, 4)\n",
    "with DebugMode(record_nn_module=True, record_output=False) as debug_mode:\n",
    "    _ = mod(inp)\n",
    "\n",
    "print(\"DebugMode output with stack traces and module boundaries:\")\n",
    "print(debug_mode.debug_string(show_stack_trace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we saw how DebugMode gives you a lightweight,\n",
    "runtime-only view of what PyTorch actually executed, whether you are\n",
    "running eager code or compiled graphs. By layering tensor hashing,\n",
    "Triton logging, and custom dispatch hooks you can quickly track down\n",
    "numerical differences. This is especially helpful in debugging bit-wise\n",
    "equivalence between runs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
