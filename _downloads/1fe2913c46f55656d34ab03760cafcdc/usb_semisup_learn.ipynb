{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Semi-Supervised Learning using USB built upon PyTorch\n\n**Author**: [Hao Chen](https://github.com/Hhhhhhao)\n\nUnified Semi-supervised learning Benchmark (USB) is a semi-supervised\nlearning (SSL) framework built upon PyTorch.\nBased on Datasets and Modules provided by PyTorch, USB becomes a flexible,\nmodular, and easy-to-use framework for semi-supervised learning.\nIt supports a variety of semi-supervised learning algorithms, including\n``FixMatch``, ``FreeMatch``, ``DeFixMatch``, ``SoftMatch``, and so on.\nIt also supports a variety of imbalanced semi-supervised learning algorithms.\nThe benchmark results across different datasets of computer vision, natural\nlanguage processing, and speech processing are included in USB.\n\nThis tutorial will walk you through the basics of using the USB lighting\npackage.\nLet's get started by training a ``FreeMatch``/``SoftMatch`` model on\nCIFAR-10 using pretrained Vision Transformers (ViT)!\nAnd we will show it is easy to change the semi-supervised algorithm and train\non imbalanced datasets.\n\n\n.. figure:: /_static/img/usb_semisup_learn/code.png\n   :alt: USB framework illustration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to ``FreeMatch`` and ``SoftMatch`` in Semi-Supervised Learning\n\nHere we provide a brief introduction to ``FreeMatch`` and ``SoftMatch``.\nFirst, we introduce a famous baseline for semi-supervised learning called ``FixMatch``.\n``FixMatch`` is a very simple framework for semi-supervised learning, where it\nutilizes a strong augmentation to generate pseudo labels for unlabeled data.\nIt adopts a confidence thresholding strategy to filter out the low-confidence\npseudo labels with a fixed threshold set.\n``FreeMatch`` and ``SoftMatch`` are two algorithms that improve upon ``FixMatch``.\n``FreeMatch`` proposes adaptive thresholding strategy to replace the fixed\nthresholding strategy in ``FixMatch``. The adaptive thresholding progressively\nincreases the threshold according to the learning status of the model on each\nclass. ``SoftMatch`` absorbs the idea of confidence thresholding as an\nweighting mechanism. It proposes a Gaussian weighting mechanism to overcome\nthe quantity-quality trade-off in pseudo-labels. In this tutorial, we will\nuse USB to train ``FreeMatch`` and ``SoftMatch``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use USB to Train ``FreeMatch``/``SoftMatch`` on CIFAR-10 with only 40 labels\n\nUSB is easy to use and extend, affordable to small groups, and comprehensive\nfor developing and evaluating SSL algorithms.\nUSB provides the implementation of 14 SSL algorithms based on Consistency\nRegularization, and 15 tasks for evaluation from CV, NLP, and Audio domain.\nIt has a modular design that allows users to easily extend the package by\nadding new algorithms and tasks.\nIt also supports a Python API for easier adaptation to different SSL\nalgorithms on new data.\n\n\nNow, let's use USB to train ``FreeMatch`` and ``SoftMatch`` on CIFAR-10.\nFirst, we need to install USB package ``semilearn`` and import necessary API\nfunctions from USB.\nIf you are running this in Google Colab, install ``semilearn`` by running:\n``!pip install semilearn``.\n\nBelow is a list of functions we will use from ``semilearn``:\n\n- ``get_dataset`` to load dataset, here we use CIFAR-10\n- ``get_data_loader`` to create train (labeled and unlabeled) and test data\nloaders, the train unlabeled loaders will provide both strong and weak\naugmentation of unlabeled data\n- ``get_net_builder`` to create a model, here we use pretrained ViT\n- ``get_algorithm`` to create the semi-supervised learning algorithm,\nhere we use ``FreeMatch`` and ``SoftMatch``\n- ``get_config``: to get default configuration of the algorithm\n- ``Trainer``: a Trainer class for training and evaluating the\nalgorithm on dataset\n\nNote that a CUDA-enabled backend is required for training with the ``semilearn`` package.\nSee [Enabling CUDA in Google Colab](https://pytorch.org/tutorials/beginner/colab#using-cuda)_ for instructions\non enabling CUDA in Google Colab.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import semilearn\nfrom semilearn import get_dataset, get_data_loader, get_net_builder, get_algorithm, get_config, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing necessary functions, we first set the hyper-parameters of the\nalgorithm.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = {\n    'algorithm': 'freematch',\n    'net': 'vit_tiny_patch2_32',\n    'use_pretrain': True, \n    'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',\n\n    # optimization configs\n    'epoch': 1,  \n    'num_train_iter': 500,\n    'num_eval_iter': 500,  \n    'num_log_iter': 50,  \n    'optim': 'AdamW',\n    'lr': 5e-4,\n    'layer_decay': 0.5,\n    'batch_size': 16,\n    'eval_batch_size': 16,\n\n\n    # dataset configs\n    'dataset': 'cifar10',\n    'num_labels': 40,\n    'num_classes': 10,\n    'img_size': 32,\n    'crop_ratio': 0.875,\n    'data_dir': './data',\n    'ulb_samples_per_class': None,\n\n    # algorithm specific configs\n    'hard_label': True,\n    'T': 0.5,\n    'ema_p': 0.999,\n    'ent_loss_ratio': 0.001,\n    'uratio': 2,\n    'ulb_loss_ratio': 1.0,\n\n    # device configs\n    'gpu': 0,\n    'world_size': 1,\n    'distributed': False,\n    \"num_workers\": 4,\n}\nconfig = get_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we load the dataset and create data loaders for training and testing.\nAnd we specify the model and algorithm to use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_dict = get_dataset(config, config.algorithm, config.dataset, config.num_labels, config.num_classes, data_dir=config.data_dir, include_lb_to_ulb=config.include_lb_to_ulb)\ntrain_lb_loader = get_data_loader(config, dataset_dict['train_lb'], config.batch_size)\ntrain_ulb_loader = get_data_loader(config, dataset_dict['train_ulb'], int(config.batch_size * config.uratio))\neval_loader = get_data_loader(config, dataset_dict['eval'], config.eval_batch_size)\nalgorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can start training the algorithms on CIFAR-10 with 40 labels now.\nWe train for 500 iterations and evaluate every 500 iterations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(config, algorithm)\ntrainer.fit(train_lb_loader, train_ulb_loader, eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's evaluate the trained model on the validation set.\nAfter training 500 iterations with ``FreeMatch`` on only 40 labels of\nCIFAR-10, we obtain a classifier that achieves around 87% accuracy on the validation set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use USB to Train ``SoftMatch`` with specific imbalanced algorithm on imbalanced CIFAR-10\n\nNow let's say we have imbalanced labeled set and unlabeled set of CIFAR-10,\nand we want to train a ``SoftMatch`` model on it.\nWe create an imbalanced labeled set and imbalanced unlabeled set of CIFAR-10,\nby setting the ``lb_imb_ratio`` and ``ulb_imb_ratio`` to 10.\nAlso, we replace the ``algorithm`` with ``softmatch`` and set the ``imbalanced``\nto ``True``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = {\n    'algorithm': 'softmatch',\n    'net': 'vit_tiny_patch2_32',\n    'use_pretrain': True, \n    'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',\n\n    # optimization configs\n    'epoch': 1,  \n    'num_train_iter': 500,\n    'num_eval_iter': 500,  \n    'num_log_iter': 50,  \n    'optim': 'AdamW',\n    'lr': 5e-4,\n    'layer_decay': 0.5,\n    'batch_size': 16,\n    'eval_batch_size': 16,\n\n\n    # dataset configs\n    'dataset': 'cifar10',\n    'num_labels': 1500,\n    'num_classes': 10,\n    'img_size': 32,\n    'crop_ratio': 0.875,\n    'data_dir': './data',\n    'ulb_samples_per_class': None,\n    'lb_imb_ratio': 10,\n    'ulb_imb_ratio': 10,\n    'ulb_num_labels': 3000,\n\n    # algorithm specific configs\n    'hard_label': True,\n    'T': 0.5,\n    'ema_p': 0.999,\n    'ent_loss_ratio': 0.001,\n    'uratio': 2,\n    'ulb_loss_ratio': 1.0,\n\n    # device configs\n    'gpu': 0,\n    'world_size': 1,\n    'distributed': False,\n    \"num_workers\": 4,\n}\nconfig = get_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we re-load the dataset and create data loaders for training and testing.\nAnd we specify the model and algorithm to use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_dict = get_dataset(config, config.algorithm, config.dataset, config.num_labels, config.num_classes, data_dir=config.data_dir, include_lb_to_ulb=config.include_lb_to_ulb)\ntrain_lb_loader = get_data_loader(config, dataset_dict['train_lb'], config.batch_size)\ntrain_ulb_loader = get_data_loader(config, dataset_dict['train_ulb'], int(config.batch_size * config.uratio))\neval_loader = get_data_loader(config, dataset_dict['eval'], config.eval_batch_size)\nalgorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can start Train the algorithms on CIFAR-10 with 40 labels now.\nWe train for 500 iterations and evaluate every 500 iterations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(config, algorithm)\ntrainer.fit(train_lb_loader, train_ulb_loader, eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's evaluate the trained model on the validation set.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References:\n- [1] USB: https://github.com/microsoft/Semi-supervised-learning\n- [2] Kihyuk Sohn et al. FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\n- [3] Yidong Wang et al. FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning\n- [4] Hao Chen et al. SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}