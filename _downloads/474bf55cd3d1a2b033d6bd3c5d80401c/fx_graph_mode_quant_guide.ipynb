{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n(prototype) FX Graph Mode Quantization User Guide\n===========================================================\n\n**Author**: `Jerry Zhang <https://github.com/jerryzh168>`_\n\nFX Graph Mode Quantization requires a symbolically traceable model. \nWe use the FX framework (TODO: link) to convert a symbolically traceable nn.Module instance to IR,\nand we operate on the IR to execute the quantization passes.  \nPlease post your question about symbolically tracing your model in `PyTorch Discussion Forum <https://discuss.pytorch.org/c/quantization/17>`_\n\nQuantization will only work on the symbolically traceable parts of your model. \nData dependent control flow (if statements / for loops etc using symbolically traced values) are one common pattern which is not supported. \nIf your model is not symbolically traceable end to end, you have a couple of options to enable FX Graph Mode Quantization only on a part of the model.\nYou can use any combination of these options:\n\n1. Non traceable code doesn\u2019t need to be quantized\n    a. Symbolically trace only the code that needs to be quantized\n    b. Skip symbolic tracing the non-traceable code\n\n2. Non traceable code needs to be quantized\n    a. Refactor your code to make it symbolically traceable\n    b. Write your own observed and quantized submodule\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the code that is not symbolically traceable does not need to be quantized, we have the following two options\nto run FX Graph Mode Quantization:\n\n1.a. Symbolically trace only the code that needs to be quantized\n-----------------------------------------------------------------\n\nWhen the whole model is not symbolically traceable but the submodule we want to quantize is \nsymbolically traceable, we can run quantization only on that submodule.\n\n\nbefore:\n\n.. code:: python\n\n  class M(nn.Module):\n\n      def forward(self, x):\n          x = non_traceable_code_1(x)\n          x = traceable_code(x)\n          x = non_traceable_code_2(x)\n          return x\n\n\nafter:\n\n.. code:: python\n\n  class FP32Traceable(nn.Module):\n\n      def forward(self, x):\n          x = traceable_code(x)\n          return x\n\n  class M(nn.Module):\n\n      def __init__(self):\n          self.traceable_submodule = FP32Traceable(...)\n\n      def forward(self, x):\n          x = self.traceable_code_1(x)\n          # We'll only symbolic trace/quantize this submodule\n          x = self.traceable_submodule(x)\n          x = self.traceable_code_2(x)\n          return x\n\n\nquantization code:\n\n.. code:: python\n\n  qconfig_dict = {\"\": qconfig}\n  model_fp32.traceable_submodule = \\\n    prepare_fx(model_fp32.traceable_submodule, qconfig_dict)\n\nNote if original model needs to be preserved, you will have to\ncopy it yourself before calling the quantization APIs.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.b. Skip symbolically trace the non-traceable code\n---------------------------------------------------\nWhen we have some non-traceable code in the module, and this part of code doesn\u2019t need to be quantized, \nwe can factor out this part of the code into a submodule and skip symbolically trace that submodule.\n\n\nbefore\n\n.. code:: python\n\n  class M(nn.Module):\n\n      def forward(self, x):\n          x = self.traceable_code_1(x)\n          x = non_traceable_code(x)\n          x = self.traceable_code_2(x)\n          return x\n\n\nafter, non-traceable parts moved to a module and marked as a leaf\n\n.. code:: python\n\n  class FP32NonTraceable(nn.Module):\n\n      def forward(self, x):\n          x = non_traceable_code(x)\n          return x\n\n  class M(nn.Module):\n\n      def __init__(self):\n          ...\n          self.non_traceable_submodule = FP32NonTraceable(...)\n\n      def forward(self, x):\n          x = self.traceable_code_1(x)\n          # we will configure the quantization call to not trace through\n          # this submodule\n          x = self.non_traceable_submodule(x)\n          x = self.traceable_code_2(x)\n          return x\n\nquantization code:\n\n.. code:: python\n\n  qconfig_dict = {\"\": qconfig}\n\n  prepare_custom_config_dict = {\n      # option 1\n      \"non_traceable_module_name\": \"non_traceable_submodule\",\n      # option 2\n      \"non_traceable_module_class\": [MNonTraceable],\n  }\n  model_prepared = prepare_fx(\n      model_fp32, \n      qconfig_dict,\n      prepare_custom_config_dict=prepare_custom_config_dict,\n  )\n\nIf the code that is not symbolically traceable needs to be quantized, we have the following two options:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.a Refactor your code to make it symbolically traceable\n--------------------------------------------------------\nIf it is easy to refactor the code and make the code symbolically traceable, \nwe can refactor the code and remove the use of non-traceable constructs in python.\n\nMore information about symbolic tracing support can be found in: (TODO: link)\n\nbefore:\n\n.. code:: python\n\n  def transpose_for_scores(self, x):\n      new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n      x = x.view(*new_x_shape)\n      return x.permute(0, 2, 1, 3)\n\n\nThis is not symbolically traceable because in x.view(*new_x_shape) \nunpacking is not supported, however, it is easy to remove the unpacking\nsince x.view also supports list input.\n\n\nafter:\n\n.. code:: python\n\n  def transpose_for_scores(self, x):\n      new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n      x = x.view(new_x_shape)\n      return x.permute(0, 2, 1, 3)\n\n\nquantization code:\n\nThis can be combined with other approaches and the quantization code\ndepends on the model.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.b. Write your own observed and quantized submodule\n-----------------------------------------------------\n\nIf the non-traceable code can\u2019t be refactored to be symbolically traceable, \nfor example it has some loops that can\u2019t be eliminated, like nn.LSTM, \nwe\u2019ll need to factor out the non-traceable code to a submodule (we call it CustomModule in fx graph mode quantization) and \ndefine the observed and quantized version of the submodule (in post training static quantization or quantization aware training for static quantization) \nor define the quantized version (in post training dynamic and weight only quantization)\n\n\nbefore:\n\n.. code:: python\n\n  class M(nn.Module):\n\n      def forward(self, x):\n          x = traceable_code_1(x)\n          x = non_traceable_code(x)\n          x = traceable_code_1(x)\n          return x\n\nafter:\n\n1. Factor out non_traceable_code to FP32NonTraceable\nnon-traceable logic, wrapped in a module\n\n.. code:: python\n\n  class FP32NonTraceable:\n      ...\n\n\n2. Define observed version of FP32NonTraceable\n\n.. code:: python\n\n  class ObservedNonTraceable:\n\n      @classmethod\n      def from_float(cls, ...):\n          ...\n\n3. Define statically quantized version of FP32NonTraceable\nand a class method \"from_observed\" to convert from ObservedNonTraceable\nto StaticQuantNonTraceable\n\n.. code:: python\n\n  class StaticQuantNonTraceable:\n\n      @classmethod\n      def from_observed(cls, ...):\n          ...\n\n\n.. code:: python\n\n  # refactor parent class to call FP32NonTraceable\n  class M(nn.Module):\n\n     def __init__(self):\n          ...\n          self.non_traceable_submodule = FP32NonTraceable(...)\n\n      def forward(self, x):\n          x = self.traceable_code_1(x)\n          # this part will be quantized manually\n          x = self.non_traceable_submodule(x)\n          x = self.traceable_code_1(x)\n          return x\n\n\nquantization code:\n\n\n.. code:: python\n\n  # post training static quantization or\n  # quantization aware training (that produces a statically quantized module)v\n  prepare_custom_config_dict = {\n      \"float_to_observed_custom_module_class\": {\n          \"static\": {\n              FP32NonTraceable: ObservedNonTraceable,\n          }\n      },\n  }\n\n  model_prepared = prepare_fx(\n      model_fp32, \n      qconfig_dict, \n      prepare_custom_config_dict=prepare_custom_config_dict)\n\ncalibrate / train (not shown)\n\n.. code:: python\n\n  convert_custom_config_dict = {\n      \"observed_to_quantized_custom_module_class\": {\n          \"static\": {\n              ObservedNonTraceable: StaticQuantNonTraceable,\n          }\n      },\n  }\n  model_quantized = convert_fx(\n      model_prepared,\n      convert_custom_config_dict)\n\npost training dynamic/weight only quantization\nin these two modes we don't need to observe the original model, so we \nonly need to define thee quantized model\n\n.. code:: python\n\n  class DynamicQuantNonTraceable: # or WeightOnlyQuantMNonTraceable\n      ...\n      @classmethod\n      def from_observed(cls, ...):\n          ...\n\n      prepare_custom_config_dict = {\n          \"non_traceable_module_class\": [\n              FP32NonTraceable\n          ]\n      }\n\n\n.. code:: python\n\n  # The example is for post training quantization\n  model_fp32.eval()\n  model_prepared = prepare_fx(\n      model_fp32, \n      qconfig_dict, \n      prepare_custom_config_dict=prepare_custom_config_dict)\n\n  convert_custom_config_dict = {\n      \"observed_to_quantized_custom_module_class\": {\n          \"dynamic\": {\n              FP32NonTraceable: DynamicQuantNonTraceable,\n          }\n      },\n  }\n  model_quantized = convert_fx(\n      model_prepared,\n      convert_custom_config_dict)\n\nYou can also find examples for custom modules in test ``test_custom_module_class`` in ``torch/test/quantization/test_quantize_fx.py``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}