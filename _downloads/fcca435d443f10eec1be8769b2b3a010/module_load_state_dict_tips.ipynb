{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tips for Loading an ``nn.Module`` from a Checkpoint\n\nIf you're loading a checkpoint and want to reduce compute and memory as much as possible,\nthis tutorial shares some recommended practices. In particular, we will discuss\n\n1.  The ``mmap`` keyword argument on ``torch.load``\n2.  The ``torch.device()`` context manager\n3.  The ``assign`` keyword argument on ``nn.Module.load_state_dict()``\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This recipe requires PyTorch 2.1.0 or later.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us consider a simple ``nn.Module`` that contains a list of Linear layers:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\nimport time\n\nclass SomeModule(torch.nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.linears = nn.ModuleList([nn.Linear(size, size) for i in range(10)])\n\n    def forward(self, x):\n        return self.linears(x)\n\n\nm = SomeModule(1000)\ntorch.save(m.state_dict(), 'checkpoint.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following snippet demonstrates the use of the the ``mmap`` keyword argument\nto ``torch.load``, the ``torch.device()`` context manager and the ``assign``\nkeyword argument to ``nn.Module.load_state_dict()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load('checkpoint.pth', mmap=True)\nwith torch.device('meta'):\n  meta_m = SomeModule(1000)\nmeta_m.load_state_dict(state_dict, assign=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the snippet below to the one above:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load('checkpoint.pth')\nm = SomeModule(1000)\nm.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second example does not use any of the features listed above and will be\nless compute and memory efficient for loading a checkpoint. In the following\nsections, we will discuss each of the features in further detail.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ``torch.load(mmap=True)``\nFirst, let us consider what happens when we load the checkpoint with ``torch.load``.\nWhen we save a checkpoint with ``torch.save``, tensor storages are tagged with the device they are\nsaved on. With ``torch.load``, tensor storages will be loaded to the device\nthey were tagged with (unless this behavior is overridden using the\n``map_location`` flag). For ease of explanation, let us assume that the tensors\nwere saved on CPU. This means that on the first line all tensor storages will be\nloaded into CPU RAM, which can be undesirable when:\n\n* CPU RAM is smaller than the size of the checkpoint.\n* Waiting for the entire checkpoint to be loaded into RAM before performing, for example, some per-tensor processing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\nstate_dict = torch.load('checkpoint.pth')\nend_time = time.time()\nprint(f\"loading time without mmap={end_time - start_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``mmap`` keyword argument to ``torch.load`` attempts to solve the above two\nproblems. As its name implies, the ``mmap`` keyword argument to ``torch.load``\nmakes use of an [mmap call](https://man7.org/linux/man-pages/man2/mmap.2.html)\nwhich maps a file on disk into virtual memory and lets the OS handle loading and\nunloading into physical memory automatically. When this flag is passed, tensor\nstorages will be memory-mapped.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\nstate_dict = torch.load('checkpoint.pth', mmap=True)\nend_time = time.time()\nprint(f\"loading time with mmap={end_time - start_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, one can use this argument to do per-tensor processing on a\ncheckpoint without loading all tensor storages into CPU memory upfront. For example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_special_routine(t, device):\n    # this could be a much fancier operation\n    return t.to(dtype=torch.bfloat16, device=device)\n\ndef my_processing_function(key, device):\n    t = state_dict[key]\n    processed_t = my_special_routine(t, device)\n    del t\n    state_dict[key] = processed_t\n\nfor key in state_dict.keys():\n    device = torch.device('cuda')\n    my_processing_function(key, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ``torch.device('meta')``\nNext, let's consider the creation of the module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m = SomeModule(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This allocates memory for all parameters/buffers and initializes them per\nthe default initialization schemes defined in ``SomeModule.__init__()``, which\nis wasteful when we want to load a checkpoint for the following reasons:\n\n* The result of the initialization kernels will be overwritten by ``load_state_dict()`` without ever being used, so\n  initialization is wasteful.\n* We are allocating memory for these parameters/buffers in RAM while ``torch.load`` of the saved state dictionary also\n  allocates memory in RAM for the parameters/buffers in the checkpoint.\n\nIn order to solve these two problems, we can use the ``torch.device()``\ncontext manager with ``device='meta'`` when we instantiate the ``nn.Module()``.\n\nThe [torch.device()](https://pytorch.org/docs/main/tensor_attributes.html#torch-device)\ncontext manager makes sure that factory calls will be performed as if they\nwere passed the specified ``device`` as an argument. Tensors on ``torch.device('meta')`` do not\ncarry data. However, they possess all other metadata a tensor carries such as ``.size()``, ``.stride()``,\n``.requires_grad``, and others.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.device('meta'):\n  new_m = SomeModule(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ``load_state_dict(assign=True)``\nNext, we consider the loading of the state dictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``nn.Module.load_state_dict()`` is usually implemented via an in-place\n``param_in_model.copy_(param_in_state_dict)``. This means that the parameter/buffer\nwith the corresponding key in the state dictionary is copied into the\nparameter/buffer in the ``nn.Module``.\n\nHowever, an in-place copy into a tensor on the ``meta`` device is a no-op.\nIn order to avoid this, we can pass the ``assign=True`` keyword argument to\n``load_state_dict()``.\n\nA caveat here is that since optimizers hold a reference to\n``nn.Module.parameters()``, the optimizer must be initialized after the module\nis loaded from state dict if ``assign=True`` is passed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_m.load_state_dict(state_dict, assign=True)\n# This MUST be done AFTER the load_state_dict with assign.\nopt = torch.optim.SGD(new_m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nTo recap, in this tutorial we learned about ``torch.load(mmap=True)``, the\n``torch.device()`` context manager with ``device=meta``, and\n``nn.Module.load_state_dict(assign=True)`` as well as how these tools could\nbe used to aid when loading a model from a checkpoint.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}