{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips for Loading an `nn.Module` from a Checkpoint\n",
    "=================================================\n",
    "\n",
    "**Author:** [Mikayla Gawarecki](https://github.com/mikaylagawarecki)\n",
    "\n",
    "If you\\'re loading a checkpoint and want to reduce compute and memory as\n",
    "much as possible, this tutorial shares some recommended practices. In\n",
    "particular, we will discuss\n",
    "\n",
    "1.  The `mmap` keyword argument on `torch.load`\n",
    "2.  The `torch.device()` context manager\n",
    "3.  The `assign` keyword argument on `nn.Module.load_state_dict()`\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>This recipe requires PyTorch 2.1.0 or later.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a simple `nn.Module` that contains a list of Linear\n",
    "layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "class SomeModule(torch.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(size, size) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linears(x)\n",
    "\n",
    "\n",
    "m = SomeModule(1000)\n",
    "torch.save(m.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet demonstrates the use of the the `mmap` keyword\n",
    "argument to `torch.load`, the `torch.device()` context manager and the\n",
    "`assign` keyword argument to `nn.Module.load_state_dict()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('checkpoint.pth', mmap=True, weights_only=True)\n",
    "with torch.device('meta'):\n",
    "  meta_m = SomeModule(1000)\n",
    "meta_m.load_state_dict(state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the snippet below to the one above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('checkpoint.pth', weights_only=True)\n",
    "m = SomeModule(1000)\n",
    "m.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example does not use any of the features listed above and\n",
    "will be less compute and memory efficient for loading a checkpoint. In\n",
    "the following sections, we will discuss each of the features in further\n",
    "detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.load(mmap=True)`\n",
    "=============================\n",
    "\n",
    "First, let us consider what happens when we load the checkpoint with\n",
    "`torch.load`. When we save a checkpoint with `torch.save`, tensor\n",
    "storages are tagged with the device they are saved on. With\n",
    "`torch.load`, tensor storages will be loaded to the device they were\n",
    "tagged with (unless this behavior is overridden using the `map_location`\n",
    "flag). For ease of explanation, let us assume that the tensors were\n",
    "saved on CPU. This means that on the first line all tensor storages will\n",
    "be loaded into CPU RAM, which can be undesirable when:\n",
    "\n",
    "-   CPU RAM is smaller than the size of the checkpoint.\n",
    "-   Waiting for the entire checkpoint to be loaded into RAM before\n",
    "    performing, for example, some per-tensor processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "state_dict = torch.load('checkpoint.pth', weights_only=True)\n",
    "end_time = time.time()\n",
    "print(f\"loading time without mmap={end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mmap` keyword argument to `torch.load` attempts to solve the above\n",
    "two problems. As its name implies, the `mmap` keyword argument to\n",
    "`torch.load` makes use of an [mmap\n",
    "call](https://man7.org/linux/man-pages/man2/mmap.2.html) which maps a\n",
    "file on disk into virtual memory and lets the OS handle loading and\n",
    "unloading into physical memory automatically. When this flag is passed,\n",
    "tensor storages will be memory-mapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "state_dict = torch.load('checkpoint.pth', mmap=True, weights_only=True)\n",
    "end_time = time.time()\n",
    "print(f\"loading time with mmap={end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, one can use this argument to do per-tensor\n",
    "processing on a checkpoint without loading all tensor storages into CPU\n",
    "memory upfront. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_special_routine(t, device):\n",
    "    # this could be a much fancier operation\n",
    "    return t.to(dtype=torch.bfloat16, device=device)\n",
    "\n",
    "def my_processing_function(key, device):\n",
    "    t = state_dict[key]\n",
    "    processed_t = my_special_routine(t, device)\n",
    "    del t\n",
    "    state_dict[key] = processed_t\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    device = torch.device('cuda')\n",
    "    my_processing_function(key, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.device('meta')`\n",
    "============================\n",
    "\n",
    "Next, let\\'s consider the creation of the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = SomeModule(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allocates memory for all parameters/buffers and initializes them\n",
    "per the default initialization schemes defined in\n",
    "`SomeModule.__init__()`, which is wasteful when we want to load a\n",
    "checkpoint for the following reasons:\n",
    "\n",
    "-   The result of the initialization kernels will be overwritten by\n",
    "    `load_state_dict()` without ever being used, so initialization is\n",
    "    wasteful.\n",
    "-   We are allocating memory for these parameters/buffers in RAM while\n",
    "    `torch.load` of the saved state dictionary also allocates memory in\n",
    "    RAM for the parameters/buffers in the checkpoint.\n",
    "\n",
    "In order to solve these two problems, we can use the `torch.device()`\n",
    "context manager with `device='meta'` when we instantiate the\n",
    "`nn.Module()`.\n",
    "\n",
    "The\n",
    "[torch.device()](https://pytorch.org/docs/main/tensor_attributes.html#torch-device)\n",
    "context manager makes sure that factory calls will be performed as if\n",
    "they were passed the specified `device` as an argument. Tensors on\n",
    "`torch.device('meta')` do not carry data. However, they possess all\n",
    "other metadata a tensor carries such as `.size()`, `.stride()`,\n",
    "`.requires_grad`, and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.device('meta'):\n",
    "  new_m = SomeModule(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `load_state_dict(assign=True)`\n",
    "====================================\n",
    "\n",
    "Next, we consider the loading of the state dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module.load_state_dict()` is usually implemented via an in-place\n",
    "`param_in_model.copy_(param_in_state_dict)`. This means that the\n",
    "parameter/buffer with the corresponding key in the state dictionary is\n",
    "copied into the parameter/buffer in the `nn.Module`.\n",
    "\n",
    "However, an in-place copy into a tensor on the `meta` device is a no-op.\n",
    "In order to avoid this, we can pass the `assign=True` keyword argument\n",
    "to `load_state_dict()`.\n",
    "\n",
    "A caveat here is that since optimizers hold a reference to\n",
    "`nn.Module.parameters()`, the optimizer must be initialized after the\n",
    "module is loaded from state dict if `assign=True` is passed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As of PyTorch 2.3.0, one can use ``torch.__future__.set_swap_module_params_on_conversion`` to\n",
    "# avoid this caveat. This `recipe <https://pytorch.org/tutorials/recipes/recipes/swap_tensors.html>`_\n",
    "# provides more details.\n",
    "\n",
    "new_m.load_state_dict(state_dict, assign=True)\n",
    "# Before 2.3.0, this MUST be done AFTER the load_state_dict with assign.\n",
    "# In versions >= 2.3.0, one can consider setting ``torch.__future__.set_swap_module_params_on_conversion``\n",
    "opt = torch.optim.SGD(new_m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "To recap, in this tutorial we learned about `torch.load(mmap=True)`, the\n",
    "`torch.device()` context manager with `device=meta`, and\n",
    "`nn.Module.load_state_dict(assign=True)` as well as how these tools\n",
    "could be used to aid when loading a model from a checkpoint.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
