{
  "metadata": {
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    },
    "kernelspec": {
      "language": "python",
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\nPyTorch: Defining new autograd functions\n----------------------------------------\n\nA fully-connected ReLU network with one hidden layer and no biases, trained to\npredict y from x by minimizing squared Euclidean distance.\n\nThis implementation computes the forward pass using operations on PyTorch\nVariables, and uses PyTorch autograd to compute gradients.\n\nIn this implementation we implement our own custom autograd function to perform\nthe ReLU function.\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "source": [
        "import torch\nfrom torch.autograd import Variable\n\n\nclass MyReLU(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input\n\n\ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold input and outputs, and wrap them in Variables.\nx = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\ny = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n\n# Create random Tensors for weights, and wrap them in Variables.\nw1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\nw2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n    relu = MyReLU.apply\n\n    # Forward pass: compute predicted y using operations on Variables; we compute\n    # ReLU using our custom autograd operation.\n    y_pred = relu(x.mm(w1)).mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.data[0])\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    w1.data -= learning_rate * w1.grad.data\n    w2.data -= learning_rate * w2.grad.data\n\n    # Manually zero the gradients after updating weights\n    w1.grad.data.zero_()\n    w2.grad.data.zero_()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}